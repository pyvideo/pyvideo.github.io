<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Padmaja Bhagwat</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 27 Apr 2022 00:00:00 +0000</lastBuildDate><item><title>Listen, Attend, and Walk : Interpreting natural language navigational instructions</title><link>https://pyvideo.org/pycon-us-2018/listen-attend-and-walk-interpreting-natural-language-navigational-instructions.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Imagine you have an appointment in a large building you do not know. Your host sent instructions describing how to reach their office. Though the instructions were fairly clear, in a few places, such as at the end, you had to infer what to do. How does a &lt;em&gt;robot (agent)&lt;/em&gt; interpret an instruction in the environment to infer the correct course of action? Enabling harmonious &lt;em&gt;Human - Robot Interaction&lt;/em&gt; is of primary importance if they are to work seamlessly alongside people.&lt;/p&gt;
&lt;p&gt;Dealing with natural language instructions in hard because of two main reasons, first being, Humans - through their prior experience know how to interpret natural language but agents can’t, and second is overcoming the ambiguity that is inherently associated with natural language instructions. This talk is about how deep learning models were used to solve such complex and ambiguous problem of converting natural language instruction into its corresponding action sequence.&lt;/p&gt;
&lt;p&gt;Following verbal route instructions requires knowledge of language, space, action and perception. In this talk I shall be presenting, a neural sequence-to-sequence model for direction following, a task that is essential to realize effective autonomous agents.&lt;/p&gt;
&lt;p&gt;At a high level, a sequence-to- sequence model is an end-to-end model made up of two recurrent neural networks:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt; - which takes the model’s input sequence as input and encodes it into a fixed-size context vector.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt; - which uses the context vector from above as a seed from which to generate an output sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this reason, sequence-to-sequence models are often referred to as &lt;em&gt;encoder-decoder&lt;/em&gt; models. The alignment based encoder-decoder model would translate the natural language instructions into corresponding action sequences. This model does not assume any prior linguistic knowledge: syntactic, semantic or lexical. The model learns the meaning of every word, including object names, verbs, spatial relations as well as syntax and the compositional semantics of the language on its own.&lt;/p&gt;
&lt;p&gt;In this talk, steps involved in pre-processing of data, training the model, testing the model and final simulation of the model in the virtual environment will be discussed. This talk will also cover some of the challenges and trade-offs made while designing the model.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Padmaja Bhagwat</dc:creator><pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-11:/pycon-us-2018/listen-attend-and-walk-interpreting-natural-language-navigational-instructions.html</guid><category>PyCon US 2018</category></item><item><title>VigNET: An intelligent camera app that assists you in understanding your surroundings</title><link>https://pyvideo.org/pycon-us-2022/vignet-an-intelligent-camera-app-that-assists-you-in-understanding-your-surroundings.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What if you can understand your surroundings with just a click of a picture?&lt;/p&gt;
&lt;p&gt;The speakers have built an intelligent camera app that assists people with visual impairment in understanding their surroundings. This application takes camera input in the form of an image and attempts to answer questions related to the image. Simply put, it's a Visual Question Answering (VQA) app.&lt;/p&gt;
&lt;p&gt;The deep learning based application is built using a transformer based model called Vision Language Transformer (ViLT) which is both computationally fast and efficient, thus providing answers to users’ questions within a fraction of seconds. The application is integrated with speech-to-text and text-to-speech capabilities to enhance accessibility.&lt;/p&gt;
&lt;p&gt;This talk would mainly cover the following: * What is the Vision Language Transformer (ViLT) model? * Advantages of ViLT over traditional vision language pre-trained models * Best practices around modularizing the application into different services * Steps to deploy this deep learning based application on cloud (GCP) * How in-built python libraries helped in implementing and deploying such complex models (viz. ViLT) easily&lt;/p&gt;
&lt;p&gt;The entire code is open-sourced and the talk will provide a walkthrough of the steps to build your own visual question answering application.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Padmaja Bhagwat</dc:creator><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2022-04-27:/pycon-us-2022/vignet-an-intelligent-camera-app-that-assists-you-in-understanding-your-surroundings.html</guid><category>PyCon US 2022</category></item></channel></rss>