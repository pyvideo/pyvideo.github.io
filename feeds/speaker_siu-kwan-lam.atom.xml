<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_siu-kwan-lam.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-13T00:00:00+00:00</updated><entry><title>How to Accelerate an Existing Codebase with Numba</title><link href="https://pyvideo.org/scipy-2019/how-to-accelerate-an-existing-codebase-with-numba.html" rel="alternate"></link><published>2019-07-13T00:00:00+00:00</published><updated>2019-07-13T00:00:00+00:00</updated><author><name>Siu Kwan Lam</name></author><id>tag:pyvideo.org,2019-07-13:scipy-2019/how-to-accelerate-an-existing-codebase-with-numba.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you have ever said to yourself &amp;quot;my code works, but it is too slow!&amp;quot; then this is the talk for you. We will describe best practices for applying the Numba just-in-time compiler to an existing project. This includes techniques for assessing whether Numba is appropriate for your use case, analyzing your program to identify where Numba can help, modifying your core algorithms to be Numba compatible, and understanding compiler errors. In addition, we'll discuss considerations for packaging and distribution of projects that depend on Numba.&lt;/p&gt;
</summary></entry><entry><title>Scaling Up and Out Programming GPU Clusters with Numba and Dask</title><link href="https://pyvideo.org/scipy-2016/scaling-up-and-out-programming-gpu-clusters-with-numba-and-dask-scipy-2016-siu-kwan-lam.html" rel="alternate"></link><published>2016-07-15T00:00:00+00:00</published><updated>2016-07-15T00:00:00+00:00</updated><author><name>Siu Kwan Lam</name></author><id>tag:pyvideo.org,2016-07-15:scipy-2016/scaling-up-and-out-programming-gpu-clusters-with-numba-and-dask-scipy-2016-siu-kwan-lam.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, we show how Python, Numba, and Dask can be used for GPU programming that easily scales from your workstation to a cluster, and can be controlled entirely from a Jupyter notebook. We will describe how the Numba JIT compiler can be used to create and compile GPU calculations entirely from the Python interpreter, and how the Dask task scheduling system can be used to farm these calculations out to a GPU cluster. Using an image processing example application, we will show how these two projects make it easy to iterate and experiment with algorithms on large data sets. Finally, we will conclude with tips and tricks for working with GPUs and distributed computing.&lt;/p&gt;
</summary><category term="SciPy 2016"></category></entry></feed>