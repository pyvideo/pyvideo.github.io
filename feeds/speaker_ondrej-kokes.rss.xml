<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Fri, 10 May 2019 00:00:00 +0000</lastBuildDate><item><title>High on Cardinality, Low on Memory</title><link>https://pyvideo.org/pydata-amsterdam-2019/high-on-cardinality-low-on-memory.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ondrej Kokes</dc:creator><pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-10:pydata-amsterdam-2019/high-on-cardinality-low-on-memory.html</guid><category>pydata</category></item><item><title>DataFrames: scaling up and out</title><link>https://pyvideo.org/pydata-amsterdam-2018/dataframes-scaling-up-and-out.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;DataFrames of all sorts have become very popular for tabular data analytics for their nifty APIs and ease of use. But as they usually operate as in-memory engines, they can be hard to scale. In my talk, I'd like to outline several ways one can scale their compute platform to handle larger datasets without incurring much cost.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ondrej Kokes</dc:creator><pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-26:pydata-amsterdam-2018/dataframes-scaling-up-and-out.html</guid></item><item><title>DataFrames: scaling up and out</title><link>https://pyvideo.org/pycon-cz-2018/dataframes-scaling-up-and-out.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;DataFrames have become ubiquitous when it comes to fast analyses of complex data. They go beyond SQL by not adhering to a strict schema and offer a rich API, where you chain methods, which fosters exploratory analytics.&lt;/p&gt;
&lt;p&gt;While newcomers to Python usually learn about pandas early on, they sometimes struggle as their underlying data grow in size. Given the in-memory nature of pandas' storage system, one can usually only scale up.&lt;/p&gt;
&lt;p&gt;I'd like to outline several workflows for adapting to the ever-increasing size of datasets:&lt;/p&gt;
&lt;p&gt;Changing application logic to handle streams rather than loading the whole dataset into memory.
Actually scaling up – locally by buying more memory and/or faster disk drives, or by deploying servers in the cloud and SSH tunneling to remote Jupyter instances.
Scaling your data source and utilizing pandas' SQL connector. This will help in other areas as well (e.g. direct connections in BI).
Using a distributed DataFrame engine – Dask or PySpark. These scale from laptops to large clusters, using the very same API the whole way through.
I will cover the various differences between these approaches and will outline their set of upsides (e.g. scaling and performance) and downsides (DevOps difficulties, cost).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ondřej Kokeš</dc:creator><pubDate>Sun, 03 Jun 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-06-03:pycon-cz-2018/dataframes-scaling-up-and-out.html</guid></item></channel></rss>