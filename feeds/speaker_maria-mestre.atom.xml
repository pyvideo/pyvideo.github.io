<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Maria Mestre</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_maria-mestre.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2022-05-12T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Efficient data labelling with weak supervision</title><link href="https://pyvideo.org/pycon-de-2022/efficient-data-labelling-with-weak-supervision.html" rel="alternate"></link><published>2022-05-12T00:00:00+00:00</published><updated>2022-05-12T00:00:00+00:00</updated><author><name>Maria Mestre</name></author><id>tag:pyvideo.org,2022-05-12:/pycon-de-2022/efficient-data-labelling-with-weak-supervision.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker:: Maria Mestre&lt;/p&gt;
&lt;p&gt;Track: PyData: Natural Language Processing
Data labelling is often considered a separate task that takes place before the real &amp;quot;machine learning work&amp;quot; happens, similar to waterfall software engineering practices. However this is typically a wrong approach that leads to failure of the whole project. In this …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker:: Maria Mestre&lt;/p&gt;
&lt;p&gt;Track: PyData: Natural Language Processing
Data labelling is often considered a separate task that takes place before the real &amp;quot;machine learning work&amp;quot; happens, similar to waterfall software engineering practices. However this is typically a wrong approach that leads to failure of the whole project. In this talk, we will show how to use weak supervision techniques to not only label large amounts of data significantly faster than with other techniques, but to also protect your ML project from issues in the annotation step which can cause catastrophic errors further downstream.&lt;/p&gt;
&lt;p&gt;Recorded at the PyConDE &amp;amp; PyData Berlin 2022 conference, April 11-13 2022.
&lt;a class="reference external" href="https://2022.pycon.de"&gt;https://2022.pycon.de&lt;/a&gt;
More details at the conference page: &lt;a class="reference external" href="https://2022.pycon.de/program/LAUL7F"&gt;https://2022.pycon.de/program/LAUL7F&lt;/a&gt;
Twitter: &lt;a class="reference external" href="https://twitter.com/pydataberlin"&gt;https://twitter.com/pydataberlin&lt;/a&gt;
Twitter: &lt;a class="reference external" href="https://twitter.com/pyconde"&gt;https://twitter.com/pyconde&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon DE 2022"></category><category term="PyCon"></category><category term="PyConDE"></category><category term="pyconde2022"></category><category term="pydata"></category><category term="PyDataBerlin"></category><category term="pydataberlin2022"></category></entry><entry><title>First Steps with Spark</title><link href="https://pyvideo.org/pydata-london-2015/first-steps-with-spark.html" rel="alternate"></link><published>2015-06-19T00:00:00+00:00</published><updated>2015-06-19T00:00:00+00:00</updated><author><name>Maria Mestre</name></author><id>tag:pyvideo.org,2015-06-19:/pydata-london-2015/first-steps-with-spark.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Spark is a distributed computing tool that offers many advantages
over more established Hadoop frameworks. Apart from its improved
memory usage and flexibility, it provides a consistent framework to
do everything from ad-hoc big data analysis to the construction of
a data processing pipeline in production.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The data …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Spark is a distributed computing tool that offers many advantages
over more established Hadoop frameworks. Apart from its improved
memory usage and flexibility, it provides a consistent framework to
do everything from ad-hoc big data analysis to the construction of
a data processing pipeline in production.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The data science team at Skimlinks has been using Spark for over a
year. We will share some of our experience of how to do large-scale
data analysis using Spark stand-alone, going from its basic
functionality to more advanced features.&lt;/p&gt;
&lt;p&gt;We will first give an introduction to Spark, explaining how
computations are distributed across the cluster using resilient
distributed datasets (RDDs). A high-level understanding of how a
workload is split into stages and tasks is important to be able to
diagnose potential problems. After showing how to get started on a
cluster of EC2 instances, we will go through a complicated part of
using Spark: how to set its configuration parameters.&lt;/p&gt;
&lt;p&gt;During the first half of the talk, we will focus on Spark core
functionality, going through some of the most common problems
encountered during basic computations. Using an example from
impression data, we will do large-scale text processing using
popular ML Python libraries.&lt;/p&gt;
&lt;p&gt;In the latter half of the talk, we will focus towards using Apache
Spark in practice by giving an overview of building a large data
processing pipeline that ingests and summarises terabytes of data
on a daily basis. Many non-technical departments such as Business
Intelligence and Marketing are familiar with Python and SQL. We
will demonstrate how Spark SQL and Dataframes can be used to query
over terabytes of data on the fly.&lt;/p&gt;
</content><category term="PyData London 2015"></category></entry></feed>