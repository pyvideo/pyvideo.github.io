<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 13 Jul 2019 00:00:00 +0000</lastBuildDate><item><title>Quantifying uncertainty in Machine Learning predictions</title><link>https://pyvideo.org/pydata-london-2019/quantifying-uncertainty-in-machine-learning-predictions.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;It is common practice to test the performance of ML models, but it is not so common to test the reliability of the predictions. Training a model, test its performance and hoping that it will produce good quality predictions is not the right approach if we are concerned with reliable ML. Hence, in this talk, we will discuss the concept of conformal predictions which quantify quality in predictions.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maria Navarro</dc:creator><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-13:pydata-london-2019/quantifying-uncertainty-in-machine-learning-predictions.html</guid></item></channel></rss>