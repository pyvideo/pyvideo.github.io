<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_aabir-abubaker-kar.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-10-12T00:00:00+00:00</updated><entry><title>Dimensionality reduction - squeezing out the good stuff with PCA</title><link href="https://pyvideo.org/pycon-za-2018/dimensionality-reduction-squeezing-out-the-good-stuff-with-pca.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>Aabir Abubaker Kar</name></author><id>tag:pyvideo.org,2018-10-12:pycon-za-2018/dimensionality-reduction-squeezing-out-the-good-stuff-with-pca.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data is often high-dimensional - millions of pixels, frequencies,
categories. A lot of this detail is unnecessary for data analysis - but
how much exactly? This talk will discuss the ideas and techniques of
dimensionality reduction, provide useful mathematical intuition about
how it's done, and show you how Netflix uses it to lead you from binge
to binge.&lt;/p&gt;
&lt;p&gt;In this session, we'll start by remembering what data really is and what
it stands for. Data is a structured set of numbers, and these numbers
typically (hopefully!) hold some information. This will lead us
naturally to the concept of a &lt;em&gt;high-dimensional dataspace&lt;/em&gt; , the
mystical realm in which data lives. It turns out that data in this space
displays an extremely useful 'selection bias' - &lt;strong&gt;*a datapoint can be
known by the company it keeps*&lt;/strong&gt;. This is one of the basic ideas behind
&lt;strong&gt;k-means clustering&lt;/strong&gt; , which we will briefly discuss.&lt;/p&gt;
&lt;p&gt;We'll then talk about the &lt;em&gt;informative-ness&lt;/em&gt; of certain dimensions of
the data space over others. This lays the mathematical foundation for
the technique of &lt;strong&gt;Principal Component Analysis&lt;/strong&gt; (PCA), which we will
run on the Netflix movie dataset using &lt;em&gt;scikit-learn&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We will also touch upon &lt;strong&gt;tSNE&lt;/strong&gt; , another popular
dimensionality-reduction algorithms.&lt;/p&gt;
&lt;p&gt;I will be using &lt;em&gt;scikit-learn&lt;/em&gt; for processing and &lt;em&gt;matplotlib&lt;/em&gt; for
visualization. The purpose of this session is to introduce
dimensionality- reduction to those who do not know it, and to provide
useful guiding intuitions to those who do. We'll also discuss some
seminal use-cases, with tips and warnings for your own applications.&lt;/p&gt;
</summary></entry><entry><title>The Ministry of Silly Talks</title><link href="https://pyvideo.org/pygotham-2017/the-ministry-of-silly-talks.html" rel="alternate"></link><published>2017-10-06T00:00:00+00:00</published><updated>2017-10-06T00:00:00+00:00</updated><author><name>Fangfei Shen</name></author><id>tag:pyvideo.org,2017-10-06:pygotham-2017/the-ministry-of-silly-talks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Lightning Talks from PyGotham 2017&lt;/p&gt;
</summary><category term="lightning talks"></category></entry></feed>