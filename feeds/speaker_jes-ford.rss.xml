<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Jes Ford</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 27 Apr 2022 00:00:00 +0000</lastBuildDate><item><title>Getting Started Testing in Data Science</title><link>https://pyvideo.org/pycon-us-2019/getting-started-testing-in-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;em&gt;How do you know if your data science results are correct?&lt;/em&gt; Robust
software usually has tests asserting that certain conditions hold, but
as a data scientist it’s often not straightforward or obvious how to
integrate these best practices. Our workflow includes exploration,
statistical models, and one-off analysis. This talk will give concrete
examples of when and how testing should play a role, and provide you
with enough introduction to get started writing your first data science
tests using &lt;tt class="docutils literal"&gt;pytest&lt;/tt&gt; &amp;amp; &lt;tt class="docutils literal"&gt;hypothesis&lt;/tt&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jes Ford</dc:creator><pubDate>Sun, 05 May 2019 13:50:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-05:/pycon-us-2019/getting-started-testing-in-data-science.html</guid><category>PyCon US 2019</category><category>talk</category></item><item><title>The Model Review: improving transparency, reproducibility, &amp; knowledge sharing using MLflow</title><link>https://pyvideo.org/pycon-us-2022/the-model-review-improving-transparency-reproducibility-knowledge-sharing-using-mlflow.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Code Review is an integral part of software development, but many teams don’t have similar processes in place for the development and deployment of Machine Learning (ML) models. I will motivate the decision to create a Model Review process, starting from the principles of transparency, reproducibility, and knowledge sharing. MLflow is a useful Python package to help simplify and automate much of the tracking necessary to create detailed records of machine learning experiments. Much of this talk will be spent introducing this tool, and demonstrating the core MLflow Tracking functionality. I’ll discuss how my team is currently running a Model Review process for any ML models that we push to production, and how we use MLflow to streamline this work and learn from each other.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jes Ford</dc:creator><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2022-04-27:/pycon-us-2022/the-model-review-improving-transparency-reproducibility-knowledge-sharing-using-mlflow.html</guid><category>PyCon US 2022</category></item></channel></rss>