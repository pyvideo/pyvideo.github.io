<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_david-warde-farley.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2014-07-09T00:00:00+00:00</updated><entry><title>Intergrating Pylearn2 and Hyperopt: Taking Deep Learning Further with Hyperparamter Optimization</title><link href="https://pyvideo.org/scipy-2014/intergrating-pylearn2-and-hyperopt-taking-deep-l.html" rel="alternate"></link><published>2014-07-09T00:00:00+00:00</published><updated>2014-07-09T00:00:00+00:00</updated><author><name>David Warde-Farley</name></author><id>tag:pyvideo.org,2014-07-09:scipy-2014/intergrating-pylearn2-and-hyperopt-taking-deep-l.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This talk/poster will outline and present recent work in integrating
Hyperopt, a package for the optimization of the hyperparameters of
machine learning algorithms, with Pylearn2, a machine learning research
and prototyping framework focused on &amp;quot;deep learning&amp;quot; algorithms, the
technical challenges we faced and how we addressed them.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Deep learning algorithms have recently garnered much attention for their
successes in solving very difficult industrial machine perception
problems. However, for many practical purposes, these algorithms are
unwieldy due to the rapid proliferation of &amp;quot;hyperparameters&amp;quot; in their
specification -- architectural and optimization constants which
ordinarily must be specified a priori by the practitioner. There is a
growing interest within the machine learning community, and acutely so
amongst deep learning researchers, in intelligently automating the
selection of hyperparameters for machine learning algorithms by through
the use of sequential model-based optimization techniques.
[Hyperopt][&lt;a class="reference external" href="http://hyperopt.github.io/hyperopt/"&gt;http://hyperopt.github.io/hyperopt/&lt;/a&gt;] is software package
designed for this purpose, architected as a general framework for
hyperparameter optimization algorithms with support for complicated,
awkward hyperparameter spaces that, e.g., involve many hyperparameters
that are only meaningful in the context of certain values of other
hyperparameters.&lt;/p&gt;
&lt;p&gt;[Pylearn2][&lt;a class="reference external" href="http://deeplearning.net/software/pylearn2"&gt;http://deeplearning.net/software/pylearn2&lt;/a&gt;] is a framework for
machine learning developed by the LISA laboratory at Université de
Montréal; it is a research and prototyping library aimed primarily at
machine learning researchers, with a focus on &amp;quot;deep learning&amp;quot;
algorithms. Despite being far from a stable release, it has had
considerable impact and developed a very active user community outside
of the laboratory that birthed it.&lt;/p&gt;
&lt;p&gt;This talk will deecribe recent efforts in building a flexible,
user-friendly bridge between Pylearn2 and Hyperopt for the purpose of
optimizing the hyperparameters of deep learning algorithms. Briefly, it
will outline the relevant problem domain and the two packages, the
technical challenges we've met in adapting the two for use with one
another and our solutions to them, in particular the development of a
novel common deferred evaluation/call-graph description language based
on &lt;tt class="docutils literal"&gt;functools.partial&lt;/tt&gt;, which we hope to make available in the near
future as a standalone package.&lt;/p&gt;
</summary><category term="machine learning"></category></entry></feed>