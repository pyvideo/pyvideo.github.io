<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_andrew-crozier.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-09-17T00:00:00+00:00</updated><entry><title>Big Data: Using Spark from Python and Jupyter</title><link href="https://pyvideo.org/pycon-uk-2018/big-data-using-spark-from-python-and-jupyter.html" rel="alternate"></link><published>2018-09-17T00:00:00+00:00</published><updated>2018-09-17T00:00:00+00:00</updated><author><name>Andrew Crozier</name></author><id>tag:pyvideo.org,2018-09-17:pycon-uk-2018/big-data-using-spark-from-python-and-jupyter.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Spark is the standard tool for processing big data, capable of
processing massive datasets often at speeds much faster than Apache
Hadoop, especially for iterative algorithms such as those of common
machine learning tasks.&lt;/p&gt;
&lt;p&gt;Spark is also relatively easy to get started with and use for
exploratory data analysis, especially as it offers interactive Scala,
Python and R shells in which a user can easily try out different ways of
manipulating their data, avoiding the slow write - compile - submit -
wait - inspect output loops of other frameworks. Spark also provides a
high level DataFrame API and scalable machine learning libraries, making
it a compelling tool for data scientists.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;In this talk Iâ€™ll introduce you to some tools for interacting with an
Apache Spark cluster from Python and Jupyter, including:&lt;/div&gt;
&lt;div class="line"&gt;* Connecting to a Spark cluster from a Jupyter notebook using
sparkmagic&lt;/div&gt;
&lt;div class="line"&gt;* Loading data into Spark from external sources&lt;/div&gt;
&lt;div class="line"&gt;* Basics of data manipulation in Spark&lt;/div&gt;
&lt;div class="line"&gt;* Getting data out of Spark and into Pandas DataFrames for plotting
or further modelling or analysis&lt;/div&gt;
&lt;div class="line"&gt;* Introduction to Spark machine learning tools&lt;/div&gt;
&lt;div class="line"&gt;* How to run Spark code from Python scripts, Plotly Dash web apps and
Flask web APIs using the pylivy client library&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;I am an engineer at ASI, a data science consultancy and developers of
the SherlockML data science platform based in London. This talk comes
directly from our experience helping our consultants and SherlockML
users to most effectively use Spark as part of an integrated data
science workflow.&lt;/p&gt;
</summary></entry><entry><title>Recipes for Productionising Data Science APIs</title><link href="https://pyvideo.org/pycon-uk-2017/recipes-for-productionising-data-science-apis.html" rel="alternate"></link><published>2017-10-29T15:00:00+01:00</published><updated>2017-10-29T15:00:00+01:00</updated><author><name>Andrew Crozier</name></author><id>tag:pyvideo.org,2017-10-29:pycon-uk-2017/recipes-for-productionising-data-science-apis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You're a data scientist developing fantastic models with the Python data science stack, and you want to release it into the wild. In this talk, I'll go over some practical solutions to deploying models as HTTP APIs on using the Flask web framework.&lt;/p&gt;
&lt;p&gt;Python has the unique advantage of not only being the most widely used language for data science, with a rich and mature stack of numerical and machine learning libraries, but also having a rich ecosystem of tooling for implementing web applications. Merging the best of these two worlds allows us to quickly and easily productionise data science models as APIs.&lt;/p&gt;
&lt;p&gt;This talk will be a practical guide for any data scientist or prospective data engineer wanting to productionise data science models, covering:     Flask     SQLAlchemy (including integration with Flask with Flask-SQLAlchemy)     Handling slow computations with job queues in python-rq     Authorization     Deployment (including gunicorn)&lt;/p&gt;
&lt;p&gt;Code samples implementing a simple application will be provided online following the talk. Participants should be able to use this as a starting point for deploying their own data science APIs online.&lt;/p&gt;
</summary></entry></feed>