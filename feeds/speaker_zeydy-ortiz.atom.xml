<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_zeydy-ortiz.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-09-14T00:00:00+00:00</updated><entry><title>Scalable Data Science with Spark and R</title><link href="https://pyvideo.org/pydata-carolinas-2016/scalable-data-science-with-spark-and-r.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Zeydy Ortiz</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/scalable-data-science-with-spark-and-r.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Processing large datasets in R have been limited by the amount of
memory in the local system. To overcome the native R limitation,
several cluster computing alternatives have recently emerged including
Apache Spark. In this session, we will discuss the architecture of
Spark and introduce the SparkR library. We will work through examples
of the API and discuss additional resources to learn more.&lt;/p&gt;
&lt;p&gt;In this tutorial, we will focus on SparkR. The outline of the tutorial
is as follows: - Introduction to cluster computing with Spark -
Getting started with SparkR - Deep dive into SparkR DataFrame API -
Additional resources&lt;/p&gt;
&lt;p&gt;In preparation for this tutorial please install.packages(&amp;quot;SparkR&amp;quot;) in
your system.&lt;/p&gt;
</summary></entry></feed>