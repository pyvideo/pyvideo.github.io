<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Fri, 23 Nov 2018 00:00:00 +0000</lastBuildDate><item><title>Distributed System in Deep Learning</title><link>https://pyvideo.org/pycon-hk-2018/distributed-system-in-deep-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The recent trends in distributed Deep Learning and how distributed
system can both massively reduce training time and enable
parallelisation. I will introduce different distributed deep learning
paradigms, including model-level parallelism and data-level parallelism,
and demonstrate how data parallelism can be used for distributed
training.In next iteration phase of deep learning there will be need of
distributed GPU computation.&lt;/p&gt;
&lt;p&gt;As data volumes increase, GPU clusters will be needed for the new
distributed methods that already produce produce the state-of-the-art
results for ImageNet and Cifar-10, such as neural architecture search.
Auto-ml is also predicated on the availability of GPU clusters. Deep
Learning systems at hyper-scale AI companies attack the toughest
problems with distributed deep learning. Distributed Deep Learning
enables both AI researchers and practitioners to be more productive and
the training of models that would be intractable on a single GPU server.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ashutosh Singh</dc:creator><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-11-23:pycon-hk-2018/distributed-system-in-deep-learning.html</guid></item></channel></rss>