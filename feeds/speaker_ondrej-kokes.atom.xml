<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_ondrej-kokes.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-05-10T00:00:00+00:00</updated><entry><title>High on Cardinality, Low on Memory</title><link href="https://pyvideo.org/pydata-amsterdam-2019/high-on-cardinality-low-on-memory.html" rel="alternate"></link><published>2019-05-10T00:00:00+00:00</published><updated>2019-05-10T00:00:00+00:00</updated><author><name>Ondrej Kokes</name></author><id>tag:pyvideo.org,2019-05-10:pydata-amsterdam-2019/high-on-cardinality-low-on-memory.html</id><summary type="html"></summary><category term="pydata"></category></entry><entry><title>DataFrames: scaling up and out</title><link href="https://pyvideo.org/pydata-amsterdam-2018/dataframes-scaling-up-and-out.html" rel="alternate"></link><published>2018-05-26T00:00:00+00:00</published><updated>2018-05-26T00:00:00+00:00</updated><author><name>Ondrej Kokes</name></author><id>tag:pyvideo.org,2018-05-26:pydata-amsterdam-2018/dataframes-scaling-up-and-out.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;DataFrames of all sorts have become very popular for tabular data analytics for their nifty APIs and ease of use. But as they usually operate as in-memory engines, they can be hard to scale. In my talk, I'd like to outline several ways one can scale their compute platform to handle larger datasets without incurring much cost.&lt;/p&gt;
</summary></entry><entry><title>DataFrames: scaling up and out</title><link href="https://pyvideo.org/pycon-cz-2018/dataframes-scaling-up-and-out.html" rel="alternate"></link><published>2018-06-03T00:00:00+00:00</published><updated>2018-06-03T00:00:00+00:00</updated><author><name>Ondřej Kokeš</name></author><id>tag:pyvideo.org,2018-06-03:pycon-cz-2018/dataframes-scaling-up-and-out.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;DataFrames have become ubiquitous when it comes to fast analyses of complex data. They go beyond SQL by not adhering to a strict schema and offer a rich API, where you chain methods, which fosters exploratory analytics.&lt;/p&gt;
&lt;p&gt;While newcomers to Python usually learn about pandas early on, they sometimes struggle as their underlying data grow in size. Given the in-memory nature of pandas' storage system, one can usually only scale up.&lt;/p&gt;
&lt;p&gt;I'd like to outline several workflows for adapting to the ever-increasing size of datasets:&lt;/p&gt;
&lt;p&gt;Changing application logic to handle streams rather than loading the whole dataset into memory.
Actually scaling up – locally by buying more memory and/or faster disk drives, or by deploying servers in the cloud and SSH tunneling to remote Jupyter instances.
Scaling your data source and utilizing pandas' SQL connector. This will help in other areas as well (e.g. direct connections in BI).
Using a distributed DataFrame engine – Dask or PySpark. These scale from laptops to large clusters, using the very same API the whole way through.
I will cover the various differences between these approaches and will outline their set of upsides (e.g. scaling and performance) and downsides (DevOps difficulties, cost).&lt;/p&gt;
</summary></entry></feed>