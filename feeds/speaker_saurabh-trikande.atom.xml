<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Saurabh Trikande</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_saurabh-trikande.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-10-16T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Cost Effectively Deploy Thousands of Fine Tuned Gen AI Models Like Llama Using TorchServe on AWS</title><link href="https://pyvideo.org/pytorch-conference-2023/cost-effectively-deploy-thousands-of-fine-tuned-gen-ai-models-like-llama-using-torchserve-on-aws.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Saurabh Trikande</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/cost-effectively-deploy-thousands-of-fine-tuned-gen-ai-models-like-llama-using-torchserve-on-aws.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As Generative AI adoption accelerates across industry, organizations want to deliver hyper-personalized experiences to end users. For building such experiences, thousands of models are being developed by fine-tuning pre-trained large models. To meet their stringent latency and throughput goals, organizations use GPU instances to deploy such models. However, inference â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As Generative AI adoption accelerates across industry, organizations want to deliver hyper-personalized experiences to end users. For building such experiences, thousands of models are being developed by fine-tuning pre-trained large models. To meet their stringent latency and throughput goals, organizations use GPU instances to deploy such models. However, inference costs can add up quickly if deploying thousands of models and provisioning dedicated hardware for each. TorchServe offers feature likes open platform, deferred distribution initialization, model sharing and heterogeneous deployment that make it easy for users to deploy fine tuned large models and save cost. Learn how organization can use these features in conjunction with fine tuning techniques like PEFT (Parameter Efficient Fine Tuning) and use Amazon SageMaker Multi-Model Endpoint (MME) to deploy multiple GenAI models on the same GPU, share GPU instances across thousands of GenAI models, and dynamically load/unload models based on incoming traffic. All of which helps you significantly reduce the cost. Finally we showcase example code for deploying multiple Llama based models which are fine tuned using PEFT on MME.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry></feed>