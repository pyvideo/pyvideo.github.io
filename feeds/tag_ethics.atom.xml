<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_ethics.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-11T00:00:00+00:00</updated><entry><title>The Dangers of Outsourcing Software Development</title><link href="https://pyvideo.org/europython-2019/the-dangers-of-outsourcing-software-development.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Sebastian Roll</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/the-dangers-of-outsourcing-software-development.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Customer lock-in. Dubious “Land and Expand” strategies. We have all
heard about outsourced software projects that run far past schedule and
obliterates the budget, yet somehow fail to meet even basic
requirements.&lt;/p&gt;
&lt;p&gt;How can this happen. How can your company reduce the risks of delegating
software development to an external party?&lt;/p&gt;
&lt;p&gt;The Principal–Agent Problem is a well researched dilemma in economic
literature. It occurs when a person or entity (“agent”), is tasked to
work on behalf of another person or entity (&amp;quot;principal&amp;quot;). When the two
parties have divergent interests, the agent might act contrary to the
best interests of the principal.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Examples of principal-agent relationships are:&lt;/div&gt;
&lt;div class="line"&gt;- Employer vs Employee&lt;/div&gt;
&lt;div class="line"&gt;- Shareholder vs Management&lt;/div&gt;
&lt;div class="line"&gt;- Voter vs Political party&lt;/div&gt;
&lt;div class="line"&gt;- Contractor vs Software Consultant&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;When a firm decides to outsource software development to a third party,
there is an economic divergence of interest at play, as both parties
seek to maximize profit. The contractor wants its requirements met at
low cost and on a predictable schedule. The third party, when acting
nefariously, can maximize its own gains by extending the project,
utilizing junior or low-cost labor, and creating a relationship of
dependency.&lt;/p&gt;
&lt;p&gt;In this session we will look into some nefarious techniques and
practices used in the IT consulting industry and how best to avoid them.
We will also learn why it is particularly hard to mitigate the risks of
outsourced software development.&lt;/p&gt;
</summary><category term="Business"></category><category term="Clients"></category><category term="Ethics"></category><category term="Management"></category><category term="freelancing"></category></entry><entry><title>Ethics in Machine Learning Panel</title><link href="https://pyvideo.org/pydata-berlin-2017/ethics-in-machine-learning-panel.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Roelof Pieters</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/ethics-in-machine-learning-panel.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Advances in AI are happening at a tremendous pace, and especially Machine Learning systems are hastily deployed in settings that span as wide as social media, search engines, advertising, military use, and legal systems. At first sight results are often promising, but more and more &amp;quot;outliers&amp;quot; are becoming visible as well. Systemic bias in culture, language, or business practices often become intensified: From the Tay-chatbot turning racist, to both Google and Flickr classifying images of African-Americans as &amp;quot;monkeys&amp;quot;, to discrimination and sexism encoded in language models used for everything from translation, to court sentencing decision. Luckily, initiatives that address some of these concerns is happening through more interpretable models, new privacy regulations, novel privacy-preserving learning algorithms, and researchers and engineers standing up for more just and transparent machine learning models and methods.&lt;/p&gt;
&lt;p&gt;The panel aims to debate these themes with speakers, and the audience, and go beyond the usual &amp;quot;AI hype&amp;quot; and discuss both the amazing progress, as well as the setbacks, in making societies better, more humane, and ready for the future!&lt;/p&gt;
&lt;p&gt;Speaking at the panel will be:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Roelof Pieters, Panel Host, Co-founder at creative.ai&lt;/li&gt;
&lt;li&gt;Marek Rosa, CEO/CTO at GoodAI&lt;/li&gt;
&lt;li&gt;Françoise Provencher, Data Team Lead at Shopify&lt;/li&gt;
&lt;li&gt;Hendrik Heuer, Researcher at Institute for Information Management (ifib) at the University of Bremen&lt;/li&gt;
&lt;li&gt;Andreas Dewes, Founder at 7scientists&lt;/li&gt;
&lt;li&gt;Katharine Jarmul, Founder at kjamistan&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Link to Q&amp;amp;A: &lt;a class="reference external" href="https://youtu.be/uOKhB1_vyAw"&gt;https://youtu.be/uOKhB1_vyAw&lt;/a&gt; (Second pyvideo tab)&lt;/p&gt;
</summary><category term="panel"></category><category term="ethics"></category><category term="machine learning"></category></entry><entry><title>Despicable machines: how computers can be assholes</title><link href="https://pyvideo.org/pydata-barcelona-2017/despicable-machines-how-computers-can-be-assholes.html" rel="alternate"></link><published>2017-05-20T12:00:00+02:00</published><updated>2017-05-20T12:00:00+02:00</updated><author><name>Maciej Siwek</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/despicable-machines-how-computers-can-be-assholes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There's a widespread belief among machine learning practitioners that algorithms are objective and allow us to deal with the messy reality in a nice, objective way, without worrying about all the yucky human nature things. This talk will argue that this belief is wrong. Algorithms, just like the humans who create them, can be severely biased and despicable indeed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When working on a new ML solution to solve a given problem, do you think that you are simply using objective reality to infer a set of unbiased rules that will allow you to predict the future? Do you think that worrying about the morality of your work is something other people should do? If so, this talk is for you.&lt;/p&gt;
&lt;p&gt;In this brief time, I will try to convince you that you hold great power over how the future world will look like and that you should incorporate thinking about morality into the set of ML tools you use every day. We will take a short journey through several problems, which surfaced over the last few years, as ML and AI generally, became more widely used. We will look at bias present in training data, at some real-world consequences of not considering it (including one or two hair-raising stories) and cutting-edge research on how to counteract this.&lt;/p&gt;
&lt;p&gt;The outline of the talk is:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Intro the problem: ML algos can be biased!&lt;/li&gt;
&lt;li&gt;Two concrete examples.&lt;/li&gt;
&lt;li&gt;What's been done so far (i.e. techniques from recently-published papers).&lt;/li&gt;
&lt;li&gt;What to do next: unanswered questions.&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="ethics"></category><category term="machine learning"></category></entry></feed>