<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Raghu Ganti</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_raghu-ganti.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-10-16T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Keynote: How to Leverage PyTorch to Scale AI Training and Inferencing</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-how-to-leverage-pytorch-to-scale-ai-training-and-inferencing.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Raghu Ganti</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-how-to-leverage-pytorch-to-scale-ai-training-and-inferencing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As generative AI models grow larger and more complex, the ability to scale these models becomes a critical challenge facing enterprises today. How can developers leverage PyTorch to maximize the value of these large, multi-billion parameter models to make them run faster, more efficiently, and more affordably both on-prem â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As generative AI models grow larger and more complex, the ability to scale these models becomes a critical challenge facing enterprises today. How can developers leverage PyTorch to maximize the value of these large, multi-billion parameter models to make them run faster, more efficiently, and more affordably both on-prem and in the cloud? This keynote will highlight various levers that PyTorch FSDP provides to scale AI model training on hundreds of GPUs and how IBM applied them to obtain state-of-the-art training throughput in models with up to 70 billion parameters. It will also discuss how we combined the latest advancements in PyTorch compile with custom tensor parallel implementation to achieve significantly reduced inferencing latency.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry></feed>