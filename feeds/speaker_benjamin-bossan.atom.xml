<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Benjamin Bossan</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_benjamin-bossan.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-07-08T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Fine-tuning large models on local hardware</title><link href="https://pyvideo.org/europython-2024/fine-tuning-large-models-on-local-hardware.html" rel="alternate"></link><published>2024-07-08T00:00:00+00:00</published><updated>2024-07-08T00:00:00+00:00</updated><author><name>Benjamin Bossan</name></author><id>tag:pyvideo.org,2024-07-08:/europython-2024/fine-tuning-large-models-on-local-hardware.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;[EuroPython 2024 — Forum Hall on 2024-07-11]&lt;/p&gt;
&lt;p&gt;Fine-tuning large models on local hardware by Benjamin Bossan&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://ep2024.europython.eu/session/fine-tuning-large-models-on-local-hardware"&gt;https://ep2024.europython.eu/session/fine-tuning-large-models-on-local-hardware&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fine-tuning big neural nets like Large Language Models (LLMs) has traditionally been prohibitive due to high hardware requirements. However, Parameter-Efficient Fine-Tuning (PEFT) and quantization enable the training of …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;[EuroPython 2024 — Forum Hall on 2024-07-11]&lt;/p&gt;
&lt;p&gt;Fine-tuning large models on local hardware by Benjamin Bossan&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://ep2024.europython.eu/session/fine-tuning-large-models-on-local-hardware"&gt;https://ep2024.europython.eu/session/fine-tuning-large-models-on-local-hardware&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fine-tuning big neural nets like Large Language Models (LLMs) has traditionally been prohibitive due to high hardware requirements. However, Parameter-Efficient Fine-Tuning (PEFT) and quantization enable the training of large models on modest hardware. Thanks to the PEFT library and the Hugging Face ecosystem, these techniques are now accessible to a broad audience.&lt;/p&gt;
&lt;p&gt;Expect to learn:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;what the challenges are of fine-tuning large models&lt;/li&gt;
&lt;li&gt;what solutions have been proposed and how they work&lt;/li&gt;
&lt;li&gt;practical examples of applying the PEFT library&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;p&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License: &lt;a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;https://creativecommons.org/licenses/by-nc-sa/4.0/&lt;/a&gt;&lt;/p&gt;
</content><category term="EuroPython 2024"></category></entry><entry><title>skorch: A scikit-learn compatible neural network library...</title><link href="https://pyvideo.org/pydata-berlin-2019/skorch-a-scikit-learn-compatible-neural-network-library.html" rel="alternate"></link><published>2019-10-10T00:00:00+00:00</published><updated>2019-10-10T00:00:00+00:00</updated><author><name>Benjamin Bossan</name></author><id>tag:pyvideo.org,2019-10-10:/pydata-berlin-2019/skorch-a-scikit-learn-compatible-neural-network-library.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker: Benjamin Bossan&lt;/p&gt;
&lt;p&gt;Track:PyData
This talk is about the open source package [skorch](https://github.com/skorch-dev/skorch), a wrapper library that allows you to combine the best of sklearn and PyTorch. It covers when it makes sense to use skorch and highlights interesting features.&lt;/p&gt;
&lt;p&gt;Recorded at the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker: Benjamin Bossan&lt;/p&gt;
&lt;p&gt;Track:PyData
This talk is about the open source package [skorch](https://github.com/skorch-dev/skorch), a wrapper library that allows you to combine the best of sklearn and PyTorch. It covers when it makes sense to use skorch and highlights interesting features.&lt;/p&gt;
&lt;p&gt;Recorded at the PyConDE &amp;amp; PyData Berlin 2019 conference.
&lt;a class="reference external" href="https://pycon.de"&gt;https://pycon.de&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More details at the conference page: &lt;a class="reference external" href="https://de.pycon.org/program/NMXSE7"&gt;https://de.pycon.org/program/NMXSE7&lt;/a&gt;
Twitter:  &lt;a class="reference external" href="https://twitter.com/pydataberlin"&gt;https://twitter.com/pydataberlin&lt;/a&gt;
Twitter:  &lt;a class="reference external" href="https://twitter.com/pyconde"&gt;https://twitter.com/pyconde&lt;/a&gt;&lt;/p&gt;
</content><category term="PyData Berlin 2019"></category></entry></feed>