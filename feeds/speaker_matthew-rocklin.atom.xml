<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_matthew-rocklin.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-13T00:00:00+00:00</updated><entry><title>Better and Faster Hyper Parameter Optimization with Dask</title><link href="https://pyvideo.org/scipy-2019/better-and-faster-hyper-parameter-optimization-with-dask.html" rel="alternate"></link><published>2019-07-13T00:00:00+00:00</published><updated>2019-07-13T00:00:00+00:00</updated><author><name>Tom Augspurger</name></author><id>tag:pyvideo.org,2019-07-13:scipy-2019/better-and-faster-hyper-parameter-optimization-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Nearly every machine learning model requires that the user specify certain parameters before training begins, aka &amp;quot;hyper-parameters&amp;quot;. Finding the optimal set of hyper-parameters is often a time- and resource-consuming process. A recent breakthrough hyper-parameter optimization algorithm, Hyperband, can find high performing hyper-parameters with minimal training and has theoretical backing. This talk will provide intuition for Hyperband, explain it's use and why it's well-suited for Dask, a Python library that scales Python to larger datasets and more computational resources. Experiments find high performing hyper-parameters more quickly in the presence of parallel computational resources and with a deep learning model.&lt;/p&gt;
</summary></entry><entry><title>Refactoring the SciPy Ecosystem for Heterogeneous Computing</title><link href="https://pyvideo.org/scipy-2019/refactoring-the-scipy-ecosystem-for-heterogeneous-computing.html" rel="alternate"></link><published>2019-07-13T00:00:00+00:00</published><updated>2019-07-13T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2019-07-13:scipy-2019/refactoring-the-scipy-ecosystem-for-heterogeneous-computing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk summarizes a number of efforts across the community to build standardization into the ecosystem. We include topics like accelerator hardware (GPUs, TPUs, multi-core CPUs), API extension points in NumPy and Pandas, file formats, and other community efforts to establish cohesion.&lt;/p&gt;
</summary></entry><entry><title>Turning HPC Systems into Interactive Data Analysis Platforms</title><link href="https://pyvideo.org/scipy-2019/turning-hpc-systems-into-interactive-data-analysis-platforms.html" rel="alternate"></link><published>2019-07-13T00:00:00+00:00</published><updated>2019-07-13T00:00:00+00:00</updated><author><name>Anderson Banihirwe</name></author><id>tag:pyvideo.org,2019-07-13:scipy-2019/turning-hpc-systems-into-interactive-data-analysis-platforms.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk demonstrates how to use Dask and Jupyter on large high-performance computing (HPC) systems to scale and accelerate large interactive data analysis tasks -- effectively turning HPC systems into interactive big-data platforms. We will introduce dask-jobqueue which allows users to seamlessly deploy and scale dask on HPC clusters that use a variety of job queuing systems such as PBS, Slurm, SGE, and LSF. We will also introduce dask-mpi, a Python package that makes deploying Dask easy from within a distributed MPI environment.&lt;/p&gt;
</summary></entry><entry><title>Democratizing Distributed Computing with Dask and JupyterHub</title><link href="https://pyvideo.org/pycon-us-2018/democratizing-distributed-computing-with-dask-and-jupyterhub.html" rel="alternate"></link><published>2018-05-12T00:00:00+00:00</published><updated>2018-05-12T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2018-05-12:pycon-us-2018/democratizing-distributed-computing-with-dask-and-jupyterhub.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We use JupyterHub, XArray, Dask, and Kubernetes to build a cloud-based system to enable scientists to analyze and manage large datasets.  We use this in practice to serve a broad community of atmospheric and climate scientists.&lt;/p&gt;
&lt;p&gt;Atmospheric and climate scientists analyze large volumes of observational and simulated data to better understand our planet.  They have historically used tools like NumPy and SciPy along with Jupyter notebooks to combine efficient computation with accessibility.  However, as datasets increase in size and collaboration extends to new populations of scientists these tools begin to feel their age.  In this talk we use more recent libraries to build a modern deployment for academic scientists.  In particular we use the following tools:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Dask:&lt;/strong&gt; to parallelize and scale NumPy computations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;XArray&lt;/strong&gt;: as a self-discribing data model and tool kit for labeled and index arrays&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JupyterLab:&lt;/strong&gt; to enable more APIs for users beyond the classic notebook&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JupyterHub:&lt;/strong&gt; to manage users and maintain environments for a new population of cloud-friendly users&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubernetes:&lt;/strong&gt; to manage everything and deploy easily on cloud hardware&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This talk will focus less on how these libraries work and will instead be a case study of using them together in an operational setting.  During the talk we will build up and deploy a running system that the audience can then use to access distributed computing resources.&lt;/p&gt;
</summary><category term="jupyterhub"></category><category term="xarray"></category><category term="dask"></category><category term="kubernetes"></category></entry><entry><title>Streaming Processing with Dask</title><link href="https://pyvideo.org/pydata-new-york-city-2017/streaming-processing-with-dask.html" rel="alternate"></link><published>2017-11-27T00:00:00+00:00</published><updated>2017-11-27T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2017-11-27:pydata-new-york-city-2017/streaming-processing-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk discusses ongoing work to build streaming data processing systems for Python with Dask, a Pythonic library for parallel computing. This talk will discuss streaming primitives, dataframes, and integration with the Jupyter notebook and use example from financial time series and cyber-security.&lt;/p&gt;
</summary></entry><entry><title>Dask: Next Steps in Parallel Python</title><link href="https://pyvideo.org/pycon-de-2017/dask-next-steps-in-parallel-python.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/dask-next-steps-in-parallel-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Matthew&lt;/strong&gt; is an open source software developer focusing on efficient computation and parallel computing, primarily within the Python ecosystem. He has contributed to many of the PyData libraries and today works on Dask a framework for parallel computing. Matthew holds a PhD in computer science from the University of Chicago where he focused on numerical linear algebra, task scheduling, and computer algebra.&lt;/p&gt;
&lt;p&gt;Matthew lives in Brooklyn, NY and is employed by Anaconda Inc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>Dask for task scheduling</title><link href="https://pyvideo.org/pygotham-2017/dask-for-task-scheduling.html" rel="alternate"></link><published>2017-10-06T00:00:00+00:00</published><updated>2017-10-06T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2017-10-06:pygotham-2017/dask-for-task-scheduling.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a library for parallel and distributed computing for Python, commonly known for parallelizing libraries like NumPy and pandas.  This talk discusses using Dask for task scheduling workloads, such as might be handled by Celery and Airflow, in a scalable and accessible manner.&lt;/p&gt;
&lt;p&gt;Most previous talks on Dask focus on &amp;quot;big data&amp;quot; collections like distributed pandas dataframes.  In this talk we'll diverge a bit and talk about more real-time and fine-grained settings.  We'll discuss dask's concurrent.futures interface, integration with await/async syntax, dynamic workload handling, and more.  This will focus more on the web-backend crowd than on the data-science crowd.&lt;/p&gt;
</summary></entry><entry><title>The Ministry of Silly Talks</title><link href="https://pyvideo.org/pygotham-2017/the-ministry-of-silly-talks.html" rel="alternate"></link><published>2017-10-06T00:00:00+00:00</published><updated>2017-10-06T00:00:00+00:00</updated><author><name>Fangfei Shen</name></author><id>tag:pyvideo.org,2017-10-06:pygotham-2017/the-ministry-of-silly-talks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Lightning Talks from PyGotham 2017&lt;/p&gt;
</summary><category term="lightning talks"></category></entry><entry><title>Dask - Advanced Techniques</title><link href="https://pyvideo.org/scipy-2017/dask-advanced-techniques.html" rel="alternate"></link><published>2017-07-14T00:00:00+00:00</published><updated>2017-07-14T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2017-07-14:scipy-2017/dask-advanced-techniques.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask enables parallel computing in Python. While commonly used for its parallel and NumPy, Pandas implementations, Dask is also capable of a variety of more advanced parallel computing workflows. This talk dives into these advanced features features and applications beyond the typical distributed dataframe to talk about asynchronicity, dynamic and self-building computations,
multi-user workflows, and more.&lt;/p&gt;
</summary></entry><entry><title>Parallel Data Analysis in Python</title><link href="https://pyvideo.org/scipy-2017/parallel-data-analysis-in-python.html" rel="alternate"></link><published>2017-07-13T00:00:00+00:00</published><updated>2017-07-13T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2017-07-13:scipy-2017/parallel-data-analysis-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Tutorial materials found here: &lt;a class="reference external" href="https://scipy2017.scipy.org/ehome/220975/493423/"&gt;https://scipy2017.scipy.org/ehome/220975/493423/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial teaches the fundamentals of parallel programming in Python. It focuses on covering a few programming techniques rather than diving into one framework or tool in particular.&lt;/p&gt;
&lt;p&gt;Student Goals:&lt;/p&gt;
&lt;p&gt;Students will walk away with a high-level understanding of both parallel problems and how to reason about parallel computing frameworks. They will also walk away with hands-on experience using a variety of frameworks easily accessible from Python.&lt;/p&gt;
</summary><category term="tutorial"></category></entry><entry><title>Dask: A Pythonic Distributed Data Science Framework</title><link href="https://pyvideo.org/pycon-us-2017/dask-a-pythonic-distributed-data-science-framework.html" rel="alternate"></link><published>2017-05-19T00:00:00+00:00</published><updated>2017-05-19T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2017-05-19:pycon-us-2017/dask-a-pythonic-distributed-data-science-framework.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a general purpose parallel computing system capable of
Celery-like task scheduling, Spark-like big data computing, and
Numpy/Pandas/Scikit-learn level complex algorithms, written in Pure
Python. Dask has been adopted by the PyData community as a Big Data
solution.&lt;/p&gt;
&lt;p&gt;This talk focuses on the distributed task scheduler that powers Dask
when running on a cluster. We'll focus on how we built a Big Data
computing system using the Python networking stack (Tornado/AsyncIO) in
service of its data science stack (NumPy/Pandas/Scikit Learn).
Additionally we'll talk about the challenges of effective task
scheduling in a data science context (data locality, resilience, load
balancing) and how we manage this dynamically with aggressive
measurement and dynamic scheduling heuristics.&lt;/p&gt;
</summary></entry><entry><title>Parallel Data Analysis</title><link href="https://pyvideo.org/pycon-us-2017/parallel-data-analysis.html" rel="alternate"></link><published>2017-05-18T00:00:00+00:00</published><updated>2017-05-18T00:00:00+00:00</updated><author><name>Ben Zaitlen</name></author><id>tag:pyvideo.org,2017-05-18:pycon-us-2017/parallel-data-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;An overview of parallel computing techniques available from Python and
hands-on experience with a variety of frameworks.&lt;/p&gt;
&lt;p&gt;This course has two primary goals: 1. Teach students how to reason about
parallel computing 2. Provide hands-on experience with a variety of
different parallel computing frameworks&lt;/p&gt;
&lt;p&gt;Students will walk away with both a high-level understanding of parallel
problems and how to select and use an appropriate parallel computing
framework for their problem. They will get hands-on experience using
tools both on their personal laptop, and on a cluster environment that
will be provided for them at the tutorial.&lt;/p&gt;
&lt;p&gt;For the first half we cover programming patterns for parallelism found
across many tools, notably map, futures, and big-data collections. We
investigate these common APIs by diving into a sequence of examples that
require increasingly complex tools. We learn the benefits and costs of
each API and the sorts of problems where each is appropriate.&lt;/p&gt;
&lt;p&gt;For the second half, we focus on the performance aspects of frameworks
and give intuition on how to pick the right tool for the job. This
includes common challenges in parallel analysis, such as communication
costs, debugging parallel code, as well as deployment and setup
strategies.&lt;/p&gt;
</summary></entry><entry><title>Dask: out of core arrays with task scheduling</title><link href="https://pyvideo.org/pydata-seattle-2015/dask-out-of-core-arrays-with-task-scheduling.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/dask-out-of-core-arrays-with-task-scheduling.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask Array implements the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores.&lt;/p&gt;
&lt;p&gt;We describe dask, dask.array, dask.dataframe, as well as task scheduling generally.&lt;/p&gt;
&lt;p&gt;NumPy and Pandas provide excellent in-memory containers and computation for the Scientific Python ecosystem. As we extend to larger-than-memory datasets these containers fail, leaving scientists with less productive options that mesh less well with the existing ecosystem.&lt;/p&gt;
&lt;p&gt;A common solution to this problem is blocking algorithms and task scheduling. Blocking algorithms define macro-scale operations on the full dataset as a network of smaller operations on in-memory blocks of the dataset. Task scheduling allows many parallel workers to execute these tasks in a way consistent to their data dependencies.&lt;/p&gt;
&lt;p&gt;We introduce dask, a task scheduling specification, and dask.array a high-level abstraction that implements a large subset of the NumPy API with blocked algorithms. In many cases dask.array provides a drop-in replacement for NumPy for out-of-core datasets with parallel execution. We discuss the design choices behind dask, dask.array, and related projects and show performance both quantitatively with benchmarks and also in usability by demonstrating integration into the larger ecosystem.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/ContinuumIO/dask-tutorial"&gt;https://github.com/ContinuumIO/dask-tutorial&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Dask for ad hoc distributed computing</title><link href="https://pyvideo.org/pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk discusses parallel and distributed computing in Python, particularly for ad-hoc and custom algorithms. It focuses on Dask, a Python solution for flexible distributed computing.&lt;/p&gt;
&lt;p&gt;The Python data science stack contains efficient algorithms with intuitive interfaces for sophisticated and friendly analysis. As the data science community tackles larger problems with larger hardware we naturally ask how best to parallelize this software stack both across many cores in a single computer and across computers in a cluster. This turns out to be harder than it looks, even with traditional Big Data tools like MapReduce, Storm, and Spark. Both the complexity of the algorithms and the high expectations for interactivity raise challenges for these systems. This talk lays out the benefits and challenges of parallelizing a numeric analytic stack, and then describes Dask, a parallel framework gaining traction within the Python community for interactive performant parallel computing, and finally goes through a few domains where this work is enabling novel science today.&lt;/p&gt;
</summary><category term="dask"></category><category term="distributed"></category></entry><entry><title>Parallel Python Analyzing Large Data Sets</title><link href="https://pyvideo.org/pydata-dc-2016/parallel-python-analyzing-large-data-sets.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Aron Ahmadia</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/parallel-python-analyzing-large-data-sets.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Students will walk away with a high-level understanding of both parallel problems and how to reason about parallel computing frameworks. They will also walk away with hands-on experience using a variety of frameworks easily accessible from Python.&lt;/p&gt;
</summary><category term="Data"></category><category term="parallel"></category><category term="sets"></category></entry><entry><title>Using Dask for Parallel Computing in Python</title><link href="https://pyvideo.org/pydata-dc-2016/using-dask-for-parallel-computing-in-python.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/using-dask-for-parallel-computing-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Dask is a relatively new library for parallel computing in Python. It builds around familiar data structures to users of the PyData stack and enables them to scale up their work on one or many machines. This tutorial will introduce users to the core concepts of dask by working through some example problems. The tutorial will be distributed via Jupyter Notebooks.&lt;/p&gt;
</summary><category term="dask"></category><category term="parallel"></category><category term="parallel computing"></category></entry><entry><title>Dask Parallel and Distributed Computing</title><link href="https://pyvideo.org/scipy-2016/dask-parallel-and-distributed-computing-scipy-2016-matthew-rocklin.html" rel="alternate"></link><published>2016-07-14T00:00:00+00:00</published><updated>2016-07-14T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2016-07-14:scipy-2016/dask-parallel-and-distributed-computing-scipy-2016-matthew-rocklin.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a pure Python library for parallel and distributed computing. Last year Dask parallelized NumPy and Pandas computations on multi-core workstations. This year we discuss using Dask to design custom algorithms and execute those algorithms efficiently on a cluster. This talk discusses Pythonic APIs for parallel algorithm development as well as strategies for intuitive and efficient distributed computing. We discuss recent results in machine learning and novel scientific applications.&lt;/p&gt;
</summary><category term="SciPy 2016"></category><category term="dask"></category><category term="parallel computing"></category></entry><entry><title>SymPy Stats - Uncertainty Modeling</title><link href="https://pyvideo.org/scipy-2012/sympy-stats-uncertainty-modeling.html" rel="alternate"></link><published>2012-07-18T00:00:00+00:00</published><updated>2012-07-18T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2012-07-18:scipy-2012/sympy-stats-uncertainty-modeling.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;SymPy is a symbolic algebra package for Python. In SymPy.Stats we add a
stochastic variable type to this package to form a language for
uncertainty modeling. This allows engineers and scientists to
symbolically declare the uncertainty in their mathematical models and to
make probabilistic queries. We provide transformations from
probabilistic statements like &lt;span class="formula"&gt;&lt;i&gt;P&lt;/i&gt;(&lt;i&gt;X&lt;/i&gt;*&lt;i&gt;Y&lt;/i&gt; &amp;gt; 3)&lt;/span&gt; or &lt;span class="formula"&gt;&lt;i&gt;E&lt;/i&gt;(&lt;i&gt;X&lt;/i&gt;**2)&lt;/span&gt; into
deterministic integrals. These integrals are then solved using SymPy's
integration routines or through numeric sampling.&lt;/p&gt;
&lt;p&gt;This talk touches on a few rising themes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The rise in interest in uncertainty quantification and&lt;/li&gt;
&lt;li&gt;The use of symbolics in scientific computing&lt;/li&gt;
&lt;li&gt;Intermediate representation layers and multi-stage compilation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Historically solutions to uncertainty quantification problems have been
expressed by writing Monte Carlo codes around individual problems. By
creating a symbolic uncertainty language we allow the expression of the
problem-to-be- solved to be written separately from the numerical
technique. SymPy.stats serves as an interface layer. The statistical
programmer doesn't need to think about the details of numerical
techniques and the computational methods programmer doesn't need to
think about the particular domain-specific questions to be solved.&lt;/p&gt;
&lt;p&gt;We have implemented multiple comptuational backends including purely
symbolic (using SymPy's integration engine), sampling, and code
generation.&lt;/p&gt;
&lt;p&gt;In the talk we discuss these ideas with a few illustrative examples
taken from basic probability and engineering. The following is one such
example&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://sympystats.wordpress.com/2011/07/02/a-lesson-in-data-%20assimilation-using-sympy/"&gt;http://sympystats.wordpress.com/2011/07/02/a-lesson-in-data-assimilation-
using-sympy/&lt;/a&gt;&lt;/p&gt;
</summary><category term="General"></category></entry><entry><title>Blaze: Building a Foundation for Array-Oriented Computing in Python</title><link href="https://pyvideo.org/scipy-2014/blaze-building-a-foundation-for-array-oriented-c.html" rel="alternate"></link><published>2014-07-10T00:00:00+00:00</published><updated>2014-07-10T00:00:00+00:00</updated><author><name>Mark Wiebe</name></author><id>tag:pyvideo.org,2014-07-10:scipy-2014/blaze-building-a-foundation-for-array-oriented-c.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The Blaze project is a collection of libraries being built towards the
goal of generalizing NumPy's data model and working on distributed data.
This talk covers each of these libraries, and how they work together to
accomplish this goal.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python's scientific computing and data analysis ecosystem, built around
NumPy, SciPy, Matplotlib, Pandas, and a host of other libraries, is a
tremendous success. NumPy provides an array object, the array-oriented
ufunc primitive, and standard practices for exposing and writing
numerical libraries to Python all of which have assisted in making it a
solid foundation for the community. Over time, however, it has become
clear that there are some limitations of NumPy that are difficult to
address via evolution from within. Notably, the way NumPy arrays are
restricted to data with regularly strided memory structure on a single
machine is not easy to change.&lt;/p&gt;
&lt;p&gt;Blaze is a project being built with the goal of addressing these
limitations, and becoming a foundation to grow Python's success in
array-oriented computing long into the future. It consists of a small
collection of libraries being built to generalize NumPy's notions of
array, dtype, and ufuncs to be more extensible, and to represent data
and computation that is distributed or does not fit in main memory.&lt;/p&gt;
&lt;p&gt;Datashape is the array type system that describes the structure of data,
including a specification of a grammar and set of basic types, and a
library for working with them. LibDyND is an in-memory array programming
library, written in C++ and exposed to Python to provide the local
representation of memory supporting the datashape array types. BLZ is a
chunked column-oriented persistence storage format for storing Blaze
data, well-suited for out of core computations. Finally, the Blaze
library ties these components together with a deferred execution graph
and execution engine, which can analyze desired computations together
with the location and size of input data, and carry out an execution
plan in memory, out of core, or in a distributed fashion as is needed.&lt;/p&gt;
</summary></entry><entry><title>SymPy Tutorial Part 1</title><link href="https://pyvideo.org/scipy-2014/sympy-tutorial-part-1.html" rel="alternate"></link><published>2014-07-09T00:00:00+00:00</published><updated>2014-07-09T00:00:00+00:00</updated><author><name>Aaron Meurer</name></author><id>tag:pyvideo.org,2014-07-09:scipy-2014/sympy-tutorial-part-1.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;SymPy is a pure Python library for symbolic mathematics. It aims to
become a full-featured computer algebra system (CAS) while keeping the
code as simple as possible in order to be comprehensible and easily
extensible. SymPy is written entirely in Python and does not require any
external libraries.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this tutorial we will introduce attendees to SymPy. We will show
basics of constructing and manipulating mathematical expressions in
SymPy, the most common issues and differences from other computer
algebra systems, and how to deal with them. In the last part of this
tutorial we will show how to solve some practical problems with SymPy.
This will include showing how to interface SymPy with popular numeric
libraries like NumPy.&lt;/p&gt;
&lt;p&gt;This knowledge should be enough for attendees to start using SymPy for
solving mathematical problems and hacking SymPy's internals (though
hacking core modules may require additional expertise).&lt;/p&gt;
</summary><category term="SymPy"></category><category term="tutorial"></category></entry><entry><title>SymPy Tutorial Part 2</title><link href="https://pyvideo.org/scipy-2014/sympy-tutorial-part-2.html" rel="alternate"></link><published>2014-07-09T00:00:00+00:00</published><updated>2014-07-09T00:00:00+00:00</updated><author><name>Aaron Meurer</name></author><id>tag:pyvideo.org,2014-07-09:scipy-2014/sympy-tutorial-part-2.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;SymPy is a pure Python library for symbolic mathematics. It aims to
become a full-featured computer algebra system (CAS) while keeping the
code as simple as possible in order to be comprehensible and easily
extensible. SymPy is written entirely in Python and does not require any
external libraries.&lt;/p&gt;
</summary><category term="SymPy"></category><category term="tutorial"></category></entry><entry><title>SymPy Tutorial Part 3</title><link href="https://pyvideo.org/scipy-2014/sympy-tutorial-part-3.html" rel="alternate"></link><published>2014-07-09T00:00:00+00:00</published><updated>2014-07-09T00:00:00+00:00</updated><author><name>Aaron Meurer</name></author><id>tag:pyvideo.org,2014-07-09:scipy-2014/sympy-tutorial-part-3.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;SymPy is a pure Python library for symbolic mathematics. It aims to
become a full-featured computer algebra system (CAS) while keeping the
code as simple as possible in order to be comprehensible and easily
extensible. SymPy is written entirely in Python and does not require any
external libraries.&lt;/p&gt;
</summary><category term="SymPy"></category><category term="tutorial"></category></entry><entry><title>SymPy Tutorial Part 4</title><link href="https://pyvideo.org/scipy-2014/sympy-tutorial-part-4.html" rel="alternate"></link><published>2014-07-09T00:00:00+00:00</published><updated>2014-07-09T00:00:00+00:00</updated><author><name>Aaron Meurer</name></author><id>tag:pyvideo.org,2014-07-09:scipy-2014/sympy-tutorial-part-4.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;SymPy is a pure Python library for symbolic mathematics. It aims to
become a full-featured computer algebra system (CAS) while keeping the
code as simple as possible in order to be comprehensible and easily
extensible. SymPy is written entirely in Python and does not require any
external libraries.&lt;/p&gt;
</summary><category term="SymPy"></category><category term="tutorial"></category></entry><entry><title>Taking Control: Enabling Mathematicians and Scientists</title><link href="https://pyvideo.org/scipy-2014/taking-control-enabling-mathematicians-and-scien.html" rel="alternate"></link><published>2014-07-09T00:00:00+00:00</published><updated>2014-07-09T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2014-07-09:scipy-2014/taking-control-enabling-mathematicians-and-scien.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Good solutions to hard problems require both domain and algorithmic
expertise. Domain experts know what to do and computer scientists know
how to do it well. This talk discusses challenges and experiences trying
to reconcile these two groups, particularly within SymPy. It proposes
concrete approaches including multiple dispatch, pattern matching, and
programmatic strategies.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Good solutions to hard problems require both domain and algorithmic
expertise. Domain experts know &lt;em&gt;what&lt;/em&gt; to do and computer scientists know
&lt;em&gt;how&lt;/em&gt; to do it well. Coordination between the algorithmic and domain
programmer is challenging to do well and difficult to scale. It is also
arguably one of the most relevant blocks to scientific progress today.&lt;/p&gt;
&lt;p&gt;This talk draws from experience supporting mathematical programmers in
the SymPy project. SymPy is a computer algebra system, a complex problem
that requires the graph manipulation algorithms of a modern compiler
alongside the mathematics of several PhD theses. SymPy draws from a
broad developer base with experienced and novice developers alike and so
struggles to maintain a cohesive organized codebase.&lt;/p&gt;
&lt;p&gt;We approach this development problem by separating software engineering
into a collection of small functions, written by domain experts,
alongside an abstract control system, written by algorithmic
programmers. We facilitate this division with techniques taken from
other languages and compiler technologies. Notably we motivate the use
of a few general purpose libraries for multiple dispatch, pattern
matching, and programmatic control.&lt;/p&gt;
</summary></entry></feed>