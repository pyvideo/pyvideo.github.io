<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_data-science.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-12-07T00:00:00+00:00</updated><entry><title>What is causal inference, and why should data scientists know?</title><link href="https://pyvideo.org/pycon-se-2019/what-is-causal-inference-and-why-should-data-scientists-know.html" rel="alternate"></link><published>2019-10-31T00:00:00+00:00</published><updated>2019-10-31T00:00:00+00:00</updated><author><name>Ludvig Hult</name></author><id>tag:pyvideo.org,2019-10-31:pycon-se-2019/what-is-causal-inference-and-why-should-data-scientists-know.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With an explosion of computation power and modern algorithms more and more people are interested in AI, Analytics and Data Science. The Python ecosystem has been one of the most important driver for developing new tools and Python holds the power of modern analytics, but knowing the tools is not enough. Drawing conclusions from data is easy; getting the right conclusions is hard. Causal Inference is the art of drawing robust conclusions from nonexperimental data. This session will be an introduction to the field.&lt;/p&gt;
</summary><category term="data science"></category></entry><entry><title>Addressing the Scarcity of Data Scientist</title><link href="https://pyvideo.org/pydata-austin-2019/addressing-the-scarcity-of-data-scientist.html" rel="alternate"></link><published>2019-12-07T00:00:00+00:00</published><updated>2019-12-07T00:00:00+00:00</updated><author><name>Joe Gartner</name></author><id>tag:pyvideo.org,2019-12-07:pydata-austin-2019/addressing-the-scarcity-of-data-scientist.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many individuals who have the job title data scientist have Ph.D.s in quantitative research such as Physics, Computer Science, and Statistics, however, the demand for analytic talent is much larger than industries ability to keep up. This talk addresses the role of immersive programs in filling the skills gap.&lt;/p&gt;
</summary><category term="Data-Scientist"></category><category term="Data Science"></category></entry><entry><title>Develop Data Science for Business Value: a Phoenix Story</title><link href="https://pyvideo.org/pydata-austin-2019/develop-data-science-for-business-value-a-phoenix-story.html" rel="alternate"></link><published>2019-12-07T00:00:00+00:00</published><updated>2019-12-07T00:00:00+00:00</updated><author><name>Morgan Cundiff</name></author><id>tag:pyvideo.org,2019-12-07:pydata-austin-2019/develop-data-science-for-business-value-a-phoenix-story.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The goal is that our Data Science projects should add value to our business. If they never make it to production or never find users this can’t happen. Projects often get blocked due to lack of commitment from other teams in the org. This talk will step through a computer vision project development and focus on how to keep engagement and excitement.&lt;/p&gt;
</summary><category term="Data Science"></category></entry><entry><title>Productionalizing a Data Science Team</title><link href="https://pyvideo.org/pydata-austin-2019/productionalizing-a-data-science-team.html" rel="alternate"></link><published>2019-12-07T00:00:00+00:00</published><updated>2019-12-07T00:00:00+00:00</updated><author><name>Nicole Carlson</name></author><id>tag:pyvideo.org,2019-12-07:pydata-austin-2019/productionalizing-a-data-science-team.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many data scientists who are used to working independently struggle when transitioning to teams. This talk is about best practices for processes on data science teams, taken from two teams I've worked on. Some topics I’ll cover are: tracking your work, organizing your code, and deploying models. These tips are meant to be utilized by any teammate including managers and individual contributors.&lt;/p&gt;
</summary><category term="Data Science"></category><category term="teamwork"></category></entry><entry><title>What the...? Data Science Questions Asked and Answered</title><link href="https://pyvideo.org/pydata-austin-2019/what-the-data-science-questions-asked-and-answered.html" rel="alternate"></link><published>2019-12-07T00:00:00+00:00</published><updated>2019-12-07T00:00:00+00:00</updated><author><name>Katrina Riehl</name></author><id>tag:pyvideo.org,2019-12-07:pydata-austin-2019/what-the-data-science-questions-asked-and-answered.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will describe the typical and not-so-typical questions around data science tools, open source, privacy, security, communication, testing, deployment, and anything else that might come up while trying to blaze the machine learning trail.&lt;/p&gt;
</summary><category term="machine learning"></category><category term="Data Science"></category></entry><entry><title>Data science in Python</title><link href="https://pyvideo.org/python-frederick/data-science-in-python.html" rel="alternate"></link><published>2017-12-14T00:00:00+00:00</published><updated>2017-12-14T00:00:00+00:00</updated><author><name>Christine Lee</name></author><id>tag:pyvideo.org,2017-12-14:python-frederick/data-science-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At the December 2017 Python Frederick meetup, Christine Lee provided an excellent overview of the Python data science ecosystem. There's a lot of info so the topics are listed below with their times.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Anaconda - &lt;a class="reference external" href="https://youtu.be/3kTOLVD0ZCg?t=236"&gt;https://youtu.be/3kTOLVD0ZCg?t=236&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NumPy - &lt;a class="reference external" href="https://youtu.be/3kTOLVD0ZCg?t=726"&gt;https://youtu.be/3kTOLVD0ZCg?t=726&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SymPy - &lt;a class="reference external" href="https://youtu.be/3kTOLVD0ZCg?t=1204"&gt;https://youtu.be/3kTOLVD0ZCg?t=1204&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SciPy - &lt;a class="reference external" href="https://youtu.be/3kTOLVD0ZCg?t=1827"&gt;https://youtu.be/3kTOLVD0ZCg?t=1827&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;scikit-learn - &lt;a class="reference external" href="https://youtu.be/3kTOLVD0ZCg?t=2432"&gt;https://youtu.be/3kTOLVD0ZCg?t=2432&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pandas - &lt;a class="reference external" href="https://youtu.be/3kTOLVD0ZCg?t=3385"&gt;https://youtu.be/3kTOLVD0ZCg?t=3385&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;matplotlib - &lt;a class="reference external" href="https://youtu.be/3kTOLVD0ZCg?t=3953"&gt;https://youtu.be/3kTOLVD0ZCg?t=3953&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bokey and Plotly - &lt;a class="reference external" href="https://youtu.be/3kTOLVD0ZCg?t=4458"&gt;https://youtu.be/3kTOLVD0ZCg?t=4458&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jupyter Notebooks - &lt;a class="reference external" href="https://youtu.be/3kTOLVD0ZCg?t=5520"&gt;https://youtu.be/3kTOLVD0ZCg?t=5520&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="data science"></category></entry><entry><title>Audio Classification with Machine Learning</title><link href="https://pyvideo.org/europython-2019/audio-classification-with-machine-learning.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Jon Nordby</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/audio-classification-with-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Sound is a rich source of information about the world around us.&lt;/div&gt;
&lt;div class="line"&gt;Modern deep learning approaches can give human-like performance on a
range of sound classifiction tasks.&lt;/div&gt;
&lt;div class="line"&gt;This makes it possible to build systems that use sound to for example:&lt;/div&gt;
&lt;div class="line"&gt;understand speech, to analyze music, to assist in medical diagnostics,
detect quality problems in manufacturing, and to study the behavior of
animals.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;This talk will show you how to build practical machine learning models
that can classify sound.&lt;/div&gt;
&lt;div class="line"&gt;We will convert sound into spectrograms, a visual representation of
sound over time,&lt;/div&gt;
&lt;div class="line"&gt;and apply machine learning models similar to what is used to for image
classification.&lt;/div&gt;
&lt;div class="line"&gt;The focus will be on Convolutional Neural Networks, which have been
shown to work very well for this task.&lt;/div&gt;
&lt;div class="line"&gt;The Keras and Tensorflow deep learning frameworks will be used. Some
tricks for getting usable results with small amounts of data will be
covered, including transfer learning, audio embeddings and data
augmentation.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;A basic understanding of machine learning is recommended.&lt;/div&gt;
&lt;div class="line"&gt;Familiarity with digital sound is a bonus.&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category></entry><entry><title>Building Data-Driven Client Relationship Management in Banking with Python</title><link href="https://pyvideo.org/europython-2019/building-data-driven-client-relationship-management-in-banking-with-python.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Paul Hughes</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/building-data-driven-client-relationship-management-in-banking-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This is a case study that documents how a small data science team in a
big bank took on the challenge to transform a fragmented sales process
into a data-driven one using Python and machine learning.&lt;/p&gt;
&lt;p&gt;This talk outlines the various ways Python has been instrumental in
delivering a production solution that serves advisers and relationship
manager on a continuous basis.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;The Challenge&lt;/div&gt;
&lt;div class="line"&gt;- A bank has many clients with diverse needs and cost pressures mean
fewer advisers resulting in reduced client coverage.&lt;/div&gt;
&lt;div class="line"&gt;- Multiple sales channels and mixed service levels meant sales
processes were uncoordinated and driven by heuristics and often very
subjective.&lt;/div&gt;
&lt;div class="line"&gt;- And... Excel sheets everywhere!&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Solution&lt;/div&gt;
&lt;div class="line"&gt;- Go data-driven!&lt;/div&gt;
&lt;div class="line"&gt;- Learn from clients and understand product usage&lt;/div&gt;
&lt;div class="line"&gt;- Empower and inform advisers and call centre agents&lt;/div&gt;
&lt;div class="line"&gt;- Build a front-to-back sales process (no more Excels!)&lt;/div&gt;
&lt;div class="line"&gt;- How? With Python!&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;The Python Bits&lt;/div&gt;
&lt;div class="line"&gt;- Scikit learn machine learning pipelines that implement two distinct
approaches to product affinity in banking and wealth management&lt;/div&gt;
&lt;div class="line"&gt;- SQL Alchemy based API for data engineering and rapid prototyping of
analytics&lt;/div&gt;
&lt;div class="line"&gt;- Pandas and Jupyter for development and collaboration&lt;/div&gt;
&lt;div class="line"&gt;- Luigi pipeline for daily processing of millions of transactions and
engineering features&lt;/div&gt;
&lt;div class="line"&gt;- Extracting features from text with NLP (Spacy)&lt;/div&gt;
&lt;div class="line"&gt;- Delivering machine learning interpretability in production, e.g.
with Random Forests and treeinterpreter&lt;/div&gt;
&lt;div class="line"&gt;- A Python module that we built with all the reusable bits: building
training and prediction datasets, developing pipelines, generating
monitoring data and enabling explainability&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Business Cases"></category><category term="Data Science"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category><category term="Windows"></category></entry><entry><title>Deep Learning with TensorFlow 2.0</title><link href="https://pyvideo.org/europython-2019/deep-learning-with-tensorflow-20.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Brad Miro</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/deep-learning-with-tensorflow-20.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Learn about the updates being made to TensorFlow in its 2.0 version.
We’ll give an overview of what’s available in the new version as well as
do a deep dive into an example using its central high-level API, Keras.
You’ll walk away with a better understanding of how you can get started
building machine learning models in Python with TensorFlow 2.0 as well
as the other exciting available features!&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Machine learning on non curated data</title><link href="https://pyvideo.org/europython-2019/machine-learning-on-non-curated-data.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Gael Varoquaux</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/machine-learning-on-non-curated-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;According to industry surveys [1], the number one hassle of data
scientists is cleaning the data to analyze it. Textbook statistical
modeling is sufficient for noisy signals, but errors of a discrete
nature break standard tools of machine learning. I will discuss how to
easily run machine learning on data tables with two common dirty-data
problems: missing values and non-normalized entries. On both problems, I
will show how to run standard machine-learning tools such as
scikit-learn in the presence of such errors. The talk will be didactic
and will discuss simple software solutions. It will build on the latest
improvements to scikit-learn for missing values and the DirtyCat package
[2] for non normalized entries. I will also summarize theoretical
analyses in recent machine learning publications.&lt;/p&gt;
&lt;p&gt;This talk targets data practitioners. Its goal are to help data
scientists to be more efficient analysing data with such errors and
understanding their impacts.&lt;/p&gt;
&lt;p&gt;With missing values, I will use simple arguments and examples to outline
how to obtain asymptotically good predictions [3]. Two components are
key: imputation and adding an indicator of missingness. I will explain
theoretical guidelines for these, and I will show how to implement these
ideas in practice, with scikit-learn as a learner, or as a preprocesser.&lt;/p&gt;
&lt;p&gt;For non-normalized categories, I will show that using their string
representations to “vectorize” them, creating vectorial representations
gives a simple but powerful solution that can be plugged in standard
statistical analysis tools [4].&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;[1] Kaggle, the state of ML and data science 2017
&lt;a class="reference external" href="https://www.kaggle.com/surveys/2017"&gt;https://www.kaggle.com/surveys/2017&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[2] &lt;a class="reference external" href="https://dirty-cat.github.io/stable/"&gt;https://dirty-cat.github.io/stable/&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[3] Josse Julie, Prost Nicolas, Scornet Erwan, and Varoquaux Gaël
(2019). “On the consistency of supervised learning with missing
values”. &lt;a class="reference external" href="https://arxiv.org/abs/1902.06931"&gt;https://arxiv.org/abs/1902.06931&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[4] Cerda Patricio, Varoquaux Gaël, and Kégl Balázs. &amp;quot;Similarity
encoding for learning with dirty categorical variables.&amp;quot; Machine
Learning 107.8-10 (2018): 1477 &lt;a class="reference external" href="https://arxiv.org/abs/1806.00979"&gt;https://arxiv.org/abs/1806.00979&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Big Data"></category><category term="Data"></category><category term="Data Science"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Natural language processing with neural networks.</title><link href="https://pyvideo.org/europython-2019/natural-language-processing-with-neural-networks.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Hubert Bryłkowski</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/natural-language-processing-with-neural-networks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Getting started with a natural language processing and neural networks
is easier nowadays thanks to the numerous talks and tutorials. The
goal is to dive deeper for those who already know the basics, or want
to expand their knowledge in a machine learning field.&lt;/div&gt;
&lt;div class="line"&gt;The talk will start with the common use cases that can be generalized
to the specific problems in a NLP world. Then I will present an
overview of possible features that we can use as input to our network,
and show that even simple feature engineering can change our results.&lt;/div&gt;
&lt;div class="line"&gt;Furthermore, I will compare different network architectures - starting
with the fully connected networks, through convolution neural networks
to recursive neural networks. I will not only considering the good
parts, but also - what is usually overlooked - pitfalls of every
solution.&lt;/div&gt;
&lt;div class="line"&gt;All of these will be done considering number of parameters, which
transfers into training and prediction costs and time. I will also
share a number of “tricks” that enables getting the best results even
out of the simple architectures, as these are usually the fastest and
quite often hard to beat, at the same time being the easiest to
interpret.&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category><category term="Natural Language Processing"></category></entry><entry><title>PlotVR - walk through your data</title><link href="https://pyvideo.org/europython-2019/plotvr-walk-through-your-data.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Philipp Thomann</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/plotvr-walk-through-your-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Are you bored by 3D-plots that only give you a simple rotatable
2d-projection? plotVR is an open source package that provides a simple
way for data scientists to plot data, pick up a phone, get a real 3d
impression - either by VR or by AR - and use the computer's keyboard to
walk through the scatter plot:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.github.com/thomann/plotVR"&gt;https://www.github.com/thomann/plotVR&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After installing and plotting your dataframe open your phone's browser
and use your GoogleVR Cardboard. Furthermore performant Android- and
iOS-apps are available - both support VR-Cardboard and the iOS-Version
also AR.&lt;/p&gt;
&lt;p&gt;Once you are immersed in your Cardboard how do you navigate through the
scatter? plotVR lets you use the computer's keyboard to walk as you
would in any first person game.&lt;/p&gt;
&lt;p&gt;You want to share your impression? Just save the HTML and publish it.&lt;/p&gt;
&lt;p&gt;The technologies beneath this project are: a web server that handles the
communication between the DataScience-session and the phone, WebSockets
to quickly proxy the keyboard events, QR-codes facilitate the simple
pairing of both, and an HTML-Page on the computer to grab the keyboard
events. And the translation of these keyboard events into 3D terms is a
nice exercise in three.js, OpenGL, and SceneKit for HTML, Android, and
iOS resp.&lt;/p&gt;
&lt;p&gt;Ready to see your data as you have never seen before? Join the talk!&lt;/p&gt;
</summary><category term="3D"></category><category term="Augmented Reality"></category><category term="Data Science"></category><category term="Open-Source"></category><category term="Visualization"></category></entry><entry><title>The state of Machine Learning Operations in 2019</title><link href="https://pyvideo.org/europython-2019/the-state-of-machine-learning-operations-in-2019.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Alejandro Saucedo</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/the-state-of-machine-learning-operations-in-2019.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will provide an overview of the key challenges and trends in
the productization of machine learning systems, including concepts such
as reproducibility, explainability and orchestration. The talk will also
provide a high level overview of several key open source tools and
frameworks available to tackle these issues, which have been identifyed
putting together the Awesome Machine Learning Operations list
(&lt;a class="reference external" href="https://github.com/EthicalML/awesome-machine-learning-operations"&gt;https://github.com/EthicalML/awesome-machine-learning-operations&lt;/a&gt;).&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;The key concepts that will be covered are:&lt;/div&gt;
&lt;div class="line"&gt;* Reproducibility&lt;/div&gt;
&lt;div class="line"&gt;* Explainability&lt;/div&gt;
&lt;div class="line"&gt;* Orchestration of models&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The reproducibility piece will cover key motivations as well as
practical requirements for model versioning, together with tools that
allow data scientists to achieve version control of model+config+data to
ensure full model lineage.&lt;/p&gt;
&lt;p&gt;The explainability piece will contain a high level overview of why this
has become an important topic in machine learning, including the high
profile incidents that tech companies have experienced where undesired
biases have slipped into data. This will also include a high level
overview of some of the tools available.&lt;/p&gt;
&lt;p&gt;Finally, the orchestration piece will cover some of the fundamental
challenges with large scale serving of models, together with some of the
key tools that are available to ensure this challenge can be tackled.&lt;/p&gt;
</summary><category term="Architecture"></category><category term="Data"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category></entry><entry><title>“When a biologist met Python”</title><link href="https://pyvideo.org/europython-2019/when-a-biologist-met-python.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Maria Molina-Contreras</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/when-a-biologist-met-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Biology and computing are closer than we usually think, for example
many algorithms are inspired in biology patterns, and complementary to
that, researchers needs special algorithms to have a better
understanding of our environment. Thus, there is a strong relation an
dependency.&lt;/div&gt;
&lt;div class="line"&gt;In the past years, Biology has been transformed into computational
biology. Therefore&lt;/div&gt;
&lt;div class="line"&gt;technological advances helps us to predict physical interactions
between atoms and DNA, because we are being able to integrate
information from biology into algorithms.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Python has become a popular programming language in biosciences because
it has a clean syntax that makes it easy to read language. In addition
to this, there are many modules (toolkits) extending to different
biological domains, like metabolomics, structure analysis,
phylogenomics, molecular biology and others. Python is currently
improving researcher’s workflow, helping us to focus on the theory or
experimental part, instead of fighting with old buggy applications.&lt;/p&gt;
&lt;p&gt;This talk aims to be oriented to all audiences (with/without biological
background) since we will go together through an amazing adventure into
the natural sciences using tools like Biopython, Bokeh, Networkx, Ecopy
and much more! Are you brave enough to follow me on this journey?&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Data Science"></category><category term="Natural Science"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category><category term="python"></category></entry><entry><title>Accelerate your Deep Learning Inferencing with the Intel® DL Boost technology</title><link href="https://pyvideo.org/europython-2019/accelerate-your-deep-learning-inferencing-with-the-intelr-dl-boost-technology.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Shailen Sobhee</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/accelerate-your-deep-learning-inferencing-with-the-intelr-dl-boost-technology.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Learn about Intel® Deep Learning Boost, also known as Vector Neural
Network Instructions (VNNI), a new set of AVX-512 instructions, that are
designed to deliver significantly more efficient Deep Learning
(Inference) acceleration. Through this technology, I will show you how
you can perform low-precision (INT8) inference much faster on hardware
that support the VNNI instruction set (for example, the 2nd generation
Intel Xeon Scalable processors, codenamed, Cascade Lake). In the live
Jupyter notebook session, you can will be able to see the benefits of
this new hardware technology.&lt;/p&gt;
&lt;p&gt;Note: This is an advanced talk. Knowledge about Deep Learning,
Inferencing and basic awareness of hardware instruction sets would be
desirable.&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Deep Learning"></category><category term="Performance"></category><category term="python"></category></entry><entry><title>Bioinformatics pipeline for revealing tumour heterogeneity</title><link href="https://pyvideo.org/europython-2019/bioinformatics-pipeline-for-revealing-tumour-heterogeneity.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Mustafa Anil Tuncel</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/bioinformatics-pipeline-for-revealing-tumour-heterogeneity.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Reproducibility of research is a common issue in science, especially
in computationally expensive research fields e.g. cancer research.&lt;/div&gt;
&lt;div class="line"&gt;A comprehensive picture of the genomic aberrations that occur during
tumour progression and the resulting intra-tumour heterogeneity, is
essential for personalised and precise cancer therapies. With the
change in the tumour environment under treatment, heterogeneity allows
the tumour additional ways to evolve resistance, such that
intra-tumour genomic diversity is a cause of relapse and treatment
failure. Earlier bulk sequencing technologies were incapable of
determining the diversity in the tumour.&lt;/div&gt;
&lt;div class="line"&gt;Single-cell DNA sequencing - a recent sequencing technology - offers
resolution down to the level of individual cells and is playing an
increasingly important role in this field.&lt;/div&gt;
&lt;div class="line"&gt;We present a reproducible and scalable Python data analysis pipeline
that employs a statistical model and an MCMC algorithm to infer the
evolutionary history of copy number alterations of a tumour from
single cells. The pipeline is built using Python, Conda environment
management system and the Snakemake workflow management system. The
pipeline starts from the raw sequencing files and a settings file for
parameter configurations. After running the data analysis, pipeline
produces report and figures to inform the treatment decision of the
cancer patient.&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Algorithms"></category><category term="Analytics"></category><category term="C-Languages"></category><category term="Command-Line"></category><category term="Data Science"></category></entry><entry><title>Building a Powerful Pet Detector in Notebooks</title><link href="https://pyvideo.org/europython-2019/building-a-powerful-pet-detector-in-notebooks.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Katherine Kampf</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/building-a-powerful-pet-detector-in-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Ever wondered what breed that dog or cat is? Let’s build a pet detector
service to recognize them in pictures! In this talk, we will walk
through the training, optimizing, and deploying of a deep learning model
using Azure Notebooks. We will use transfer learning to retrain a
MobileNet model using TensorFlow to recognize dog and cat breeds using
the Oxford IIIT Pet Dataset. Next, we’ll optimize the model and tune our
hyperparameters to improve the model accuracy. Finally, we will deploy
the model as a web service in. Come to learn how you can quickly create
accurate image recognition models with a few simple techniques!&lt;/p&gt;
</summary><category term="Data"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Jupyter"></category><category term="Machine-Learning"></category></entry><entry><title>Dash: Interactive Data Visualization Web Apps with no Javascript</title><link href="https://pyvideo.org/europython-2019/dash-interactive-data-visualization-web-apps-with-no-javascript.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Dom Weldon</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/dash-interactive-data-visualization-web-apps-with-no-javascript.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Your data science or machine learning project probably won't just
produce a written report. Instead, projects are increasingly expected to
produce interactive tools to allow end-users to explore data and results
with rich, interactive visualizations. Inevitably, this will be done in
a web browser, meaning you'll need to add a quantitatively trained web
developer to your team, or have your data scientists spend time learning
HTML, Javascript and CSS. Dash, a project by the team that makes Plotly,
solves some of these problems by allowing data scientists to build rich
and interactive websites in pure python, with minimal knowledge of HTML
and absolutely no Javascript.&lt;/p&gt;
&lt;p&gt;At decisionLab, a London-based data science consultancy producing
decision tools, we've embraced Dash to produce proof-of-concept models
for our projects in alpha. Although we're not officially connected to
the plotly/Dash project, by using the library daily across many
projects, we've learned many lessons and what we feel are best practises
we'd like to share, and hear feedback on!&lt;/p&gt;
&lt;p&gt;This talk will give an overview of Dash, how it works and what it can be
used for, before outlining some of the common problems that emerge when
data scientists are let loose to produce web applications, and web
developers have to work with the pydata ecosystem. The talk also covers
effective working practises to start producing cool interactive
statistical web applications, fast. We'll also identify some of the
pitfalls of Dash, and how and when to make the decision to stop using
Dash and start building a proper web application.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://domweldon-europython-2019-dash.s3.eu"&gt;http://domweldon-europython-2019-dash.s3.eu&lt;/a&gt;-
west-2.amazonaws.com/index.html&lt;/p&gt;
</summary><category term="Data Science"></category><category term="JavaScript"></category><category term="Visualization"></category><category term="Web"></category><category term="Web Servers and MicroFWs"></category></entry><entry><title>Do we have a diversity problem in Python community?</title><link href="https://pyvideo.org/europython-2019/do-we-have-a-diversity-problem-in-python-community.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Cheuk Ho</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/do-we-have-a-diversity-problem-in-python-community.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The diversity statement quoted as follows: “The Python Software
Foundation and the global Python community welcome and encourage
participation by everyone. Our community is based on mutual respect,
tolerance, and encouragement, and we are working to help each other live
up to these principles. We want our community to be more diverse:
whoever you are, and whatever your background, we welcome you.”&lt;/p&gt;
&lt;p&gt;Diversity, big deal! As an active members and event organisers (and also
on the minority side of the gender) in the Python community, we have
alway been concern by the question of: Do we truly have a problem in
diversity? Especially, gender diversity. We would like to find out the
truth, by data science, and see if we can find a clue why and how we can
fix it.&lt;/p&gt;
&lt;p&gt;First, we will show the research others did regarding the representation
of women in the R and Python communities [1]. Then, we will show the
research that we did based on our experience and statistic. Including
static analysis of the speakers diversity (regarding gender) at major
PyCon and PyData conferences. Finally, as we all care about diversity
and want improvements, we would like to find out the reason and what we
can do about it. We would propose what we, the minorities and allies,
could do against this seemingly unbalance situation and make the
community better.&lt;/p&gt;
&lt;p&gt;This talk is for all that who cares about diversity in our community.&lt;/p&gt;
&lt;p&gt;[1]
&lt;a class="reference external" href="https://reshamas.github.io/why-women-are-flourishing-in-r-community-but"&gt;https://reshamas.github.io/why-women-are-flourishing-in-r-community-but&lt;/a&gt;-
lagging-in-python/&lt;/p&gt;
&lt;p&gt;Update: slides at
&lt;a class="reference external" href="https://slides.com/cheukting_ho/do-we-have-a-diversity"&gt;https://slides.com/cheukting_ho/do-we-have-a-diversity&lt;/a&gt;-
problem-in-python-community&lt;/p&gt;
</summary><category term="Community"></category><category term="Conferences and Meet-Ups"></category><category term="Data Science"></category><category term="Static Analysis"></category></entry><entry><title>Image processing with scikit-image and Dash</title><link href="https://pyvideo.org/europython-2019/image-processing-with-scikit-image-and-dash.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Emmanuelle Gouillart</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/image-processing-with-scikit-image-and-dash.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Images are an ubiquitous form of data in various fields of science and&lt;/div&gt;
&lt;div class="line"&gt;industry. Images often need to be transformed and processed, for
example for helping medical diagnosis by extracting regions of
interest or measures, or for building training sets for machine
learning.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;In this talk, I will present and discuss several tools for automatic
and&lt;/div&gt;
&lt;div class="line"&gt;interactive image processing with Python. I will start by a short&lt;/div&gt;
&lt;div class="line"&gt;introduction to scikit-image (&lt;a class="reference external" href="https://scikit-image.org/"&gt;https://scikit-image.org/&lt;/a&gt;), the
open-source&lt;/div&gt;
&lt;div class="line"&gt;image processing toolkit of the Pydata ecosystem, which aims at&lt;/div&gt;
&lt;div class="line"&gt;processing images from a large class of modalities (2-D, 3-D, etc.)
and&lt;/div&gt;
&lt;div class="line"&gt;strives to have a gentle learning curve with pedagogical example-based&lt;/div&gt;
&lt;div class="line"&gt;documentation. scikit-image provides users with a simple API based on
a large number of functions, which can be used to build pipelines of
image processing workflows.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;In a second part, I will explain how to use Dash for building
interactive&lt;/div&gt;
&lt;div class="line"&gt;image processing operations. Dash (&lt;a class="reference external" href="https://dash.plot.ly/"&gt;https://dash.plot.ly/&lt;/a&gt;) is an&lt;/div&gt;
&lt;div class="line"&gt;open-source Python web application framework developed by Plotly.
Written on top of Flask, Plotly.js, and React.js, Dash is meant for
building data visualization apps with highly custom user interfaces in
pure Python. The dash-canvas component library of Dash
(&lt;a class="reference external" href="https://dash.plot.ly/canvas"&gt;https://dash.plot.ly/canvas&lt;/a&gt;) is an interactive component for
annotating images with several tools (freehand brush, lines, bounding
boxes, ...). It also provides utility functions for using
user-provided annotations for several image processing tasks such as
segmentation, transformation, measures, etc. The latter functions are
based on libraries such scikit-image and openCV. A gallery of examples
showcases some typical uses of Dash for image processing on&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://dash-canvas.plotly.host/"&gt;https://dash-canvas.plotly.host/&lt;/a&gt;. Also, other components of Dash can
be leveraged easily to build powerful image processing applications,
such as widgets to tune parameters or data tables for inspecting
object&lt;/div&gt;
&lt;div class="line"&gt;properties.&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Computer Vision"></category><category term="Data Science"></category><category term="Image Processing"></category><category term="JavaScript Web Frameworks (AngularJS/ReactJS/...)"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Opt Out of Online Sexism – Open Source Activism</title><link href="https://pyvideo.org/europython-2019/opt-out-of-online-sexism-open-source-activism.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Teresa Ingram</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/opt-out-of-online-sexism-open-source-activism.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Although people of all genders can experience violence and abuse
online, the abuse experienced by women is often sexist or misogynistic
in nature, and online threats of violence against women are often
sexualized and include specific references to women’s bodies. &amp;quot; -
Amnesty International. This abuse pushes women offline, affecting their
social well-being, representation and economic potential.&lt;/p&gt;
&lt;p&gt;In this talk I will discuss how we plan to help resolve this with our
browser extension, Opt Out. I will discuss the online global tragedy
that is online sexual harassment, our idea and where we’re at with
current implementation. I will also talk about what it’s like to build
an open source activism project, one which aims to be lead by the
community it’s trying to protect.&lt;/p&gt;
&lt;p&gt;We will cover current research and results from our own engagement with
the community, where the idea came from and challenges we have faced and
plan to face in the future. I will also dive into the intricate world of
natural language processing (NLP) for online harassment and talk about
balancing state-of-the-art data science with web development in an open
source community, one being managed by someone relatively new to tech.&lt;/p&gt;
</summary><category term="Communication"></category><category term="Data Science"></category><category term="Open-Source"></category><category term="TDD"></category><category term="Web"></category></entry><entry><title>Supercharge your Deep Learning algorithms with optimized software</title><link href="https://pyvideo.org/europython-2019/supercharge-your-deep-learning-algorithms-with-optimized-software.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Shailen Sobhee</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/supercharge-your-deep-learning-algorithms-with-optimized-software.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, you will learn various optimization techniques to improve
the runtime performance of your deep learning algorithms on Intel
architecture. The presentation will cover how to accelerate the training
of your deep neural networks with Tensorflow thanks to the highly
optimized Intel® Math Kernel Library (Intel® MKL) and how we boost
inferencing with Intel® nGraph and with the Intel® Distribution of
OpenVINO™.&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Deep Learning"></category><category term="Open-Source"></category></entry><entry><title>What about recommendation engines?</title><link href="https://pyvideo.org/europython-2019/what-about-recommendation-engines.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Adriana Dorneles</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/what-about-recommendation-engines.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;How recommendation engines are taking part in our daily routine and
how companies as Netflix and Amazon implement it?&lt;/div&gt;
&lt;div class="line"&gt;This talk aims to show the elements that compound a recommendation
engine to people who have never been in touch with the matter or want
to know a bit more. At the end of this session, you might be able to
reproduce your own recommendation system and also know where to find
more about it.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Talk structure:&lt;/div&gt;
&lt;div class="line"&gt;1. What is and why use a recommendation engine?&lt;/div&gt;
&lt;div class="line"&gt;2. Recommendation engine importance&lt;/div&gt;
&lt;div class="line"&gt;3. Steps of a recommendation&lt;/div&gt;
&lt;div class="line"&gt;4. Recommendation algorithms&lt;/div&gt;
&lt;div class="line"&gt;5. Basic Statistics for distance and correlation&lt;/div&gt;
&lt;div class="line"&gt;6. Example&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Business"></category><category term="Data Science"></category><category term="Python 3"></category></entry><entry><title>Predicting Human Activity using Time Series Analysis</title><link href="https://pyvideo.org/pycon-italia-2019/predicting-human-activity-using-time-series-analysis.html" rel="alternate"></link><published>2019-05-04T00:00:00+00:00</published><updated>2019-05-04T00:00:00+00:00</updated><author><name>Akul Mehra</name></author><id>tag:pyvideo.org,2019-05-04:pycon-italia-2019/predicting-human-activity-using-time-series-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Time series is referred to as sequence of data points measured over
successive intervals of time. Time series analysis is performed to
extract meaningful statistics and characteristics of the data. For
example, this analysis can be used to predict future based on the past
events. Such use cases of this analysis are stock market prediction,
weather prediction and web traffic prediction.&lt;/p&gt;
&lt;p&gt;In this talk, we’ll be understanding the basics and how to perform time
series analysis and machine learning for a given dataset. For this talk,
we’ll be using the UCI Human Activity Recognition (HAR) Data Set to
classify a given activity performed by the human as one of the following
activities: Walking, Sitting, Standing or Laying.&lt;/p&gt;
&lt;p&gt;The talk would be intermediate level and basics of Python and Pandas are
required. Understanding of Machine Learning algorithms and signal
processing would be helpful.&lt;/p&gt;
&lt;p&gt;Feedback form: &lt;a class="reference external" href="https://python.it/feedback-1576"&gt;https://python.it/feedback-1576&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Saturday 4 May&lt;/strong&gt; at 18:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="TimeSeriesAnalysis"></category><category term="data-science"></category><category term="machine-language"></category><category term="Python"></category><category term="data-exploration"></category><category term="Data-Scientist"></category><category term="MachineLearning"></category><category term="TimeSeries"></category><category term="pandas"></category><category term="DataEngineering"></category></entry><entry><title>Open Sourcing at Work</title><link href="https://pyvideo.org/pycon-ca-2018/open-sourcing-at-work.html" rel="alternate"></link><published>2018-11-11T00:00:00+00:00</published><updated>2018-11-11T00:00:00+00:00</updated><author><name>Faisal Dosani</name></author><id>tag:pyvideo.org,2018-11-11:pycon-ca-2018/open-sourcing-at-work.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We just open sourced 2 projects (datacompy, and locopy) with roots in Data Science and Engineering which we will showcase. While is it exciting and rewarding to share your ideas with the world it isn't always easy. Thinking about licenses, copyrights, and protecting confidential information is a must!&lt;/p&gt;
&lt;p&gt;Working in a large organization which is embracing the mantra 'open source first' is really exciting. Part of this journey is to make sure we give back to the open source community when we can. Two of our projects had gained traction internally: datacompy, and locopy. As part of our commitment we wanted to make sure we could open source these projects for others to use and contribute back to. DataComPy is a package to compare two Pandas DataFrames. Originally started to be something of a replacement for SAS's PROC COMPARE for Pandas DataFrames with some more functionality than just Pandas.DataFrame.equals(Pandas.DataFrame) (in that it prints out some stats, and lets you tweak how accurate matches have to be). Then extended to carry that functionality over to Spark Dataframes. Locopy helps load flat files to S3 and then to Amazon Redshift, and assist with ETL processing. It is DB Driver (Adapter) agnostic, provides basic functionality to move data to S3 buckets, execute COPY commands to load data to S3, and into Redshift, and UNLOAD commands to unload data from Redshift into S3. While building these products was exciting and fun, some of the legal considerations were as interesting, complex, and required collaboration between many teams, from security, licensing, brand, and IP/copyright. We'll explore the projects, and some of these other considerations which can make or break if you decide to release a project into the wild, along with the road blocks we faced with in these areas.&lt;/p&gt;
</summary><category term="open source"></category><category term="licensing"></category><category term="copyright"></category><category term="data"></category><category term="security"></category><category term="testing"></category><category term="best practices"></category><category term="data science"></category></entry><entry><title>Data Science and Machine Learning with Python</title><link href="https://pyvideo.org/pycon-philippines-2019/data-science-and-machine-learning-with-python.html" rel="alternate"></link><published>2019-02-23T00:00:00+00:00</published><updated>2019-02-23T00:00:00+00:00</updated><author><name>Argeo Tuble Alecha</name></author><id>tag:pyvideo.org,2019-02-23:pycon-philippines-2019/data-science-and-machine-learning-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This is a combination of slides and interactive presentation/demo in coding and notebook operations. Presenting Python packages and tools for Data Science and Machine Learning like Numpy, Pandas, ScikitLearn, Matplotlib, etc.&lt;/p&gt;
</summary><category term="data science"></category><category term="machine learning"></category></entry><entry><title>An Idiots Guide to (Open) Data Science</title><link href="https://pyvideo.org/pycon-ireland-2018/an-idiots-guide-to-open-data-science.html" rel="alternate"></link><published>2018-11-10T00:00:00+00:00</published><updated>2018-11-10T00:00:00+00:00</updated><author><name>Andrew Bolster</name></author><id>tag:pyvideo.org,2018-11-10:pycon-ireland-2018/an-idiots-guide-to-open-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Setup, configuration, and use of Python Data Science tools, highlighting some of the technical pitfalls / statistical failings people often come across in the cleaning and analysis of data. Focus on using multiple datasets from OpenDataNI to generate insights into economic policy and educational attainment in Northern Ireland.&lt;/p&gt;
</summary><category term="data-science"></category><category term="opendata"></category></entry><entry><title>Functional Programming for Data Science</title><link href="https://pyvideo.org/pycon-ireland-2018/functional-programming-for-data-science.html" rel="alternate"></link><published>2018-11-10T00:00:00+00:00</published><updated>2018-11-10T00:00:00+00:00</updated><author><name>Neal Ó Riain</name></author><id>tag:pyvideo.org,2018-11-10:pycon-ireland-2018/functional-programming-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python is a versatile language and it supports a wide variety of programming paradigms. At its heart it's object-oriented, but in this talk I want to discuss how you can use Python to write clean, efficient, and modular functional code. I'll begin by giving a little background on what functional programming is and why you might use it. I'll talk through some of the simple primitives of functional programming, and I'll give some useful examples of functional code for data analysis. The aim is to give a practical and pragmatic introduction to these ideas, covering some of the strengths and weaknesses of Python as a functional language.&lt;/p&gt;
</summary><category term="functional programming"></category><category term="data science"></category></entry><entry><title>Build text classification models ( CBOW and Skip-gram) with FastText in python</title><link href="https://pyvideo.org/pycon-de-2018/build-text-classification-models-cbow-and-skip-gram-with-fasttext-in-python.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Kajal Puri</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/build-text-classification-models-cbow-and-skip-gram-with-fasttext-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;FastText has been open-sourced by Facebook in 2016 and with its release,
it became the fastest and most accurate library in Python for text
classification and word representation. It is to be seen as a substitute
for gensim package's word2vec. It includes the implementation of two
extremely important methodologies in NLP i.e Continuous Bag of Words and
Skip-gram model. Fasttext performs exceptionally well with supervised as
well as unsupervised learning.&lt;/p&gt;
&lt;p&gt;The tutorial will be divided in following four segments :&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="NLP"></category><category term="Machine Learning"></category></entry><entry><title>Enabling the chip technologies of tomorrow – how Python helps us</title><link href="https://pyvideo.org/pycon-de-2018/enabling-the-chip-technologies-of-tomorrow-how-python-helps-us.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Tim Hoffmann</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/enabling-the-chip-technologies-of-tomorrow-how-python-helps-us.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Carl Zeiss SMT GmbH is the leading manufacturer of lithography optics.
Our optics allow chipmakers to produce smaller, faster and more energy
efficient computer chips. As we move to smaller and smaller structures,
the necessary optics grow more and more complex. Customized simulations
and data analytics by highly qualified technical domain experts are
essential. These people are not experienced software developers.
However, with Python and the right support, we can give them powerful
tools to accomplish their task efficiently.&lt;/p&gt;
&lt;p&gt;Pioneering Python in a larger enterprise can be challenging. At present,
we use Python in selected areas of our product development and
production processes. We'd like to share our challenges and solutions
with using Python in a heterogeneous company environment. In particular,
how can we make Python accessible to non-programmers? How do we ensure
consistent development? How do we embed in the non-Python ecosystem of
the company?&lt;/p&gt;
</summary><category term="Data Science"></category></entry><entry><title>From exploration to deployment - combining PyTorch and TensorFlow for Deep Learning</title><link href="https://pyvideo.org/pycon-de-2018/from-exploration-to-deployment-combining-pytorch-and-tensorflow-for-deep-learning.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Marcel Kurovski</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/from-exploration-to-deployment-combining-pytorch-and-tensorflow-for-deep-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Despite the many deep learning frameworks out in the wild few have
achieved widespread adoption. Two of them are TensorFlow and PyTorch.
Where PyTorch relies on a dynamic computation graph TensorFlow goes for
a static graph. Where TensorFlow shows greater adoption and additional
useful extensions with TensorFlow Serving and TensorBoard, Pytorch
proves useful trough its easy and more pythonic API.&lt;/p&gt;
&lt;p&gt;Data scientists are confronted with explorative challenges, but also
need to be aware of model deployment and production. Do we need to
single out frameworks until we end up with the only one or is there a
case for joint usage of two deep learning frameworks? Can we leverage
the strengths of the frameworks for different tasks along the path from
exploration to production?&lt;/p&gt;
&lt;p&gt;In my talk, I want to present a case combining the benefits of PyTorch
and TensorFlow using the first for explorative and latter for deployment
tasks. Therefore, I will choose a common deep learning challenge and
discuss the strengths and weaknesses of both frameworks along a demo
that brings a model from development into production.&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="Machine Learning"></category></entry><entry><title>Prototyping to tested code</title><link href="https://pyvideo.org/pycon-de-2018/prototyping-to-tested-code.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Christopher Prohm</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/prototyping-to-tested-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter notebooks are a great environment to prototype solutions and
explore their design. Turning these solutions into reusable components
usually requires moving them out of the notebook environment into
external python packages. Often, at this stage, the code is refactored
and test are written.&lt;/p&gt;
&lt;p&gt;In this talk, I will demo
&lt;tt class="docutils literal"&gt;`ipytest&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="https://github.com/chmp/ipytest"&gt;https://github.com/chmp/ipytest&lt;/a&gt;&amp;gt;`__, a small tool to run
tests inside notebooks. It supports &lt;tt class="docutils literal"&gt;`pytest&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="http://pytest.org/"&gt;http://pytest.org/&lt;/a&gt;&amp;gt;`__
as well as the standard
&lt;tt class="docutils literal"&gt;`unittest&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="https://docs.python.org/3/library/unittest.html"&gt;https://docs.python.org/3/library/unittest.html&lt;/a&gt;&amp;gt;`__
framework. It allows to start prototypes in a notebook and to develop
the tests with the code in an highly interactive environment. As the
code grows, it can be transparently moved outside notebooks and
transformed into reusable components. By bringing support for tests to
the notebook environment,
&lt;tt class="docutils literal"&gt;`ipytest&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="https://github.com/chmp/ipytest"&gt;https://github.com/chmp/ipytest&lt;/a&gt;&amp;gt;`__ bridges the artificial
gap between notebooks and reusable components.&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Machine Learning"></category></entry><entry><title>Satellite data is for everyone: insights into modern remote sensing research with open data and Python</title><link href="https://pyvideo.org/pycon-de-2018/satellite-data-is-for-everyone-insights-into-modern-remote-sensing-research-with-open-data-and-python.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Felix M. Riese</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/satellite-data-is-for-everyone-insights-into-modern-remote-sensing-research-with-open-data-and-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The largest earth observation programme Copernicus
(&lt;a class="reference external" href="http://copernicus.eu"&gt;http://copernicus.eu&lt;/a&gt;) makes it possible to perform terrestrial
observations providing data for all kinds of purposes. One important
objective is to monitor the land-use and land-cover changes with the
Sentinel-2 satellite mission. These satellites measure the sun
reflectance on the earth surface with multispectral cameras (13 channels
between 440 nm to 2190 nm). Machine learning techniques like
convolutional neural networks (CNN) are able to learn the link between
the satellite image (spectrum) and the ground truth (land use class). In
this talk, we give an overview about the state-of-the-art land-use
classification with CNNs based on an open dataset.&lt;/p&gt;
&lt;p&gt;The EuroSAT benchmark dataset (&lt;a class="reference external" href="http://madm.dfki.de/downloads"&gt;http://madm.dfki.de/downloads&lt;/a&gt;) is freely
provided by German Research Center for Artificial Intelligence (DFKI).
It consists of 27.000 image patches for ten different land use/cover
classes, e.g. industrial and residential areas, different crop and
vegetation types and forests. All samples have 64 by 64 pixel dimension
and include either only the RGB images or all 13 bands.&lt;/p&gt;
&lt;p&gt;We will use different out-of-box CNNs for the Keras deep learning
library (&lt;a class="reference external" href="https://keras.io/"&gt;https://keras.io/&lt;/a&gt;). All networks are either included in Keras
itself or are available from Github repositories. We will show the
process of transfer learning for the RGB datasets. Furthermore, the
minimal changes required to apply commonly used CNNs to multispectral
data are demonstrated. Thus, the interested audience will be able to
perform their own classification of remote sensing data within a very
short time. Results of different network structures are visually
compared. Especially the differences of transfer learning and learning
from scratch are demonstrated. This also includes the amount of
necessary training epochs, progress of training and validation error and
visual comparison of the results of the trained networks.&lt;/p&gt;
&lt;p&gt;Finally, we give a quick overview about the current research topics
including recurrent neural networks for spatio-temporal land-use
classification and further applications of multi- and hyperspectral
data, e.g. for the estimation of water parameters and soil
characteristics. Additionally, we provide links to the code and dataset
used in this talk.&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Computer Vision"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="Machine Learning"></category><category term="Science"></category></entry><entry><title>Strongly typed datasets in a weakly typed world</title><link href="https://pyvideo.org/pycon-de-2018/strongly-typed-datasets-in-a-weakly-typed-world.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Marco Neumann</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/strongly-typed-datasets-in-a-weakly-typed-world.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We at Blue Yonder use Pandas quite a lot during our daily data science
and engineering work. This choice, together with Python as an underlying
programming language gives us flexibility, a feature-rich interface, and
access to a large community and ecosystem. When it comes to preserving
the data and exchanging it with different software stacks, we rely on
Parquet Datasets / Hive Tables. During the write process, there is a
shift from a rather weakly typed world to a strongly typed one. For
example, Pandas may convert integers to floats for many operations
without asking, but parquet files and the schema information stored
alongside them dictate very precise types. The type situation may get
even more &amp;quot;colorful&amp;quot;, when datasets are written by multiple code
versions or different software solutions over time. This then results in
important questions regarding type compatibility.&lt;/p&gt;
&lt;p&gt;This talk will first represent an overview on types at different layers
(like NumPy, Pandas, Arrow and Parquet) and the transition between this
layers. The second part of the talk will present examples of type
compatibility we have seen and why+how we think they should be handled.
At the end there will be a Q+A, which can be seen as the start of a
potentially longer RFC process to align different software stacks (like
Hive and Dask) to handle types in a similar way.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category></entry><entry><title>Beyond Jupyter Notebooks - Building your own Data Science platform with Python &amp; Docker</title><link href="https://pyvideo.org/pycon-de-2018/beyond-jupyter-notebooks-building-your-own-data-science-platform-with-python-docker.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Joshua Görner</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/beyond-jupyter-notebooks-building-your-own-data-science-platform-with-python-docker.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Interactive notebooks like Jupyter have become more and more popular in
the recent past and build the core of many data scientist's workplace.
Being accessed via web browser they allow scientists to easily structure
their work by combining code and documentation.&lt;/p&gt;
&lt;p&gt;Yet notebooks often lead to isolated and disposable analysis artefacts.
Keeping the computation inside those notebooks does not allow for
convenient concurrent model training, model exposure or scheduled model
retraining.&lt;/p&gt;
&lt;p&gt;Those issues can be addressed by taking advantage of recent developments
in the discipline of software engineering. Over the past years
containerization became the technology of choice for crafting and
deploying applications. Building a data science platform that allows for
easy access (via notebooks), flexibility and reproducibility (via
containerization) combines the best of both worlds and addresses Data
Scientist's hidden needs.&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Algorithms"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Infrastructure"></category><category term="Jupyter"></category><category term="Machine Learning"></category><category term="Programming"></category><category term="Python"></category></entry><entry><title>Big Data Systems Performance: The Little Shop of Horrors</title><link href="https://pyvideo.org/pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Jens Dittrich</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The confusion around terms such as like NoSQL, Big Data, Data Science,
Spark, SQL, and Data Lakes often creates more fog than clarity. However,
clarity about the underlying technologies is crucial to designing the
best technical solution in any field relying on huge amounts of data
including data science, machine learning, but also more traditional
analytical systems such as data integration, data warehousing,
reporting, and OLAP.&lt;/p&gt;
&lt;p&gt;In my presentation, I will show that often at least three dimensions are
cluttered and confused in discussions when it comes to data management:
First, buzzwords (labels &amp;amp; terms like &amp;quot;big data&amp;quot;, &amp;quot;AI&amp;quot;, &amp;quot;data lake&amp;quot;);
second, data design patterns (principles &amp;amp; best practices like:
selection push-down, materialization, indexing); and Third, software
platforms (concrete implementations &amp;amp; frameworks like: Python, DBMS,
Spark, and NoSQL-systems).&lt;/p&gt;
&lt;p&gt;Only by keeping these three dimensions apart, it is possible to create
technically-sound architectures in the field of big data analytics.&lt;/p&gt;
&lt;p&gt;I will show concrete examples, which through a simple redesign and wise
choice of the right tools and technologies, run thereby up to 1000 times
faster. This in turn triggers tremendous savings in terms of development
time, hardware costs, and maintenance effort.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Infrastructure"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>Bonobo, Airflow and Grafana to visualize your business</title><link href="https://pyvideo.org/pycon-de-2018/bonobo-airflow-and-grafana-to-visualize-your-business.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Romain Dorgueil</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/bonobo-airflow-and-grafana-to-visualize-your-business.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Zero-to-one hands-on introduction to building a business dashboard using
Bonobo ETL, Airflow, and a bit of Grafana (because graphs are cool).
Although the opposite is better, there is no need of prior knowledge
about any of those tools.&lt;/p&gt;
&lt;p&gt;After a short introduction about the tools, we'll go through the
following topics, using the real data of a small SaaS software:&lt;/p&gt;
&lt;p&gt;One can expect to be able to build a similar system at the end of the
talk in a few days (of course, the implementation is only a small part
of this process, data is what really matters).&lt;/p&gt;
&lt;p&gt;«Metrics you watch tend to improve over time»&lt;/p&gt;
</summary><category term="Business &amp; Start-Ups"></category><category term="Data Science"></category><category term="Visualisation"></category><category term="Web"></category></entry><entry><title>Data science complexity and solutions in real industrial projects</title><link href="https://pyvideo.org/pycon-de-2018/data-science-complexity-and-solutions-in-real-industrial-projects.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Artur Miller</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/data-science-complexity-and-solutions-in-real-industrial-projects.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As data scientists we usually like to apply fancy machine learning
models to well-groomed datasets. Everyone working on industrial problems
will eventually learn, that this does not reflect reality. The amount of
time spent on modeling is small compared to data gathering, -warehousing
and -cleaning. Even after training and deployment of the model, the work
is not done. Continuous monitoring of the performance and input data is
still necessary.&lt;/p&gt;
&lt;p&gt;In this talk I discuss how important data handling is for successful
data science projects. Each milestone, from finding the business case to
continuously monitoring the performance of the solution, is addressed.
This is exemplary shown on a project, with the goal of improving a
productive system.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Infrastructure"></category><category term="Machine Learning"></category></entry><entry><title>Data Science meets Data Protection: Keeping your data secure while learning from it.</title><link href="https://pyvideo.org/pycon-de-2018/data-science-meets-data-protection-keeping-your-data-secure-while-learning-from-it.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Andreas Dewes</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/data-science-meets-data-protection-keeping-your-data-secure-while-learning-from-it.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will discuss anonymization and pseudonymization techniques that you
can apply to your data to keep it secure and comply with the law(s)
while still being able to gain useful insights from it.&lt;/p&gt;
&lt;p&gt;We will show concrete Python implementations of various techniques and
use example data sets to show how applying pseudonymization and
anonymization will affect our ability to do machine learning / data
science.&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Business &amp; Start-Ups"></category><category term="Big Data"></category><category term="Data Science"></category></entry><entry><title>Driving simulation and data analysis of magnetic nanostructures through Jupyter Notebook</title><link href="https://pyvideo.org/pycon-de-2018/driving-simulation-and-data-analysis-of-magnetic-nanostructures-through-jupyter-notebook.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Hans Fangohr</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/driving-simulation-and-data-analysis-of-magnetic-nanostructures-through-jupyter-notebook.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We present ongoing work from a project that makes a particular computer
simulation (implemented in C++ and Tk/Tcl) accessible through a Python
interface, and through the Jupyter Notebook. The talk describes the
motivation and current status of the project.&lt;/p&gt;
&lt;p&gt;In more detail, the computer simulation in question is the Object
Oriented Micromagnetic Modelling Framework
(&lt;a class="reference external" href="http://math.nist.gov/oommf/"&gt;OOMMF&lt;/a&gt;) which is likely the most
widely used micromagnetic simulation package. It can be driven through a
graphical (Tk) user interface or through a configuration file that
defines a simulation run.&lt;/p&gt;
&lt;p&gt;In this talk, we first show a Python interface to OOMMF that allows the
driving of OOMMF simulations from a Python program or interpreter
prompt. This way we embed a widely used scientific code from 1990s in a
general purpose programming language
[&lt;a class="reference external" href="https://doi.org/10.1063/1.4977225"&gt;1&lt;/a&gt;] and enable the full use of
the ecosystem of scientific libraries available for Python. For example,
design optimisation, specialised post-processing, and the creation of
figures can all be carried out using a single script; making the work
more easily reproducible.&lt;/p&gt;
&lt;p&gt;Second, we integrate the Python interface to OOMMF into a Jupyter
notebook, so that all existing benefits of using Jupyter are inherited
for the use in computational micromagnetics, which is the reason we
named our code Jupyter- OOMMF (&lt;a class="reference external" href="http://joommf.github.io/"&gt;JOOMMF&lt;/a&gt;). A
&lt;a class="reference external" href="https://tryjoommf.soton.ac.uk/"&gt;JupyterHub installation&lt;/a&gt; of the tool
reduces barriers in uptake, and all the code is &lt;a class="reference external" href="https://github.com/joommf"&gt;on
github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We discuss the benefits of driving computer simulation and data analysis
through Jupyter Notebooks.&lt;/p&gt;
&lt;p&gt;This project is a part of the Jupyter-OOMMF (JOOMMF) activity in the
&lt;a class="reference external" href="http://opendreamkit.org/"&gt;OpenDreamKit&lt;/a&gt; project and we acknowledge
financial support from Horizon 2020 European Research Infrastructures
project (676541). The work is also supported by the EPSRC CDT in Next
Generation Computational Modelling EP/L015382/1, and the EPSRC grants
EP/M022668/1 and EP/N032128/1.&lt;/p&gt;
&lt;p&gt;For additional context: micromagnetic modelling is a key research method
in academia and industry to support development of high-capacity
magnetic storage devices that are cheap, fast, and reliable, and to
enable research into future alternative storage and processing
technologies such as spintronics. The OOMMF modelling package has been
used in &lt;a class="reference external" href="https://math.nist.gov/oommf/oommf_cites.html"&gt;over 2500
publications&lt;/a&gt; since
1999.&lt;/p&gt;
&lt;p&gt;[1] Beg, M., Pepper, R. A., and Fangohr, H. User interfaces for
computational science: A domain specific language for OOMMF embedded in
Python. AIP Advances 7, 056025 (2017), &lt;a class="reference external" href="https://doi.org/10.1063/1.4977225"&gt;https://doi.org/10.1063/1.4977225&lt;/a&gt;&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Jupyter"></category><category term="Programming"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>Fulfilling Apache Arrow's Promises: Pandas on JVM memory without a copy</title><link href="https://pyvideo.org/pycon-de-2018/fulfilling-apache-arrows-promises-pandas-on-jvm-memory-without-a-copy.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/fulfilling-apache-arrows-promises-pandas-on-jvm-memory-without-a-copy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Arrow established a standard for columnar in-memory analytics to
redefine the performance and interoperability of most Big Data
technologies in early 2016. Since then implementations in Java, C++,
Python, Glib, Ruby, Go, JavaScript and Rust have been added. Although
Apache Arrow (&lt;tt class="docutils literal"&gt;pyarrow&lt;/tt&gt;) is already known to many Python/Pandas users
for reading Apache Parquet files, its main benefit is the cross-language
interoperability. With feather and PySpark, you can already benefit from
this in Python and R/Java via the filesystem or network. While they
improve data sharing and remove serialization overhead, data still needs
to be copied as it is passed between processes.&lt;/p&gt;
&lt;p&gt;In the 0.23 release of Pandas, the concept of ExtensionArrays was
introduced. They allow the extension of Pandas DataFrames and Series
with custom, user- defined typed. The most prominent example is
&lt;tt class="docutils literal"&gt;cyberpandas&lt;/tt&gt; which adds an IP dtype that is backed by the appropriate
representation using NumPy arrays. These ExtensionArrays are not limited
to arrays backed by NumPy but can take an arbitrary storage as long as
they fulfill a certain interfaces. Using Apache Arrow we can implement
ExtensionArrays that are of the same dtype as the built-in types of
Pandas but memory management is not tied to Pandas' internal
BlockManager. On the other hand Apache Arrow has a much more wider set
of efficient types that we can also expose as an ExtensionArray. These
types include a native string type as well as a arbitrarily nested types
such as &lt;tt class="docutils literal"&gt;list of …&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;struct of (…, …, …)&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;To show the real-world benefits of this, we take the example of a data
pipeline that pulls data from a relational store, transforms it and then
passes it into a machine learning model. A typical setup nowadays most
likely involves a data lake that is queried with a JVM based query
engine. The machine learning model is then normally implemented in
Python using popular frameworks like CatBoost or Tensorflow.&lt;/p&gt;
&lt;p&gt;While sometimes these query engines provide Python clients, their
performance is normally not optimized for large results sets. In the
case of a machine learning model, we will do some feature
transformations and possibly aggregations with the query engine but feed
as many rows as possible into the model. This will lead then to result
sets that have above a million rows. In contrast to the Python clients,
these engines often come with efficient JDBC drivers that can cope with
result sets of this size but then the conversion from Java objects to
Python objects in the JVM bridge will slow things down again. In our
example, we will show how to use Arrow to retrieve a large result in the
JVM and then pass it on to Python without running into these
bottlenecks.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category></entry><entry><title>Interactive Visualization of Traffic Data using Bokeh</title><link href="https://pyvideo.org/pycon-de-2018/interactive-visualization-of-traffic-data-using-bokeh.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Dr. Patrik Hlobil</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/interactive-visualization-of-traffic-data-using-bokeh.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk covers the creation of highly interactive and dynamic
visualization (as HTML) using Python, that can still be opened with any
modern browser. Using real-world examples we will show our usual
workflow for processing and creating visualizations using Bokeh. The
following topics will be covered:&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Visualisation"></category><category term="Web"></category></entry><entry><title>Introduction to Docker for Pythonistas</title><link href="https://pyvideo.org/pycon-de-2018/introduction-to-docker-for-pythonistas.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Jan Wagner</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/introduction-to-docker-for-pythonistas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;My Talk aims to introduce you to Docker and how it works, how you can
use prebuild Images from the Docker-Hub and how you can make your own
Images.&lt;/div&gt;
&lt;div class="line"&gt;In more Detail, the following Points will be covered:&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Jupyter"></category><category term="Machine Learning"></category></entry><entry><title>Measuring the hay in the haystack: quantifying hidden variables using Bayesian Inference</title><link href="https://pyvideo.org/pycon-de-2018/measuring-the-hay-in-the-haystack-quantifying-hidden-variables-using-bayesian-inference.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Omer Yuksel</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/measuring-the-hay-in-the-haystack-quantifying-hidden-variables-using-bayesian-inference.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Technology-driven trading is a field with many challenges, and
performance and availability of the network communication is essential
to the business. To have a good understanding on the performance and
availability, we monitor certain metrics - however not every interesting
metric is readily available to measure. Some of these have to be
inferred from the data we see in production by incorporating our own
knowledge. What complicates this further is that the relationship
between the hidden variables and the output data is not a deterministic
one, as we are often dealing with a stochastic system.&lt;/p&gt;
&lt;p&gt;Bayesian inference is a suitable way to tackle this issue - it allows
encoding our knowledge as a prior distribution of the model parameters.
Here we will go through real-world uses of Bayesian inference at IMC,
using PyMC3 to make an estimate for the hidden metrics in the network
traffic.&lt;/p&gt;
&lt;p&gt;Knowledge: No prior knowledge of PyMC3 is required. Since this is a
short presentation, the talk with approach the problem and the solution
at a high level instead of implementation details.&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Networks"></category></entry><entry><title>Microservices from the trenches: how we delivery fancy sofas across Europe</title><link href="https://pyvideo.org/pycon-de-2018/microservices-from-the-trenches-how-we-delivery-fancy-sofas-across-europe.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Christian Barra</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/microservices-from-the-trenches-how-we-delivery-fancy-sofas-across-europe.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At made.com we use micro-services written in Python to power our entire
backend system and deliver an incredible amount of orders each week.
During this presentation I will explain what are micro-services and
compare them to monolith applications. By analysing what are the
differences between micro- services and monolith applications I will
show why you need them and for which situations they are extremely
useful. I will also talk about what you need on different levels
regarding infrastructure, knowledge and experience to get the most out
of a micro-services architecture. The last part of the presentation is
dedicated to the drawbacks of running a micro-services architecture and
sharing some solutions. I will conclude sharing some useful resources
about micro services and taking some questions.&amp;quot;&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Jupyter"></category></entry><entry><title>Processing Geodata using Python</title><link href="https://pyvideo.org/pycon-de-2018/processing-geodata-using-python.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Martin Christen</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/processing-geodata-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a large amount of Python modules available suitable for spatial
data processing. In this talk, it is shown how to analyze, manipulate
and visualize geospatial data by using open source modules. The
following modules will be introduced:&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="Jupyter"></category><category term="Python"></category><category term="Visualisation"></category></entry><entry><title>Solving Data Science Problems using a Jupyter Notebook and SAP HANA's in-database Machine Learning Libraries</title><link href="https://pyvideo.org/pycon-de-2018/solving-data-science-problems-using-a-jupyter-notebook-and-sap-hanas-in-database-machine-learning-libraries.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Dr Frank Gottfried</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/solving-data-science-problems-using-a-jupyter-notebook-and-sap-hanas-in-database-machine-learning-libraries.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Companies store their data in databases with highly restricted access
regulations. The latest regulatorily changes enforces the need to work
on the datasets in this controlled environment without created
additional external copies. However Data Scientists prefer to work with
tools they are most familiar like Python, R and Jupyter Notebooks using
to a large amount of open- source packages (numpy, matplotlib, pandas,
..). SAP HANA provides highly optimized in-database machine learning
libraries. In this talk we will present how a Data Scientist can work in
an environment he/she is most familiar with and access the data stored
in SAP HANA using SAP HANA machine learning libraries with a
scikit-learn type interface. Data will remain in the database and will
be exposed as dataframes (similar to Pandas dataframes). We will explain
the software architecture and present a complete end-to-end use case by
using a Jupyter Notebook.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="Jupyter"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>Stretchy - NoSQL Database behind REST API</title><link href="https://pyvideo.org/pycon-de-2018/stretchy-nosql-database-behind-rest-api.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Artur Scholz</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/stretchy-nosql-database-behind-rest-api.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Stretchy is built as a microservice that provides a simple and intuitive
REST API with a NoSQL database as backend. No need for database
migrations or upfront schema design. The basic CRUD (create, read,
update, delete) operations are available for getting data in and out
from the database.&lt;/p&gt;
&lt;p&gt;Stretchy is free and open source software built with Python 3, using
Flask web framework. It currently uses MongoDB as its backend database.
Since it is interfaced through the REST API however, Stretchy is
technology agnostic and developers can create bindings to other
databases, including SQL databases.&lt;/p&gt;
&lt;p&gt;This presentation reviews the reasons for creating Stretchy, its current
applications, an short tutorial on how to use it, and tips on how to
deploy it.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="Web"></category></entry><entry><title>A Day Has Only 24±1 Hours: import pytz</title><link href="https://pyvideo.org/pycon-de-2018/a-day-has-only-24-1-hours-import-pytz.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Miroslav Šedivý</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/a-day-has-only-24-1-hours-import-pytz.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;On the last Sunday of October “we get one more hour of sleep” but may
spend much more time debugging code dealing with the timezones, daylight
saving time shifts and datetime stuff in general.&lt;/p&gt;
&lt;p&gt;We'll look at a few pitfalls you may encounter when working with
datetimes in Python. We'll discover the &lt;tt class="docutils literal"&gt;pytz&lt;/tt&gt; module and explain why
&lt;tt class="docutils literal"&gt;pytz.all_timezones&lt;/tt&gt; contains over 500 individual timezones. We'll
also find the reason why &lt;tt class="docutils literal"&gt;pytz&lt;/tt&gt; is not part of the standard Python,
why it gets updated so often and why even that won't solve all your
problems.&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Science"></category></entry><entry><title>Active Learning - Building Semi-supervised Classifiers when Labeled Data is not Available</title><link href="https://pyvideo.org/pycon-de-2018/active-learning-building-semi-supervised-classifiers-when-labeled-data-is-not-available.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Dr. Hendrik Niemeyer</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/active-learning-building-semi-supervised-classifiers-when-labeled-data-is-not-available.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In many situations large datasets are available but unfortunately
labeling is expensive and time consuming. Active Learning is a concept
for building classifiers by letting the algorithm choose the training
data it uses. This achieves greater accuracy than just labeling a random
subset of the available dataset.&lt;/p&gt;
&lt;p&gt;The active learning algorithm selects some unlabeled data instances
which are then labeled by a human annotator. Given this information a
classifier is trained and new instances for the human annotator to label
are selected. This iterative process tries to label as few instances as
possible while achieving high classification accuracy.&lt;/p&gt;
&lt;p&gt;In this talk I will give a general overview of the core concepts and
techniques of active learning like algorithms for selecting the queries
and convergence criteria.&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Algorithms"></category><category term="Data Science"></category><category term="Machine Learning"></category><category term="Python"></category></entry><entry><title>Binder - lowering the bar to sharing interactive software</title><link href="https://pyvideo.org/pycon-de-2018/binder-lowering-the-bar-to-sharing-interactive-software.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Tim Head</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/binder-lowering-the-bar-to-sharing-interactive-software.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Binder project drastically lowers the bar to sharing and re-using
software. As a user wanting to try out someone else’s work I only have
to click a single link. As the author preparing a binder-ready project
is much easier than having to support many different platforms and for
many projects involves little additional work.&lt;/p&gt;
&lt;p&gt;In this talk I will introduce the audience to the concepts and ideas
behind the Binder project. I will showcase examples from the community
to illustrate use-cases and show off the power of Binder.&lt;/p&gt;
&lt;p&gt;Three pieces of software power Binder:
&lt;a class="reference external" href="http://repo2docker.readthedocs.io/en/latest/"&gt;repo2docker&lt;/a&gt;,
&lt;a class="reference external" href="https://binderhub.readthedocs.io/en/latest/"&gt;BinderHub&lt;/a&gt; and
&lt;a class="reference external" href="http://jupyterhub.readthedocs.io/en/stable/"&gt;JupyterHub&lt;/a&gt;. Using an
example repository I will go through the steps required to make a
repository binder- ready and what happens when a user launches it. At
each step I will illustrate the role that each of the three software
components play and how they interact.&lt;/p&gt;
&lt;p&gt;Binder is a project created by its community. I will present pathways
for getting involved with the community.&lt;/p&gt;
&lt;p&gt;To wrap up I will highlight plans for future developments and features
of Binder.&lt;/p&gt;
</summary><category term="Community"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Jupyter"></category><category term="Science"></category><category term="Web"></category></entry><entry><title>Distributed Hyperparameter search with sklearn and kubernetes</title><link href="https://pyvideo.org/pycon-de-2018/distributed-hyperparameter-search-with-sklearn-and-kubernetes.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Jakob Karalus</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/distributed-hyperparameter-search-with-sklearn-and-kubernetes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While sklearn provides a good interface to do hyperparameter search on
large &amp;amp; complex model (pipelines), doing these can take up a lot of
time. The traditional way usually includes one beefy machine and a lot
of waiting. In other cases, people tend to “manually” schedule parameter
ranges between nodes, but that can also be problematic since these won't
talk to each other. Kubernetes itself is currently the most prominent
scheduler and shines at distributing task, but is a pretty complex
system in itself.&lt;/p&gt;
&lt;p&gt;In this talk, I will show how you can harness the scheduling of
kubernetes for distributing hyperparameter search with sklearn onto a
cluster of nodes. This can be achieved quite easily and with just a few
changes to the original code, so the Data Scientist won't be bothered by
complex kubernetes internals.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Infrastructure"></category><category term="Machine Learning"></category></entry><entry><title>Germany's next topic model</title><link href="https://pyvideo.org/pycon-de-2018/germanys-next-topic-model.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Thomas Mayer</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/germanys-next-topic-model.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Identifying topic models for user generated content like hotel reviews
turns out to be difficult with the standard approach of LDA (Latent
Dirichlet Allocation; Blei et al., 2003). Hotel review texts usually
don't differ as much in the topics that are covered as is typical with
other genres such as Wikipedia or newsgroup articles where there is
commonly only a very small set of topics present in each document.&lt;/p&gt;
&lt;p&gt;To this end, we developed our own approach to topic modeling that is
especially tailored to non-edited texts like hotel reviews. The approach
can be divided into three major steps. First, using the concept of
second-order cooccurrences we define a contextual similarity score that
enables us to identify words that are similar with respect to certain
topics. This score allows us to build up a topic network where nodes are
words and edges the contextual similarity between the words. With the
help of algorithms from graph theory, like the Infomap algorithm
(Rosvall and Bergstrom, 2008), we are able to detect clusters of highly
connected words that can be identified as topics in our review texts. In
a further step, we use these clusters and the respective words to get a
topic similarity score for each word in the network. In other words, we
transform a hard clustering of words into topics into a probability
score of how likely a certain word belongs to a given topic/cluster.&lt;/p&gt;
&lt;p&gt;The presentation is structured as follows:&lt;/p&gt;
&lt;p&gt;References: David M. Blei, Andrew Y. Ng, Michael I. Jordan: Latent
dirichlet allocation. In: Journal of Machine Learning Research, Jg. 3
(2003), S. 993–1022, ISSN 1532-4435 M. Rosvall and C. T. Bergstrom, Maps
of information flow reveal community structure in complex networks, PNAS
105, 1118 (2008) &lt;a class="reference external" href="http://dx.doi.org/10.1073/pnas.0706851105"&gt;http://dx.doi.org/10.1073/pnas.0706851105&lt;/a&gt;,
&lt;a class="reference external" href="http://arxiv.org/abs/0707.0609"&gt;http://arxiv.org/abs/0707.0609&lt;/a&gt;&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="Networks"></category><category term="NLP"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>Productionizing your ML code seamlessly</title><link href="https://pyvideo.org/pycon-de-2018/productionizing-your-ml-code-seamlessly.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Lauris Jullien</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/productionizing-your-ml-code-seamlessly.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data science and Machine Learning are hot topics right now for Software
Engineers and beyond. There are a lot of python tools that allow you to
hack together a notebook to quickly get insight on your data, or train a
model to predict or classify. Or you might have inherited some data
wrangling and modeling {Jupyter/Zeppelin} notebook code from someone
else, like the resident data scientist.&lt;/p&gt;
&lt;p&gt;The code works on test data, when you run the cells in the right order
(skipping cell 22), and you believe that the insight gained from this
work would be a valuable game changer. But now how do you take this
experimental code into production, and keep it up-to-date with a regular
retraining schedule? And what do you need to do after that, to ensure
that it remains reliable and brings value in the long term?&lt;/p&gt;
&lt;p&gt;These will be the questions this talk will answer, focusing on 2 main
themes: What does running an ML model in production involve? How to
improve your development workflow to make the path to production easier?&lt;/p&gt;
&lt;p&gt;This talk will draw examples from real projects at Yelp, like migrating
a pandas/sklearn classification project into production with pyspark,
while aiming to give advice that is not dependent on specific
frameworks, or tools, and is useful for listeners from all backgrounds.&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Data Science"></category><category term="Machine Learning"></category></entry><entry><title>reticulate: R interface to Python</title><link href="https://pyvideo.org/pycon-de-2018/reticulate-r-interface-to-python.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Jens Bruno Wittek</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/reticulate-r-interface-to-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python and R are the preferred languages for data science. In 2018,
RStudio introduced its package reticulate and clearly demonstrates that
it favours to join forces. Both languages have strengths and weaknesses.
Tools to combine the strengths will enable easier collaboration in
projects and more possibilities to succeed. Using Python from R gives R
users wider access to functions and makes it easier for Python beginners
to just run scripts and being able to collaborate in Python projects.
The talk will show the possibilities of reticulate: The main part starts
with demonstrating the Python interpreter within R. It will show how to
source Python scripts as well as install and import modules. Then it
will deal with the most important types of Python objects, how they are
represented in R and how to further manipulate them. Thereby, a special
focus is on using Python for data science. In addition, it will be
presented how Conda environments can be created and used from R. A
further application will be the creation of reports with Markdown and
LaTeX where R and Python can be used within one document and share
objects. A last topic is about showing the possibilities for easier
development in RStudio (help regarding Python functions, auto
completion).&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="NLP"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>Scalable Scientific Computing using Dask</title><link href="https://pyvideo.org/pycon-de-2018/scalable-scientific-computing-using-dask.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/scalable-scientific-computing-using-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category><category term="Python"></category></entry><entry><title>Databases for Data Science</title><link href="https://pyvideo.org/pycon-italia-2018/databases-for-data-science.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Alexander Hendorf</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/databases-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I’ll present the usefulness of databases for data science
projects.&lt;/p&gt;
&lt;p&gt;Databases have been around for decades and were highly optimised for
data aggregations during that time. Not only &lt;em&gt;Big data&lt;/em&gt; has changed the
landscape of databases massively in the past years - we nowadays can
find many &lt;em&gt;Open Source&lt;/em&gt; projects among the most popular dbs.&lt;/p&gt;
&lt;p&gt;After this talk you will be enabled to decide if a database can make
your work more efficient and which direction to look to.&lt;/p&gt;
&lt;p&gt;Outline:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A quick recap on database history, &lt;em&gt;it all starts in Florence - where
else…&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;What is a database? What’s a data store? data lake?…&lt;/li&gt;
&lt;li&gt;Relational SQL systems and their benefits for DS&lt;/li&gt;
&lt;li&gt;NoSQL systems and their benefits for DS&lt;/li&gt;
&lt;li&gt;How to chose the db fitting your needs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 17:00 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="data-structures"></category><category term="Deep-Learning"></category><category term="data-science"></category><category term="database"></category></entry><entry><title>Recent advancements in NLP and Deep Learning: A Quant's Perspective</title><link href="https://pyvideo.org/pycon-italia-2018/recent-advancements-in-nlp-and-deep-learning-a-quants-perspective.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Umit Mert Cakmak</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/recent-advancements-in-nlp-and-deep-learning-a-quants-perspective.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a gold-rush among hedge-funds for text mining algorithms to
quantify textual data and generate trading signals. Harnessing the power
of alternative data sources became crucial to find novel ways of
enhancing trading strategies.&lt;/p&gt;
&lt;p&gt;With the proliferation of new data sources, natural language data became
one of the most important data sources which could represent the public
sentiment and opinion about market events, which then can be used to
predict financial markets.&lt;/p&gt;
&lt;p&gt;Talk is split into 5 parts;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Who is a quant and how do they use NLP?&lt;/li&gt;
&lt;li&gt;How deep learning has changed NLP?&lt;/li&gt;
&lt;li&gt;Let’s get dirty with word embeddings&lt;/li&gt;
&lt;li&gt;Performant deep learning layer for NLP: The Recurrent Layer&lt;/li&gt;
&lt;li&gt;Using all that to make money&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="who-is-a-quant-and-how-do-they-use-nlp"&gt;
&lt;h4&gt;1. Who is a quant and how do they use NLP?&lt;/h4&gt;
&lt;p&gt;Quants use mathematical and statistical methods to create algorithmic
trading strategies.&lt;/p&gt;
&lt;p&gt;Due to recent advances in available deep learning frameworks and
datasets (time series, text, video etc) together with decreasing cost of
parallelisable hardware, quants are experimenting with various NLP
methods which are applicable to quantitative trading.&lt;/p&gt;
&lt;p&gt;In this section, we will get familiar with the brief history of text
mining work that quants have done so far and recent advancements.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-deep-learning-has-changed-nlp"&gt;
&lt;h4&gt;2. How deep learning has changed NLP?&lt;/h4&gt;
&lt;p&gt;In recent years, data representation and modeling methods are vastly
improved. For example when it comes to textual data, rather than using
high dimensional sparse matrices and suffering from curse of
dimensionality, distributional vectors are more efficient to work with.&lt;/p&gt;
&lt;p&gt;In this section, I will talk about distributional vectors a.k.a. word
embeddings and recent neural network architectures used when building
NLP models.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lets-get-dirty-with-word-embeddings"&gt;
&lt;h4&gt;3. Let’s get dirty with word embeddings&lt;/h4&gt;
&lt;p&gt;Models such as Word2vec or GloVe helps us create word embeddings from
large unlabeled corpus which represent the relation between words, their
contextual relationships in numerical vector spaces and these
representations not only work for words but also could be used for
phrases and sentences.&lt;/p&gt;
&lt;p&gt;In this section, I will talk about inner workings of these models and
important points when creating domain-specific embeddings (e.g. for
sentiment analysis in financial domain).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="performant-deep-learning-layer-for-nlp-the-recurrent-layer"&gt;
&lt;h4&gt;4. Performant deep learning layer for NLP: The Recurrent Layer&lt;/h4&gt;
&lt;p&gt;Recurrent Neural Networks (RNNs) can capture and hold the information
which was seen before (context), which is important for dealing with
unbounded context in NLP tasks.&lt;/p&gt;
&lt;p&gt;Long Short Term Memory (LSTM) networks, which is a special type of RNN,
can understand the context even if words have long term dependencies,
words which are far back in their sequence.&lt;/p&gt;
&lt;p&gt;In this talk, I will compare LSTMs with other deep learning
architectures and will look at LSTM unit from a technical point of view.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-all-that-to-make-money"&gt;
&lt;h4&gt;5. Using all that to make money&lt;/h4&gt;
&lt;p&gt;Financial news, especially if it’s major, can change the sentiment among
investors and affect the related asset price with immediate price
corrections.&lt;/p&gt;
&lt;p&gt;For example, what’s been communicated in quarterly earnings calls might
indicate whether the price of share will drop or increase based on the
language used. If the message of the company is not direct and featuring
complex sounding language, it usually indicates that there’s some shady
stuff going on and if this information extracted right, it’s a valuable
trading signal. For similar reasons, scanning announcements and
financial disclosures for trading signals became a common NLP practice
in investment industry.&lt;/p&gt;
&lt;p&gt;In this section, I will talk about the various data sources that
researchers can use and also explain common NLP workflows and deep
learning practices for quantifying textual data for generating trading
signals.&lt;/p&gt;
&lt;p&gt;I will end with summary with application architecture in case anyone
would like to implement similar systems for their own use.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 14:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</summary><category term="nlp"></category><category term="data-science"></category><category term="Keras"></category><category term="Python"></category><category term="Deep-Learning"></category><category term="machine-learning"></category><category term="spaCy"></category><category term="nltk"></category></entry><entry><title>Python &amp; Industry 4.0: a real world case</title><link href="https://pyvideo.org/pycon-italia-2018/python-industry-40-a-real-world-case.html" rel="alternate"></link><published>2018-04-20T00:00:00+00:00</published><updated>2018-04-20T00:00:00+00:00</updated><author><name>Gianluca Emireni</name></author><id>tag:pyvideo.org,2018-04-20:pycon-italia-2018/python-industry-40-a-real-world-case.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;IT Con l’avvento dell’Industry 4.0 sempre più linee di produzione sono
in grado di generare informazioni di funzionamento ad alta frequenza. E’
quindi possibile raccogliere ed elaborare i dati provenienti dai sensori
(per es. consumi elettrici, numero di pezzi prodotti, allarmi, ecc.) per
ottimizzare i processi produttivi fino ad arrivare alla Manutenzione
Predittiva. In questo talk vedremo come poter sfruttare lo stack Python
per implementare la pipeline di attività quali Data Cleansing, Data
Wrangling, Feature Engineering e Machine Learning modeling su un caso
reale.&lt;/p&gt;
&lt;p&gt;EN Thanks to the Industry 4.0 wave, an increasing number of production
lines are able to generate high frequency operational data. It is
possible to gather and process sensor data (e.g., power consumption,
processed items, alerts, etc.) in order to optimize production processes
and to realize the so called Predictive Maintenance. In this talk we
will show how we applied the Python stack to a real world scenario by
implementing data pipelines (i.e., Data Cleansing, Data Wrangling,
Feature Engineering, and Machine Learning modeling).&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 20 April&lt;/strong&gt; at 11:45 &lt;a class="reference external" href="/en/sprints/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="industry4.0"></category><category term="data-science"></category><category term="machine-learning"></category></entry><entry><title>Docker for Data Science</title><link href="https://pyvideo.org/pycon-us-2018/docker-for-data-science.html" rel="alternate"></link><published>2018-05-10T00:00:00+00:00</published><updated>2018-05-10T00:00:00+00:00</updated><author><name>Aly Sivji</name></author><id>tag:pyvideo.org,2018-05-10:pycon-us-2018/docker-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter notebooks simplify the process of developing and sharing Data Science projects across groups and organizations. However, when we want to deploy our work into production, we need to extract the model from the notebook and package it up with the required artifacts (data, dependencies, configurations, etc) to ensure it works in other environments. Containerization technologies such as Docker can be used to streamline this workflow.&lt;/p&gt;
&lt;p&gt;This hands-on tutorial presents Docker in the context of Reproducible Data Science - from idea to application deployment. You will get a thorough introduction to the world of containers; learn how to incorporate Docker into various Data Science projects; and walk through the process of building a Machine Learning model in Jupyter and deploying it as a containerized Flask REST API.&lt;/p&gt;
</summary><category term="jupyter"></category><category term="docker"></category><category term="data science"></category></entry><entry><title>An introduction to PyMC3</title><link href="https://pyvideo.org/pycon-de-2017/an-introduction-to-pymc3.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Adrian Seyboldt</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/an-introduction-to-pymc3.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Adrian Seyboldt&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I studied Mathematics and Bioinformatics in Bonn and Tübingen and I am a core developer of pymc3 since Feb 2017. Currently, I work for Quantopian on the development of Bayesian Methods for portfolio allocation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;PyMC3 allows you to build statistical models for a wide range of datasets, use those models to estimate underlying parameters, and compute the uncertainty about those parameters. In this talk I will try to give a gentle introduction to PyMC3, and help avoid common pitfalls for new users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some of the problems that are discussed in the context of the reproducibility crisis in science and statistics can be solved or alleviated by tools like PyMC3 or Stan. They allow users to build much more realistic models and get a full distribution of the possible values for parameters as output – instead of p-values that are often hard to interpret correctly. Thanks to Hamiltonian and Variational methods, they are more flexible and can be applied to larger problems than predecessors like JAGS and BUGS. However, these new methods also come with challenges. Writing good models isn't easy, and when inference algorithms cry out in pain, they need someone who listens to them. This talk uses some real-world applications to give an introduction to PyMC3, without requiring a lot of background in math, statistics or programming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="python"></category><category term="data-science"></category><category term="machine learning"></category><category term="analysis"></category></entry><entry><title>Connecting PyData to other Big Data Landscapes using Arrow and Parquet</title><link href="https://pyvideo.org/pycon-de-2017/connecting-pydata-to-other-big-data-landscapes-using-arrow-and-parquet.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/connecting-pydata-to-other-big-data-landscapes-using-arrow-and-parquet.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Uwe L. Korn&lt;/strong&gt; (&amp;#64;xhochy)&lt;/p&gt;
&lt;p&gt;Uwe Korn is a Data Scientist at the Karlsruhe-based RetailTec company Blue Yonder. His expertise is on building architectures for machine learning services that are scalably usable for multiple customers aiming at high service availability as well as rapid prototyping of solutions to evaluate the feasibility of his design decisions. As part of his work to provide an efficient data interchange he became a core committer to the Apache Parquet and Apache Arrow projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While Python itself hosts a wide range of machine learning and data tools, other ecosystems like the Hadoop world also provide beneficial tools that can be either connected via Apache Parquet files or in memory using Arrow. This talks shows recent developments that allow interoperation at speed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Python has a vast amount of libraries and tools in its machine learning and data analysis ecosystem. Although it is clearly in competition with R here about the leadership, the world that has sprung out of the Hadoop ecosystem has established itself in the space of data engineering and also tries to provide tools for distributed machine learning. As these stacks run in different environments and are mostly developed by distinct groups of people, using them together has been a pain. While Apache Parquet has already proven itself as the gold standard for the exchange of DataFrames serialized to files, Apache Arrow recently got traction as the in-memory format for DataFrame exchange between different ecosystems.&lt;/p&gt;
&lt;p&gt;This talk will outline how Apache Parquet files can be used in Python and how they are structured to provide efficient DataFrame exchange. In addition to small code sample, this also includes an explanation of some interesting details of the file format. Additionally, the idea of Apache Arrow will be presented and taking Apache Spark (2.3) as an example to showcase how performance increases once DataFrames can be efficiently shared between Python and JVM processes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data-science"></category><category term="hadoop"></category><category term="apache"></category><category term="arrow"></category><category term="parquet"></category><category term="pandas"></category><category term="pydata"></category></entry><entry><title>Data Plumbing 101 - ETL Pipelines for Everyday Projects</title><link href="https://pyvideo.org/pycon-de-2017/data-plumbing-101-etl-pipelines-for-everyday-projects.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Eberhard Hansis</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/data-plumbing-101-etl-pipelines-for-everyday-projects.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Eberhard Hansis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have been writing code for more than two thirds of my life, mostly for scientific computing and data analysis. In the past couple of years, I have worked on a range of different data science projects, all using Python at their core. During this time, I repeatedly was tasked with making data usable by joining multiple sources into a clearly defined data model. Once you have done that, it is amazing how much real-life value you can generate with little more than a bit of statistics and visualization. The world is full of underused data, let’s change that!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There is no data science without ETL! This presentation is about implementing maintainable data integration for your projects. We will have a first look a ‘Ozelot’, a library based on Luigi and SQLAlchemy that helps you get started with building ETL pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ETL, the hard way&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You are starting a new data science project, and you can’t wait to perform some machine learning magic. However, before getting to ML, you have to deal with its ugly sibling: ETL. Extracting, transforming and loading data (or, more generally, data integration) is an indispensable first step in almost any data project.&lt;/p&gt;
&lt;p&gt;In your project you will, most likely, have to extract data from various sources, clean it, link it and prepare it to your needs. You will start writing a first data integration script for some part of the process, then a second, then a third. At some point you will write an ugly ‘master’ script to keep your 17 import scripts in check and run them in just the right order. When you come back to the code later, you will have a hard time deciphering what you did and why, and what format the output data is in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pipelines to the rescue&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Implementing a proper data integration pipeline and a well-defined data model helps document your data flows and makes them traceable. More importantly, it simplifies the ETL development process, because it lets you easily re-run the whole process or parts of it. And you will have to modify and re-run your ETL, because your code changes, your output requirements change or the data changes.&lt;/p&gt;
&lt;p&gt;In this talk I propose a setup for building maintainable data integration pipelines for everyday projects. This setup is embodied by ‘Ozelot’, my brand-new Python library for ETL. It is based on Luigi for pipeline management and SQLAlchemy as ORM layer. Ozelot gives you core functionality to quickly start building your own solution, including an ORM base class, database connection management and Luigi task classes that play nice with the ORM. It comes with extensively documented examples that walk you through various aspects of data integration.&lt;/p&gt;
&lt;p&gt;The proposed setup works well for many small- to medium-sized projects -- projects, for which you previously might not have implemented a proper data integration pipeline. For big-data projects or those requiring live streaming data you probably want to consider alternative solutions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Core principles of data integration&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Taking one step back, I propose the following core principles for maintainable data integration:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Any and all data manipulation happens in the pipeline, in a single code base.&lt;/li&gt;
&lt;li&gt;The pipeline represents all dependencies between data integration tasks.&lt;/li&gt;
&lt;li&gt;Each task has a method for rolling back its operations.&lt;/li&gt;
&lt;li&gt;Data is loaded into a single database, in a clearly defined, object-based data model that also encodes object relationships.&lt;/li&gt;
&lt;li&gt;The whole process is fully automatic and thereby reproducible and traceable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will discuss why I think that these principles are important, and how they are reflected in the proposed setup.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data-science"></category><category term="analytics"></category><category term="python"></category></entry><entry><title>Data Science Project for Beginners</title><link href="https://pyvideo.org/pycon-de-2017/data-science-project-for-beginners.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Natalie Speiser</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/data-science-project-for-beginners.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Natalie Speiser,Jens Beyer&lt;/strong&gt; (&amp;#64;natalie_lavrio)&lt;/p&gt;
&lt;p&gt;Natalie is a psychologist focused on statistics and machine learning for predictions. Jens is a pyhsicist turned consultant for IBM and d-fine and helped big companies with statistical models since 2009. Together, we founded LAVRIO.solutions and help our clients to make the most out of their data. In our recent data science projects, we faced specific hurdles which seem to be typical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;AI and Machine Learning are taking over the world - but how do you actually start with understanding your data and predicting events? And what kind of &amp;quot;political&amp;quot; trouble could you run into? With examples from real projects, we try to give you a feeling for data science projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will be talking about examples from projects we did as data science consultants. What should be organized before you go to a client (internal or external). How should the data look like. What are problems you have to face while working with the clients IT department. You get to see a rough draft of our code. It won't be a thorough manual, just our experiences and how we dealt with problems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="business"></category><category term="data-science"></category><category term="use-case"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Deep Learning for Computer Vision</title><link href="https://pyvideo.org/pycon-de-2017/deep-learning-for-computer-vision.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Alex Conway</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/deep-learning-for-computer-vision.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Alex Conway&lt;/strong&gt; (&amp;#64;alxcnwy)
Alex is the founder &amp;amp; CTO of NumberBoost, a startup that builds deep learning applications. He previously worked as a quant for a hedge fund and as a data scientist for an e-commerce company. He has an honours degree in actuarial science and a MSc in statistics. He is one of the organizers of the Cape Town Deep Learning meet-up and has built numerous computer vision systems that run at scale in production predicting labels for millions of images per day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The state-of-the-art in image classification has skyrocketed thanks to the development of deep convolutional neural networks and increases in the amount of data and computing power available to train them. The top-5 error rate in the ImageNet competition to predict which of 1000 classes an image belongs to has plummeted from 28% error in 2010 to just 2.25% in 2017 (human level error is around 5%).&lt;/p&gt;
&lt;p&gt;In addition to being able to classify objects in images (including not hotdogs), deep learning can be used to automatically generate captions for images, convert photos into paintings, detect cancer in pathology slide images, and help self-driving cars ‘see’.&lt;/p&gt;
&lt;p&gt;The talk will give an overview of the cutting edge and some of the core mathematical concepts and will also include a short code-first tutorial to show how easy it is to get started using deep learning for computer vision in python…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk is a crash course on convolutional neural networks and how to use them to solve 2 real-world applications at scale. The first is an image moderation system and the second is a visual similarity system where a user uploads an image of an item and the system returns visually similar items.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="business"></category><category term="data-science"></category><category term="use-case"></category><category term="deep learning"></category><category term="ai"></category><category term="machine learning"></category></entry><entry><title>Effective Data Analysis with Pandas Indexes</title><link href="https://pyvideo.org/pycon-de-2017/effective-data-analysis-with-pandas-indexes.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Alexander Hendorf</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/effective-data-analysis-with-pandas-indexes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is the Swiss-Multipurpose Knife for Data Analysis in Python. In this talk we will look deeper into how to gain productivity utilizing Pandas powerful indexing and make advanced analytics a piece of cake. Pandas features multiple index types. This talk will give you a deep insight into the Pandas indexes and showcase the handiness of special Indexes as the TimeSeriesIndex.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="machine learning"></category><category term="analytics"></category><category term="data-science"></category><category term="business analytics"></category></entry><entry><title>Flow is in the Air: Best Practices of Building Analytical Data Pipelines with Apache Airflow</title><link href="https://pyvideo.org/pycon-de-2017/flow-is-in-the-air-best-practices-of-building-analytical-data-pipelines-with-apache-airflow.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Dominik Benz</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/flow-is-in-the-air-best-practices-of-building-analytical-data-pipelines-with-apache-airflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dominik Benz&lt;/strong&gt; (&amp;#64;john_maverick)&lt;/p&gt;
&lt;p&gt;Dominik Benz holds a PhD from the University of Kassel in the field of Data Mining on the Social Web. Since 2012 he is working as a Big Data Engineer at Inovex GmbH. In this time, he was involved in several projects concerned with establishing analytical data platforms in various companies. He is most experienced in tools around the Hadoop Ecosystem like Apache Hive and Spark, and has hands-on experience with productionizing analytical applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apache Airflow is an Open-Source python project which facilitates an intuitive programmatic definition of analytical data pipelines. Based on 2+ years of productive experience, we summarize its core concepts, detail on lessons learned and set it in context with the Big Data Analytics Ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Motivation &amp;amp; Outline&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Creating, orchestrating and running multiple data processing or analysis steps may cover a substantial portion of a Data Engineer and Data Scientist business. A widely adopted notion for this process is a &amp;quot;data pipeline&amp;quot; - which consists mainly of a set of &amp;quot;operators&amp;quot; which perform a particular action on data, with the possibility to specify dependencies among those. Real-Life examples may include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Importing several files with different formats into a Hadoop platform, perform data cleansing, and training a machine learning model on the result&lt;/li&gt;
&lt;li&gt;perform feature extraction on a given dataset, apply an existing deep learning model to it, and write the results in the backend of a microservice&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Apache Airflow is an open-source Python project developed by AirBnB which facilitates the programmatic definition of such pipelines. Features which differentiate Airflow from similar projects like Apache Oozie, Luigi or Azkaban include (i) its pluggable architecture with several extension points (ii) the programmatic approach of &amp;quot;workflow is code&amp;quot; and (iii) its tight relationship with the the Python as well as the Big Data Analytics Ecosystem. Based on several years of productive usage, we briefly summarize the core concepts of Airflow, and detail in-depth on lessons learned and best practices from our experience. These include hints for getting efficient quickly with Airflow, approaches to structure workflows, integrating it in an enterprise landscape, writing plugins and extentions, and maintaining it in productive environment. We conclude with a comparison with other analytical workflow engines and summarize why we have chosen Airflow.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Questions answered by this talk&lt;/em&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What are the core concepts of Apache Airflow?&lt;/li&gt;
&lt;li&gt;How can Airflow help me with moving data pipelines from analytics to production?&lt;/li&gt;
&lt;li&gt;Which concepts of Airflow make it more slim and more efficient compared to Apache Oozie?&lt;/li&gt;
&lt;li&gt;How can I specify dynamic dependencies at runtime between my analytical data processing steps?&lt;/li&gt;
&lt;li&gt;Which facilities does Airflow offer to enable automation and orchestration of analytical tasks?&lt;/li&gt;
&lt;li&gt;How can I extend the built-in facilities of Airflow by writing Python plugins?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;People who benefit most from this talk&lt;/em&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Data Scientists who are looking for a slim library to automate and control their data processing steps&lt;/li&gt;
&lt;li&gt;Data Engineers who want to save time debugging static workflow definitions (e.g. in XML)&lt;/li&gt;
&lt;li&gt;Project leaders interested in tools which lower the burden of moving from analytics to production&lt;/li&gt;
&lt;li&gt;Hadoop Cluster administrators eager to save cluster resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="workflow"></category><category term="data pipeline"></category><category term="data-science"></category><category term="analytics"></category></entry><entry><title>Getting Scikit-Learn To Run On Top Of Pandas</title><link href="https://pyvideo.org/pycon-de-2017/getting-scikit-learn-to-run-on-top-of-pandas.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Ami Tavory</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/getting-scikit-learn-to-run-on-top-of-pandas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Ami Tavory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ami is a data scientist at Facebook Research's Core Data Science group. He previously worked as a machine learning researcher in the fields of bioinformatics and algorithmic trading. In 2010 he received a Ph.D in Electrical Engineering from Tel Aviv University, in the field of financial information theory. His bachelor's and master's are from Tel Aviv University too.&lt;/p&gt;
&lt;p&gt;Ami uses Python and C++ for data analysis. He contributed to various open source projects, and is the author of a libstd C++ extension shipped with g++ (pb_ds: policy-based data structures).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Scikit-Learn is built directly over numpy, Python's numerical array library. Pandas adds to numpy metadata and higher-level munging capabilities. This talk describes how to intelligently auto-wrap Scikit-Learn for creating a version that can leverage pandas's added features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Scikit-Learn is the de-facto standard Python library for general-purpose machine learning. It operates over NumPy, an efficient, but low-level, homogeneic array library. Pandas adds to NumPy metadata, heterogeneity, and higher-leve munging capabilities.&lt;/p&gt;
&lt;p&gt;In the field of visualization, newer generation libraries, e.g., Seaborn and Bokeh, are providing safer, more readable, and higher-level functionality, by operating over Pandas data structures. Some of these are implemented using Matplotlib, a lower-level NumPy-based plotting library.&lt;/p&gt;
&lt;p&gt;This talk describes a library for a Pandas-based version of sickit-learn. Here, too, giving a Pandas interface to a machine-learning library, provides code which is safer to use, more readable, and allows direct integration with Pandas's higher-level munging capabilities.&lt;/p&gt;
&lt;p&gt;Due to the large-scale, and evolving nature, of sicikit-learn's codebase, it is infeasible to manually wrap it. Except for a small number of intentional deviations from sickit-learn, the library wraps Scikit-Learn modules lazily through module and class introspection, and dynamic module loading.&lt;/p&gt;
&lt;p&gt;Following a short review of the relevant points of Pandas and Scikit-Learn, the talk is roughly divided into two aspects:     Scikit-Learn And Pandas User Perspective     Safety Advantages Of Pandas-Based Estimators     Using Metadata For Inter-Instance Aggregated Features And Cross-Validation     Using Metadata For Advanced Meta-Algorithms: Stacking, Nested Labeled And Stratified Cross-Valdiation     Python Develop Perspective     Unique Challenges Of Scikit-Learn Introspection And Decoration     Two Approaches For Wrapping Scikit-Learn Estimators     Lazy Dynamic Module Loading&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="code-introspection"></category><category term="scikit-learn"></category><category term="pandas"></category><category term="data-science"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Large-scale machine learning pipelines using Luigi, PySpark and scikit-learn</title><link href="https://pyvideo.org/pycon-de-2017/large-scale-machine-learning-pipelines-using-luigi-pyspark-and-scikit-learn.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Alexander Bauer</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/large-scale-machine-learning-pipelines-using-luigi-pyspark-and-scikit-learn.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Alexander Bauer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alexander Bauer holds a Ph.D. in computer science. He has around 10 years industry experience, currently leading a team of data scientists at Lidl, one of the largest global discount supermarket chains. He is a Kaggle Master and regular speaker at the Frankfurt Predictive Analytics Meetup. He believes in agile software development practices and promotes Python as a primary language for data science applications in production.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For prescriptive analytics applications, data science teams need to design, build and maintain complex machine learning pipelines. In this talk, we demonstrate how such pipelines can be implemented in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data science teams working on real-world prescriptive analytics applications face the challenge to design, build and maintain considerably complex machine learning pipelines on a daily basis. Such pipelines include parsing data from multiple data sources, extracting relevant predictive features, executing training, validation, prediction steps and finally optimizing actions to meet desired business outcome so that they can be shared and visualized to business users. In this talk, we demonstrate how such pipelines can be implemented end-to-end in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn. We will share our lessons learned from using this framework in a real-world demand forecasting use case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data-science"></category><category term="analytics"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Modern ETL-ing with Python and Airflow (and Spark)</title><link href="https://pyvideo.org/pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Tamara Mendt</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Tamara Mendt&lt;/strong&gt; (&amp;#64;TamaraMendt)&lt;/p&gt;
&lt;p&gt;Tamara Mendt is a Data Engineer at HelloFresh, a meal kit delivery service headquartered in Berlin, and one of the top 3 tech startups to come out of Europe over the past 4 years. She devotes her time to building data pipelines and designing and maintaining the company's data infrastructure. Tamara has a computer engineering degree from her native country Venezuela, and an Erasmus Mundus Masters degree in IT for Business Intelligence. She wrote her Master thesis at the TU Berlin with the research group where Apache Flink was born. At HelloFresh she is continuing to work with distributed technologies such has Apache Hadoop, Apache Kafka and Apache Spark to cope with the scalability that the fast growing company requires for dealing with their data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The challenge of data integration is real. The sheer amount of tools that exist to address this problem is proof that organizations struggle with it. This talk will discuss the inherent challenges of data integration, and show how it can be tackled using Python and Apache Airflow and Apache Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The way organizations analyze their data has evolved very quickly since the beginning of the millennium. The development of Hadoop, and the explosion in the variety of data that companies are dealing with nowadays, has fostered the appearance of the concept of data lake, and the shift of traditional ETL (extract, transform, load), to ELT (extract, load, transform). Yet, the challenge of integrating data to obtain valuable insights still remains, and despite the hype and attention being focused on data, very few organizations have actually managed to become data driven. In this talk I will present insights into how we are currently building data pipelines using Python (as a replacement to high level ETL software), Apache Airflow as a scheduler to our coded transformations, and Apache Spark for achieve scalability. Though building data pipelines is not the only element required to become data driven, it is a crucial one, and I hope to encourage the audience to use these open source technologies in their own ETL-ing (or ELT-ing) efforts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data"></category><category term="data-science"></category><category term="pipeline"></category></entry><entry><title>Sport analysis with Python</title><link href="https://pyvideo.org/pycon-de-2017/sport-analysis-with-python.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Thuy Le</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/sport-analysis-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Give an example data of the IoT sport case for instance the information of football match of a team (the positions, velocities of each player with are recorded in every 20 millisecond).&lt;/li&gt;
&lt;li&gt;We use Python to analysis and processing data (calculate the match time, analyst the activities of each player such as time in the bench, time in the pitch, ... )&lt;/li&gt;
&lt;li&gt;We use Tableau to visualize data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="devops"></category><category term="analytics"></category><category term="data-science"></category><category term="python"></category></entry><entry><title>Synthetic Data for Machine Learning Applications</title><link href="https://pyvideo.org/pycon-de-2017/synthetic-data-for-machine-learning-applications.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Hendrik Niemeyer</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/synthetic-data-for-machine-learning-applications.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dr. Hendrik Niemeyer&lt;/strong&gt; (&amp;#64;hniemeye)&lt;/p&gt;
&lt;p&gt;Data Scientist working on predictive analytics with data from pipeline inspection measurements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk I will show how we use real and synthetic data to create successful models for risk assessing pipeline anomalies. The main focus is the estimation of the difference in the statistical properties of real and generated data by machine learning methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ROSEN provides predictive analytics for pipelines by detecting and risk assessing anomalies from data gathered by inline inspection measurement devices. Due to budget reasons (pipelines need to be dug up to get acess) ground truth data for machine learning applications in this field are usually scarce, imbalanced and not available for all existing configurations of measurement devices. This creates the need for synthetic data (using FEM simulations and unsupervised learning algorithms) in order to be able to create successful models.&lt;/p&gt;
&lt;p&gt;But a naive mixture of real-world and synthetic samples in a model does not necessarily yield to an increased predictive performance because of differences in the statistical distributions in feature space. I will show how we evaluate the use of synthetic data besides simple visual inspection. Manifold learning (e.g. TSNE) can be used to gain an insight whether real and generated data are inherently different.
Quantitative approaches like classifiers trained to discriminate between these types of data provide a non visual insight whether a &amp;quot;synthetic gap&amp;quot; in the feature distributions exists.&lt;/p&gt;
&lt;p&gt;If the synthetic data is useful for model building careful considerations have to be applied when constructing cross validation folds and test sets to prevent biased estimates of the model performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data-science"></category><category term="python"></category><category term="machine learning"></category><category term="ai"></category></entry><entry><title>The eye of the Python, an eye tracking system. From zero to... what eye learned</title><link href="https://pyvideo.org/pycon-de-2017/the-eye-of-the-python-an-eye-tracking-system-from-zero-to-what-eye-learned.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Samuel Muñoz Hidalgo</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/the-eye-of-the-python-an-eye-tracking-system-from-zero-to-what-eye-learned.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Samuel Muñoz Hidalgo&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I am Samuel (Samu for friends). With a curious mind I studied computer science, then focused in machine learning and IoT as a professional career. I haven't given up on my plan to take over the world; that's why my coworkers know a crazy idea is coming out, when I can't hide any longer a mischievous smile. I like to meet people and understand other points of view, and in return I like to show what I can do and teach what I have mastered. But, what drives me crazy is rollerskating with disco music.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Is it possible to predict the point in the screen where a person is looking at? Easy to say but hard to do. An eye tracking system is the perfect project to learn the difficulties of applied machine learning. From gathering training data to building the final software with an acceptable performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk I will show how to build an eye tracking system from scratch.&lt;/p&gt;
&lt;p&gt;Once the approach (machine learning) and the tools (Python ecosystem) are set, the important tasks are:     Making users addicted to a game built with Pygame in order to generate data.     Unleashing the power of deep learning over GPU with Tensorflow to train an Artificial Neural Network.     Exploiting the training model in real time so as to control a computer with the eyes with PyAutoGUI.&lt;/p&gt;
&lt;p&gt;The path is full of pitfalls and every clear single step to the goal turns out to be a mountain of small but very important subtasks. Every iteration is a continuous struggle just to gain a bit of accuracy.&lt;/p&gt;
&lt;p&gt;Despite these efforts, at the very end we will see the difference between the theoretical and the real world. Our engineering skills will determine the usability of the new interface that allows us to move the mouse with our eyes; and we will learn that things don't need to be perfect, because humans are ...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data-science"></category><category term="use-case"></category><category term="python"></category><category term="ai"></category><category term="deep learning"></category></entry><entry><title>The Python Ecosystem for Data Science: A Guided Tour</title><link href="https://pyvideo.org/pycon-de-2017/the-python-ecosystem-for-data-science-a-guided-tour.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Christian Staudt</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/the-python-ecosystem-for-data-science-a-guided-tour.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Christian Staudt&lt;/strong&gt; (&amp;#64;C_L_Staudt)&lt;/p&gt;
&lt;p&gt;I am an independent data scientist with a background in computer science, in-depth in algorithms, data analysis, high-performance computing and software engineering. My current interests include machine learning and data visualization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pythonistas have access to an extensive collection of tools for data analysis. The space of tools is best understood as an ecosystem: Libraries build upon each other, and a good library fills an ecological niche by doing certain jobs well. This is a guided tour of the Python data science ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Python Ecosystem for Data Science: A Guided Tour&lt;/p&gt;
&lt;p&gt;Python is on its way to becoming the lingua franca of data science, and Pythonistas have access to an impressive and extensive collection of tools for data analysis. Here, a data scientist needs to see the forest for the trees: The space of tools is best understood as an ecosystem, where libraries build upon each other, and where a good library fills an ecological niche by doing certain jobs well. This talk is a guided tour of the Python data science ecosystem. More than a list of libraries, it aims to provide some structure, classing tools by type of data, size of data, and type of analysis. In our tour, we visit a number of areas, including working with tabular data (numpy, pandas, dask, ...) and graph data (e.g. networkx), statistics (e.g. statsmodels), machine learning (scikit-learn, ...), data visualization (matplotlib, seaborn, bokeh, ...). Aspiring data scientists, and everyone else working with data, should find this useful for selecting the right tools for their next data-driven project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="use-case"></category><category term="business"></category><category term="ai"></category><category term="analytics"></category><category term="data-science"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Time series feature extraction with tsfresh - “get rich or die overfitting”</title><link href="https://pyvideo.org/pycon-de-2017/time-series-feature-extraction-with-tsfresh-get-rich-or-die-overfitting.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Nils Braun</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/time-series-feature-extraction-with-tsfresh-get-rich-or-die-overfitting.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Nils Braun&lt;/strong&gt; (&amp;#64;_nilsbraun)&lt;/p&gt;
&lt;p&gt;Currently I am doing my PhD in Particle Physics - which mainly involves development of software in a large collaboration. I love working with Python and C++ to process large amounts of data. Of course it needs to be processed as quickly as possible. I am working on the core reconstruction algorithms for our experiment, which are steered and controlled using Python. Apart from that, I was working as a Data Science Engineer for Blue Yonder, a leading machine learning company, where the idea for tsfresh was born. I am still heavily involved in the project. When I am not writing code, I am updating myself on the newest technical geek stuff (mostly cloud computing and deep learning) or play the guitar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Have you ever thought about developing a time series model to predict stock prices? Or do you consider log time series from the operation of cloud resources as being more compelling? In this case you really should consider using the time series feature extraction package tsfresh for your project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Trends such as the Internet of Things (IoT), Industry 4.0, and precision medicine are driven by the availability of cheap sensors and advancing connectivity, which among others increases the availability of temporally annotated data. The resulting time series are the basis for manifold machine learning applications. Examples are the classification of hard drives into risk classes concerning specific defect, the log analysis of server farms for detecting intruders, or regression tasks like the prediction of the remaining lifespan of machinery. Tsfresh also allows to easily setup a machine learning pipeline that predicts stock prices, which we will demonstrate live during the presentation ;). The problem of extracting and selecting relevant features for classification or regression is these domains is especially hard to solve, if each label or regression target is associated with several time series and meta-information simultaneously – which is a common pattern in industrial applications. This talk introduces a distributed and parallel feature extraction and selection algorithm – the recently published Python library tsfresh. The fully automated extraction and importance selection does not only allow to reach better machine learning classification scores, but in combination with the speed of the package, also allows to incorporate tsfresh into automated AI-pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="pydata"></category><category term="time series"></category><category term="data-science"></category><category term="machine learning"></category><category term="python"></category><category term="ai"></category></entry><entry><title>Turbodbc: Turbocharged database access for data scientists</title><link href="https://pyvideo.org/pycon-de-2017/turbodbc-turbocharged-database-access-for-data-scientists.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Michael König</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/turbodbc-turbocharged-database-access-for-data-scientists.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Michael König&lt;/strong&gt; (&amp;#64;turbodbc)&lt;/p&gt;
&lt;p&gt;Michael is a senior software engineer at Blue Yonder GmbH. He holds a PhD in physics, practices test-driven development, and digs Clean Code in C++ and Python. In the last five years, he invested more money in table tennis gear than in smartphones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Python's database API 2.0 is well suited for transactional database workflows, but not so much for column-heavy data science. This talk explains how the ODBC-based turbodbc database module extends this API with first-class, efficient support for familiar NumPy and Apache Arrow data structures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk introduces the open source Python database module turbodbc. It uses standard ODBC drivers to connect with virtually any database and is a viable (and often faster) alternative to &amp;quot;native&amp;quot; Python drivers.&lt;/p&gt;
&lt;p&gt;Briefly recounting the painful story of how data scientists previously used our analytics database, I explain why turbodbc was created and what distinguishes it from other ODBC modules. Sketching the flow of data from databases via drivers and Python modules to consumable Python objects, I motivate a few extensions to the standard database API 2.0 that turbodbc has made. These extensions heavily use NumPy arrays and Apache Arrow tables to provide data scientists with both familiar and efficient binary data structures they can further work on. I conclude my talk with benchmark results for a few databases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="numpy"></category><category term="database"></category><category term="python"></category><category term="data-science"></category><category term="analytics"></category></entry><entry><title>How to use pandas the wrong way</title><link href="https://pyvideo.org/pycon-italia-2017/how-to-use-pandas-the-wrong-way.html" rel="alternate"></link><published>2017-04-08T00:00:00+00:00</published><updated>2017-04-08T00:00:00+00:00</updated><author><name>Pietro Battiston</name></author><id>tag:pyvideo.org,2017-04-08:pycon-italia-2017/how-to-use-pandas-the-wrong-way.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The &lt;strong&gt;pandas&lt;/strong&gt; library represents a very efficient and convenient tool
for data manipulation, but sometimes hides unexpected pitfalls which can
arise in various and sometimes unintelligible ways.&lt;/p&gt;
&lt;p&gt;By briefly referring to some aspects of the internals, I will review
specific situations in which a change of approach can, for instance,
make a difference in terms of performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE (April 12, 2017) - SLIDES:&lt;/strong&gt; the talk had very few slides;
still, you can find those few, together with the notebooks I used live,
&lt;a class="reference external" href="https://pietrobattiston.it/python:pycon"&gt;here&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data-science"></category><category term="pandas"></category><category term="pydata"></category></entry><entry><title>Data Science &amp; Data Visualization in Python. How to harness power of Python for social good?</title><link href="https://pyvideo.org/pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Radovan Kavicky</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python as an Open Data Science tool offers many libraries for data visualization and I will show you how to use and combine the best. I strongly believe that power of data is not only in the information &amp;amp; insight that data can provide us, Data is and can be really beautiful and can not only transform our perception but also the world that we all live in.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my talk I will primarily focus on answering/offer the answer to these questions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Why we need data science and why more and more people should be really interested in analyzing data and data visualization? (motivation)&lt;/li&gt;
&lt;li&gt;What is data science and how to start doing it in Python? (introduction of procedures, tools, most popular IDE-s for Python, etc.)&lt;/li&gt;
&lt;li&gt;What tools for data analysis and data visualization Python offers? (in each stage of analysis the best libraries will be shown for the specific purpose; as for data visualization we will focus particularly on Bokeh, Seaborn, Plotly and use of Jupyter Notebook and Plotly)&lt;/li&gt;
&lt;li&gt;How to 'unlock' the insight hidden in data through Python and how to use it to transform not only public administration or business, but ultimately the transformation of the whole society and economy towards the insight &amp;amp; knowledge based? (potential of data science)&lt;/li&gt;
&lt;li&gt;Open Data, Open Government Partnership, Open Public Administration &amp;amp; all the advantages of Open Data Science &amp;amp; Python. Data-Driven Approach. Everywhere. Now. (the end of talk +vision)&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="python"></category><category term="data-science"></category><category term="data-visualization"></category><category term="analytics"></category><category term="PyData"></category><category term="PyDataBLN"></category><category term="PyDataBerlin"></category><category term="PyDataBA"></category><category term="PyDataBratislava"></category><category term="talk"></category><category term="Data"></category><category term="Bokeh"></category><category term="Social Good"></category><category term="datascience"></category><category term="jupyter"></category><category term="open science"></category><category term="open data science"></category><category term="DataVisualization"></category><category term="data-analysis"></category><category term="analysis"></category><category term="matplotlib"></category><category term="numpy"></category><category term="data wrangling"></category><category term="jupyter notebook"></category><category term="pandas"></category><category term="machine learning"></category><category term="deep learning"></category><category term="Open Data"></category><category term="Citizen Data Science"></category></entry><entry><title>Marketing Data Science</title><link href="https://pyvideo.org/pydata-barcelona-2017/marketing-data-science.html" rel="alternate"></link><published>2017-05-21T15:45:00+02:00</published><updated>2017-05-21T15:45:00+02:00</updated><author><name>Joaquin Pais</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/marketing-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Marketing Data Science: how digital marketing needs data science to survive.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Digital Marketing is changing the way corporations and brands communicates with customers. The investments in Digital Marketing are skyrocketing around the world in a multi-billion industry.&lt;/p&gt;
&lt;p&gt;But consumers and customers (specially millenials) does not trust advertising. Different approaches are being made by corporations like Inbound Marketing Strategies. My presentation is about how Digital Marketing needs Data Science in order to better understand the customer needs and generate new niches of interest for companies. Companies investing in Digital Marketing should take a close look at Data Science Platforms like Python in order to better gather inisghts, create segments and personalice a customer experience.&lt;/p&gt;
&lt;p&gt;I will provide some short examples about how we are using python jupyter notebook environment in order to gain inisghts from customers using IBM Watson API, generating new segmentation and customer experiences.&lt;/p&gt;
</summary><category term="marketing"></category><category term="data science"></category><category term="pandas"></category></entry><entry><title>Matplotlib Plot Tutorial: Histograms, Scatter Plots &amp; Legend</title><link href="https://pyvideo.org/datacamp/Matplotlib-Plot-Tutorial-For-Beginners.html" rel="alternate"></link><published>2016-02-01T00:00:00+00:00</published><updated>2016-02-01T00:00:00+00:00</updated><author><name>Filip Schouwenaars</name></author><id>tag:pyvideo.org,2016-02-01:datacamp/Matplotlib-Plot-Tutorial-For-Beginners.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Matplotlib makes it easy to create meaningful and insightful plots. In this beginner video, you will learn how to build various types of data visualizations such as histograms, scatter plots and line plots. You will also see how to customize them to make them more visually appealing and interpretable.&lt;/p&gt;
&lt;p&gt;Want to do the corresponding exercises? Go to our &lt;cite&gt;Python For Data Science Tutorial &amp;lt;https://www.datacamp.com/courses/intro-to-python-for-data-science&amp;gt;&lt;/cite&gt; where you can do them for free.&lt;/p&gt;
</summary><category term="Matplotlib"></category><category term="data science"></category><category term="data visualization"></category><category term="tutorial"></category><category term="DataCamp"></category></entry><entry><title>Dev Ops meets Data Science Taking models from prototype to production with Docker</title><link href="https://pyvideo.org/pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Andy Terrel</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We present the evolution of a model to a production API that can scale to large e-commerce needs. On the journey we discuss metrics of success and how to use the Kubernetes cluster manager and associated tools for deploy. In addition to the use of these tools we highlight how to make use of the cluster management system for further testing and experimentation with your models.&lt;/p&gt;
&lt;p&gt;The chasm between data science and dev ops is often wide and impenetrable, but the two fields have more in common than meets the eye. Every data scientist will be able to lean in and help their career by investing in a basic understanding the basic principles of dev ops. In this talk I present the notions of service level indicators, objectives, and agreements. I cover the rigorous monitoring and testing of services. Finally we demonstrate how to build a basic data science workflow and push to production level APIs with Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes is an opinionated container cluster manager with an easy to use, robust interface. It can be use on very small and very large clusters. Docker is a container system that allows one to build code in an isolated environment. Paired with a container manager such as Kubernetes we are able to manage millions of instances as needed for a production deployment. These tools are two of many different options but are considered among the best open source solutions available.&lt;/p&gt;
</summary><category term="Data"></category><category term="data science"></category><category term="docker"></category><category term="models"></category><category term="science"></category></entry><entry><title>You got your engineering in my Data Science: Addressing the reproducibility crisis</title><link href="https://pyvideo.org/pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Jon Bodner</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering"&gt;http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. Unfortunately, multiple recent studies have been found to be unreliable and non-reproducible. Adopting techniques from software engineering might help mitigate some of these problems.&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. It makes sense of everything from cancer trials to package delivery logistics. But all is not well with data science. Over the past decade, multiple studies have been found to be unreliable and non-reproducible when other scientists tried to recreate their results. This is due to a variety of factors, including fraud, pressure to publish, improper data handling practices, and bugs in analytic tools.&lt;/p&gt;
&lt;p&gt;The problems faced by data science mirror problems that software engineering has been trying to solve. While there are no silver bullets to guarantee quality software, techniques have been developed over time that have improved quality and reliability. Some of these techniques, including open source, version control, automation, and fuzzing could be adapted to the data science domain to improve reliability and help address the reproducibility crisis.&lt;/p&gt;
</summary><category term="Data"></category><category term="data science"></category><category term="engineering"></category><category term="reproducibility"></category></entry><entry><title>Keynote: How Open Data Science Opens the World of Innovation</title><link href="https://pyvideo.org/pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Robert Cohn</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Innovation today appears to be instantaneous in large part due to open source technology. Open Data Science is no exception. Python, a pillar in the Open Data Science bedrock, is well positioned to harvest innovation in software and with Anaconda, it’s also well positioned to capitalize on the latest hardware innovations. Anaconda and Intel are blazing a path for the Python community to take advantage of cognitive computing, including machine learning and deep learning.&lt;/p&gt;
&lt;p&gt;In this keynote, Peter and Robert will talk about how Open Data Science––a connected ecosystem of data, analytics and compute––streamlines the path to high performance and innovation to achieve breakthrough results.&lt;/p&gt;
</summary><category term="Data"></category><category term="data science"></category><category term="science"></category></entry><entry><title>Scaling up to Big Data Devops for Data Science</title><link href="https://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Marck Vaisman</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</summary><category term="big data"></category><category term="Data"></category><category term="data science"></category><category term="devops"></category><category term="scaling"></category><category term="science"></category></entry><entry><title>When Worlds Collide: Productionalizing a Data Science Model</title><link href="https://pyvideo.org/pydata-chicago-2016/when-worlds-collide-productionalizing-a-data-science-model.html" rel="alternate"></link><published>2016-08-28T00:00:00+00:00</published><updated>2016-08-28T00:00:00+00:00</updated><author><name>Tudor Radoaca</name></author><id>tag:pyvideo.org,2016-08-28:pydata-chicago-2016/when-worlds-collide-productionalizing-a-data-science-model.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;On our first data science project at Shiftgig, the data science and engineering teams had to build software that was production-ready while maintaining the flexibility of a data science sandbox. Although these seem like irreconcilable goals, they forced us to improve inter-team communication and ultimately helped create a great product. We’ll walk through our process and the lessons we learned.&lt;/p&gt;
</summary><category term="Data"></category><category term="data science"></category><category term="model"></category><category term="science"></category></entry><entry><title>Keynote: Using Data Science for Social Good: Examples, Opportunities, and Challenges</title><link href="https://pyvideo.org/pydata-chicago-2016/keynote-using-data-science-for-social-good-examples-opportunities-and-challenges.html" rel="alternate"></link><published>2016-08-27T00:00:00+00:00</published><updated>2016-08-27T00:00:00+00:00</updated><author><name>Rayid Ghani</name></author><id>tag:pyvideo.org,2016-08-27:pydata-chicago-2016/keynote-using-data-science-for-social-good-examples-opportunities-and-challenges.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
</summary><category term="Data"></category><category term="data science"></category><category term="science"></category></entry><entry><title>How do I apply a function to a pandas Series or DataFrame?</title><link href="https://pyvideo.org/data-school/pandas-30-apply-function.html" rel="alternate"></link><published>2016-08-23T00:00:00+00:00</published><updated>2016-08-23T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-08-23:data-school/pandas-30-apply-function.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever struggled to figure out the differences between apply, map, and applymap? In this video, I'll explain when you should use each of these methods and demonstrate a few common use cases. Watch the end of the video for three important announcements!&lt;/p&gt;
&lt;p&gt;This is video 30 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="NumPy"></category></entry><entry><title>How do I create a pandas DataFrame from another object?</title><link href="https://pyvideo.org/data-school/pandas-29-dummy-dataframe.html" rel="alternate"></link><published>2016-08-16T00:00:00+00:00</published><updated>2016-08-16T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-08-16:data-school/pandas-29-dummy-dataframe.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever needed to create a DataFrame of &amp;quot;dummy&amp;quot; data, but without reading from a file? In this video, I'll demonstrate how to create a DataFrame from a dictionary, a list, and a NumPy array. I'll also show you how to create a new Series and attach it to the DataFrame.&lt;/p&gt;
&lt;p&gt;This is video 29 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="NumPy"></category></entry><entry><title>How do I change display options in pandas?</title><link href="https://pyvideo.org/data-school/pandas-28-customize-display.html" rel="alternate"></link><published>2016-08-09T00:00:00+00:00</published><updated>2016-08-09T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-08-09:data-school/pandas-28-customize-display.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever wanted to change the way your DataFrame is displayed? Perhaps you needed to see more rows or columns, or modify the formatting of numbers? In this video, I'll demonstrate how to change the settings for five common display options in pandas.&lt;/p&gt;
&lt;p&gt;This is video 28 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I avoid a SettingWithCopyWarning in pandas?</title><link href="https://pyvideo.org/data-school/pandas-27-setting-with-copy-warning.html" rel="alternate"></link><published>2016-08-02T00:00:00+00:00</published><updated>2016-08-02T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-08-02:data-school/pandas-27-setting-with-copy-warning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you've been using pandas for a while, you've likely encountered a SettingWithCopyWarning. The proper response is to modify your code appropriately, not to turn off the warning! In this video, I'll show you two common scenarios in which this warning arises, explain why it's occurring, and then demonstrate how to address it.&lt;/p&gt;
&lt;p&gt;This is video 27 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="missing data"></category></entry><entry><title>How do I find and remove duplicate rows in pandas?</title><link href="https://pyvideo.org/data-school/pandas-26-duplicate-data.html" rel="alternate"></link><published>2016-07-26T00:00:00+00:00</published><updated>2016-07-26T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-07-26:data-school/pandas-26-duplicate-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;During the data cleaning process, you will often need to figure out whether you have duplicate data, and if so, how to deal with it. In this video, I'll demonstrate the two key methods for finding and removing duplicate rows, as well as how to modify their behavior to suit your specific needs.&lt;/p&gt;
&lt;p&gt;This is video 26 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="duplicate data"></category></entry><entry><title>How do I work with dates and times in pandas?</title><link href="https://pyvideo.org/data-school/pandas-25-dates-and-times.html" rel="alternate"></link><published>2016-07-19T00:00:00+00:00</published><updated>2016-07-19T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-07-19:data-school/pandas-25-dates-and-times.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you have dates and times in your DataFrame and you want to analyze your data by minute, month, or year. What should you do? In this video, I'll demonstrate how you can convert your data to &amp;quot;datetime&amp;quot; format, enabling you to access a ton of convenient attributes and perform datetime comparisons and mathematical operations.&lt;/p&gt;
&lt;p&gt;This is video 25 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="data visualization"></category></entry><entry><title>How do I create dummy variables in pandas?</title><link href="https://pyvideo.org/data-school/pandas-24-dummy-variables.html" rel="alternate"></link><published>2016-07-12T00:00:00+00:00</published><updated>2016-07-12T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-07-12:data-school/pandas-24-dummy-variables.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you want to include a categorical feature in your machine learning model, one common solution is to create dummy variables. In this video, I'll demonstrate three different ways you can create dummy variables from your existing DataFrame columns. I'll also show you a trick for simplifying your code that was introduced in pandas 0.18.&lt;/p&gt;
&lt;p&gt;This is video 24 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="machine learning"></category></entry><entry><title>More of your pandas questions answered!</title><link href="https://pyvideo.org/data-school/pandas-23-viewer-questions.html" rel="alternate"></link><published>2016-07-05T00:00:00+00:00</published><updated>2016-07-05T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-07-05:data-school/pandas-23-viewer-questions.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, I'm answering a few of the pandas questions I've received in the YouTube comments: Could you explain how to read the pandas documentation? What is the difference between ufo.isnull() and pd.isnull(ufo)? Why are DataFrame slices inclusive when using .loc, but exclusive when using .iloc? How do I randomly sample rows from a DataFrame?&lt;/p&gt;
&lt;p&gt;This is video 23 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="reproducibility"></category></entry><entry><title>How do I use pandas with scikit-learn to create Kaggle submissions?</title><link href="https://pyvideo.org/data-school/pandas-22-prepare-for-machine-learning.html" rel="alternate"></link><published>2016-06-28T00:00:00+00:00</published><updated>2016-06-28T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-28:data-school/pandas-22-prepare-for-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you been using scikit-learn for machine learning, and wondering whether pandas could help you to prepare your data and export your predictions? In this video, I'll demonstrate the simplest way to integrate pandas into your machine learning workflow, and will create a submission for Kaggle's Titanic competition in just a few lines of code!&lt;/p&gt;
&lt;p&gt;This is video 22 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="scikit-learn"></category><category term="machine learning"></category></entry><entry><title>How do I make my pandas DataFrame smaller and faster?</title><link href="https://pyvideo.org/data-school/pandas-21-reduce-dataframe-size.html" rel="alternate"></link><published>2016-06-21T00:00:00+00:00</published><updated>2016-06-21T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-21:data-school/pandas-21-reduce-dataframe-size.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Are you working with a large dataset in pandas, and wondering if you can reduce its memory footprint or improve its efficiency? In this video, I'll show you how to do exactly that in one line of code using the &amp;quot;category&amp;quot; data type, introduced in pandas 0.15. I'll explain how it works, and how to know when you shouldn't use it.&lt;/p&gt;
&lt;p&gt;This is video 21 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>When should I use the "inplace" parameter in pandas?</title><link href="https://pyvideo.org/data-school/pandas-20-inplace-parameter.html" rel="alternate"></link><published>2016-06-14T00:00:00+00:00</published><updated>2016-06-14T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-14:data-school/pandas-20-inplace-parameter.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We've used the &amp;quot;inplace&amp;quot; parameter many times during this video series, but what exactly does it do, and when should you use it? In this video, I'll explain how &amp;quot;inplace&amp;quot; affects methods such as &amp;quot;drop&amp;quot; and &amp;quot;dropna&amp;quot;, and why it is always False by default.&lt;/p&gt;
&lt;p&gt;This is video 20 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="missing data"></category></entry><entry><title>How do I select multiple rows and columns from a pandas DataFrame?</title><link href="https://pyvideo.org/data-school/pandas-19-select-dataframe-rows-and-columns.html" rel="alternate"></link><published>2016-06-07T00:00:00+00:00</published><updated>2016-06-07T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-07:data-school/pandas-19-select-dataframe-rows-and-columns.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever been confused about the &amp;quot;right&amp;quot; way to select rows and columns from a DataFrame? pandas gives you an incredible number of options for doing so, but in this video, I'll outline the current best practices for row and column selection using the loc, iloc, and ix methods.&lt;/p&gt;
&lt;p&gt;This is video 19 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>What do I need to know about the pandas index? (Part 2)</title><link href="https://pyvideo.org/data-school/pandas-18-index-part-2.html" rel="alternate"></link><published>2016-06-02T00:00:00+00:00</published><updated>2016-06-02T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-02:data-school/pandas-18-index-part-2.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In part two of our discussion of the index, we'll switch our focus from the DataFrame index to the Series index. After discussing index-based selection and sorting, I'll demonstrate how automatic index alignment during mathematical operations and concatenation enables us to easily work with incomplete data in pandas.&lt;/p&gt;
&lt;p&gt;This is video 18 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="missing data"></category></entry><entry><title>What do I need to know about the pandas index? (Part 1)</title><link href="https://pyvideo.org/data-school/pandas-17-index-part-1.html" rel="alternate"></link><published>2016-05-31T00:00:00+00:00</published><updated>2016-05-31T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-31:data-school/pandas-17-index-part-1.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The DataFrame index is core to the functionality of pandas, yet it's confusing to many users. In this video, I'll explain what the index is used for and why you might want to store your data in the index. I'll also demonstrate how to set and reset the index, and show how that affects the DataFrame's shape and contents.&lt;/p&gt;
&lt;p&gt;This is video 17 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</summary><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry></feed>