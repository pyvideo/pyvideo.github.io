<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Data Science</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_data-science.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-07-23T00:00:00+00:00</updated><subtitle></subtitle><entry><title>How do I apply a function to a pandas Series or DataFrame?</title><link href="https://pyvideo.org/data-school/pandas-30-apply-function.html" rel="alternate"></link><published>2016-08-23T00:00:00+00:00</published><updated>2016-08-23T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-08-23:/data-school/pandas-30-apply-function.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever struggled to figure out the differences between apply, map, and applymap? In this video, I'll explain when you should use each of these methods and demonstrate a few common use cases. Watch the end of the video for three important announcements!&lt;/p&gt;
&lt;p&gt;This is video 30 of …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever struggled to figure out the differences between apply, map, and applymap? In this video, I'll explain when you should use each of these methods and demonstrate a few common use cases. Watch the end of the video for three important announcements!&lt;/p&gt;
&lt;p&gt;This is video 30 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="NumPy"></category></entry><entry><title>How do I create a pandas DataFrame from another object?</title><link href="https://pyvideo.org/data-school/pandas-29-dummy-dataframe.html" rel="alternate"></link><published>2016-08-16T00:00:00+00:00</published><updated>2016-08-16T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-08-16:/data-school/pandas-29-dummy-dataframe.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever needed to create a DataFrame of &amp;quot;dummy&amp;quot; data, but without reading from a file? In this video, I'll demonstrate how to create a DataFrame from a dictionary, a list, and a NumPy array. I'll also show you how to create a new Series and attach it …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever needed to create a DataFrame of &amp;quot;dummy&amp;quot; data, but without reading from a file? In this video, I'll demonstrate how to create a DataFrame from a dictionary, a list, and a NumPy array. I'll also show you how to create a new Series and attach it to the DataFrame.&lt;/p&gt;
&lt;p&gt;This is video 29 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="NumPy"></category></entry><entry><title>How do I change display options in pandas?</title><link href="https://pyvideo.org/data-school/pandas-28-customize-display.html" rel="alternate"></link><published>2016-08-09T00:00:00+00:00</published><updated>2016-08-09T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-08-09:/data-school/pandas-28-customize-display.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever wanted to change the way your DataFrame is displayed? Perhaps you needed to see more rows or columns, or modify the formatting of numbers? In this video, I'll demonstrate how to change the settings for five common display options in pandas.&lt;/p&gt;
&lt;p&gt;This is video 28 of …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever wanted to change the way your DataFrame is displayed? Perhaps you needed to see more rows or columns, or modify the formatting of numbers? In this video, I'll demonstrate how to change the settings for five common display options in pandas.&lt;/p&gt;
&lt;p&gt;This is video 28 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I avoid a SettingWithCopyWarning in pandas?</title><link href="https://pyvideo.org/data-school/pandas-27-setting-with-copy-warning.html" rel="alternate"></link><published>2016-08-02T00:00:00+00:00</published><updated>2016-08-02T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-08-02:/data-school/pandas-27-setting-with-copy-warning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you've been using pandas for a while, you've likely encountered a SettingWithCopyWarning. The proper response is to modify your code appropriately, not to turn off the warning! In this video, I'll show you two common scenarios in which this warning arises, explain why it's occurring, and then demonstrate …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you've been using pandas for a while, you've likely encountered a SettingWithCopyWarning. The proper response is to modify your code appropriately, not to turn off the warning! In this video, I'll show you two common scenarios in which this warning arises, explain why it's occurring, and then demonstrate how to address it.&lt;/p&gt;
&lt;p&gt;This is video 27 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="missing data"></category></entry><entry><title>How do I find and remove duplicate rows in pandas?</title><link href="https://pyvideo.org/data-school/pandas-26-duplicate-data.html" rel="alternate"></link><published>2016-07-26T00:00:00+00:00</published><updated>2016-07-26T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-07-26:/data-school/pandas-26-duplicate-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;During the data cleaning process, you will often need to figure out whether you have duplicate data, and if so, how to deal with it. In this video, I'll demonstrate the two key methods for finding and removing duplicate rows, as well as how to modify their behavior to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;During the data cleaning process, you will often need to figure out whether you have duplicate data, and if so, how to deal with it. In this video, I'll demonstrate the two key methods for finding and removing duplicate rows, as well as how to modify their behavior to suit your specific needs.&lt;/p&gt;
&lt;p&gt;This is video 26 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="duplicate data"></category></entry><entry><title>How do I work with dates and times in pandas?</title><link href="https://pyvideo.org/data-school/pandas-25-dates-and-times.html" rel="alternate"></link><published>2016-07-19T00:00:00+00:00</published><updated>2016-07-19T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-07-19:/data-school/pandas-25-dates-and-times.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you have dates and times in your DataFrame and you want to analyze your data by minute, month, or year. What should you do? In this video, I'll demonstrate how you can convert your data to &amp;quot;datetime&amp;quot; format, enabling you to access a ton of convenient …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you have dates and times in your DataFrame and you want to analyze your data by minute, month, or year. What should you do? In this video, I'll demonstrate how you can convert your data to &amp;quot;datetime&amp;quot; format, enabling you to access a ton of convenient attributes and perform datetime comparisons and mathematical operations.&lt;/p&gt;
&lt;p&gt;This is video 25 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="data visualization"></category></entry><entry><title>How do I create dummy variables in pandas?</title><link href="https://pyvideo.org/data-school/pandas-24-dummy-variables.html" rel="alternate"></link><published>2016-07-12T00:00:00+00:00</published><updated>2016-07-12T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-07-12:/data-school/pandas-24-dummy-variables.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you want to include a categorical feature in your machine learning model, one common solution is to create dummy variables. In this video, I'll demonstrate three different ways you can create dummy variables from your existing DataFrame columns. I'll also show you a trick for simplifying your code …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you want to include a categorical feature in your machine learning model, one common solution is to create dummy variables. In this video, I'll demonstrate three different ways you can create dummy variables from your existing DataFrame columns. I'll also show you a trick for simplifying your code that was introduced in pandas 0.18.&lt;/p&gt;
&lt;p&gt;This is video 24 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="machine learning"></category></entry><entry><title>More of your pandas questions answered!</title><link href="https://pyvideo.org/data-school/pandas-23-viewer-questions.html" rel="alternate"></link><published>2016-07-05T00:00:00+00:00</published><updated>2016-07-05T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-07-05:/data-school/pandas-23-viewer-questions.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, I'm answering a few of the pandas questions I've received in the YouTube comments: Could you explain how to read the pandas documentation? What is the difference between ufo.isnull() and pd.isnull(ufo)? Why are DataFrame slices inclusive when using .loc, but exclusive when using …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, I'm answering a few of the pandas questions I've received in the YouTube comments: Could you explain how to read the pandas documentation? What is the difference between ufo.isnull() and pd.isnull(ufo)? Why are DataFrame slices inclusive when using .loc, but exclusive when using .iloc? How do I randomly sample rows from a DataFrame?&lt;/p&gt;
&lt;p&gt;This is video 23 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="reproducibility"></category></entry><entry><title>How do I use pandas with scikit-learn to create Kaggle submissions?</title><link href="https://pyvideo.org/data-school/pandas-22-prepare-for-machine-learning.html" rel="alternate"></link><published>2016-06-28T00:00:00+00:00</published><updated>2016-06-28T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-28:/data-school/pandas-22-prepare-for-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you been using scikit-learn for machine learning, and wondering whether pandas could help you to prepare your data and export your predictions? In this video, I'll demonstrate the simplest way to integrate pandas into your machine learning workflow, and will create a submission for Kaggle's Titanic competition in …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you been using scikit-learn for machine learning, and wondering whether pandas could help you to prepare your data and export your predictions? In this video, I'll demonstrate the simplest way to integrate pandas into your machine learning workflow, and will create a submission for Kaggle's Titanic competition in just a few lines of code!&lt;/p&gt;
&lt;p&gt;This is video 22 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="scikit-learn"></category><category term="machine learning"></category></entry><entry><title>How do I make my pandas DataFrame smaller and faster?</title><link href="https://pyvideo.org/data-school/pandas-21-reduce-dataframe-size.html" rel="alternate"></link><published>2016-06-21T00:00:00+00:00</published><updated>2016-06-21T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-21:/data-school/pandas-21-reduce-dataframe-size.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Are you working with a large dataset in pandas, and wondering if you can reduce its memory footprint or improve its efficiency? In this video, I'll show you how to do exactly that in one line of code using the &amp;quot;category&amp;quot; data type, introduced in pandas 0.15. I'll …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Are you working with a large dataset in pandas, and wondering if you can reduce its memory footprint or improve its efficiency? In this video, I'll show you how to do exactly that in one line of code using the &amp;quot;category&amp;quot; data type, introduced in pandas 0.15. I'll explain how it works, and how to know when you shouldn't use it.&lt;/p&gt;
&lt;p&gt;This is video 21 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>When should I use the "inplace" parameter in pandas?</title><link href="https://pyvideo.org/data-school/pandas-20-inplace-parameter.html" rel="alternate"></link><published>2016-06-14T00:00:00+00:00</published><updated>2016-06-14T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-14:/data-school/pandas-20-inplace-parameter.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We've used the &amp;quot;inplace&amp;quot; parameter many times during this video series, but what exactly does it do, and when should you use it? In this video, I'll explain how &amp;quot;inplace&amp;quot; affects methods such as &amp;quot;drop&amp;quot; and &amp;quot;dropna&amp;quot;, and why it is always False by default.&lt;/p&gt;
&lt;p&gt;This is video 20 …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We've used the &amp;quot;inplace&amp;quot; parameter many times during this video series, but what exactly does it do, and when should you use it? In this video, I'll explain how &amp;quot;inplace&amp;quot; affects methods such as &amp;quot;drop&amp;quot; and &amp;quot;dropna&amp;quot;, and why it is always False by default.&lt;/p&gt;
&lt;p&gt;This is video 20 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="missing data"></category></entry><entry><title>How do I select multiple rows and columns from a pandas DataFrame?</title><link href="https://pyvideo.org/data-school/pandas-19-select-dataframe-rows-and-columns.html" rel="alternate"></link><published>2016-06-07T00:00:00+00:00</published><updated>2016-06-07T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-07:/data-school/pandas-19-select-dataframe-rows-and-columns.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever been confused about the &amp;quot;right&amp;quot; way to select rows and columns from a DataFrame? pandas gives you an incredible number of options for doing so, but in this video, I'll outline the current best practices for row and column selection using the loc, iloc, and ix …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever been confused about the &amp;quot;right&amp;quot; way to select rows and columns from a DataFrame? pandas gives you an incredible number of options for doing so, but in this video, I'll outline the current best practices for row and column selection using the loc, iloc, and ix methods.&lt;/p&gt;
&lt;p&gt;This is video 19 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>What do I need to know about the pandas index? (Part 2)</title><link href="https://pyvideo.org/data-school/pandas-18-index-part-2.html" rel="alternate"></link><published>2016-06-02T00:00:00+00:00</published><updated>2016-06-02T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-06-02:/data-school/pandas-18-index-part-2.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In part two of our discussion of the index, we'll switch our focus from the DataFrame index to the Series index. After discussing index-based selection and sorting, I'll demonstrate how automatic index alignment during mathematical operations and concatenation enables us to easily work with incomplete data in pandas.&lt;/p&gt;
&lt;p&gt;This …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In part two of our discussion of the index, we'll switch our focus from the DataFrame index to the Series index. After discussing index-based selection and sorting, I'll demonstrate how automatic index alignment during mathematical operations and concatenation enables us to easily work with incomplete data in pandas.&lt;/p&gt;
&lt;p&gt;This is video 18 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="missing data"></category></entry><entry><title>What do I need to know about the pandas index? (Part 1)</title><link href="https://pyvideo.org/data-school/pandas-17-index-part-1.html" rel="alternate"></link><published>2016-05-31T00:00:00+00:00</published><updated>2016-05-31T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-31:/data-school/pandas-17-index-part-1.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The DataFrame index is core to the functionality of pandas, yet it's confusing to many users. In this video, I'll explain what the index is used for and why you might want to store your data in the index. I'll also demonstrate how to set and reset the index …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The DataFrame index is core to the functionality of pandas, yet it's confusing to many users. In this video, I'll explain what the index is used for and why you might want to store your data in the index. I'll also demonstrate how to set and reset the index, and show how that affects the DataFrame's shape and contents.&lt;/p&gt;
&lt;p&gt;This is video 17 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I handle missing values in pandas?</title><link href="https://pyvideo.org/data-school/pandas-16-missing-values.html" rel="alternate"></link><published>2016-05-26T00:00:00+00:00</published><updated>2016-05-26T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-26:/data-school/pandas-16-missing-values.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most datasets contain &amp;quot;missing values&amp;quot;, meaning that the data is incomplete. Deciding how to handle missing values can be challenging! In this video, I'll cover all of the basics: how missing values are represented in pandas, how to locate them, and options for how to drop them or fill …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most datasets contain &amp;quot;missing values&amp;quot;, meaning that the data is incomplete. Deciding how to handle missing values can be challenging! In this video, I'll cover all of the basics: how missing values are represented in pandas, how to locate them, and options for how to drop them or fill them in.&lt;/p&gt;
&lt;p&gt;This is video 16 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="missing data"></category></entry><entry><title>How do I explore a pandas Series?</title><link href="https://pyvideo.org/data-school/pandas-15-explore-series.html" rel="alternate"></link><published>2016-05-24T00:00:00+00:00</published><updated>2016-05-24T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-24:/data-school/pandas-15-explore-series.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;When you start working with a new dataset, how should you go about exploring it? In this video, I'll demonstrate some of the basic tools in pandas for exploring both numeric and non-numeric data. I'll also show you how to create simple visualizations in a single line of code …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;When you start working with a new dataset, how should you go about exploring it? In this video, I'll demonstrate some of the basic tools in pandas for exploring both numeric and non-numeric data. I'll also show you how to create simple visualizations in a single line of code!&lt;/p&gt;
&lt;p&gt;This is video 15 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="data visualization"></category></entry><entry><title>When should I use a "groupby" in pandas?</title><link href="https://pyvideo.org/data-school/pandas-14-analyze-data-by-category.html" rel="alternate"></link><published>2016-05-19T00:00:00+00:00</published><updated>2016-05-19T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-19:/data-school/pandas-14-analyze-data-by-category.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The pandas &amp;quot;groupby&amp;quot; method allows you to split a DataFrame into groups, apply a function to each group independently, and then combine the results back together. This is called the &amp;quot;split-apply-combine&amp;quot; pattern, and is a powerful tool for analyzing data across different categories. In this video, I'll explain when …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The pandas &amp;quot;groupby&amp;quot; method allows you to split a DataFrame into groups, apply a function to each group independently, and then combine the results back together. This is called the &amp;quot;split-apply-combine&amp;quot; pattern, and is a powerful tool for analyzing data across different categories. In this video, I'll explain when you should use a groupby and then demonstrate its flexibility using four different examples.&lt;/p&gt;
&lt;p&gt;This is video 14 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="data visualization"></category></entry><entry><title>How do I change the data type of a pandas Series?</title><link href="https://pyvideo.org/data-school/pandas-13-change-data-type-of-series.html" rel="alternate"></link><published>2016-05-17T00:00:00+00:00</published><updated>2016-05-17T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-17:/data-school/pandas-13-change-data-type-of-series.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever tried to do math with a pandas Series that you thought was numeric, but it turned out that your numbers were stored as strings? In this video, I'll demonstrate two different ways to change the data type of a Series so that you can fix incorrect …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever tried to do math with a pandas Series that you thought was numeric, but it turned out that your numbers were stored as strings? In this video, I'll demonstrate two different ways to change the data type of a Series so that you can fix incorrect data types. I'll also show you the easiest way to convert a boolean Series to integers, which is useful for creating dummy/indicator variables for machine learning.&lt;/p&gt;
&lt;p&gt;This is video 13 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I use string methods in pandas?</title><link href="https://pyvideo.org/data-school/pandas-12-string-methods.html" rel="alternate"></link><published>2016-05-12T00:00:00+00:00</published><updated>2016-05-12T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-12:/data-school/pandas-12-string-methods.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas includes powerful string manipulation capabilities that you can easily apply to any Series of strings. In this video, I'll show you how to access string methods in pandas (along with a few examples), and then end with two bonus tips to help you maximize your efficiency.&lt;/p&gt;
&lt;p&gt;This is …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas includes powerful string manipulation capabilities that you can easily apply to any Series of strings. In this video, I'll show you how to access string methods in pandas (along with a few examples), and then end with two bonus tips to help you maximize your efficiency.&lt;/p&gt;
&lt;p&gt;This is video 12 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="regular expressions"></category><category term="string processing"></category></entry><entry><title>How do I use the "axis" parameter in pandas?</title><link href="https://pyvideo.org/data-school/pandas-11-dataframe-axis.html" rel="alternate"></link><published>2016-05-10T00:00:00+00:00</published><updated>2016-05-10T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-10:/data-school/pandas-11-dataframe-axis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;When performing operations on a pandas DataFrame, such as dropping columns or calculating row means, it is often necessary to specify the &amp;quot;axis&amp;quot;. But what exactly is an axis? In this video, I'll help you to build a mental model for understanding the axis parameter so that you will …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;When performing operations on a pandas DataFrame, such as dropping columns or calculating row means, it is often necessary to specify the &amp;quot;axis&amp;quot;. But what exactly is an axis? In this video, I'll help you to build a mental model for understanding the axis parameter so that you will know when and how to use it.&lt;/p&gt;
&lt;p&gt;This is video 11 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>Your pandas questions answered!</title><link href="https://pyvideo.org/data-school/pandas-10-viewer-questions.html" rel="alternate"></link><published>2016-05-05T00:00:00+00:00</published><updated>2016-05-05T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-05:/data-school/pandas-10-viewer-questions.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, I'm answering a few of the pandas questions I've received in the YouTube comments: When reading from a file, how do I read in only a subset of the columns or rows? How do I iterate through a Series or a DataFrame? How do I drop …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, I'm answering a few of the pandas questions I've received in the YouTube comments: When reading from a file, how do I read in only a subset of the columns or rows? How do I iterate through a Series or a DataFrame? How do I drop all non-numeric columns from a DataFrame? How do I know whether I should pass an argument as a string or a list?&lt;/p&gt;
&lt;p&gt;This is video 10 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I apply multiple filter criteria to a pandas DataFrame?</title><link href="https://pyvideo.org/data-school/pandas-09-multiple-filter-criteria.html" rel="alternate"></link><published>2016-05-03T00:00:00+00:00</published><updated>2016-05-03T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-05-03:/data-school/pandas-09-multiple-filter-criteria.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you want to filter the rows of a DataFrame by multiple conditions. In this video, I'll demonstrate how to do this using two different logical operators. I'll also explain the special rules in pandas for combining filter criteria, and end with a trick for simplifying chained …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you want to filter the rows of a DataFrame by multiple conditions. In this video, I'll demonstrate how to do this using two different logical operators. I'll also explain the special rules in pandas for combining filter criteria, and end with a trick for simplifying chained conditions!&lt;/p&gt;
&lt;p&gt;This is video 9 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I filter rows of a pandas DataFrame by column value?</title><link href="https://pyvideo.org/data-school/pandas-08-filter-dataframe-rows.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-04-28:/data-school/pandas-08-filter-dataframe-rows.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you only want to display the rows of a DataFrame which have a certain column value. How would you do it? pandas makes it easy, but the notation can be confusing and thus difficult to remember. In this video, I'll work up to the solution step-by-step …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you only want to display the rows of a DataFrame which have a certain column value. How would you do it? pandas makes it easy, but the notation can be confusing and thus difficult to remember. In this video, I'll work up to the solution step-by-step using regular Python code so that you can truly understand the logic behind pandas filtering notation.&lt;/p&gt;
&lt;p&gt;This is video 8 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I sort a pandas DataFrame or a Series?</title><link href="https://pyvideo.org/data-school/pandas-07-sort-dataframe-or-series.html" rel="alternate"></link><published>2016-04-26T00:00:00+00:00</published><updated>2016-04-26T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-04-26:/data-school/pandas-07-sort-dataframe-or-series.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas allows you to sort a DataFrame by one of its columns (known as a &amp;quot;Series&amp;quot;), and also allows you to sort a Series alone. The sorting API changed in pandas version 0.17, so in this video, I'll demonstrate both the &amp;quot;old way&amp;quot; and the &amp;quot;new way&amp;quot; to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas allows you to sort a DataFrame by one of its columns (known as a &amp;quot;Series&amp;quot;), and also allows you to sort a Series alone. The sorting API changed in pandas version 0.17, so in this video, I'll demonstrate both the &amp;quot;old way&amp;quot; and the &amp;quot;new way&amp;quot; to sort. I'll also show you how to sort a DataFrame by multiple columns at once!&lt;/p&gt;
&lt;p&gt;This is video 7 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I remove columns from a pandas DataFrame?</title><link href="https://pyvideo.org/data-school/pandas-06-remove-dataframe-column.html" rel="alternate"></link><published>2016-04-21T00:00:00+00:00</published><updated>2016-04-21T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-04-21:/data-school/pandas-06-remove-dataframe-column.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you have DataFrame columns that you're never going to use, you may want to remove them entirely in order to focus on the columns that you do use. In this video, I'll show you how to remove columns (and rows), and will briefly explain the meaning of the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you have DataFrame columns that you're never going to use, you may want to remove them entirely in order to focus on the columns that you do use. In this video, I'll show you how to remove columns (and rows), and will briefly explain the meaning of the &amp;quot;axis&amp;quot; and &amp;quot;inplace&amp;quot; parameters.&lt;/p&gt;
&lt;p&gt;This is video 6 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I rename columns in a pandas DataFrame?</title><link href="https://pyvideo.org/data-school/pandas-05-rename-dataframe-column.html" rel="alternate"></link><published>2016-04-19T00:00:00+00:00</published><updated>2016-04-19T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-04-19:/data-school/pandas-05-rename-dataframe-column.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You will often want to rename the columns of a DataFrame so that their names are descriptive, easy to type, and don't contain any spaces. In this video, I'll demonstrate three different strategies for renaming columns so that you can choose the best strategy to fit your particular situation …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You will often want to rename the columns of a DataFrame so that their names are descriptive, easy to type, and don't contain any spaces. In this video, I'll demonstrate three different strategies for renaming columns so that you can choose the best strategy to fit your particular situation.&lt;/p&gt;
&lt;p&gt;This is video 5 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>Why do some pandas commands end with parentheses (and others don't)?</title><link href="https://pyvideo.org/data-school/pandas-04-methods-and-attributes.html" rel="alternate"></link><published>2016-04-14T00:00:00+00:00</published><updated>2016-04-14T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-04-14:/data-school/pandas-04-methods-and-attributes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;To access most of the functionality in pandas, you have to call the methods and attributes of DataFrame and Series objects. In this video, I'll discuss some common methods and attributes, and show you how to tell the difference between them. (Hint: It's all about the parentheses!)&lt;/p&gt;
&lt;p&gt;This is …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;To access most of the functionality in pandas, you have to call the methods and attributes of DataFrame and Series objects. In this video, I'll discuss some common methods and attributes, and show you how to tell the difference between them. (Hint: It's all about the parentheses!)&lt;/p&gt;
&lt;p&gt;This is video 4 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I select a pandas Series from a DataFrame?</title><link href="https://pyvideo.org/data-school/pandas-03-select-series-from-dataframe.html" rel="alternate"></link><published>2016-04-12T00:00:00+00:00</published><updated>2016-04-12T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-04-12:/data-school/pandas-03-select-series-from-dataframe.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;DataFrames and Series are the two main object types in pandas for data storage: a DataFrame is like a table, and each column of the table is called a Series. You will often select a Series in order to analyze or manipulate it. In this video, I'll show you …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;DataFrames and Series are the two main object types in pandas for data storage: a DataFrame is like a table, and each column of the table is called a Series. You will often select a Series in order to analyze or manipulate it. In this video, I'll show you how to select a Series using &amp;quot;bracket notation&amp;quot; and &amp;quot;dot notation&amp;quot;, and will discuss the limitations of dot notation. I'll also demonstrate how to create a new Series in a DataFrame.&lt;/p&gt;
&lt;p&gt;This is video 3 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>What is pandas? (Introduction to the Q&amp;A series)</title><link href="https://pyvideo.org/data-school/pandas-01-introduction.html" rel="alternate"></link><published>2016-04-07T00:00:00+00:00</published><updated>2016-04-07T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-04-07:/data-school/pandas-01-introduction.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas is a full-featured Python library for data analysis, manipulation, and visualization. This video series is for anyone who wants to work with data in Python, regardless of whether you are brand new to pandas or have some experience.&lt;/p&gt;
&lt;p&gt;This is video 1 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas is a full-featured Python library for data analysis, manipulation, and visualization. This video series is for anyone who wants to work with data in Python, regardless of whether you are brand new to pandas or have some experience.&lt;/p&gt;
&lt;p&gt;This is video 1 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category></entry><entry><title>How do I read a tabular data file into pandas?</title><link href="https://pyvideo.org/data-school/pandas-02-read-tabular-data-file.html" rel="alternate"></link><published>2016-04-07T00:00:00+00:00</published><updated>2016-04-07T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-04-07:/data-school/pandas-02-read-tabular-data-file.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Tabular data&amp;quot; is just data that has been formatted as a table, with rows and columns (like a spreadsheet). You can easily read a tabular data file into pandas, even directly from a URL! In this video, I'll walk you through how to do that, including how to modify …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Tabular data&amp;quot; is just data that has been formatted as a table, with rows and columns (like a spreadsheet). You can easily read a tabular data file into pandas, even directly from a URL! In this video, I'll walk you through how to do that, including how to modify some of the default arguments of the read_table function to solve common problems.&lt;/p&gt;
&lt;p&gt;This is video 2 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="data science"></category><category term="data analysis"></category><category term="data wrangling"></category><category term="data processing"></category><category term="pandas"></category><category term="tutorial"></category><category term="Data School"></category><category term="csv"></category></entry><entry><title>How to evaluate a classifier in scikit-learn</title><link href="https://pyvideo.org/data-school/scikit-learn-09-evaluating-classification-models.html" rel="alternate"></link><published>2015-10-23T00:00:00+00:00</published><updated>2015-10-23T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2015-10-23:/data-school/scikit-learn-09-evaluating-classification-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, you'll learn how to properly evaluate a classification model using a variety of common tools and metrics, as well as how to adjust the performance of a classifier to best match your business objectives. I'll start by demonstrating the weaknesses of classification accuracy as an evaluation …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, you'll learn how to properly evaluate a classification model using a variety of common tools and metrics, as well as how to adjust the performance of a classifier to best match your business objectives. I'll start by demonstrating the weaknesses of classification accuracy as an evaluation metric. I'll then discuss the confusion matrix, the ROC curve and AUC, and metrics such as sensitivity, specificity, and precision. By the end of the video, you will have a solid foundation for intelligently evaluating your own classification model.&lt;/p&gt;
&lt;p&gt;This is the ninth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="machine learning"></category><category term="data science"></category><category term="scikit-learn"></category><category term="tutorial"></category><category term="Data School"></category><category term="model evaluation"></category><category term="classification"></category><category term="confusion matrix"></category><category term="ROC curve"></category><category term="AUC"></category></entry><entry><title>How to find the best model parameters in scikit-learn</title><link href="https://pyvideo.org/data-school/scikit-learn-08-parameter-tuning-with-grid-search.html" rel="alternate"></link><published>2015-07-15T00:00:00+00:00</published><updated>2015-07-15T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2015-07-15:/data-school/scikit-learn-08-parameter-tuning-with-grid-search.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, you'll learn how to efficiently search for the optimal tuning parameters (or &amp;quot;hyperparameters&amp;quot;) for your machine learning model in order to maximize its performance. I'll start by demonstrating an exhaustive &amp;quot;grid search&amp;quot; process using scikit-learn's GridSearchCV class, and then I'll compare it with RandomizedSearchCV, which can …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, you'll learn how to efficiently search for the optimal tuning parameters (or &amp;quot;hyperparameters&amp;quot;) for your machine learning model in order to maximize its performance. I'll start by demonstrating an exhaustive &amp;quot;grid search&amp;quot; process using scikit-learn's GridSearchCV class, and then I'll compare it with RandomizedSearchCV, which can often achieve similar results in far less time.&lt;/p&gt;
&lt;p&gt;This is the eighth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="machine learning"></category><category term="data science"></category><category term="scikit-learn"></category><category term="tutorial"></category><category term="Data School"></category><category term="cross-validation"></category><category term="model evaluation"></category><category term="parameter tuning"></category><category term="grid search"></category></entry><entry><title>Selecting the best model in scikit-learn using cross-validation</title><link href="https://pyvideo.org/data-school/scikit-learn-07-model-evaluation-with-cross-validation.html" rel="alternate"></link><published>2015-06-28T00:00:00+00:00</published><updated>2015-06-28T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2015-06-28:/data-school/scikit-learn-07-model-evaluation-with-cross-validation.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, we'll learn about K-fold cross-validation and how it can be used for selecting optimal tuning parameters, choosing between models, and selecting features. We'll compare cross-validation with the train/test split procedure, and we'll also discuss some variations of cross-validation that can result in more accurate estimates …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, we'll learn about K-fold cross-validation and how it can be used for selecting optimal tuning parameters, choosing between models, and selecting features. We'll compare cross-validation with the train/test split procedure, and we'll also discuss some variations of cross-validation that can result in more accurate estimates of model performance.&lt;/p&gt;
&lt;p&gt;This is the seventh video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="machine learning"></category><category term="data science"></category><category term="scikit-learn"></category><category term="tutorial"></category><category term="Data School"></category><category term="cross-validation"></category><category term="model evaluation"></category><category term="feature selection"></category><category term="parameter tuning"></category></entry><entry><title>Data science in Python: pandas, seaborn, scikit-learn</title><link href="https://pyvideo.org/data-school/scikit-learn-06-data-science-pipeline.html" rel="alternate"></link><published>2015-05-28T00:00:00+00:00</published><updated>2015-05-28T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2015-05-28:/data-school/scikit-learn-06-data-science-pipeline.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, we'll cover the data science pipeline from data ingestion (with pandas) to data visualization (with seaborn) to machine learning (with scikit-learn). We'll learn how to train and interpret a linear regression model, and then compare three possible evaluation metrics for regression problems. Finally, we'll apply the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, we'll cover the data science pipeline from data ingestion (with pandas) to data visualization (with seaborn) to machine learning (with scikit-learn). We'll learn how to train and interpret a linear regression model, and then compare three possible evaluation metrics for regression problems. Finally, we'll apply the train/test split procedure to decide which features to include in our model.&lt;/p&gt;
&lt;p&gt;This is the sixth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="machine learning"></category><category term="data science"></category><category term="scikit-learn"></category><category term="tutorial"></category><category term="Data School"></category><category term="pandas"></category><category term="seaborn"></category><category term="linear regression"></category><category term="model evaluation"></category><category term="feature selection"></category><category term="visualization"></category></entry><entry><title>Comparing machine learning models in scikit-learn</title><link href="https://pyvideo.org/data-school/scikit-learn-05-comparing-machine-learning-models.html" rel="alternate"></link><published>2015-05-14T00:00:00+00:00</published><updated>2015-05-14T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2015-05-14:/data-school/scikit-learn-05-comparing-machine-learning-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We've learned how to train different machine learning models and make predictions, but how do we actually choose which model is &amp;quot;best&amp;quot;? We'll cover the train/test split process for model evaluation, which allows you to avoid &amp;quot;overfitting&amp;quot; by estimating how well a model is likely to perform on …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We've learned how to train different machine learning models and make predictions, but how do we actually choose which model is &amp;quot;best&amp;quot;? We'll cover the train/test split process for model evaluation, which allows you to avoid &amp;quot;overfitting&amp;quot; by estimating how well a model is likely to perform on new data. We'll use that same process to locate optimal tuning parameters for a KNN model, and then we'll re-train our model so that it's ready to make real predictions.&lt;/p&gt;
&lt;p&gt;This is the fifth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="machine learning"></category><category term="data science"></category><category term="scikit-learn"></category><category term="tutorial"></category><category term="Data School"></category><category term="model evaluation"></category><category term="overfitting"></category></entry><entry><title>Training a machine learning model with scikit-learn</title><link href="https://pyvideo.org/data-school/scikit-learn-04-training-a-machine-learning-model.html" rel="alternate"></link><published>2015-04-29T00:00:00+00:00</published><updated>2015-04-29T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2015-04-29:/data-school/scikit-learn-04-training-a-machine-learning-model.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Now that we're familiar with the famous iris dataset, let's actually use a classification model in scikit-learn to predict the species of an iris! We'll learn how the K-nearest neighbors (KNN) model works, and then walk through the four steps for model training and prediction in scikit-learn. Finally, we'll …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Now that we're familiar with the famous iris dataset, let's actually use a classification model in scikit-learn to predict the species of an iris! We'll learn how the K-nearest neighbors (KNN) model works, and then walk through the four steps for model training and prediction in scikit-learn. Finally, we'll see how easy it is to try out a different classification model, namely logistic regression.&lt;/p&gt;
&lt;p&gt;This is the fourth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="machine learning"></category><category term="data science"></category><category term="scikit-learn"></category><category term="tutorial"></category><category term="Data School"></category><category term="classification"></category><category term="KNN"></category></entry><entry><title>Getting started in scikit-learn with the famous iris dataset</title><link href="https://pyvideo.org/data-school/scikit-learn-03-getting-started-with-machine-learning.html" rel="alternate"></link><published>2015-04-21T00:00:00+00:00</published><updated>2015-04-21T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2015-04-21:/data-school/scikit-learn-03-getting-started-with-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Now that we've set up Python for machine learning, let's get started by loading an example dataset into scikit-learn! We'll explore the famous &amp;quot;iris&amp;quot; dataset, learn some important machine learning terminology, and discuss the four key requirements for working with data in scikit-learn.&lt;/p&gt;
&lt;p&gt;This is the third video in …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Now that we've set up Python for machine learning, let's get started by loading an example dataset into scikit-learn! We'll explore the famous &amp;quot;iris&amp;quot; dataset, learn some important machine learning terminology, and discuss the four key requirements for working with data in scikit-learn.&lt;/p&gt;
&lt;p&gt;This is the third video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="machine learning"></category><category term="data science"></category><category term="scikit-learn"></category><category term="tutorial"></category><category term="Data School"></category><category term="NumPy"></category></entry><entry><title>Setting up Python for machine learning: scikit-learn and IPython Notebook</title><link href="https://pyvideo.org/data-school/scikit-learn-02-machine-learning-setup.html" rel="alternate"></link><published>2015-04-15T00:00:00+00:00</published><updated>2015-04-15T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2015-04-15:/data-school/scikit-learn-02-machine-learning-setup.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Want to get started with machine learning in Python? I'll discuss the pros and cons of the scikit-learn library, show how to install my preferred Python distribution, and demonstrate the basic functionality of the IPython Notebook. If you don't yet know any Python, I'll also provide four recommended resources …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Want to get started with machine learning in Python? I'll discuss the pros and cons of the scikit-learn library, show how to install my preferred Python distribution, and demonstrate the basic functionality of the IPython Notebook. If you don't yet know any Python, I'll also provide four recommended resources for learning Python.&lt;/p&gt;
&lt;p&gt;This is the second video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="machine learning"></category><category term="data science"></category><category term="scikit-learn"></category><category term="tutorial"></category><category term="Data School"></category><category term="IPython notebook"></category><category term="Jupyter notebook"></category></entry><entry><title>What is machine learning, and how does it work?</title><link href="https://pyvideo.org/data-school/scikit-learn-01-what-is-machine-learning.html" rel="alternate"></link><published>2015-04-07T00:00:00+00:00</published><updated>2015-04-07T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2015-04-07:/data-school/scikit-learn-01-what-is-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you heard of &amp;quot;machine learning&amp;quot;, and you're trying to figure out exactly what that means? I'll give you my definition, provide some examples of machine learning, and explain at a high level how machine learning &amp;quot;works&amp;quot;.&lt;/p&gt;
&lt;p&gt;This is the first video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you heard of &amp;quot;machine learning&amp;quot;, and you're trying to figure out exactly what that means? I'll give you my definition, provide some examples of machine learning, and explain at a high level how machine learning &amp;quot;works&amp;quot;.&lt;/p&gt;
&lt;p&gt;This is the first video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content><category term="Data School"></category><category term="machine learning"></category><category term="data science"></category><category term="scikit-learn"></category><category term="tutorial"></category><category term="Data School"></category><category term="supervised learning"></category><category term="unsupervised learning"></category></entry><entry><title>Matplotlib Plot Tutorial: Histograms, Scatter Plots &amp; Legend</title><link href="https://pyvideo.org/datacamp/Matplotlib-Plot-Tutorial-For-Beginners.html" rel="alternate"></link><published>2016-02-01T00:00:00+00:00</published><updated>2016-02-01T00:00:00+00:00</updated><author><name>Filip Schouwenaars</name></author><id>tag:pyvideo.org,2016-02-01:/datacamp/Matplotlib-Plot-Tutorial-For-Beginners.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Matplotlib makes it easy to create meaningful and insightful plots. In this beginner video, you will learn how to build various types of data visualizations such as histograms, scatter plots and line plots. You will also see how to customize them to make them more visually appealing and interpretable …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Matplotlib makes it easy to create meaningful and insightful plots. In this beginner video, you will learn how to build various types of data visualizations such as histograms, scatter plots and line plots. You will also see how to customize them to make them more visually appealing and interpretable.&lt;/p&gt;
&lt;p&gt;Want to do the corresponding exercises? Go to our &lt;cite&gt;Python For Data Science Tutorial &amp;lt;https://www.datacamp.com/courses/intro-to-python-for-data-science&amp;gt;&lt;/cite&gt; where you can do them for free.&lt;/p&gt;
</content><category term="DataCamp"></category><category term="Matplotlib"></category><category term="data science"></category><category term="data visualization"></category><category term="tutorial"></category><category term="DataCamp"></category></entry><entry><title>Audio Classification with Machine Learning</title><link href="https://pyvideo.org/europython-2019/audio-classification-with-machine-learning.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Jon Nordby</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/audio-classification-with-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Sound is a rich source of information about the world around us.&lt;/div&gt;
&lt;div class="line"&gt;Modern deep learning approaches can give human-like performance on a
range of sound classifiction tasks.&lt;/div&gt;
&lt;div class="line"&gt;This makes it possible to build systems that use sound to for example:&lt;/div&gt;
&lt;div class="line"&gt;understand speech, to analyze music, to assist in medical …&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Sound is a rich source of information about the world around us.&lt;/div&gt;
&lt;div class="line"&gt;Modern deep learning approaches can give human-like performance on a
range of sound classifiction tasks.&lt;/div&gt;
&lt;div class="line"&gt;This makes it possible to build systems that use sound to for example:&lt;/div&gt;
&lt;div class="line"&gt;understand speech, to analyze music, to assist in medical diagnostics,
detect quality problems in manufacturing, and to study the behavior of
animals.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;This talk will show you how to build practical machine learning models
that can classify sound.&lt;/div&gt;
&lt;div class="line"&gt;We will convert sound into spectrograms, a visual representation of
sound over time,&lt;/div&gt;
&lt;div class="line"&gt;and apply machine learning models similar to what is used to for image
classification.&lt;/div&gt;
&lt;div class="line"&gt;The focus will be on Convolutional Neural Networks, which have been
shown to work very well for this task.&lt;/div&gt;
&lt;div class="line"&gt;The Keras and Tensorflow deep learning frameworks will be used. Some
tricks for getting usable results with small amounts of data will be
covered, including transfer learning, audio embeddings and data
augmentation.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;A basic understanding of machine learning is recommended.&lt;/div&gt;
&lt;div class="line"&gt;Familiarity with digital sound is a bonus.&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category></entry><entry><title>Building Data-Driven Client Relationship Management in Banking with Python</title><link href="https://pyvideo.org/europython-2019/building-data-driven-client-relationship-management-in-banking-with-python.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Paul Hughes</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/building-data-driven-client-relationship-management-in-banking-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This is a case study that documents how a small data science team in a
big bank took on the challenge to transform a fragmented sales process
into a data-driven one using Python and machine learning.&lt;/p&gt;
&lt;p&gt;This talk outlines the various ways Python has been instrumental in
delivering a …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This is a case study that documents how a small data science team in a
big bank took on the challenge to transform a fragmented sales process
into a data-driven one using Python and machine learning.&lt;/p&gt;
&lt;p&gt;This talk outlines the various ways Python has been instrumental in
delivering a production solution that serves advisers and relationship
manager on a continuous basis.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;The Challenge&lt;/div&gt;
&lt;div class="line"&gt;- A bank has many clients with diverse needs and cost pressures mean
fewer advisers resulting in reduced client coverage.&lt;/div&gt;
&lt;div class="line"&gt;- Multiple sales channels and mixed service levels meant sales
processes were uncoordinated and driven by heuristics and often very
subjective.&lt;/div&gt;
&lt;div class="line"&gt;- And... Excel sheets everywhere!&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Solution&lt;/div&gt;
&lt;div class="line"&gt;- Go data-driven!&lt;/div&gt;
&lt;div class="line"&gt;- Learn from clients and understand product usage&lt;/div&gt;
&lt;div class="line"&gt;- Empower and inform advisers and call centre agents&lt;/div&gt;
&lt;div class="line"&gt;- Build a front-to-back sales process (no more Excels!)&lt;/div&gt;
&lt;div class="line"&gt;- How? With Python!&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;The Python Bits&lt;/div&gt;
&lt;div class="line"&gt;- Scikit learn machine learning pipelines that implement two distinct
approaches to product affinity in banking and wealth management&lt;/div&gt;
&lt;div class="line"&gt;- SQL Alchemy based API for data engineering and rapid prototyping of
analytics&lt;/div&gt;
&lt;div class="line"&gt;- Pandas and Jupyter for development and collaboration&lt;/div&gt;
&lt;div class="line"&gt;- Luigi pipeline for daily processing of millions of transactions and
engineering features&lt;/div&gt;
&lt;div class="line"&gt;- Extracting features from text with NLP (Spacy)&lt;/div&gt;
&lt;div class="line"&gt;- Delivering machine learning interpretability in production, e.g.
with Random Forests and treeinterpreter&lt;/div&gt;
&lt;div class="line"&gt;- A Python module that we built with all the reusable bits: building
training and prediction datasets, developing pipelines, generating
monitoring data and enabling explainability&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Business Cases"></category><category term="Data Science"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category><category term="Windows"></category></entry><entry><title>Deep Learning with TensorFlow 2.0</title><link href="https://pyvideo.org/europython-2019/deep-learning-with-tensorflow-20.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Brad Miro</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/deep-learning-with-tensorflow-20.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Learn about the updates being made to TensorFlow in its 2.0 version.
We’ll give an overview of what’s available in the new version as well as
do a deep dive into an example using its central high-level API, Keras.
You’ll walk away with a better …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Learn about the updates being made to TensorFlow in its 2.0 version.
We’ll give an overview of what’s available in the new version as well as
do a deep dive into an example using its central high-level API, Keras.
You’ll walk away with a better understanding of how you can get started
building machine learning models in Python with TensorFlow 2.0 as well
as the other exciting available features!&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Machine learning on non curated data</title><link href="https://pyvideo.org/europython-2019/machine-learning-on-non-curated-data.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Gael Varoquaux</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/machine-learning-on-non-curated-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;According to industry surveys [1], the number one hassle of data
scientists is cleaning the data to analyze it. Textbook statistical
modeling is sufficient for noisy signals, but errors of a discrete
nature break standard tools of machine learning. I will discuss how to
easily run machine learning on …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;According to industry surveys [1], the number one hassle of data
scientists is cleaning the data to analyze it. Textbook statistical
modeling is sufficient for noisy signals, but errors of a discrete
nature break standard tools of machine learning. I will discuss how to
easily run machine learning on data tables with two common dirty-data
problems: missing values and non-normalized entries. On both problems, I
will show how to run standard machine-learning tools such as
scikit-learn in the presence of such errors. The talk will be didactic
and will discuss simple software solutions. It will build on the latest
improvements to scikit-learn for missing values and the DirtyCat package
[2] for non normalized entries. I will also summarize theoretical
analyses in recent machine learning publications.&lt;/p&gt;
&lt;p&gt;This talk targets data practitioners. Its goal are to help data
scientists to be more efficient analysing data with such errors and
understanding their impacts.&lt;/p&gt;
&lt;p&gt;With missing values, I will use simple arguments and examples to outline
how to obtain asymptotically good predictions [3]. Two components are
key: imputation and adding an indicator of missingness. I will explain
theoretical guidelines for these, and I will show how to implement these
ideas in practice, with scikit-learn as a learner, or as a preprocesser.&lt;/p&gt;
&lt;p&gt;For non-normalized categories, I will show that using their string
representations to “vectorize” them, creating vectorial representations
gives a simple but powerful solution that can be plugged in standard
statistical analysis tools [4].&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;[1] Kaggle, the state of ML and data science 2017
&lt;a class="reference external" href="https://www.kaggle.com/surveys/2017"&gt;https://www.kaggle.com/surveys/2017&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[2] &lt;a class="reference external" href="https://dirty-cat.github.io/stable/"&gt;https://dirty-cat.github.io/stable/&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[3] Josse Julie, Prost Nicolas, Scornet Erwan, and Varoquaux Gaël
(2019). “On the consistency of supervised learning with missing
values”. &lt;a class="reference external" href="https://arxiv.org/abs/1902.06931"&gt;https://arxiv.org/abs/1902.06931&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[4] Cerda Patricio, Varoquaux Gaël, and Kégl Balázs. &amp;quot;Similarity
encoding for learning with dirty categorical variables.&amp;quot; Machine
Learning 107.8-10 (2018): 1477 &lt;a class="reference external" href="https://arxiv.org/abs/1806.00979"&gt;https://arxiv.org/abs/1806.00979&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Big Data"></category><category term="Data"></category><category term="Data Science"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Natural language processing with neural networks.</title><link href="https://pyvideo.org/europython-2019/natural-language-processing-with-neural-networks.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Hubert Bryłkowski</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/natural-language-processing-with-neural-networks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Getting started with a natural language processing and neural networks
is easier nowadays thanks to the numerous talks and tutorials. The
goal is to dive deeper for those who already know the basics, or want
to expand their knowledge in a machine learning field.&lt;/div&gt;
&lt;div class="line"&gt;The talk will start with …&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Getting started with a natural language processing and neural networks
is easier nowadays thanks to the numerous talks and tutorials. The
goal is to dive deeper for those who already know the basics, or want
to expand their knowledge in a machine learning field.&lt;/div&gt;
&lt;div class="line"&gt;The talk will start with the common use cases that can be generalized
to the specific problems in a NLP world. Then I will present an
overview of possible features that we can use as input to our network,
and show that even simple feature engineering can change our results.&lt;/div&gt;
&lt;div class="line"&gt;Furthermore, I will compare different network architectures - starting
with the fully connected networks, through convolution neural networks
to recursive neural networks. I will not only considering the good
parts, but also - what is usually overlooked - pitfalls of every
solution.&lt;/div&gt;
&lt;div class="line"&gt;All of these will be done considering number of parameters, which
transfers into training and prediction costs and time. I will also
share a number of “tricks” that enables getting the best results even
out of the simple architectures, as these are usually the fastest and
quite often hard to beat, at the same time being the easiest to
interpret.&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category><category term="Natural Language Processing"></category></entry><entry><title>PlotVR - walk through your data</title><link href="https://pyvideo.org/europython-2019/plotvr-walk-through-your-data.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Philipp Thomann</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/plotvr-walk-through-your-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Are you bored by 3D-plots that only give you a simple rotatable
2d-projection? plotVR is an open source package that provides a simple
way for data scientists to plot data, pick up a phone, get a real 3d
impression - either by VR or by AR - and use the computer's …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Are you bored by 3D-plots that only give you a simple rotatable
2d-projection? plotVR is an open source package that provides a simple
way for data scientists to plot data, pick up a phone, get a real 3d
impression - either by VR or by AR - and use the computer's keyboard to
walk through the scatter plot:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.github.com/thomann/plotVR"&gt;https://www.github.com/thomann/plotVR&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After installing and plotting your dataframe open your phone's browser
and use your GoogleVR Cardboard. Furthermore performant Android- and
iOS-apps are available - both support VR-Cardboard and the iOS-Version
also AR.&lt;/p&gt;
&lt;p&gt;Once you are immersed in your Cardboard how do you navigate through the
scatter? plotVR lets you use the computer's keyboard to walk as you
would in any first person game.&lt;/p&gt;
&lt;p&gt;You want to share your impression? Just save the HTML and publish it.&lt;/p&gt;
&lt;p&gt;The technologies beneath this project are: a web server that handles the
communication between the DataScience-session and the phone, WebSockets
to quickly proxy the keyboard events, QR-codes facilitate the simple
pairing of both, and an HTML-Page on the computer to grab the keyboard
events. And the translation of these keyboard events into 3D terms is a
nice exercise in three.js, OpenGL, and SceneKit for HTML, Android, and
iOS resp.&lt;/p&gt;
&lt;p&gt;Ready to see your data as you have never seen before? Join the talk!&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="3D"></category><category term="Augmented Reality"></category><category term="Data Science"></category><category term="Open-Source"></category><category term="Visualization"></category></entry><entry><title>The state of Machine Learning Operations in 2019</title><link href="https://pyvideo.org/europython-2019/the-state-of-machine-learning-operations-in-2019.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Alejandro Saucedo</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/the-state-of-machine-learning-operations-in-2019.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will provide an overview of the key challenges and trends in
the productization of machine learning systems, including concepts such
as reproducibility, explainability and orchestration. The talk will also
provide a high level overview of several key open source tools and
frameworks available to tackle these issues …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will provide an overview of the key challenges and trends in
the productization of machine learning systems, including concepts such
as reproducibility, explainability and orchestration. The talk will also
provide a high level overview of several key open source tools and
frameworks available to tackle these issues, which have been identifyed
putting together the Awesome Machine Learning Operations list
(&lt;a class="reference external" href="https://github.com/EthicalML/awesome-machine-learning-operations"&gt;https://github.com/EthicalML/awesome-machine-learning-operations&lt;/a&gt;).&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;The key concepts that will be covered are:&lt;/div&gt;
&lt;div class="line"&gt;* Reproducibility&lt;/div&gt;
&lt;div class="line"&gt;* Explainability&lt;/div&gt;
&lt;div class="line"&gt;* Orchestration of models&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The reproducibility piece will cover key motivations as well as
practical requirements for model versioning, together with tools that
allow data scientists to achieve version control of model+config+data to
ensure full model lineage.&lt;/p&gt;
&lt;p&gt;The explainability piece will contain a high level overview of why this
has become an important topic in machine learning, including the high
profile incidents that tech companies have experienced where undesired
biases have slipped into data. This will also include a high level
overview of some of the tools available.&lt;/p&gt;
&lt;p&gt;Finally, the orchestration piece will cover some of the fundamental
challenges with large scale serving of models, together with some of the
key tools that are available to ensure this challenge can be tackled.&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Architecture"></category><category term="Data"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category></entry><entry><title>“When a biologist met Python”</title><link href="https://pyvideo.org/europython-2019/when-a-biologist-met-python.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Maria Molina-Contreras</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/when-a-biologist-met-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Biology and computing are closer than we usually think, for example
many algorithms are inspired in biology patterns, and complementary to
that, researchers needs special algorithms to have a better
understanding of our environment. Thus, there is a strong relation an
dependency.&lt;/div&gt;
&lt;div class="line"&gt;In the past years, Biology has been …&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Biology and computing are closer than we usually think, for example
many algorithms are inspired in biology patterns, and complementary to
that, researchers needs special algorithms to have a better
understanding of our environment. Thus, there is a strong relation an
dependency.&lt;/div&gt;
&lt;div class="line"&gt;In the past years, Biology has been transformed into computational
biology. Therefore&lt;/div&gt;
&lt;div class="line"&gt;technological advances helps us to predict physical interactions
between atoms and DNA, because we are being able to integrate
information from biology into algorithms.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Python has become a popular programming language in biosciences because
it has a clean syntax that makes it easy to read language. In addition
to this, there are many modules (toolkits) extending to different
biological domains, like metabolomics, structure analysis,
phylogenomics, molecular biology and others. Python is currently
improving researcher’s workflow, helping us to focus on the theory or
experimental part, instead of fighting with old buggy applications.&lt;/p&gt;
&lt;p&gt;This talk aims to be oriented to all audiences (with/without biological
background) since we will go together through an amazing adventure into
the natural sciences using tools like Biopython, Bokeh, Networkx, Ecopy
and much more! Are you brave enough to follow me on this journey?&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Algorithms"></category><category term="Data Science"></category><category term="Natural Science"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category><category term="python"></category></entry><entry><title>Accelerate your Deep Learning Inferencing with the Intel® DL Boost technology</title><link href="https://pyvideo.org/europython-2019/accelerate-your-deep-learning-inferencing-with-the-intelr-dl-boost-technology.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Shailen Sobhee</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/accelerate-your-deep-learning-inferencing-with-the-intelr-dl-boost-technology.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Learn about Intel® Deep Learning Boost, also known as Vector Neural
Network Instructions (VNNI), a new set of AVX-512 instructions, that are
designed to deliver significantly more efficient Deep Learning
(Inference) acceleration. Through this technology, I will show you how
you can perform low-precision (INT8) inference much faster on …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Learn about Intel® Deep Learning Boost, also known as Vector Neural
Network Instructions (VNNI), a new set of AVX-512 instructions, that are
designed to deliver significantly more efficient Deep Learning
(Inference) acceleration. Through this technology, I will show you how
you can perform low-precision (INT8) inference much faster on hardware
that support the VNNI instruction set (for example, the 2nd generation
Intel Xeon Scalable processors, codenamed, Cascade Lake). In the live
Jupyter notebook session, you can will be able to see the benefits of
this new hardware technology.&lt;/p&gt;
&lt;p&gt;Note: This is an advanced talk. Knowledge about Deep Learning,
Inferencing and basic awareness of hardware instruction sets would be
desirable.&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Performance"></category><category term="python"></category></entry><entry><title>Bioinformatics pipeline for revealing tumour heterogeneity</title><link href="https://pyvideo.org/europython-2019/bioinformatics-pipeline-for-revealing-tumour-heterogeneity.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Mustafa Anil Tuncel</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/bioinformatics-pipeline-for-revealing-tumour-heterogeneity.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Reproducibility of research is a common issue in science, especially
in computationally expensive research fields e.g. cancer research.&lt;/div&gt;
&lt;div class="line"&gt;A comprehensive picture of the genomic aberrations that occur during
tumour progression and the resulting intra-tumour heterogeneity, is
essential for personalised and precise cancer therapies. With the
change in the …&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Reproducibility of research is a common issue in science, especially
in computationally expensive research fields e.g. cancer research.&lt;/div&gt;
&lt;div class="line"&gt;A comprehensive picture of the genomic aberrations that occur during
tumour progression and the resulting intra-tumour heterogeneity, is
essential for personalised and precise cancer therapies. With the
change in the tumour environment under treatment, heterogeneity allows
the tumour additional ways to evolve resistance, such that
intra-tumour genomic diversity is a cause of relapse and treatment
failure. Earlier bulk sequencing technologies were incapable of
determining the diversity in the tumour.&lt;/div&gt;
&lt;div class="line"&gt;Single-cell DNA sequencing - a recent sequencing technology - offers
resolution down to the level of individual cells and is playing an
increasingly important role in this field.&lt;/div&gt;
&lt;div class="line"&gt;We present a reproducible and scalable Python data analysis pipeline
that employs a statistical model and an MCMC algorithm to infer the
evolutionary history of copy number alterations of a tumour from
single cells. The pipeline is built using Python, Conda environment
management system and the Snakemake workflow management system. The
pipeline starts from the raw sequencing files and a settings file for
parameter configurations. After running the data analysis, pipeline
produces report and figures to inform the treatment decision of the
cancer patient.&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Algorithms"></category><category term="Analytics"></category><category term="C-Languages"></category><category term="Command-Line"></category><category term="Data Science"></category></entry><entry><title>Building a Powerful Pet Detector in Notebooks</title><link href="https://pyvideo.org/europython-2019/building-a-powerful-pet-detector-in-notebooks.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Katherine Kampf</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/building-a-powerful-pet-detector-in-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Ever wondered what breed that dog or cat is? Let’s build a pet detector
service to recognize them in pictures! In this talk, we will walk
through the training, optimizing, and deploying of a deep learning model
using Azure Notebooks. We will use transfer learning to retrain a …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Ever wondered what breed that dog or cat is? Let’s build a pet detector
service to recognize them in pictures! In this talk, we will walk
through the training, optimizing, and deploying of a deep learning model
using Azure Notebooks. We will use transfer learning to retrain a
MobileNet model using TensorFlow to recognize dog and cat breeds using
the Oxford IIIT Pet Dataset. Next, we’ll optimize the model and tune our
hyperparameters to improve the model accuracy. Finally, we will deploy
the model as a web service in. Come to learn how you can quickly create
accurate image recognition models with a few simple techniques!&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Data"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Jupyter"></category><category term="Machine-Learning"></category></entry><entry><title>Dash: Interactive Data Visualization Web Apps with no Javascript</title><link href="https://pyvideo.org/europython-2019/dash-interactive-data-visualization-web-apps-with-no-javascript.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Dom Weldon</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/dash-interactive-data-visualization-web-apps-with-no-javascript.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Your data science or machine learning project probably won't just
produce a written report. Instead, projects are increasingly expected to
produce interactive tools to allow end-users to explore data and results
with rich, interactive visualizations. Inevitably, this will be done in
a web browser, meaning you'll need to add …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Your data science or machine learning project probably won't just
produce a written report. Instead, projects are increasingly expected to
produce interactive tools to allow end-users to explore data and results
with rich, interactive visualizations. Inevitably, this will be done in
a web browser, meaning you'll need to add a quantitatively trained web
developer to your team, or have your data scientists spend time learning
HTML, Javascript and CSS. Dash, a project by the team that makes Plotly,
solves some of these problems by allowing data scientists to build rich
and interactive websites in pure python, with minimal knowledge of HTML
and absolutely no Javascript.&lt;/p&gt;
&lt;p&gt;At decisionLab, a London-based data science consultancy producing
decision tools, we've embraced Dash to produce proof-of-concept models
for our projects in alpha. Although we're not officially connected to
the plotly/Dash project, by using the library daily across many
projects, we've learned many lessons and what we feel are best practises
we'd like to share, and hear feedback on!&lt;/p&gt;
&lt;p&gt;This talk will give an overview of Dash, how it works and what it can be
used for, before outlining some of the common problems that emerge when
data scientists are let loose to produce web applications, and web
developers have to work with the pydata ecosystem. The talk also covers
effective working practises to start producing cool interactive
statistical web applications, fast. We'll also identify some of the
pitfalls of Dash, and how and when to make the decision to stop using
Dash and start building a proper web application.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://domweldon-europython-2019-dash.s3.eu"&gt;http://domweldon-europython-2019-dash.s3.eu&lt;/a&gt;-
west-2.amazonaws.com/index.html&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Data Science"></category><category term="JavaScript"></category><category term="Visualization"></category><category term="Web"></category><category term="Web Servers and MicroFWs"></category></entry><entry><title>Do we have a diversity problem in Python community?</title><link href="https://pyvideo.org/europython-2019/do-we-have-a-diversity-problem-in-python-community.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Cheuk Ho</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/do-we-have-a-diversity-problem-in-python-community.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The diversity statement quoted as follows: “The Python Software
Foundation and the global Python community welcome and encourage
participation by everyone. Our community is based on mutual respect,
tolerance, and encouragement, and we are working to help each other live
up to these principles. We want our community to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The diversity statement quoted as follows: “The Python Software
Foundation and the global Python community welcome and encourage
participation by everyone. Our community is based on mutual respect,
tolerance, and encouragement, and we are working to help each other live
up to these principles. We want our community to be more diverse:
whoever you are, and whatever your background, we welcome you.”&lt;/p&gt;
&lt;p&gt;Diversity, big deal! As an active members and event organisers (and also
on the minority side of the gender) in the Python community, we have
alway been concern by the question of: Do we truly have a problem in
diversity? Especially, gender diversity. We would like to find out the
truth, by data science, and see if we can find a clue why and how we can
fix it.&lt;/p&gt;
&lt;p&gt;First, we will show the research others did regarding the representation
of women in the R and Python communities [1]. Then, we will show the
research that we did based on our experience and statistic. Including
static analysis of the speakers diversity (regarding gender) at major
PyCon and PyData conferences. Finally, as we all care about diversity
and want improvements, we would like to find out the reason and what we
can do about it. We would propose what we, the minorities and allies,
could do against this seemingly unbalance situation and make the
community better.&lt;/p&gt;
&lt;p&gt;This talk is for all that who cares about diversity in our community.&lt;/p&gt;
&lt;p&gt;[1]
&lt;a class="reference external" href="https://reshamas.github.io/why-women-are-flourishing-in-r-community-but"&gt;https://reshamas.github.io/why-women-are-flourishing-in-r-community-but&lt;/a&gt;-
lagging-in-python/&lt;/p&gt;
&lt;p&gt;Update: slides at
&lt;a class="reference external" href="https://slides.com/cheukting_ho/do-we-have-a-diversity"&gt;https://slides.com/cheukting_ho/do-we-have-a-diversity&lt;/a&gt;-
problem-in-python-community&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Community"></category><category term="Conferences and Meet-Ups"></category><category term="Data Science"></category><category term="Static Analysis"></category></entry><entry><title>Image processing with scikit-image and Dash</title><link href="https://pyvideo.org/europython-2019/image-processing-with-scikit-image-and-dash.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Emmanuelle Gouillart</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/image-processing-with-scikit-image-and-dash.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Images are an ubiquitous form of data in various fields of science and&lt;/div&gt;
&lt;div class="line"&gt;industry. Images often need to be transformed and processed, for
example for helping medical diagnosis by extracting regions of
interest or measures, or for building training sets for machine
learning.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;In this talk, I will present …&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Images are an ubiquitous form of data in various fields of science and&lt;/div&gt;
&lt;div class="line"&gt;industry. Images often need to be transformed and processed, for
example for helping medical diagnosis by extracting regions of
interest or measures, or for building training sets for machine
learning.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;In this talk, I will present and discuss several tools for automatic
and&lt;/div&gt;
&lt;div class="line"&gt;interactive image processing with Python. I will start by a short&lt;/div&gt;
&lt;div class="line"&gt;introduction to scikit-image (&lt;a class="reference external" href="https://scikit-image.org/"&gt;https://scikit-image.org/&lt;/a&gt;), the
open-source&lt;/div&gt;
&lt;div class="line"&gt;image processing toolkit of the Pydata ecosystem, which aims at&lt;/div&gt;
&lt;div class="line"&gt;processing images from a large class of modalities (2-D, 3-D, etc.)
and&lt;/div&gt;
&lt;div class="line"&gt;strives to have a gentle learning curve with pedagogical example-based&lt;/div&gt;
&lt;div class="line"&gt;documentation. scikit-image provides users with a simple API based on
a large number of functions, which can be used to build pipelines of
image processing workflows.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;In a second part, I will explain how to use Dash for building
interactive&lt;/div&gt;
&lt;div class="line"&gt;image processing operations. Dash (&lt;a class="reference external" href="https://dash.plot.ly/"&gt;https://dash.plot.ly/&lt;/a&gt;) is an&lt;/div&gt;
&lt;div class="line"&gt;open-source Python web application framework developed by Plotly.
Written on top of Flask, Plotly.js, and React.js, Dash is meant for
building data visualization apps with highly custom user interfaces in
pure Python. The dash-canvas component library of Dash
(&lt;a class="reference external" href="https://dash.plot.ly/canvas"&gt;https://dash.plot.ly/canvas&lt;/a&gt;) is an interactive component for
annotating images with several tools (freehand brush, lines, bounding
boxes, ...). It also provides utility functions for using
user-provided annotations for several image processing tasks such as
segmentation, transformation, measures, etc. The latter functions are
based on libraries such scikit-image and openCV. A gallery of examples
showcases some typical uses of Dash for image processing on&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://dash-canvas.plotly.host/"&gt;https://dash-canvas.plotly.host/&lt;/a&gt;. Also, other components of Dash can
be leveraged easily to build powerful image processing applications,
such as widgets to tune parameters or data tables for inspecting
object&lt;/div&gt;
&lt;div class="line"&gt;properties.&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Computer Vision"></category><category term="Data Science"></category><category term="Image Processing"></category><category term="JavaScript Web Frameworks (AngularJS/ReactJS/...)"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Opt Out of Online Sexism – Open Source Activism</title><link href="https://pyvideo.org/europython-2019/opt-out-of-online-sexism-open-source-activism.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Teresa Ingram</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/opt-out-of-online-sexism-open-source-activism.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Although people of all genders can experience violence and abuse
online, the abuse experienced by women is often sexist or misogynistic
in nature, and online threats of violence against women are often
sexualized and include specific references to women’s bodies. &amp;quot; -
Amnesty International. This abuse pushes women offline, affecting …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Although people of all genders can experience violence and abuse
online, the abuse experienced by women is often sexist or misogynistic
in nature, and online threats of violence against women are often
sexualized and include specific references to women’s bodies. &amp;quot; -
Amnesty International. This abuse pushes women offline, affecting their
social well-being, representation and economic potential.&lt;/p&gt;
&lt;p&gt;In this talk I will discuss how we plan to help resolve this with our
browser extension, Opt Out. I will discuss the online global tragedy
that is online sexual harassment, our idea and where we’re at with
current implementation. I will also talk about what it’s like to build
an open source activism project, one which aims to be lead by the
community it’s trying to protect.&lt;/p&gt;
&lt;p&gt;We will cover current research and results from our own engagement with
the community, where the idea came from and challenges we have faced and
plan to face in the future. I will also dive into the intricate world of
natural language processing (NLP) for online harassment and talk about
balancing state-of-the-art data science with web development in an open
source community, one being managed by someone relatively new to tech.&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Communication"></category><category term="Data Science"></category><category term="Open-Source"></category><category term="TDD"></category><category term="Web"></category></entry><entry><title>Supercharge your Deep Learning algorithms with optimized software</title><link href="https://pyvideo.org/europython-2019/supercharge-your-deep-learning-algorithms-with-optimized-software.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Shailen Sobhee</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/supercharge-your-deep-learning-algorithms-with-optimized-software.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, you will learn various optimization techniques to improve
the runtime performance of your deep learning algorithms on Intel
architecture. The presentation will cover how to accelerate the training
of your deep neural networks with Tensorflow thanks to the highly
optimized Intel® Math Kernel Library (Intel® MKL …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, you will learn various optimization techniques to improve
the runtime performance of your deep learning algorithms on Intel
architecture. The presentation will cover how to accelerate the training
of your deep neural networks with Tensorflow thanks to the highly
optimized Intel® Math Kernel Library (Intel® MKL) and how we boost
inferencing with Intel® nGraph and with the Intel® Distribution of
OpenVINO™.&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Open-Source"></category></entry><entry><title>What about recommendation engines?</title><link href="https://pyvideo.org/europython-2019/what-about-recommendation-engines.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Adriana Dorneles</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/what-about-recommendation-engines.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;How recommendation engines are taking part in our daily routine and
how companies as Netflix and Amazon implement it?&lt;/div&gt;
&lt;div class="line"&gt;This talk aims to show the elements that compound a recommendation
engine to people who have never been in touch with the matter or want
to know a bit more …&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;How recommendation engines are taking part in our daily routine and
how companies as Netflix and Amazon implement it?&lt;/div&gt;
&lt;div class="line"&gt;This talk aims to show the elements that compound a recommendation
engine to people who have never been in touch with the matter or want
to know a bit more. At the end of this session, you might be able to
reproduce your own recommendation system and also know where to find
more about it.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Talk structure:&lt;/div&gt;
&lt;div class="line"&gt;1. What is and why use a recommendation engine?&lt;/div&gt;
&lt;div class="line"&gt;2. Recommendation engine importance&lt;/div&gt;
&lt;div class="line"&gt;3. Steps of a recommendation&lt;/div&gt;
&lt;div class="line"&gt;4. Recommendation algorithms&lt;/div&gt;
&lt;div class="line"&gt;5. Basic Statistics for distance and correlation&lt;/div&gt;
&lt;div class="line"&gt;6. Example&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Business"></category><category term="Data Science"></category><category term="Python 3"></category></entry><entry><title>A Brief History of Jupyter Notebooks</title><link href="https://pyvideo.org/europython-2020/a-brief-history-of-jupyter-notebooks.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>William Horton</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/a-brief-history-of-jupyter-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Jupyter Notebook: many Python users love it, many other Python users love to hate it. But where did it come from? How did we come to have a tool that combines code execution, visualization, Markdown, and more? In this talk, we will dive into the development of the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Jupyter Notebook: many Python users love it, many other Python users love to hate it. But where did it come from? How did we come to have a tool that combines code execution, visualization, Markdown, and more? In this talk, we will dive into the development of the Jupyter Notebook and the older ideas that it built upon.&lt;/p&gt;
&lt;p&gt;To start, we will look at tools that popularized the “computational notebook” interface. In 1988, Mathematica introduced this interface to the scientific community. In the 90s, tools like Maple competed with Mathematica to provide the best scientific programming environment. The early 2000s saw the rise in popularity of open-source scientific tools in Python, including IPython, leading to IPython Notebook and then Jupyter.&lt;/p&gt;
&lt;p&gt;Turning to the present, we look at the expanding ecosystem beyond the Notebook. JupyterLab provides a richer programming environment. Voilà and Binder give users better options for sharing their notebooks. And increased language support has led to Jupyter being a tool not only for Julia, Python, and R, but for dozens of other languages.&lt;/p&gt;
&lt;p&gt;Finally: what is still to come? JupyerLab 2.0 promises even greater IDE-like capabilities, while IDEs increase their own Notebook support. Projects like Deepnote and CoCalc promise real-time collaboration on top of the Notebook interface. And the frustrations of working with Git are the source of a growing number of possible solutions. These efforts point us toward what the Jupyter Notebook could become.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Data Science"></category><category term="Ipython"></category><category term="Jupyter"></category><category term="Open-Source"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Accessible Python education for schoolgirls using Avocados, Zombies, and Korean!</title><link href="https://pyvideo.org/europython-2020/accessible-python-education-for-schoolgirls-using-avocados-zombies-and-korean.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Chiin-Rui Tan</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/accessible-python-education-for-schoolgirls-using-avocados-zombies-and-korean.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Imagine this school scenario: an entire year group of students aged 11-12, the majority completely new to coding, undergoing 6 hours of compulsory lessons on Python for Scientific Computing.&lt;/p&gt;
&lt;p&gt;Now imagine these outcomes:
•       Students wanting to continue coding from the lessons outside of class in their own time
•       Students …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Imagine this school scenario: an entire year group of students aged 11-12, the majority completely new to coding, undergoing 6 hours of compulsory lessons on Python for Scientific Computing.&lt;/p&gt;
&lt;p&gt;Now imagine these outcomes:
•       Students wanting to continue coding from the lessons outside of class in their own time
•       Students asking to replicate the lesson computing environment at home
•       Students disappointed for the lessons to come to an end and asking for more
•       Students struggling in Science discovering intrinsic ability in computing, bringing new enjoyment and confidence&lt;/p&gt;
&lt;p&gt;And lastly, imagine that all the students are girls!&lt;/p&gt;
&lt;p&gt;This talk will share this actual case study of a pioneering Python education initiative implemented at a secondary school for girls in London, UK for a cohort of 120 students.&lt;/p&gt;
&lt;p&gt;The audience will gain actionable insights of the factors that enabled these children to develop basic but working proficiency of a mainstream scientific data stack using typical school IT resources.&lt;/p&gt;
&lt;p&gt;Ultimately, this talk aims to increase awareness of Scientific Computing &amp;amp; Data Science as potentially effective and empowering Python education for young people.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Case Study"></category><category term="Data Science"></category><category term="Education"></category><category term="Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Automating machine learning workflow with DVC</title><link href="https://pyvideo.org/europython-2020/automating-machine-learning-workflow-with-dvc.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Hongjoo Lee</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/automating-machine-learning-workflow-with-dvc.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What data scientist / ML engineer wants to do while software engineers are busy with CI/CD.&lt;/p&gt;
&lt;p&gt;As software engineers work on CI/CD process as soon as they start a new project, data scientists and ML engineers define a pipeline for data as it flows through a typical workflow …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What data scientist / ML engineer wants to do while software engineers are busy with CI/CD.&lt;/p&gt;
&lt;p&gt;As software engineers work on CI/CD process as soon as they start a new project, data scientists and ML engineers define a pipeline for data as it flows through a typical workflow. Each step of the pipeline is fed data processed from its preceding step as CI/CD process starts from code changes.&lt;/p&gt;
&lt;p&gt;&amp;quot;Pipelining ML project&amp;quot; is sometimes misleading as it implies a large project with a group of engineers working on some large systems , being considered to be hard for an individual and unnecessary for a small project. Regardless of its size, having well organized pipelines for any ML projects is essential to succeed and actually it could be done easily with utilizing a proper tool.&lt;/p&gt;
&lt;p&gt;In this talk, we will go through a machine learning workflow divided into a few steps composing a ML pipeline from data ingestion to model deployment. Each step depends on data produced by previous step, which are controlled by DVC. DVC is open-source version control system for data scientist and ML engineer helping them to organize data, models and experiments for some ML projects. The presentation will not only introduce how to use the tool but also show how to organize a ML pipeline with some examples.&lt;/p&gt;
&lt;p&gt;The goal of this talk is to motivate data scientists and ML engineer to start building machine learning pipeline with DVC. Audience might expect a guide to using DVC  for automating the pipeline. Also I will give some explanation about concepts of machine learning related techniques necessary for understanding the pipeline.&lt;/p&gt;
&lt;p&gt;This session is designed to be accessible to everyone in beginners level. Understandings of basic concepts of machine learning and version control system (preferably, Git) might be helpful but not mandatory for the audience.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Big Data"></category><category term="Data"></category><category term="Data Science"></category><category term="Deployment/Continuous Integration and Delivery"></category></entry><entry><title>Building The Perfect Personalised Menu Using Python</title><link href="https://pyvideo.org/europython-2020/building-the-perfect-personalised-menu-using-python.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Irene Iriarte</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/building-the-perfect-personalised-menu-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How Gousto is building an algorithm to offer personalised menus to their customers using python&lt;/p&gt;
&lt;p&gt;This talk will describe how Gousto, a leading recipe box service based in the UK, is using python to build a personalisation ecosystem. Our menu planning optimisation algorithm allows us to create the perfect …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How Gousto is building an algorithm to offer personalised menus to their customers using python&lt;/p&gt;
&lt;p&gt;This talk will describe how Gousto, a leading recipe box service based in the UK, is using python to build a personalisation ecosystem. Our menu planning optimisation algorithm allows us to create the perfect mix of recipes, ensuring a variety of dish types, cuisines and ingredients. Our recommendation engine sitting on top of this can then offer each customer a personally curated menu, making sure that users have meaningful choice. All this while ensuring that we are also optimising for maximum performance from the operations point of view!&lt;/p&gt;
&lt;p&gt;To build this, we have used a range of Python packages, such as DEAP for implementing genetic algorithms, and integrations, such as the one for graph database neo4j.&lt;/p&gt;
&lt;p&gt;The talk will give an overview of our methods, our infrastructure, our results and everything that we have learnt along the way!&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Algorithms"></category><category term="Case Study"></category><category term="Data Science"></category><category term="E-Commerce"></category><category term="Machine-Learning"></category></entry><entry><title>Corona-Net</title><link href="https://pyvideo.org/europython-2020/corona-net.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Ching Lam Choi</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/corona-net.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Fighting COVID-19 with Machine Learning&lt;/p&gt;
&lt;p&gt;Identified in December 2019, the novel Coronavirus has infected 2.7 million worldwide, and claimed the lives of 0.2 million. Amidst this deadly pandemic, I started my open source project, Corona-Net, in the hopes of contributing to the global fight against the Coronavirus …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Fighting COVID-19 with Machine Learning&lt;/p&gt;
&lt;p&gt;Identified in December 2019, the novel Coronavirus has infected 2.7 million worldwide, and claimed the lives of 0.2 million. Amidst this deadly pandemic, I started my open source project, Corona-Net, in the hopes of contributing to the global fight against the Coronavirus. Corona-Net is a 3-part project dedicated to the classification, binary segmentation and multi-class segmentation of COVID-19. I first leverage the EfficientNet model for COVID-19 diagnosis, then utilise and refine the U-Net architecture for both binary and 3-class (ground-glass, consolidation, pleural effusion) segmentation of COVID-19 symptoms, through inference on the COVID-19 CT segmentation (chest axial CT) dataset. Through Corona-Net, I aim to develop a reliable, visual-semantically balanced method for automatic COVID-19 diagnosis, as well as extend an invitation to all to collaborate and stand together against this pandemic. My PyTorch code is publicly available at &lt;a class="reference external" href="https://github.com/chinglamchoi/Corona-Net"&gt;https://github.com/chinglamchoi/Corona-Net&lt;/a&gt;.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Computer Vision"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Image Processing"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Decision Science with Probabilistic Programming</title><link href="https://pyvideo.org/europython-2020/decision-science-with-probabilistic-programming.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Mattia Ferrini</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/decision-science-with-probabilistic-programming.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Make robust optimal decisions with Python!&lt;/p&gt;
&lt;p&gt;Generative Models are the Swiss Army Knife for the Decision Scientist. Generative models allow the simulation of scenarios based on different business hypotheses (Bayesian priors). With Probabilistic Programming, decision makers can simulate the impact of business drivers in times of great uncertainty.&lt;/p&gt;
&lt;p&gt;Furthermore …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Make robust optimal decisions with Python!&lt;/p&gt;
&lt;p&gt;Generative Models are the Swiss Army Knife for the Decision Scientist. Generative models allow the simulation of scenarios based on different business hypotheses (Bayesian priors). With Probabilistic Programming, decision makers can simulate the impact of business drivers in times of great uncertainty.&lt;/p&gt;
&lt;p&gt;Furthermore, Probabilistic Programming Languages provide all the inference tools necessary to identify the assumptions that have most likely generated an outcome. Inference is a statistical tool that enables optimal decision-making based on models that explicitly quantify uncertainty.&lt;/p&gt;
&lt;p&gt;Generative models of key optimization parameters are necessary input to Robust Optimization and Stochastic Programming problems.
Python provides all the tools to successfully integrate Probabilitistic Programs with Robust and Stochastic Optimization and therefore cope with high uncertainty in optimization.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Functional Programming"></category><category term="Science"></category></entry><entry><title>Developing a match-making algorithm between customers and Go-Jek products!</title><link href="https://pyvideo.org/europython-2020/developing-a-match-making-algorithm-between-customers-and-go-jek-products.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Gunjan Dewan</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/developing-a-match-making-algorithm-between-customers-and-go-jek-products.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;GoJek has millions of monthly active users in Indonesia across our 20+ products and services. A major problem we faced was targeting these customers with promos and vouchers that were relevant to them. We developed a generalized model that takes into account the transaction history of users and gives …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;GoJek has millions of monthly active users in Indonesia across our 20+ products and services. A major problem we faced was targeting these customers with promos and vouchers that were relevant to them. We developed a generalized model that takes into account the transaction history of users and gives a ranked list of our services that they are most likely to use next. From here on, we are able to determine the vouchers that we can target these customers with.
In this poster, I will be presenting our process while developing the model, the challenges we faced during the time, how we used PySpark to tackle these challenges and the impact it had on our conversion rates.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Data"></category><category term="Data Science"></category><category term="Machine-Learning"></category></entry><entry><title>Docker and Python: making them play nicely and securely for Data Science and ML</title><link href="https://pyvideo.org/europython-2020/docker-and-python-making-them-play-nicely-and-securely-for-data-science-and-ml.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Tania Allard</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/docker-and-python-making-them-play-nicely-and-securely-for-data-science-and-ml.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Docker has become a standard tool for developers around the world to deploy applications in a reproducible and robust manner. The existence of Docker and Docker compose have reduced the time needed to set up new software and implementing complex technology stacks for our applications. Now, six years after …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Docker has become a standard tool for developers around the world to deploy applications in a reproducible and robust manner. The existence of Docker and Docker compose have reduced the time needed to set up new software and implementing complex technology stacks for our applications. Now, six years after the initial release of Docker, we can say with confidence that containers and containers orchestration have become some of the defaults in the current technology stacks.&lt;/p&gt;
&lt;p&gt;There are thousands of tutorials and getting started documents for those wanting to adopt Docker for apps deployment. However, if you are a Data Scientist, a researcher or someone working on scientific computing wanting to adopt Docker, the story is quite different. There are very few tutorials (in comparison to app/web) and documents focused on Docker best practices for DS and scientific computing. If you are working on DS, ML or scientific computing, this talk is for you. We'll cover best practices when building Docker containers for data-intensive applications, from optimising your image build, to ensuring your containers are secure and efficient deployment workflows. We will talk about the most common problems faced while using Docker with data intensive applications and how you can overcome most of them. Finally I'll give some practical and useful tips for you to improve your Docker workflows and practises.&lt;/p&gt;
&lt;p&gt;Attendees will leave the talk feeling confident about adopting Docker across a range of DS, ML and research projects.&lt;/p&gt;
&lt;p&gt;Who and Why (audience)
This talk is designed for folks working in data-intensive environments (i.e. Machine Learning, Data Science, research and scientific computing) and that are either using Docker or want to learn more about how to use Docker in these environments. Attendees will leave the talk feeling confident about adopting Docker in their workflows as well as have acquired several best practices and guidelines to do this robustly.
Introduction (5 minutes)
About me
When is Docker the right choice?
Docker for all Python users: introduction to Docker in Machine Learning (ML), Data Science (DS) and research contexts
The usual culprits
Optimising for data-oriented application (10 minutes)
Creating a data-oriented Docker image - how is this different from an app/web image?
Choosing the right base image - set yourself for success
Dependencies, volumes and code best practices
Security and performance (10 minutes)
Finding vulnerabilities in your images
Image consistency and reproducibility
Optimising image building - cache and image size considerations
Do not reinvent the wheel - automate! (10 minutes)
Consider tools to assist with Dockerfile generation - e.g. repo2docker, dokta
Creating templates for projects
Automating image build and publishing - e.g. GitHub actions
Automated deployment strategies - going from local to deploying your containerised application
Conclusions (5 minutes)
Top 10 best practices when working with Docker and Python for DS/ML and research
Additional resources
Thanks and getting in touch&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Conda / conda forge"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Making Pandas Fly</title><link href="https://pyvideo.org/europython-2020/making-pandas-fly.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Ian Ozsvald</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/making-pandas-fly.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Process bigger-than-RAM data using Pandas, Dask and Vaex&lt;/p&gt;
&lt;p&gt;Larger datasets can't fit into RAM - suddenly you can't use Pandas any more - but we need to analyse that data! First we'll review techniques to compress our data (maybe cutting our DataFrame RAM usage in half!) so we can process more …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Process bigger-than-RAM data using Pandas, Dask and Vaex&lt;/p&gt;
&lt;p&gt;Larger datasets can't fit into RAM - suddenly you can't use Pandas any more - but we need to analyse that data! First we'll review techniques to compress our data (maybe cutting our DataFrame RAM usage in half!) so we can process more rows using regular Pandas. Next we'll look at clever ways to make common operations run faster on DataFrames including dropping down to numpy, compiling with Numba and running multi-core. Finally for still-larger datasets we'll review Dask on Pandas and the new Vaex competitor solution. You'll leave with new techniques to make your DataFrames smaller and ideas for processing your data faster.
This talk is inspired by Ian's work updating his O'Reilly book High Performance Python to the 2nd edition for 2020. With over 10 years of evolution the Pandas DataFrame library has gained a huge amount of functionality and it is used by millions of Pythonistas - but the most obvious way to solve a task isn't always the fastest or most RAM efficient. This talk will help any Pandas user (beginner or beyond) process more data faster, making them more effective at their jobs.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Multi-Processing"></category><category term="Performance"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Mastering a data pipeline with Python: 6 years of learned lessons from mistakes</title><link href="https://pyvideo.org/europython-2020/mastering-a-data-pipeline-with-python-6-years-of-learned-lessons-from-mistakes.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Robson Junior</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/mastering-a-data-pipeline-with-python-6-years-of-learned-lessons-from-mistakes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Building data pipelines are a consolidated task, there are a vast number of tools that automate and help developers to create data pipelines with few clicks on the cloud. It might solve non-complex or well-defined standard problems. This presentation is a demystification of years of experience and painful mistakes …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Building data pipelines are a consolidated task, there are a vast number of tools that automate and help developers to create data pipelines with few clicks on the cloud. It might solve non-complex or well-defined standard problems. This presentation is a demystification of years of experience and painful mistakes using Python as a core to create reliable data pipelines and manage insanely amount of valuable data. Let's cover how each piece fits into this puzzle: data acquisition, ingestion, transformation, storage, workflow management and serving. Also, we'll walk through best practices and possible issues. We'll cover PySpark vs Dask and Pandas, Airflow, and Apache Arrow as a new approach.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Beginners"></category><category term="Big Data"></category><category term="Case Study"></category><category term="Data Science"></category><category term="Open-Source"></category></entry><entry><title>NLPeasy - a Workflow to Analyse, Enrich, and Explore Textual Data</title><link href="https://pyvideo.org/europython-2020/nlpeasy-a-workflow-to-analyse-enrich-and-explore-textual-data.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Philipp Thomann</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/nlpeasy-a-workflow-to-analyse-enrich-and-explore-textual-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Use pre-trained NLP-models, ingest into Elastic Search, and enjoy auto-generated Kibana dashboards!&lt;/p&gt;
&lt;p&gt;Ever wanted to try out NLP methods but it felt it too cumbersome to set up a workflow for textual data? How to enrich your data based on textual features and explore the results?&lt;/p&gt;
&lt;p&gt;NLPeasy (&lt;a class="reference external" href="https://github.com/d-one/NLPeasy"&gt;https://github …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Use pre-trained NLP-models, ingest into Elastic Search, and enjoy auto-generated Kibana dashboards!&lt;/p&gt;
&lt;p&gt;Ever wanted to try out NLP methods but it felt it too cumbersome to set up a workflow for textual data? How to enrich your data based on textual features and explore the results?&lt;/p&gt;
&lt;p&gt;NLPeasy (&lt;a class="reference external" href="https://github.com/d-one/NLPeasy"&gt;https://github.com/d-one/NLPeasy&lt;/a&gt;) does that: Enrich the data using well-known pre-trained models (Word embeddings, Sentiment Analysics, POS, Dependency Parsing). Then start the Elastic Stack on your Docker. Set-up indices and ingest it in bulk. And finally generate Kibana dashboards to explore the results.&lt;/p&gt;
&lt;p&gt;Complicated? Not at all! Just do it in a simple Jupyter Notebook.&lt;/p&gt;
&lt;p&gt;In this presentation we will give an architecture overview of the different components and demonstrate the capabilities of this Python package.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Data Science"></category><category term="Elastic Search"></category><category term="Machine-Learning"></category><category term="Natural Language Processing"></category></entry><entry><title>Painless Machine Learning in Production</title><link href="https://pyvideo.org/europython-2020/painless-machine-learning-in-production.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Chase Stevens</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/painless-machine-learning-in-production.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Developing machine learning models is easy; training, deploying, monitoring, scaling, and maintaining them in an automated fashion - all while maintaining your sanity - is hard.&lt;/p&gt;
&lt;p&gt;In this session, I'll discuss the infrastructure and tooling my small team of data science practitioners and engineers is using to manage and orchestrate the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Developing machine learning models is easy; training, deploying, monitoring, scaling, and maintaining them in an automated fashion - all while maintaining your sanity - is hard.&lt;/p&gt;
&lt;p&gt;In this session, I'll discuss the infrastructure and tooling my small team of data science practitioners and engineers is using to manage and orchestrate the machine learning model lifecycle, including pitfalls we've encountered along the way. Particular attention will be paid to where we've opted to use off-the-shelf solutions versus developing our own, the importance of developer ergonomics, and how to maximally empower data scientists to get their work into production without the need for a dedicated MLOps team.&lt;/p&gt;
&lt;p&gt;The talk will cover our ML stack as it exists in production today, and will touch on our application of a number of technologies and techniques, including:
- AWS SageMaker
- Airflow
- Docker
- Cookiecutter
- Property-based testing
- Jsonschema
- Linting
- Slack integration
- Model artifacts and diagnostics
- Automated deployments and rollbacks
- Healthchecks
- Autoscaling
- DBT&lt;/p&gt;
&lt;p&gt;At the end of the session, attendees should expect to leave with new insights that they can apply immediately to their own ML systems and infrastructure, as well as a better understanding of how to minimize engineering and ops overhead, in the real world, across data science teams of any size and composition.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Case Study"></category><category term="Data Science"></category><category term="DevOps general"></category><category term="Infrastructure"></category><category term="Machine-Learning"></category></entry><entry><title>Practical Optimisations for Pandas</title><link href="https://pyvideo.org/europython-2020/practical-optimisations-for-pandas.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Eyal Trabelsi</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/practical-optimisations-for-pandas.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Writing performant pandas code is not an easy task, in this talk I will explain how to find the bottlenecks and how to write proper code with computational efficiency, and memory optimisation in mind.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Best Practice"></category><category term="Data"></category><category term="Data Science"></category></entry><entry><title>Probabilistic Forecasting with DeepAR and AWS SageMaker</title><link href="https://pyvideo.org/europython-2020/probabilistic-forecasting-with-deepar-and-aws-sagemaker.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Nicolas Kuhaupt</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/probabilistic-forecasting-with-deepar-and-aws-sagemaker.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In time series forecasting we are interested in how the time series is going to continue in the future. This is of high importance in areas like forecasting energy production from renewable resources, forecasting demand of customers or the price of products. Many forecasting algorithms provide only the prediction …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In time series forecasting we are interested in how the time series is going to continue in the future. This is of high importance in areas like forecasting energy production from renewable resources, forecasting demand of customers or the price of products. Many forecasting algorithms provide only the prediction. However, oftentimes we are also interested in the likelihood of the prediction and how much it may vary. This is what probabilistic forecasting is for. With every forecast, we also obtain an upper and lower bound with certain probabilities. For a long time, probabilistic forecasting was limited to traditional techniques like ARIMA. DeepAR is an algorithm that allows us to combine Deep Learning techniques with probabilistic forecasting. Additionally, in contrast to training a model for each time series individually, DeepAR suggests training one large forecasting model for all related time series. The algorithm was developed by Amazon and is also provided in AWS SageMaker.
In this talk, we will understand the theoretical basics of DeepAR, have a look at a practical time series example and will demonstrate an implementation. In the end, you will be prepared to get started with your own forecasts.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category><category term="Predictions"></category><category term="Public Cloud (AWS/Google/...)"></category></entry><entry><title>Sharing Reproducible Python Environments with Binder</title><link href="https://pyvideo.org/europython-2020/sharing-reproducible-python-environments-with-binder.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Sarah Gibson</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/sharing-reproducible-python-environments-with-binder.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As reproducibility gains traction in the data science and research communities, the need to package code, data and the computational environment is growing.&lt;/p&gt;
&lt;p&gt;There are many tools that address different aspects of this type of packaging, such as Jupyter Notebooks for literate programming, Docker for containerising and porting computational …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As reproducibility gains traction in the data science and research communities, the need to package code, data and the computational environment is growing.&lt;/p&gt;
&lt;p&gt;There are many tools that address different aspects of this type of packaging, such as Jupyter Notebooks for literate programming, Docker for containerising and porting computational environments, and so on. But they represent barriers to reproducibility as each one requires time and effort to learn.&lt;/p&gt;
&lt;p&gt;Project Binder integrates Notebooks and Docker for generating reproducible computational analyses and combines them with a web-based interface and cloud orchestration engines. This means that analysts do not have to worry about all the moving parts so long as they have followed basic software best practices: their code is version controlled and they've captured the dependencies the analysis needs to run. Binder then hosts the compute in the cloud and makes it easily shareable by providing a unique URL to the code repository, without imposing additional overheads on the analyst.&lt;/p&gt;
&lt;p&gt;During this talk, Sarah will introduce Binder (the service), BinderHub (the technological infrastructure) and mybinder.org (a public instance of a Binder service, free for anyone to use) and demonstrate how it can be used to share Python environments and analyses.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Best Practice"></category><category term="Data Science"></category><category term="Jupyter"></category><category term="Open-Source"></category><category term="Public Cloud (AWS/Google/...)"></category></entry><entry><title>Speed Up Your Data Processing</title><link href="https://pyvideo.org/europython-2020/speed-up-your-data-processing.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Chin Hwee Ong</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/speed-up-your-data-processing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Parallel and Asynchronous Programming in Data Science&lt;/p&gt;
&lt;p&gt;In a data science project, one of the biggest bottlenecks (in terms of time) is the constant wait for the data processing code to finish executing. Slow code, as well as connectivity issues, affect every step of a typical data science workflow …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Parallel and Asynchronous Programming in Data Science&lt;/p&gt;
&lt;p&gt;In a data science project, one of the biggest bottlenecks (in terms of time) is the constant wait for the data processing code to finish executing. Slow code, as well as connectivity issues, affect every step of a typical data science workflow — be it for network I/O operations or computation-driven workloads. In this talk, I will be sharing about common bottlenecks in data processing within a typical data science workflow, and exploring the use of parallel and asynchronous programming using concurrent.futures module in Python to speed up your data processing pipelines so that you could focus more on getting value out of your data. Through real-life analogies, you will learn about:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Sequential vs parallel processing,&lt;/li&gt;
&lt;li&gt;Synchronous vs asynchronous execution,&lt;/li&gt;
&lt;li&gt;Network I/O operations vs computation-driven workloads in a data science workflow,&lt;/li&gt;
&lt;li&gt;When is parallelism and asynchronous programming a good idea,&lt;/li&gt;
&lt;li&gt;How to implement parallel and asynchronous programming using concurrent.futures module to speed up your data processing pipelines&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This talk assumes basic understanding of data pipelines and data science workflows. While the main target audience are data scientists and engineers building data pipelines, the talk is designed such that anyone with a basic understanding of the Python language would be able to understand the illustrated concepts and use cases.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="ASYNC / Concurreny"></category><category term="Data"></category><category term="Data Science"></category><category term="Multi-Processing"></category><category term="Multi-Threading"></category></entry><entry><title>Supercharge your Data Science workflow with Notebooks, VS Code, and Azure</title><link href="https://pyvideo.org/europython-2020/supercharge-your-data-science-workflow-with-notebooks-vs-code-and-azure.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Jeffrey Mew</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/supercharge-your-data-science-workflow-with-notebooks-vs-code-and-azure.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter Notebooks and interactive programming have become one of the most popular tools for developing Python due to its flexibility and ease of use. Visual Studio Code now offers a first class Notebooks experience along with many great features for Data Scientists and Python developers alike. Come see how …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter Notebooks and interactive programming have become one of the most popular tools for developing Python due to its flexibility and ease of use. Visual Studio Code now offers a first class Notebooks experience along with many great features for Data Scientists and Python developers alike. Come see how you can explore, experiment and productionize machine learning models using the flexibility of Notebooks combined with the power and productivity of VS Code. Along the way we’ll also be connecting to several Azure services from within VS Code, including Azure Virtual Machines for running the training job, Azure Storage for storing the dataset and model, Azure Functions for deploying our model and endpoint, and Azure Web Service for running a web application for making predictions.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Data Science"></category><category term="Jupyter"></category></entry><entry><title>The Python Data Visualization Landscape in 2020</title><link href="https://pyvideo.org/europython-2020/the-python-data-visualization-landscape-in-2020.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Bence Arató</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/the-python-data-visualization-landscape-in-2020.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python offers many different data visualization libraries, and the sheer number of alternatives  can be daunting to newcomers.&lt;/p&gt;
&lt;p&gt;This talk aims to introduce the most important visualization libraries, covering Matplotlib, Plotly, Bokeh and Altair, among others. It also provides a summary of the quickly developing dashboarding solutions, including Dash …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python offers many different data visualization libraries, and the sheer number of alternatives  can be daunting to newcomers.&lt;/p&gt;
&lt;p&gt;This talk aims to introduce the most important visualization libraries, covering Matplotlib, Plotly, Bokeh and Altair, among others. It also provides a summary of the quickly developing dashboarding solutions, including Dash, Panel and Voila.&lt;/p&gt;
&lt;p&gt;The goal of talk is not just to provide a simple list of libraries, but also to highlight the main characteristics and inspirations for each, and summarize the recent developments as well.&lt;/p&gt;
&lt;p&gt;This talk is aimed to people who have some basic experience working with data in Python and would like to get a better understanding of the data visualization tool landscape.  Some existing knowledge of pandas DataFrames is beneficial for understanding the examples, but not required.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Analytics"></category><category term="Data"></category><category term="Data Science"></category><category term="Visualization"></category></entry><entry><title>Top 15 Python Tips for Data Cleaning/ Understanding</title><link href="https://pyvideo.org/europython-2020/top-15-python-tips-for-data-cleaning-understanding.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Hui Xiang Chua</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/top-15-python-tips-for-data-cleaning-understanding.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data cleaning is one of the most important tasks in data science but it is unglamorous, underappreciated and under-discussed. These are some common tasks involved in data cleaning but not limited to:
-       Merging/ appending
-       Checking completeness of data
-       Checking of valid values
-       De-duplication
-       Handling of missing values
-       Recoding&lt;/p&gt;
&lt;p&gt;Most …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data cleaning is one of the most important tasks in data science but it is unglamorous, underappreciated and under-discussed. These are some common tasks involved in data cleaning but not limited to:
-       Merging/ appending
-       Checking completeness of data
-       Checking of valid values
-       De-duplication
-       Handling of missing values
-       Recoding&lt;/p&gt;
&lt;p&gt;Most, if not all, of the time, the datasets that we have to analyze are unclean. i.e. they are not necessarily complete/ accurate/ valid. This will impact the accuracy of our analysis if we do not clean them properly.&lt;/p&gt;
&lt;p&gt;This talk covers how to perform data cleaning and understanding using primarily Pandas and Numpy. If you’re new to data analytics/ data science and are interested how to use Python to perform analysis, or if you're an Excel user hoping to move to Python, this talk might be for you.&lt;/p&gt;
&lt;p&gt;Participants should be at least familiar with the basics of Python programming.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Beginners"></category><category term="Data"></category><category term="Data Science"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category><category term="Use Case"></category></entry><entry><title>Train. Serve. Deploy! Story of a NLP Model ft. PyTorch, Docker, Uwsgi and Nginx</title><link href="https://pyvideo.org/europython-2020/train-serve-deploy-story-of-a-nlp-model-ft-pytorch-docker-uwsgi-and-nginx.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Shreya Khurana</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/train-serve-deploy-story-of-a-nlp-model-ft-pytorch-docker-uwsgi-and-nginx.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Quickly prototype a machine translation model from scratch and learn how to serve it in production&lt;/p&gt;
&lt;p&gt;Natural language processing has seen leaps of technology progress with Machine Learning becoming the norm of solving the major problems in this area, with Machine translation being one of the major problems in …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Quickly prototype a machine translation model from scratch and learn how to serve it in production&lt;/p&gt;
&lt;p&gt;Natural language processing has seen leaps of technology progress with Machine Learning becoming the norm of solving the major problems in this area, with Machine translation being one of the major problems in this area. Neural machine translation systems are now used to convert sentences or phrases from one language to another, or in general, for sequence to sequence modeling. In this talk, we’ll be covering the steps from scratch to preprocess, train and serve a NMT model using PyTorch. While building a highly accurate model is a prerequisite to getting good quality translations, often in industry, we also need to make sure we can serve the model to customers without getting timeouts or delays. The practice of serving models requires creating a web app to get client requests and process them in a way the model would understand. For this, we’ll use  the various components of the application server environment - Flask, Docker, uwsgi and nginx. This talk is suitable for audience who is working in general with ML models and want to learn how to serve them or working specifically with NMT and want to learn about some quick prototyping tips.&lt;/p&gt;
&lt;p&gt;Prerequisites: Audience should be comfortable with the basic ML terminology and procedure of training models. NLP knowledge will be good, but is not a necessity as the focus will be on quick prototyping in production.&lt;/p&gt;
&lt;p&gt;By the end of the talk, the audience will have:
- Learnt how to preprocess data for NLP systems
- Learnt how to quickly prototype and train a translation model
- Learnt how to create a web app for the NLP model using Flask
- Learnt how to containerize a pytorch model using Docker
- Learnt how to serve the model as an app using uwsgi, nginx and&lt;/p&gt;
&lt;p&gt;Outline:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to translation systems, machine translation framework&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ML Modelling
- Preprocessing data
- Training
- Generating new translations&lt;/p&gt;
&lt;p&gt;Serving and prototyping
- Flask app
- Docker container
- Nginx + uwsgi + supervisord configurations
- Putting it all together&lt;/p&gt;
&lt;p&gt;Good practices
Q/A (optional?)&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Machine-Learning"></category><category term="Natural Language Processing"></category><category term="Web Servers and MicroFWs (Flask/Tornado/Nginx/...)"></category></entry><entry><title>Writing Extensions and Bindings for GPU made Easy</title><link href="https://pyvideo.org/europython-2020/writing-extensions-and-bindings-for-gpu-made-easy.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Krishna Kanta Singh</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/writing-extensions-and-bindings-for-gpu-made-easy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Writing Bindings for C and CUDA code and Packaging it with setup.py in 30 min or less&lt;/p&gt;
&lt;p&gt;As Deep Learning Engineer and Researcher we are always trying to optimize some bottleneck computation in our programs. Sometimes we are faced with situations when scientific libraries like NumPy, SciPy aren't …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Writing Bindings for C and CUDA code and Packaging it with setup.py in 30 min or less&lt;/p&gt;
&lt;p&gt;As Deep Learning Engineer and Researcher we are always trying to optimize some bottleneck computation in our programs. Sometimes we are faced with situations when scientific libraries like NumPy, SciPy aren't just cutting it or worse there are no libraries that implement the esoteric function on our expensive GPU hardware. Writing Custom C and Cuda Extension becomes an important skill and necessity for applications that require really fast computation.&lt;/p&gt;
&lt;p&gt;In this talk, we go through a detailed example of image search on billions of items, we write custom C and Cuda kernel for distance computation and learn how to connect them seamlessly with our python codebase. We compare methods for writing these extensions and bindings for python in terms of both speed and ease of use. Finally, we make it all work together by hacking the setup.py file for easy deployment and sharing of the&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="C-Languages"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="GPU"></category><category term="Packaging"></category></entry><entry><title>Open Sourcing at Work</title><link href="https://pyvideo.org/pycon-ca-2018/open-sourcing-at-work.html" rel="alternate"></link><published>2018-11-11T00:00:00+00:00</published><updated>2018-11-11T00:00:00+00:00</updated><author><name>Faisal Dosani</name></author><id>tag:pyvideo.org,2018-11-11:/pycon-ca-2018/open-sourcing-at-work.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We just open sourced 2 projects (datacompy, and locopy) with roots in Data Science and Engineering which we will showcase. While is it exciting and rewarding to share your ideas with the world it isn't always easy. Thinking about licenses, copyrights, and protecting confidential information is a must!&lt;/p&gt;
&lt;p&gt;Working …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We just open sourced 2 projects (datacompy, and locopy) with roots in Data Science and Engineering which we will showcase. While is it exciting and rewarding to share your ideas with the world it isn't always easy. Thinking about licenses, copyrights, and protecting confidential information is a must!&lt;/p&gt;
&lt;p&gt;Working in a large organization which is embracing the mantra 'open source first' is really exciting. Part of this journey is to make sure we give back to the open source community when we can. Two of our projects had gained traction internally: datacompy, and locopy. As part of our commitment we wanted to make sure we could open source these projects for others to use and contribute back to. DataComPy is a package to compare two Pandas DataFrames. Originally started to be something of a replacement for SAS's PROC COMPARE for Pandas DataFrames with some more functionality than just Pandas.DataFrame.equals(Pandas.DataFrame) (in that it prints out some stats, and lets you tweak how accurate matches have to be). Then extended to carry that functionality over to Spark Dataframes. Locopy helps load flat files to S3 and then to Amazon Redshift, and assist with ETL processing. It is DB Driver (Adapter) agnostic, provides basic functionality to move data to S3 buckets, execute COPY commands to load data to S3, and into Redshift, and UNLOAD commands to unload data from Redshift into S3. While building these products was exciting and fun, some of the legal considerations were as interesting, complex, and required collaboration between many teams, from security, licensing, brand, and IP/copyright. We'll explore the projects, and some of these other considerations which can make or break if you decide to release a project into the wild, along with the road blocks we faced with in these areas.&lt;/p&gt;
</content><category term="PyCon CA 2018"></category><category term="open source"></category><category term="licensing"></category><category term="copyright"></category><category term="data"></category><category term="security"></category><category term="testing"></category><category term="best practices"></category><category term="data science"></category></entry><entry><title>An introduction to PyMC3</title><link href="https://pyvideo.org/pycon-de-2017/an-introduction-to-pymc3.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Adrian Seyboldt</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/an-introduction-to-pymc3.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Adrian Seyboldt&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I studied Mathematics and Bioinformatics in Bonn and Tübingen and I am a core developer of pymc3 since Feb 2017. Currently, I work for Quantopian on the development of Bayesian Methods for portfolio allocation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;PyMC3 allows you to build statistical models for a wide range of …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Adrian Seyboldt&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I studied Mathematics and Bioinformatics in Bonn and Tübingen and I am a core developer of pymc3 since Feb 2017. Currently, I work for Quantopian on the development of Bayesian Methods for portfolio allocation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;PyMC3 allows you to build statistical models for a wide range of datasets, use those models to estimate underlying parameters, and compute the uncertainty about those parameters. In this talk I will try to give a gentle introduction to PyMC3, and help avoid common pitfalls for new users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some of the problems that are discussed in the context of the reproducibility crisis in science and statistics can be solved or alleviated by tools like PyMC3 or Stan. They allow users to build much more realistic models and get a full distribution of the possible values for parameters as output – instead of p-values that are often hard to interpret correctly. Thanks to Hamiltonian and Variational methods, they are more flexible and can be applied to larger problems than predecessors like JAGS and BUGS. However, these new methods also come with challenges. Writing good models isn't easy, and when inference algorithms cry out in pain, they need someone who listens to them. This talk uses some real-world applications to give an introduction to PyMC3, without requiring a lot of background in math, statistics or programming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="python"></category><category term="data-science"></category><category term="machine learning"></category><category term="analysis"></category></entry><entry><title>Connecting PyData to other Big Data Landscapes using Arrow and Parquet</title><link href="https://pyvideo.org/pycon-de-2017/connecting-pydata-to-other-big-data-landscapes-using-arrow-and-parquet.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/connecting-pydata-to-other-big-data-landscapes-using-arrow-and-parquet.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Uwe L. Korn&lt;/strong&gt; (&amp;#64;xhochy)&lt;/p&gt;
&lt;p&gt;Uwe Korn is a Data Scientist at the Karlsruhe-based RetailTec company Blue Yonder. His expertise is on building architectures for machine learning services that are scalably usable for multiple customers aiming at high service availability as well as rapid prototyping of solutions to evaluate the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Uwe L. Korn&lt;/strong&gt; (&amp;#64;xhochy)&lt;/p&gt;
&lt;p&gt;Uwe Korn is a Data Scientist at the Karlsruhe-based RetailTec company Blue Yonder. His expertise is on building architectures for machine learning services that are scalably usable for multiple customers aiming at high service availability as well as rapid prototyping of solutions to evaluate the feasibility of his design decisions. As part of his work to provide an efficient data interchange he became a core committer to the Apache Parquet and Apache Arrow projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While Python itself hosts a wide range of machine learning and data tools, other ecosystems like the Hadoop world also provide beneficial tools that can be either connected via Apache Parquet files or in memory using Arrow. This talks shows recent developments that allow interoperation at speed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Python has a vast amount of libraries and tools in its machine learning and data analysis ecosystem. Although it is clearly in competition with R here about the leadership, the world that has sprung out of the Hadoop ecosystem has established itself in the space of data engineering and also tries to provide tools for distributed machine learning. As these stacks run in different environments and are mostly developed by distinct groups of people, using them together has been a pain. While Apache Parquet has already proven itself as the gold standard for the exchange of DataFrames serialized to files, Apache Arrow recently got traction as the in-memory format for DataFrame exchange between different ecosystems.&lt;/p&gt;
&lt;p&gt;This talk will outline how Apache Parquet files can be used in Python and how they are structured to provide efficient DataFrame exchange. In addition to small code sample, this also includes an explanation of some interesting details of the file format. Additionally, the idea of Apache Arrow will be presented and taking Apache Spark (2.3) as an example to showcase how performance increases once DataFrames can be efficiently shared between Python and JVM processes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="data-science"></category><category term="hadoop"></category><category term="apache"></category><category term="arrow"></category><category term="parquet"></category><category term="pandas"></category><category term="pydata"></category></entry><entry><title>Data Plumbing 101 - ETL Pipelines for Everyday Projects</title><link href="https://pyvideo.org/pycon-de-2017/data-plumbing-101-etl-pipelines-for-everyday-projects.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Eberhard Hansis</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/data-plumbing-101-etl-pipelines-for-everyday-projects.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Eberhard Hansis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have been writing code for more than two thirds of my life, mostly for scientific computing and data analysis. In the past couple of years, I have worked on a range of different data science projects, all using Python at their core. During this time, I …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Eberhard Hansis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have been writing code for more than two thirds of my life, mostly for scientific computing and data analysis. In the past couple of years, I have worked on a range of different data science projects, all using Python at their core. During this time, I repeatedly was tasked with making data usable by joining multiple sources into a clearly defined data model. Once you have done that, it is amazing how much real-life value you can generate with little more than a bit of statistics and visualization. The world is full of underused data, let’s change that!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There is no data science without ETL! This presentation is about implementing maintainable data integration for your projects. We will have a first look a ‘Ozelot’, a library based on Luigi and SQLAlchemy that helps you get started with building ETL pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ETL, the hard way&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You are starting a new data science project, and you can’t wait to perform some machine learning magic. However, before getting to ML, you have to deal with its ugly sibling: ETL. Extracting, transforming and loading data (or, more generally, data integration) is an indispensable first step in almost any data project.&lt;/p&gt;
&lt;p&gt;In your project you will, most likely, have to extract data from various sources, clean it, link it and prepare it to your needs. You will start writing a first data integration script for some part of the process, then a second, then a third. At some point you will write an ugly ‘master’ script to keep your 17 import scripts in check and run them in just the right order. When you come back to the code later, you will have a hard time deciphering what you did and why, and what format the output data is in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pipelines to the rescue&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Implementing a proper data integration pipeline and a well-defined data model helps document your data flows and makes them traceable. More importantly, it simplifies the ETL development process, because it lets you easily re-run the whole process or parts of it. And you will have to modify and re-run your ETL, because your code changes, your output requirements change or the data changes.&lt;/p&gt;
&lt;p&gt;In this talk I propose a setup for building maintainable data integration pipelines for everyday projects. This setup is embodied by ‘Ozelot’, my brand-new Python library for ETL. It is based on Luigi for pipeline management and SQLAlchemy as ORM layer. Ozelot gives you core functionality to quickly start building your own solution, including an ORM base class, database connection management and Luigi task classes that play nice with the ORM. It comes with extensively documented examples that walk you through various aspects of data integration.&lt;/p&gt;
&lt;p&gt;The proposed setup works well for many small- to medium-sized projects -- projects, for which you previously might not have implemented a proper data integration pipeline. For big-data projects or those requiring live streaming data you probably want to consider alternative solutions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Core principles of data integration&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Taking one step back, I propose the following core principles for maintainable data integration:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Any and all data manipulation happens in the pipeline, in a single code base.&lt;/li&gt;
&lt;li&gt;The pipeline represents all dependencies between data integration tasks.&lt;/li&gt;
&lt;li&gt;Each task has a method for rolling back its operations.&lt;/li&gt;
&lt;li&gt;Data is loaded into a single database, in a clearly defined, object-based data model that also encodes object relationships.&lt;/li&gt;
&lt;li&gt;The whole process is fully automatic and thereby reproducible and traceable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will discuss why I think that these principles are important, and how they are reflected in the proposed setup.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="data-science"></category><category term="analytics"></category><category term="python"></category></entry><entry><title>Data Science Project for Beginners</title><link href="https://pyvideo.org/pycon-de-2017/data-science-project-for-beginners.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Natalie Speiser</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/data-science-project-for-beginners.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Natalie Speiser,Jens Beyer&lt;/strong&gt; (&amp;#64;natalie_lavrio)&lt;/p&gt;
&lt;p&gt;Natalie is a psychologist focused on statistics and machine learning for predictions. Jens is a pyhsicist turned consultant for IBM and d-fine and helped big companies with statistical models since 2009. Together, we founded LAVRIO.solutions and help our clients to make the most …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Natalie Speiser,Jens Beyer&lt;/strong&gt; (&amp;#64;natalie_lavrio)&lt;/p&gt;
&lt;p&gt;Natalie is a psychologist focused on statistics and machine learning for predictions. Jens is a pyhsicist turned consultant for IBM and d-fine and helped big companies with statistical models since 2009. Together, we founded LAVRIO.solutions and help our clients to make the most out of their data. In our recent data science projects, we faced specific hurdles which seem to be typical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;AI and Machine Learning are taking over the world - but how do you actually start with understanding your data and predicting events? And what kind of &amp;quot;political&amp;quot; trouble could you run into? With examples from real projects, we try to give you a feeling for data science projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will be talking about examples from projects we did as data science consultants. What should be organized before you go to a client (internal or external). How should the data look like. What are problems you have to face while working with the clients IT department. You get to see a rough draft of our code. It won't be a thorough manual, just our experiences and how we dealt with problems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="business"></category><category term="data-science"></category><category term="use-case"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Deep Learning for Computer Vision</title><link href="https://pyvideo.org/pycon-de-2017/deep-learning-for-computer-vision.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Alex Conway</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/deep-learning-for-computer-vision.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Alex Conway&lt;/strong&gt; (&amp;#64;alxcnwy)
Alex is the founder &amp;amp; CTO of NumberBoost, a startup that builds deep learning applications. He previously worked as a quant for a hedge fund and as a data scientist for an e-commerce company. He has an honours degree in actuarial science and a MSc in statistics …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Alex Conway&lt;/strong&gt; (&amp;#64;alxcnwy)
Alex is the founder &amp;amp; CTO of NumberBoost, a startup that builds deep learning applications. He previously worked as a quant for a hedge fund and as a data scientist for an e-commerce company. He has an honours degree in actuarial science and a MSc in statistics. He is one of the organizers of the Cape Town Deep Learning meet-up and has built numerous computer vision systems that run at scale in production predicting labels for millions of images per day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The state-of-the-art in image classification has skyrocketed thanks to the development of deep convolutional neural networks and increases in the amount of data and computing power available to train them. The top-5 error rate in the ImageNet competition to predict which of 1000 classes an image belongs to has plummeted from 28% error in 2010 to just 2.25% in 2017 (human level error is around 5%).&lt;/p&gt;
&lt;p&gt;In addition to being able to classify objects in images (including not hotdogs), deep learning can be used to automatically generate captions for images, convert photos into paintings, detect cancer in pathology slide images, and help self-driving cars ‘see’.&lt;/p&gt;
&lt;p&gt;The talk will give an overview of the cutting edge and some of the core mathematical concepts and will also include a short code-first tutorial to show how easy it is to get started using deep learning for computer vision in python…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk is a crash course on convolutional neural networks and how to use them to solve 2 real-world applications at scale. The first is an image moderation system and the second is a visual similarity system where a user uploads an image of an item and the system returns visually similar items.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="business"></category><category term="data-science"></category><category term="use-case"></category><category term="deep learning"></category><category term="ai"></category><category term="machine learning"></category></entry><entry><title>Effective Data Analysis with Pandas Indexes</title><link href="https://pyvideo.org/pycon-de-2017/effective-data-analysis-with-pandas-indexes.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Alexander Hendorf</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/effective-data-analysis-with-pandas-indexes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is the Swiss-Multipurpose Knife for Data Analysis in Python. In this talk we will look deeper into how to gain productivity utilizing Pandas powerful indexing and make advanced analytics a piece of cake. Pandas features multiple index types. This talk will give you a deep insight into the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is the Swiss-Multipurpose Knife for Data Analysis in Python. In this talk we will look deeper into how to gain productivity utilizing Pandas powerful indexing and make advanced analytics a piece of cake. Pandas features multiple index types. This talk will give you a deep insight into the Pandas indexes and showcase the handiness of special Indexes as the TimeSeriesIndex.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="machine learning"></category><category term="analytics"></category><category term="data-science"></category><category term="business analytics"></category></entry><entry><title>Flow is in the Air: Best Practices of Building Analytical Data Pipelines with Apache Airflow</title><link href="https://pyvideo.org/pycon-de-2017/flow-is-in-the-air-best-practices-of-building-analytical-data-pipelines-with-apache-airflow.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Dominik Benz</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/flow-is-in-the-air-best-practices-of-building-analytical-data-pipelines-with-apache-airflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dominik Benz&lt;/strong&gt; (&amp;#64;john_maverick)&lt;/p&gt;
&lt;p&gt;Dominik Benz holds a PhD from the University of Kassel in the field of Data Mining on the Social Web. Since 2012 he is working as a Big Data Engineer at Inovex GmbH. In this time, he was involved in several projects concerned with establishing analytical …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dominik Benz&lt;/strong&gt; (&amp;#64;john_maverick)&lt;/p&gt;
&lt;p&gt;Dominik Benz holds a PhD from the University of Kassel in the field of Data Mining on the Social Web. Since 2012 he is working as a Big Data Engineer at Inovex GmbH. In this time, he was involved in several projects concerned with establishing analytical data platforms in various companies. He is most experienced in tools around the Hadoop Ecosystem like Apache Hive and Spark, and has hands-on experience with productionizing analytical applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apache Airflow is an Open-Source python project which facilitates an intuitive programmatic definition of analytical data pipelines. Based on 2+ years of productive experience, we summarize its core concepts, detail on lessons learned and set it in context with the Big Data Analytics Ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Motivation &amp;amp; Outline&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Creating, orchestrating and running multiple data processing or analysis steps may cover a substantial portion of a Data Engineer and Data Scientist business. A widely adopted notion for this process is a &amp;quot;data pipeline&amp;quot; - which consists mainly of a set of &amp;quot;operators&amp;quot; which perform a particular action on data, with the possibility to specify dependencies among those. Real-Life examples may include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Importing several files with different formats into a Hadoop platform, perform data cleansing, and training a machine learning model on the result&lt;/li&gt;
&lt;li&gt;perform feature extraction on a given dataset, apply an existing deep learning model to it, and write the results in the backend of a microservice&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Apache Airflow is an open-source Python project developed by AirBnB which facilitates the programmatic definition of such pipelines. Features which differentiate Airflow from similar projects like Apache Oozie, Luigi or Azkaban include (i) its pluggable architecture with several extension points (ii) the programmatic approach of &amp;quot;workflow is code&amp;quot; and (iii) its tight relationship with the the Python as well as the Big Data Analytics Ecosystem. Based on several years of productive usage, we briefly summarize the core concepts of Airflow, and detail in-depth on lessons learned and best practices from our experience. These include hints for getting efficient quickly with Airflow, approaches to structure workflows, integrating it in an enterprise landscape, writing plugins and extentions, and maintaining it in productive environment. We conclude with a comparison with other analytical workflow engines and summarize why we have chosen Airflow.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Questions answered by this talk&lt;/em&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What are the core concepts of Apache Airflow?&lt;/li&gt;
&lt;li&gt;How can Airflow help me with moving data pipelines from analytics to production?&lt;/li&gt;
&lt;li&gt;Which concepts of Airflow make it more slim and more efficient compared to Apache Oozie?&lt;/li&gt;
&lt;li&gt;How can I specify dynamic dependencies at runtime between my analytical data processing steps?&lt;/li&gt;
&lt;li&gt;Which facilities does Airflow offer to enable automation and orchestration of analytical tasks?&lt;/li&gt;
&lt;li&gt;How can I extend the built-in facilities of Airflow by writing Python plugins?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;People who benefit most from this talk&lt;/em&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Data Scientists who are looking for a slim library to automate and control their data processing steps&lt;/li&gt;
&lt;li&gt;Data Engineers who want to save time debugging static workflow definitions (e.g. in XML)&lt;/li&gt;
&lt;li&gt;Project leaders interested in tools which lower the burden of moving from analytics to production&lt;/li&gt;
&lt;li&gt;Hadoop Cluster administrators eager to save cluster resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="workflow"></category><category term="data pipeline"></category><category term="data-science"></category><category term="analytics"></category></entry><entry><title>Getting Scikit-Learn To Run On Top Of Pandas</title><link href="https://pyvideo.org/pycon-de-2017/getting-scikit-learn-to-run-on-top-of-pandas.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Ami Tavory</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/getting-scikit-learn-to-run-on-top-of-pandas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Ami Tavory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ami is a data scientist at Facebook Research's Core Data Science group. He previously worked as a machine learning researcher in the fields of bioinformatics and algorithmic trading. In 2010 he received a Ph.D in Electrical Engineering from Tel Aviv University, in the field of financial …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Ami Tavory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ami is a data scientist at Facebook Research's Core Data Science group. He previously worked as a machine learning researcher in the fields of bioinformatics and algorithmic trading. In 2010 he received a Ph.D in Electrical Engineering from Tel Aviv University, in the field of financial information theory. His bachelor's and master's are from Tel Aviv University too.&lt;/p&gt;
&lt;p&gt;Ami uses Python and C++ for data analysis. He contributed to various open source projects, and is the author of a libstd C++ extension shipped with g++ (pb_ds: policy-based data structures).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Scikit-Learn is built directly over numpy, Python's numerical array library. Pandas adds to numpy metadata and higher-level munging capabilities. This talk describes how to intelligently auto-wrap Scikit-Learn for creating a version that can leverage pandas's added features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Scikit-Learn is the de-facto standard Python library for general-purpose machine learning. It operates over NumPy, an efficient, but low-level, homogeneic array library. Pandas adds to NumPy metadata, heterogeneity, and higher-leve munging capabilities.&lt;/p&gt;
&lt;p&gt;In the field of visualization, newer generation libraries, e.g., Seaborn and Bokeh, are providing safer, more readable, and higher-level functionality, by operating over Pandas data structures. Some of these are implemented using Matplotlib, a lower-level NumPy-based plotting library.&lt;/p&gt;
&lt;p&gt;This talk describes a library for a Pandas-based version of sickit-learn. Here, too, giving a Pandas interface to a machine-learning library, provides code which is safer to use, more readable, and allows direct integration with Pandas's higher-level munging capabilities.&lt;/p&gt;
&lt;p&gt;Due to the large-scale, and evolving nature, of sicikit-learn's codebase, it is infeasible to manually wrap it. Except for a small number of intentional deviations from sickit-learn, the library wraps Scikit-Learn modules lazily through module and class introspection, and dynamic module loading.&lt;/p&gt;
&lt;p&gt;Following a short review of the relevant points of Pandas and Scikit-Learn, the talk is roughly divided into two aspects:     Scikit-Learn And Pandas User Perspective     Safety Advantages Of Pandas-Based Estimators     Using Metadata For Inter-Instance Aggregated Features And Cross-Validation     Using Metadata For Advanced Meta-Algorithms: Stacking, Nested Labeled And Stratified Cross-Valdiation     Python Develop Perspective     Unique Challenges Of Scikit-Learn Introspection And Decoration     Two Approaches For Wrapping Scikit-Learn Estimators     Lazy Dynamic Module Loading&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="code-introspection"></category><category term="scikit-learn"></category><category term="pandas"></category><category term="data-science"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Large-scale machine learning pipelines using Luigi, PySpark and scikit-learn</title><link href="https://pyvideo.org/pycon-de-2017/large-scale-machine-learning-pipelines-using-luigi-pyspark-and-scikit-learn.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Alexander Bauer</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/large-scale-machine-learning-pipelines-using-luigi-pyspark-and-scikit-learn.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Alexander Bauer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alexander Bauer holds a Ph.D. in computer science. He has around 10 years industry experience, currently leading a team of data scientists at Lidl, one of the largest global discount supermarket chains. He is a Kaggle Master and regular speaker at the Frankfurt Predictive Analytics Meetup …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Alexander Bauer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alexander Bauer holds a Ph.D. in computer science. He has around 10 years industry experience, currently leading a team of data scientists at Lidl, one of the largest global discount supermarket chains. He is a Kaggle Master and regular speaker at the Frankfurt Predictive Analytics Meetup. He believes in agile software development practices and promotes Python as a primary language for data science applications in production.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For prescriptive analytics applications, data science teams need to design, build and maintain complex machine learning pipelines. In this talk, we demonstrate how such pipelines can be implemented in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data science teams working on real-world prescriptive analytics applications face the challenge to design, build and maintain considerably complex machine learning pipelines on a daily basis. Such pipelines include parsing data from multiple data sources, extracting relevant predictive features, executing training, validation, prediction steps and finally optimizing actions to meet desired business outcome so that they can be shared and visualized to business users. In this talk, we demonstrate how such pipelines can be implemented end-to-end in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn. We will share our lessons learned from using this framework in a real-world demand forecasting use case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="data-science"></category><category term="analytics"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Modern ETL-ing with Python and Airflow (and Spark)</title><link href="https://pyvideo.org/pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Tamara Mendt</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Tamara Mendt&lt;/strong&gt; (&amp;#64;TamaraMendt)&lt;/p&gt;
&lt;p&gt;Tamara Mendt is a Data Engineer at HelloFresh, a meal kit delivery service headquartered in Berlin, and one of the top 3 tech startups to come out of Europe over the past 4 years. She devotes her time to building data pipelines and designing and maintaining …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Tamara Mendt&lt;/strong&gt; (&amp;#64;TamaraMendt)&lt;/p&gt;
&lt;p&gt;Tamara Mendt is a Data Engineer at HelloFresh, a meal kit delivery service headquartered in Berlin, and one of the top 3 tech startups to come out of Europe over the past 4 years. She devotes her time to building data pipelines and designing and maintaining the company's data infrastructure. Tamara has a computer engineering degree from her native country Venezuela, and an Erasmus Mundus Masters degree in IT for Business Intelligence. She wrote her Master thesis at the TU Berlin with the research group where Apache Flink was born. At HelloFresh she is continuing to work with distributed technologies such has Apache Hadoop, Apache Kafka and Apache Spark to cope with the scalability that the fast growing company requires for dealing with their data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The challenge of data integration is real. The sheer amount of tools that exist to address this problem is proof that organizations struggle with it. This talk will discuss the inherent challenges of data integration, and show how it can be tackled using Python and Apache Airflow and Apache Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The way organizations analyze their data has evolved very quickly since the beginning of the millennium. The development of Hadoop, and the explosion in the variety of data that companies are dealing with nowadays, has fostered the appearance of the concept of data lake, and the shift of traditional ETL (extract, transform, load), to ELT (extract, load, transform). Yet, the challenge of integrating data to obtain valuable insights still remains, and despite the hype and attention being focused on data, very few organizations have actually managed to become data driven. In this talk I will present insights into how we are currently building data pipelines using Python (as a replacement to high level ETL software), Apache Airflow as a scheduler to our coded transformations, and Apache Spark for achieve scalability. Though building data pipelines is not the only element required to become data driven, it is a crucial one, and I hope to encourage the audience to use these open source technologies in their own ETL-ing (or ELT-ing) efforts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="data"></category><category term="data-science"></category><category term="pipeline"></category></entry><entry><title>Sport analysis with Python</title><link href="https://pyvideo.org/pycon-de-2017/sport-analysis-with-python.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Thuy Le</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/sport-analysis-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Give an example data of the IoT sport case for instance the information of football match of a team (the positions, velocities of each player with are recorded in every 20 millisecond).&lt;/li&gt;
&lt;li&gt;We use Python to analysis and processing data (calculate the match time, analyst the activities of each …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Give an example data of the IoT sport case for instance the information of football match of a team (the positions, velocities of each player with are recorded in every 20 millisecond).&lt;/li&gt;
&lt;li&gt;We use Python to analysis and processing data (calculate the match time, analyst the activities of each player such as time in the bench, time in the pitch, ... )&lt;/li&gt;
&lt;li&gt;We use Tableau to visualize data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="devops"></category><category term="analytics"></category><category term="data-science"></category><category term="python"></category></entry><entry><title>Synthetic Data for Machine Learning Applications</title><link href="https://pyvideo.org/pycon-de-2017/synthetic-data-for-machine-learning-applications.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Hendrik Niemeyer</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/synthetic-data-for-machine-learning-applications.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dr. Hendrik Niemeyer&lt;/strong&gt; (&amp;#64;hniemeye)&lt;/p&gt;
&lt;p&gt;Data Scientist working on predictive analytics with data from pipeline inspection measurements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk I will show how we use real and synthetic data to create successful models for risk assessing pipeline anomalies. The main focus is the estimation of the difference in …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dr. Hendrik Niemeyer&lt;/strong&gt; (&amp;#64;hniemeye)&lt;/p&gt;
&lt;p&gt;Data Scientist working on predictive analytics with data from pipeline inspection measurements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk I will show how we use real and synthetic data to create successful models for risk assessing pipeline anomalies. The main focus is the estimation of the difference in the statistical properties of real and generated data by machine learning methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ROSEN provides predictive analytics for pipelines by detecting and risk assessing anomalies from data gathered by inline inspection measurement devices. Due to budget reasons (pipelines need to be dug up to get acess) ground truth data for machine learning applications in this field are usually scarce, imbalanced and not available for all existing configurations of measurement devices. This creates the need for synthetic data (using FEM simulations and unsupervised learning algorithms) in order to be able to create successful models.&lt;/p&gt;
&lt;p&gt;But a naive mixture of real-world and synthetic samples in a model does not necessarily yield to an increased predictive performance because of differences in the statistical distributions in feature space. I will show how we evaluate the use of synthetic data besides simple visual inspection. Manifold learning (e.g. TSNE) can be used to gain an insight whether real and generated data are inherently different.
Quantitative approaches like classifiers trained to discriminate between these types of data provide a non visual insight whether a &amp;quot;synthetic gap&amp;quot; in the feature distributions exists.&lt;/p&gt;
&lt;p&gt;If the synthetic data is useful for model building careful considerations have to be applied when constructing cross validation folds and test sets to prevent biased estimates of the model performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="data-science"></category><category term="python"></category><category term="machine learning"></category><category term="ai"></category></entry><entry><title>The eye of the Python, an eye tracking system. From zero to... what eye learned</title><link href="https://pyvideo.org/pycon-de-2017/the-eye-of-the-python-an-eye-tracking-system-from-zero-to-what-eye-learned.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Samuel Muñoz Hidalgo</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/the-eye-of-the-python-an-eye-tracking-system-from-zero-to-what-eye-learned.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Samuel Muñoz Hidalgo&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I am Samuel (Samu for friends). With a curious mind I studied computer science, then focused in machine learning and IoT as a professional career. I haven't given up on my plan to take over the world; that's why my coworkers know a crazy idea is …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Samuel Muñoz Hidalgo&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I am Samuel (Samu for friends). With a curious mind I studied computer science, then focused in machine learning and IoT as a professional career. I haven't given up on my plan to take over the world; that's why my coworkers know a crazy idea is coming out, when I can't hide any longer a mischievous smile. I like to meet people and understand other points of view, and in return I like to show what I can do and teach what I have mastered. But, what drives me crazy is rollerskating with disco music.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Is it possible to predict the point in the screen where a person is looking at? Easy to say but hard to do. An eye tracking system is the perfect project to learn the difficulties of applied machine learning. From gathering training data to building the final software with an acceptable performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk I will show how to build an eye tracking system from scratch.&lt;/p&gt;
&lt;p&gt;Once the approach (machine learning) and the tools (Python ecosystem) are set, the important tasks are:     Making users addicted to a game built with Pygame in order to generate data.     Unleashing the power of deep learning over GPU with Tensorflow to train an Artificial Neural Network.     Exploiting the training model in real time so as to control a computer with the eyes with PyAutoGUI.&lt;/p&gt;
&lt;p&gt;The path is full of pitfalls and every clear single step to the goal turns out to be a mountain of small but very important subtasks. Every iteration is a continuous struggle just to gain a bit of accuracy.&lt;/p&gt;
&lt;p&gt;Despite these efforts, at the very end we will see the difference between the theoretical and the real world. Our engineering skills will determine the usability of the new interface that allows us to move the mouse with our eyes; and we will learn that things don't need to be perfect, because humans are ...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="data-science"></category><category term="use-case"></category><category term="python"></category><category term="ai"></category><category term="deep learning"></category></entry><entry><title>The Python Ecosystem for Data Science: A Guided Tour</title><link href="https://pyvideo.org/pycon-de-2017/the-python-ecosystem-for-data-science-a-guided-tour.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Christian Staudt</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/the-python-ecosystem-for-data-science-a-guided-tour.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Christian Staudt&lt;/strong&gt; (&amp;#64;C_L_Staudt)&lt;/p&gt;
&lt;p&gt;I am an independent data scientist with a background in computer science, in-depth in algorithms, data analysis, high-performance computing and software engineering. My current interests include machine learning and data visualization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pythonistas have access to an extensive collection of tools for data analysis. The space …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Christian Staudt&lt;/strong&gt; (&amp;#64;C_L_Staudt)&lt;/p&gt;
&lt;p&gt;I am an independent data scientist with a background in computer science, in-depth in algorithms, data analysis, high-performance computing and software engineering. My current interests include machine learning and data visualization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pythonistas have access to an extensive collection of tools for data analysis. The space of tools is best understood as an ecosystem: Libraries build upon each other, and a good library fills an ecological niche by doing certain jobs well. This is a guided tour of the Python data science ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Python Ecosystem for Data Science: A Guided Tour&lt;/p&gt;
&lt;p&gt;Python is on its way to becoming the lingua franca of data science, and Pythonistas have access to an impressive and extensive collection of tools for data analysis. Here, a data scientist needs to see the forest for the trees: The space of tools is best understood as an ecosystem, where libraries build upon each other, and where a good library fills an ecological niche by doing certain jobs well. This talk is a guided tour of the Python data science ecosystem. More than a list of libraries, it aims to provide some structure, classing tools by type of data, size of data, and type of analysis. In our tour, we visit a number of areas, including working with tabular data (numpy, pandas, dask, ...) and graph data (e.g. networkx), statistics (e.g. statsmodels), machine learning (scikit-learn, ...), data visualization (matplotlib, seaborn, bokeh, ...). Aspiring data scientists, and everyone else working with data, should find this useful for selecting the right tools for their next data-driven project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="use-case"></category><category term="business"></category><category term="ai"></category><category term="analytics"></category><category term="data-science"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Time series feature extraction with tsfresh - “get rich or die overfitting”</title><link href="https://pyvideo.org/pycon-de-2017/time-series-feature-extraction-with-tsfresh-get-rich-or-die-overfitting.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Nils Braun</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/time-series-feature-extraction-with-tsfresh-get-rich-or-die-overfitting.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Nils Braun&lt;/strong&gt; (&amp;#64;_nilsbraun)&lt;/p&gt;
&lt;p&gt;Currently I am doing my PhD in Particle Physics - which mainly involves development of software in a large collaboration. I love working with Python and C++ to process large amounts of data. Of course it needs to be processed as quickly as possible. I am working …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Nils Braun&lt;/strong&gt; (&amp;#64;_nilsbraun)&lt;/p&gt;
&lt;p&gt;Currently I am doing my PhD in Particle Physics - which mainly involves development of software in a large collaboration. I love working with Python and C++ to process large amounts of data. Of course it needs to be processed as quickly as possible. I am working on the core reconstruction algorithms for our experiment, which are steered and controlled using Python. Apart from that, I was working as a Data Science Engineer for Blue Yonder, a leading machine learning company, where the idea for tsfresh was born. I am still heavily involved in the project. When I am not writing code, I am updating myself on the newest technical geek stuff (mostly cloud computing and deep learning) or play the guitar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Have you ever thought about developing a time series model to predict stock prices? Or do you consider log time series from the operation of cloud resources as being more compelling? In this case you really should consider using the time series feature extraction package tsfresh for your project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Trends such as the Internet of Things (IoT), Industry 4.0, and precision medicine are driven by the availability of cheap sensors and advancing connectivity, which among others increases the availability of temporally annotated data. The resulting time series are the basis for manifold machine learning applications. Examples are the classification of hard drives into risk classes concerning specific defect, the log analysis of server farms for detecting intruders, or regression tasks like the prediction of the remaining lifespan of machinery. Tsfresh also allows to easily setup a machine learning pipeline that predicts stock prices, which we will demonstrate live during the presentation ;). The problem of extracting and selecting relevant features for classification or regression is these domains is especially hard to solve, if each label or regression target is associated with several time series and meta-information simultaneously – which is a common pattern in industrial applications. This talk introduces a distributed and parallel feature extraction and selection algorithm – the recently published Python library tsfresh. The fully automated extraction and importance selection does not only allow to reach better machine learning classification scores, but in combination with the speed of the package, also allows to incorporate tsfresh into automated AI-pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="pydata"></category><category term="time series"></category><category term="data-science"></category><category term="machine learning"></category><category term="python"></category><category term="ai"></category></entry><entry><title>Turbodbc: Turbocharged database access for data scientists</title><link href="https://pyvideo.org/pycon-de-2017/turbodbc-turbocharged-database-access-for-data-scientists.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Michael König</name></author><id>tag:pyvideo.org,2017-10-25:/pycon-de-2017/turbodbc-turbocharged-database-access-for-data-scientists.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Michael König&lt;/strong&gt; (&amp;#64;turbodbc)&lt;/p&gt;
&lt;p&gt;Michael is a senior software engineer at Blue Yonder GmbH. He holds a PhD in physics, practices test-driven development, and digs Clean Code in C++ and Python. In the last five years, he invested more money in table tennis gear than in smartphones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Python's database …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Michael König&lt;/strong&gt; (&amp;#64;turbodbc)&lt;/p&gt;
&lt;p&gt;Michael is a senior software engineer at Blue Yonder GmbH. He holds a PhD in physics, practices test-driven development, and digs Clean Code in C++ and Python. In the last five years, he invested more money in table tennis gear than in smartphones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Python's database API 2.0 is well suited for transactional database workflows, but not so much for column-heavy data science. This talk explains how the ODBC-based turbodbc database module extends this API with first-class, efficient support for familiar NumPy and Apache Arrow data structures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk introduces the open source Python database module turbodbc. It uses standard ODBC drivers to connect with virtually any database and is a viable (and often faster) alternative to &amp;quot;native&amp;quot; Python drivers.&lt;/p&gt;
&lt;p&gt;Briefly recounting the painful story of how data scientists previously used our analytics database, I explain why turbodbc was created and what distinguishes it from other ODBC modules. Sketching the flow of data from databases via drivers and Python modules to consumable Python objects, I motivate a few extensions to the standard database API 2.0 that turbodbc has made. These extensions heavily use NumPy arrays and Apache Arrow tables to provide data scientists with both familiar and efficient binary data structures they can further work on. I conclude my talk with benchmark results for a few databases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</content><category term="PyCon DE 2017"></category><category term="numpy"></category><category term="database"></category><category term="python"></category><category term="data-science"></category><category term="analytics"></category></entry><entry><title>Build text classification models ( CBOW and Skip-gram) with FastText in python</title><link href="https://pyvideo.org/pycon-de-2018/build-text-classification-models-cbow-and-skip-gram-with-fasttext-in-python.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Kajal Puri</name></author><id>tag:pyvideo.org,2018-10-26:/pycon-de-2018/build-text-classification-models-cbow-and-skip-gram-with-fasttext-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;FastText has been open-sourced by Facebook in 2016 and with its release,
it became the fastest and most accurate library in Python for text
classification and word representation. It is to be seen as a substitute
for gensim package's word2vec. It includes the implementation of two
extremely important methodologies …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;FastText has been open-sourced by Facebook in 2016 and with its release,
it became the fastest and most accurate library in Python for text
classification and word representation. It is to be seen as a substitute
for gensim package's word2vec. It includes the implementation of two
extremely important methodologies in NLP i.e Continuous Bag of Words and
Skip-gram model. Fasttext performs exceptionally well with supervised as
well as unsupervised learning.&lt;/p&gt;
&lt;p&gt;The tutorial will be divided in following four segments :&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Artificial Intelligence"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="NLP"></category><category term="Machine Learning"></category></entry><entry><title>Enabling the chip technologies of tomorrow – how Python helps us</title><link href="https://pyvideo.org/pycon-de-2018/enabling-the-chip-technologies-of-tomorrow-how-python-helps-us.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Tim Hoffmann</name></author><id>tag:pyvideo.org,2018-10-26:/pycon-de-2018/enabling-the-chip-technologies-of-tomorrow-how-python-helps-us.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Carl Zeiss SMT GmbH is the leading manufacturer of lithography optics.
Our optics allow chipmakers to produce smaller, faster and more energy
efficient computer chips. As we move to smaller and smaller structures,
the necessary optics grow more and more complex. Customized simulations
and data analytics by highly qualified …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Carl Zeiss SMT GmbH is the leading manufacturer of lithography optics.
Our optics allow chipmakers to produce smaller, faster and more energy
efficient computer chips. As we move to smaller and smaller structures,
the necessary optics grow more and more complex. Customized simulations
and data analytics by highly qualified technical domain experts are
essential. These people are not experienced software developers.
However, with Python and the right support, we can give them powerful
tools to accomplish their task efficiently.&lt;/p&gt;
&lt;p&gt;Pioneering Python in a larger enterprise can be challenging. At present,
we use Python in selected areas of our product development and
production processes. We'd like to share our challenges and solutions
with using Python in a heterogeneous company environment. In particular,
how can we make Python accessible to non-programmers? How do we ensure
consistent development? How do we embed in the non-Python ecosystem of
the company?&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Data Science"></category></entry><entry><title>From exploration to deployment - combining PyTorch and TensorFlow for Deep Learning</title><link href="https://pyvideo.org/pycon-de-2018/from-exploration-to-deployment-combining-pytorch-and-tensorflow-for-deep-learning.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Marcel Kurovski</name></author><id>tag:pyvideo.org,2018-10-26:/pycon-de-2018/from-exploration-to-deployment-combining-pytorch-and-tensorflow-for-deep-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Despite the many deep learning frameworks out in the wild few have
achieved widespread adoption. Two of them are TensorFlow and PyTorch.
Where PyTorch relies on a dynamic computation graph TensorFlow goes for
a static graph. Where TensorFlow shows greater adoption and additional
useful extensions with TensorFlow Serving and …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Despite the many deep learning frameworks out in the wild few have
achieved widespread adoption. Two of them are TensorFlow and PyTorch.
Where PyTorch relies on a dynamic computation graph TensorFlow goes for
a static graph. Where TensorFlow shows greater adoption and additional
useful extensions with TensorFlow Serving and TensorBoard, Pytorch
proves useful trough its easy and more pythonic API.&lt;/p&gt;
&lt;p&gt;Data scientists are confronted with explorative challenges, but also
need to be aware of model deployment and production. Do we need to
single out frameworks until we end up with the only one or is there a
case for joint usage of two deep learning frameworks? Can we leverage
the strengths of the frameworks for different tasks along the path from
exploration to production?&lt;/p&gt;
&lt;p&gt;In my talk, I want to present a case combining the benefits of PyTorch
and TensorFlow using the first for explorative and latter for deployment
tasks. Therefore, I will choose a common deep learning challenge and
discuss the strengths and weaknesses of both frameworks along a demo
that brings a model from development into production.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Artificial Intelligence"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="Machine Learning"></category></entry><entry><title>Prototyping to tested code</title><link href="https://pyvideo.org/pycon-de-2018/prototyping-to-tested-code.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Christopher Prohm</name></author><id>tag:pyvideo.org,2018-10-26:/pycon-de-2018/prototyping-to-tested-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter notebooks are a great environment to prototype solutions and
explore their design. Turning these solutions into reusable components
usually requires moving them out of the notebook environment into
external python packages. Often, at this stage, the code is refactored
and test are written.&lt;/p&gt;
&lt;p&gt;In this talk, I will …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter notebooks are a great environment to prototype solutions and
explore their design. Turning these solutions into reusable components
usually requires moving them out of the notebook environment into
external python packages. Often, at this stage, the code is refactored
and test are written.&lt;/p&gt;
&lt;p&gt;In this talk, I will demo
&lt;tt class="docutils literal"&gt;`ipytest&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="https://github.com/chmp/ipytest"&gt;https://github.com/chmp/ipytest&lt;/a&gt;&amp;gt;`__, a small tool to run
tests inside notebooks. It supports &lt;tt class="docutils literal"&gt;`pytest&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="http://pytest.org/"&gt;http://pytest.org/&lt;/a&gt;&amp;gt;`__
as well as the standard
&lt;tt class="docutils literal"&gt;`unittest&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="https://docs.python.org/3/library/unittest.html"&gt;https://docs.python.org/3/library/unittest.html&lt;/a&gt;&amp;gt;`__
framework. It allows to start prototypes in a notebook and to develop
the tests with the code in an highly interactive environment. As the
code grows, it can be transparently moved outside notebooks and
transformed into reusable components. By bringing support for tests to
the notebook environment,
&lt;tt class="docutils literal"&gt;`ipytest&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="https://github.com/chmp/ipytest"&gt;https://github.com/chmp/ipytest&lt;/a&gt;&amp;gt;`__ bridges the artificial
gap between notebooks and reusable components.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Data Science"></category><category term="Machine Learning"></category></entry><entry><title>Satellite data is for everyone: insights into modern remote sensing research with open data and Python</title><link href="https://pyvideo.org/pycon-de-2018/satellite-data-is-for-everyone-insights-into-modern-remote-sensing-research-with-open-data-and-python.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Felix M. Riese</name></author><id>tag:pyvideo.org,2018-10-26:/pycon-de-2018/satellite-data-is-for-everyone-insights-into-modern-remote-sensing-research-with-open-data-and-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The largest earth observation programme Copernicus
(&lt;a class="reference external" href="http://copernicus.eu"&gt;http://copernicus.eu&lt;/a&gt;) makes it possible to perform terrestrial
observations providing data for all kinds of purposes. One important
objective is to monitor the land-use and land-cover changes with the
Sentinel-2 satellite mission. These satellites measure the sun
reflectance on the earth surface …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The largest earth observation programme Copernicus
(&lt;a class="reference external" href="http://copernicus.eu"&gt;http://copernicus.eu&lt;/a&gt;) makes it possible to perform terrestrial
observations providing data for all kinds of purposes. One important
objective is to monitor the land-use and land-cover changes with the
Sentinel-2 satellite mission. These satellites measure the sun
reflectance on the earth surface with multispectral cameras (13 channels
between 440 nm to 2190 nm). Machine learning techniques like
convolutional neural networks (CNN) are able to learn the link between
the satellite image (spectrum) and the ground truth (land use class). In
this talk, we give an overview about the state-of-the-art land-use
classification with CNNs based on an open dataset.&lt;/p&gt;
&lt;p&gt;The EuroSAT benchmark dataset (&lt;a class="reference external" href="http://madm.dfki.de/downloads"&gt;http://madm.dfki.de/downloads&lt;/a&gt;) is freely
provided by German Research Center for Artificial Intelligence (DFKI).
It consists of 27.000 image patches for ten different land use/cover
classes, e.g. industrial and residential areas, different crop and
vegetation types and forests. All samples have 64 by 64 pixel dimension
and include either only the RGB images or all 13 bands.&lt;/p&gt;
&lt;p&gt;We will use different out-of-box CNNs for the Keras deep learning
library (&lt;a class="reference external" href="https://keras.io/"&gt;https://keras.io/&lt;/a&gt;). All networks are either included in Keras
itself or are available from Github repositories. We will show the
process of transfer learning for the RGB datasets. Furthermore, the
minimal changes required to apply commonly used CNNs to multispectral
data are demonstrated. Thus, the interested audience will be able to
perform their own classification of remote sensing data within a very
short time. Results of different network structures are visually
compared. Especially the differences of transfer learning and learning
from scratch are demonstrated. This also includes the amount of
necessary training epochs, progress of training and validation error and
visual comparison of the results of the trained networks.&lt;/p&gt;
&lt;p&gt;Finally, we give a quick overview about the current research topics
including recurrent neural networks for spatio-temporal land-use
classification and further applications of multi- and hyperspectral
data, e.g. for the estimation of water parameters and soil
characteristics. Additionally, we provide links to the code and dataset
used in this talk.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Artificial Intelligence"></category><category term="Computer Vision"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="Machine Learning"></category><category term="Science"></category></entry></feed>