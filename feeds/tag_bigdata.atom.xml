<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_bigdata.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-04-22T00:00:00+00:00</updated><entry><title>Scegliere le armi per la battaglia del calcolo intensivo</title><link href="https://pyvideo.org/europython-2013/scegliere-le-armi-per-la-battaglia-del-calcolo-intensivo.html" rel="alternate"></link><published>2013-07-05T00:00:00+00:00</published><updated>2013-07-05T00:00:00+00:00</updated><author><name>Enrico Franchi</name></author><id>tag:pyvideo.org,2013-07-05:europython-2013/scegliere-le-armi-per-la-battaglia-del-calcolo-intensivo.html</id><summary type="html"></summary><category term="bigdata"></category><category term="optimization"></category><category term="data-analysis"></category><category term="hpc"></category><category term="performance"></category><category term="scientific-computing"></category></entry><entry><title>Python and PostgreSQL for Huge Data Warehouses</title><link href="https://pyvideo.org/europython-2013/python-and-postgresql-for-huge-data-warehouses.html" rel="alternate"></link><published>2013-07-04T00:00:00+00:00</published><updated>2013-07-04T00:00:00+00:00</updated><author><name>Hannu Krosing</name></author><id>tag:pyvideo.org,2013-07-04:europython-2013/python-and-postgresql-for-huge-data-warehouses.html</id><summary type="html"></summary><category term="postgresql"></category><category term="nosql"></category><category term="parallelization"></category><category term="bigdata"></category><category term="scalability"></category><category term="pl/python"></category><category term="olap"></category><category term="optimization"></category><category term="architecture"></category><category term="sql"></category><category term="performance"></category></entry><entry><title>pl/python now as powerful as C</title><link href="https://pyvideo.org/europython-2013/plpython-now-as-powerful-as-c.html" rel="alternate"></link><published>2013-07-03T00:00:00+00:00</published><updated>2013-07-03T00:00:00+00:00</updated><author><name>Hannu Krosing</name></author><id>tag:pyvideo.org,2013-07-03:europython-2013/plpython-now-as-powerful-as-c.html</id><summary type="html"></summary><category term="postgresql"></category><category term="nosql"></category><category term="database"></category><category term="mongodb"></category><category term="bigdata"></category><category term="pl/python"></category><category term="optimization"></category><category term="sql"></category><category term="Full Text Search"></category></entry><entry><title>PostgreSQL is Web-Scale (Really :) )</title><link href="https://pyvideo.org/europython-2013/postgresql-is-web-scale-really.html" rel="alternate"></link><published>2013-07-02T00:00:00+00:00</published><updated>2013-07-02T00:00:00+00:00</updated><author><name>Hannu Krosing</name></author><id>tag:pyvideo.org,2013-07-02:europython-2013/postgresql-is-web-scale-really.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I show you how to set up a python and PostgreSQL based
system which is easy to set up and easy to scale, provides ACID
guarantees where they are needed and delays time-consistency between
unrelated objects for scalability and availability where the latter are
deemed more important.&lt;/p&gt;
&lt;p&gt;The best thing is that this kind of scalability work for both OLTP and
OLAP workloads, so with some planning you can have just a single large
“database” which can take almost any type of load.&lt;/p&gt;
&lt;p&gt;Also, if you hate SQL, you can do all the OLTP stuff in a pythonic way
using an automagically generated ORM layer inside the database, near the
data. If you are really masochistic, you can use the same ORM also for
map-reduce type distributed data processing, though on this side the
small effort of learning SQL usually pays off when queries get more
complex. But as I said, everything runs inside the databse, near the
data and thus even the ORM &amp;amp; map-reduce analytics works fast.&lt;/p&gt;
</summary><category term="postgresql"></category><category term="nosql"></category><category term="datamining"></category><category term="parallelization"></category><category term="distributed"></category><category term="bigdata"></category><category term="scalability"></category><category term="pl/python"></category><category term="olap"></category><category term="optimization"></category><category term="orm"></category><category term="sql"></category><category term="performance"></category></entry><entry><title>Python e Elasticsearch: dal Text Search a NLP e oltre</title><link href="https://pyvideo.org/pycon-italia-2018/python-e-elasticsearch-dal-text-search-a-nlp-e-oltre.html" rel="alternate"></link><published>2018-04-22T00:00:00+00:00</published><updated>2018-04-22T00:00:00+00:00</updated><author><name>Dario Balinzo</name></author><id>tag:pyvideo.org,2018-04-22:pycon-italia-2018/python-e-elasticsearch-dal-text-search-a-nlp-e-oltre.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Il Talk è rivolto a sviluppatori Python intermedi. Non è richiesta
nessuna conoscenza su Elasticsearch.&lt;/p&gt;
&lt;p&gt;Nell’introduzione presenteremo le librerie elasticsearch-py e
elasticsearch- dsl introducendo i concetti base di ElasticSearch.
Saranno prima presentate le metodologie di indicizzazione per
ottimizzare la ricerca su grandi quantità di dati, mostrando come
inserire i propri dati nel motore di ricerca.&lt;/p&gt;
&lt;p&gt;Dopo passeremo alle query (dal text search alle geo queries ) e relative
aggregazioni, facendo vedere come estrarre informazioni dai dati in
maniera veloce e migliorare così la user experience.&lt;/p&gt;
&lt;p&gt;In seguito saranno presentate funzionalità di ricerca avanzate,
spiegando come arricchire le proprie webapp con le funzionalità
dinamiche di “search as you type”, autocompletamento e suggerimento.&lt;/p&gt;
&lt;p&gt;Infine mostreremo come utilizzare tecniche di Data Analytics avanzate
come il NLP: analizzando i testi sarà possibile fare “language
detection”, “text classification” e “keyword extraction”. In tal modo
non solo si può trovare velocemente cosa si sta cercando, ma analizzare
commenti e recensioni per capire se i clienti sono soddisfatti.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;domenica 22 aprile&lt;/strong&gt; at 14:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="nlp"></category><category term="nosql"></category><category term="Python"></category><category term="bigdata"></category><category term="elasticsearch"></category><category term="Full Text Search"></category></entry><entry><title>GPU-accelerated data analysis in Python: a study case in Material Sciences</title><link href="https://pyvideo.org/pycon-italia-2018/gpu-accelerated-data-analysis-in-python-a-study-case-in-material-sciences.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Giuseppe Di Bernardo</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/gpu-accelerated-data-analysis-in-python-a-study-case-in-material-sciences.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Max Planck Computing and Data Facility is engaged in the development
and optimization of algorithms and applications for high performance
computing as well as for data-intensive projects. As programming
language in data science, Python is now used at MPCDF in the scientific
area of “atom probe crystallography” (APT): a Fourier analysis in 3D
space can be simulated in order to reveal composition and
crystallographic structure at the atomic scale of billions APT
experimental data sets.&lt;/p&gt;
&lt;p&gt;The Python data ecosystem has proved to be well suited to this, as it
has grown beyond the confines of single machines to embrace scalability.
The talk aims to describe our approach to scaling across multiple GPUs,
and the role of visualization methods too.&lt;/p&gt;
&lt;p&gt;Our data workflow analysis relies on the GPU-accelerated Python software
package PyNX, an open source library which provides fast parallel
computation scattering. The code takes advantage of the high throughput
of GPUs, using the pyCUDA library.&lt;/p&gt;
&lt;p&gt;Exploratory data analysis, high productivity and rapid prototyping with
high performance are enabled through Jupyter Notebooks and Python
packages e.g., pandas, matplotlib/plotly. In production stage,
interactive visualization is realized by using standard scientific tool,
e.g. Paraview, an open-source 3D visualization program which requires
Python modules to generate visualization components within VTK files.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 14:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="GPUComputing"></category><category term="visualization"></category><category term="mathematical-modelling"></category><category term="image-processing"></category><category term="bigdata"></category><category term="matplotlib"></category><category term="analytics"></category><category term="data-visualization"></category><category term="data-analysis"></category><category term="Data Mining"></category><category term="scientific-computing"></category><category term="physics"></category><category term="python3"></category></entry><entry><title>Using Python to bring democracy to the A.I. age</title><link href="https://pyvideo.org/pycon-italia-2018/using-python-to-bring-democracy-to-the-ai-age.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Felipe Cabral</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/using-python-to-bring-democracy-to-the-ai-age.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;TL;DR&lt;/p&gt;
&lt;div class="section" id="when-you-go-full-big-data-at-public-data-and-become-a-citzen"&gt;
&lt;h4&gt;When you go full Big Data at public data and become a citzen.&lt;/h4&gt;
&lt;p&gt;Audience type: developers, data scientists of any level of expertise.&lt;/p&gt;
&lt;p&gt;After a political coup Brazil drowned in scandals and political
disbelief. That was the final straw for us.&lt;/p&gt;
&lt;p&gt;We created a bot persona who uses Machine Learning to analyze public
spending, launching our own data journalism investigations. As expected
we use the internet publicize our findings and icing on it was to use
Twitter to directly engage the public and politicians under the topic of
suspicious expenses.&lt;/p&gt;
&lt;p&gt;Come with me and I’ll show some figures from Brazilian corruption, share
some code and cherry-pick the best of our toolbox to deal with public
data and machine learning. I’ll introduce our public dashboard that
makes visualization and browsing government data easy peasy. And surely
we can take a look in some tweets from Rosie, the robot, and how some
politicians are now vociferating with a ROBOT on social media.&lt;/p&gt;
&lt;p&gt;And you guessed it right: everything is open-source and our mission is
to create a global community to bring democracy to the A.I. age.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 18:30 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</summary><category term="machine-learning"></category><category term="Python"></category><category term="agile"></category><category term="Data Mining"></category><category term="bigdata"></category><category term="data-visualization"></category><category term="OpenSource"></category><category term="data-analysis"></category><category term="e-gov"></category><category term="data"></category></entry><entry><title>Sports performance evaluation: from cognitive mechanisms to data-driven algorithms</title><link href="https://pyvideo.org/pycon-italia-2018/sports-performance-evaluation-from-cognitive-mechanisms-to-data-driven-algorithms.html" rel="alternate"></link><published>2018-04-20T00:00:00+00:00</published><updated>2018-04-20T00:00:00+00:00</updated><author><name>Luca Pappalardo</name></author><id>tag:pyvideo.org,2018-04-20:pycon-italia-2018/sports-performance-evaluation-from-cognitive-mechanisms-to-data-driven-algorithms.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;blockquote&gt;
Not everything that counts can be counted, and not everything that
can be counted counts. A. Einstein&lt;/blockquote&gt;
&lt;p&gt;Humans are routinely asked to evaluate the performance of other
individuals, separating success from failure and affecting outcomes from
science to education and sports. Yet, little is known about the aspects
that determine the human perception of performance. How do expert
reviewers, as well as ordinary people, arrive to their evaluations? To
what extent these evaluations are based on objective performance
features? How are they affected by subjective biases or contextual
influences?&lt;/p&gt;
&lt;p&gt;This talk will answer these fascinating questions focusing on &lt;em&gt;soccer&lt;/em&gt; ,
the most popular sport in the world. Firstly, we will show how machine
learning can accurately reproduce the mechanisms human judges use to
evaluate the performance of soccer players, uncovering limits and
characteristics of the human evaluation process. Second, we design a
Python package that allows, in a completely unsupervised and data-driven
way, to (i) evaluate the quality of a player’s performance and (ii) rank
soccer players based on their performances.&lt;/p&gt;
&lt;p&gt;The first part of the talk will show how soccer ratings assigned to
every player of a game by sport-specialized newspapers are associated
with a high- dimensional vector of features extracted by massive data
which describe any quantifiable aspects of soccer games. The talk will
show how, by using Scikit- learn, we can train an &lt;em&gt;artificial judge&lt;/em&gt;
which learns the relation between technical performance and soccer
ratings, hence &lt;em&gt;accurately reproducing&lt;/em&gt; the human evaluation process. By
inspecting the structure of the artificial judge, the talk will show
that the human evaluation criteria follow a simplistic cognitive process
based on a simple heuristic: judges first select a limited number of
features which attract their attention and then rate a performance based
on the presence of noticeable values, i.e., features values far from the
norm that can be easily brought to mind.&lt;/p&gt;
&lt;p&gt;The second part of the talk will show how to overcome the simplicity of
the human evaluation process presenting &lt;strong&gt;PlayeRank&lt;/strong&gt; , a Python package
which implements an unsupervised data-driven framework to evaluate the
performance of soccer players in the main European leagues. The talk
will show how to use PlayeRank to construct a data-driven ranking of
players and highlight the factors which determine why celebrated
players, like Messi and Cristiano Ronaldo, actually result to be the top
players in the world. The modules composing PlayeRank will be presented,
showing how they allow the user to define the features characterizing a
performance, to detect in an automatic way the relevance of each
player’s action to a game outcome, to detect the role of a player given
his game data, to rate every performance as well as to obtain a final
ranking of all players in Europe. A short demo will be provided during
the talk through a Jupyter notebook, exploiting interactive data
visualization with the Bokeh package.&lt;/p&gt;
&lt;p&gt;The audience will learn how to use Python to construct evaluation
algorithms entirely based on machine learning and big data, a step
forward to a thorough and objective evaluation of performance which
overcomes the biases and the limitations of human perception of
performance. Just a basic knowledge of Python and of data mining
principles is required for a full understanding of the talk.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;venerdì 20 aprile&lt;/strong&gt; at 15:15 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="mathematical-modelling"></category><category term="Python"></category><category term="bigdata"></category><category term="sports-analytics"></category><category term="Machine Learning"></category><category term="analytics"></category><category term="Algorithms"></category><category term="Data Mining"></category><category term="sklearn"></category></entry><entry><title>Machine Learning con Python: previsione in real-time della richiesta di energia elettrica</title><link href="https://pyvideo.org/pycon-italia-2017/machine-learning-con-python-previsione-in-real-time-della-richiesta-di-energia-elettrica.html" rel="alternate"></link><published>2017-04-08T00:00:00+00:00</published><updated>2017-04-08T00:00:00+00:00</updated><author><name>Felice Tuosto</name></author><id>tag:pyvideo.org,2017-04-08:pycon-italia-2017/machine-learning-con-python-previsione-in-real-time-della-richiesta-di-energia-elettrica.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Nel talk si parlerà di come attraverso il linguaggio Python sia
possibile risolvere un problema reale e complesso relativamente alla
trasmissione di energia elettrica. Verrà spiegato il progetto
&lt;strong&gt;RealtimeLoadForecast&lt;/strong&gt; che è stato sviluppato per un importante TSO
(Transmission System Operator). Si tratta di sistema predittivo che
permette di fornire in tempo reale ogni 15 minuti ed entro 5 minuti, le
previsioni delle serie storiche dei consumi di energia elettrica
relativi a circa 500 nodi elettrici.&lt;/p&gt;
&lt;p&gt;Si parlerà dei passi che occorre seguire per ottenere da un semplice
prototipo, un sistema &lt;em&gt;ingegnerizzato&lt;/em&gt; che lavori in tempo reale e di
come sono state utilizzate le librerie di Python per l’acquisizione,
manipolazione e processamento dei dati elettrici ed ambientali.&lt;/p&gt;
&lt;p&gt;Saranno descritte alcune tecniche algoritmiche e di Machine Learning per
ottenere dei modelli predittivi capaci di fornire previsioni accurate ma
con tempi di risposta sfidanti.&lt;/p&gt;
&lt;p&gt;Verrà mostrato un &lt;em&gt;esempio concreto&lt;/em&gt; di implementazione di un algoritmo
predittivo basato sulla libreria Deep Learning &lt;strong&gt;Keras&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Per la comprensione del talk non sono necessari particolari requisiti se
non una conoscenza di base di programmazione in Python e di Machine
Learning.&lt;/p&gt;
</summary><category term="Forecasting"></category><category term="Genetic Algorithms"></category><category term="Keras"></category><category term="Data Mining"></category><category term="programming-paradigms"></category><category term="scikit-learn"></category><category term="bigdata"></category><category term="scalability"></category><category term="Deep-Learning"></category><category term="threading"></category><category term="realtime"></category><category term="Data-Scientist"></category><category term="database"></category><category term="machine-learning"></category><category term="mysql"></category><category term="signal-processing"></category><category term="LoadForecasting"></category><category term="cassandra"></category></entry><entry><title>Big data with python</title><link href="https://pyvideo.org/pycon-au-2012/big-data-with-python.html" rel="alternate"></link><published>2012-08-22T00:00:00+00:00</published><updated>2012-08-22T00:00:00+00:00</updated><author><name>Alex Sharp</name></author><id>tag:pyvideo.org,2012-08-22:pycon-au-2012/big-data-with-python.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Dealing with big data isn't a particularly new problem. There are all
sorts of new solutions, each with their own niche, their own hype. It's
important to remember that python is not &amp;quot;too slow&amp;quot; for big data, and
that with projects such as&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dealing with big data isn't a particularly new problem. There are all
sorts of new solutions, each with their own niche, their own hype. It's
important to remember that python is not &amp;quot;too slow&amp;quot; for big data, and
that with projects such as scipy, numpy, cython and rpy, python is
becoming a better tool then ever for data processing. In this talk we'll
be explaining some of the theory behind big data problems, where python
fits in and some of the more interesting things you can do.&lt;/p&gt;
</summary><category term="bigdata"></category></entry><entry><title>Backup Is Hard; Let's Go Shopping</title><link href="https://pyvideo.org/pycon-us-2011/pycon-2011--backup-is-hard--let--39-s-go-shopping.html" rel="alternate"></link><published>2011-03-11T00:00:00+00:00</published><updated>2011-03-11T00:00:00+00:00</updated><author><name>Gary Bernhardt</name></author><id>tag:pyvideo.org,2011-03-11:pycon-us-2011/pycon-2011--backup-is-hard--let--39-s-go-shopping.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Backup Is Hard; Let's Go Shopping&lt;/p&gt;
&lt;p&gt;Presented by Gary Bernhardt&lt;/p&gt;
&lt;p&gt;We'll fly through the most clever bits of BitBacker, an online backup
app developed as a startup for three years and eventually abandoned.
Highlights: a hacked-up httplib/asyncore HTTP client; a real-life,
HATEOAS-respecting RESTful API, and an encryption scheme that can
quickly diff a file system against the server while leaking no
information – not even file timestamps.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;This is the story of a solution to a huge problem: fast, secure online
backup. A single client generates a hundred gigabytes, millions of data
chunks, and thousands of file system snapshots. To appreciate the
problem's scale, consider that a Python array holding content hashes for
1,000,000 files consumes 100 MB of memory. File hashes are only a
portion of the required per- file metadata, and that's only one for
snapshot of thousands.&lt;/p&gt;
&lt;p&gt;We'll tour the hard parts of this system with no apology for their
difficulty. The httplib/asyncore hybrid monster that served millions of
parallel requests, transparently retrying on failures and timeouts, with
only 300 lines of python. The RESTful API – fully respecting hypertext,
with every request safely repeatable, even POSTs, and not a single
hard-coded URL in the client. The encryption scheme that leaked nothing
– not even modification times – but could quickly diff local file
systems against the server. And, that one time that a client
accidentally requested a 4.76 megabyte URL in production.&lt;/p&gt;
</summary><category term="backup"></category><category term="bigdata"></category><category term="bitbacker"></category><category term="pycon"></category><category term="pycon2011"></category></entry><entry><title>Handling ridiculous amounts of data with probabilistic data structures</title><link href="https://pyvideo.org/pycon-us-2011/pycon-2011--handling-ridiculous-amounts-of-data-w.html" rel="alternate"></link><published>2011-03-11T00:00:00+00:00</published><updated>2011-03-11T00:00:00+00:00</updated><author><name>C. Titus Brown</name></author><id>tag:pyvideo.org,2011-03-11:pycon-us-2011/pycon-2011--handling-ridiculous-amounts-of-data-w.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Handling ridiculous amounts of data with probabilistic data structures&lt;/p&gt;
&lt;p&gt;Presented by C. Titus Brown&lt;/p&gt;
&lt;p&gt;Part of my job as a scientist involves playing with rather large amounts
of data (200 gb+). In doing so we stumbled across some neat CS
techniques that scale well, and are easy to understand and trivial to
implement. These techniques allow us to make some or many types of data
analysis map-reducable. I'll talk about interesting implementation
details, fun science, and neat computer science.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;If an extreme talk, I will talk about interesting details/issues in:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Python as the backbone for a non-SciPy scientific software package:
using Python as a frontend to C++ code, esp for parallelization and
testing purposes.&lt;/li&gt;
&lt;li&gt;Implementing probabilistic data structures with one-sided error as
pre-filters for data retrieval and analysis, in ways that are
generally useful.&lt;/li&gt;
&lt;li&gt;Efficiently breaking down certain types of sparse graph problems
using these probabilistic data structures, so that large graphs can
be analyzed straightforwardly. This will be applied to plagiarism
detection and/or duplicate code detection.&lt;/li&gt;
&lt;/ol&gt;
</summary><category term="bigdata"></category><category term="parallelization"></category><category term="pycon"></category><category term="pycon2011"></category><category term="testing"></category></entry><entry><title>Rapid Python used on Big Data to Discover Human Genetic Variation</title><link href="https://pyvideo.org/pycon-us-2011/pycon-2011--rapid-python-used-on-big-data-to-disc.html" rel="alternate"></link><published>2011-03-11T00:00:00+00:00</published><updated>2011-03-11T00:00:00+00:00</updated><author><name>Deniz Kural</name></author><id>tag:pyvideo.org,2011-03-11:pycon-us-2011/pycon-2011--rapid-python-used-on-big-data-to-disc.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Rapid Python used on Big Data to Discover Human Genetic Variation&lt;/p&gt;
&lt;p&gt;Presented by Deniz Kural&lt;/p&gt;
&lt;p&gt;Advances in genome sequencing has enabled large-scale projects such as
the 1000 Genomes Project to sequence genomes across diverse populations
around the world, resulting in very large data sets. I use Python for
rapid development of algorithms for processing &amp;amp; analyzing genomes and
discovering thousands of new variants, including &amp;quot;Mobile Elements&amp;quot; that
copy&amp;amp;paste; themselves across the genome.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Recent advances in high-throughput sequencing now enables accurate
sequencing human genomes at a low cost &amp;amp; high speed. This technology is
now used to initiate projects involving large-scale sequencing of many
genomes. The 1000 Genomes project aims to sequence 2500 genomes across
27 world populations, and has initially completed its Pilot phase. The
aim of the project is to discover &amp;amp; characterize novel variants. These
variants enable association studies that investigate the link between
genomic variation &amp;amp; phenotypes, including disease.&lt;/p&gt;
&lt;p&gt;A class of variants, known as &amp;quot;Structural Variants&amp;quot; represent a
heterogenous class of larger variants, such as inversions, duplications,
deletions, and various kinds of insertions.&lt;/p&gt;
&lt;p&gt;I use Python to for rapid development of algorithms to process, analyze,
and annotate very large data sets. In particular, I focus on Mobile
Elements, pieces of DNA that copy&amp;amp;paste; across the genome. These
elements constitute roughly half of the genome, whereas protein-coding
genes account for roughly 1.5 % of the genome.&lt;/p&gt;
&lt;p&gt;I will discuss distributed computing, genomics, and big data within the
context of Python.&lt;/p&gt;
</summary><category term="bigdata"></category><category term="casestudy"></category><category term="dna"></category><category term="genomes"></category><category term="pycon"></category><category term="pycon2011"></category></entry></feed>