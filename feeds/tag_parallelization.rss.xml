<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sun, 05 May 2019 00:00:00 +0000</lastBuildDate><item><title>Deep Learning for brain MRI segmentation: Big Data, AI and HPC meet together</title><link>https://pyvideo.org/pycon-italia-2019/deep-learning-for-brain-mri-segmentation-big-data-ai-and-hpc-meet-together.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With ever-increasing advancements in technology, neuroscientists are
able to collect data in greater volumes and with finer resolution. There
has been a growing interest in leveraging this vast volume of data
across levels of analysis, measurement techniques, and experimental
paradigms to gain more insight into brain function. At multiple stages
and levels of neuroscience investigation, ML holds great promise as an
addition to the arsenal of analysis tools for discovering how the brain
works. As quantitative analysis of brain MRI is routine for many
neurological diseases and conditions, deep learning-based segmentation
approaches for brain Magnetic Resonance Imaging (MRI) are gaining
interest due to their self-learning and generalisation ability over
large amounts of data. On the other hand, High Performance Computing
(HPC) and AI will increasingly intertwine as we transition to an
exascale future using new computing, storage, and communications
technologies. In this talk I will walk you through fundamentals of
generating high- performance deep-learning models in TensorFlow platform
using Python on large computing system (e.g NVIDIA® Tesla® GPUs powered
by Tensor Cores), in order to infer and segment thousands of cell
centroids out of the brain objects of interest. From a more
technological perspective, although astonishing results have been
achieved concerning the distribution of training large convolutional
neural networks on big data, to date the Python scientific ecosystem is
still missing tools for an optimised and, above all, distributed
inference of deep learning models. In this talk I will show you how a
tiling-based inferencing approach could be a good solution to remedy the
problem. The talk is intended for intermediate PyData researchers and
practitioners. Basic to intermediate level experience in image
recognition/object detection deep learning applications is assumed.
Overall, a good proficiency with the Python language and with scientific
python libraries (e.g. numpy, TensorFlow, Keras) are required for the
entire talk.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1794"&gt;https://python.it/feedback-1794&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Sunday 5 May&lt;/strong&gt; at 11:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Giuseppe Di Bernardo</dc:creator><pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-05:pycon-italia-2019/deep-learning-for-brain-mri-segmentation-big-data-ai-and-hpc-meet-together.html</guid><category>GPUComputing</category><category>parallelization</category><category>bio-informatics</category><category>Machine Learning</category><category>ComputerVision</category><category>optimization</category><category>data-analysis</category><category>Artificial Intelligence</category></item><item><title>Python and PostgreSQL for Huge Data Warehouses</title><link>https://pyvideo.org/europython-2013/python-and-postgresql-for-huge-data-warehouses.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hannu Krosing</dc:creator><pubDate>Thu, 04 Jul 2013 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2013-07-04:europython-2013/python-and-postgresql-for-huge-data-warehouses.html</guid><category>postgresql</category><category>nosql</category><category>parallelization</category><category>bigdata</category><category>scalability</category><category>pl/python</category><category>olap</category><category>optimization</category><category>architecture</category><category>sql</category><category>performance</category></item><item><title>Greenlet-based concurrency</title><link>https://pyvideo.org/europython-2013/greenlet-based-concurrency.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Goran Peretin</dc:creator><pubDate>Wed, 03 Jul 2013 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2013-07-03:europython-2013/greenlet-based-concurrency.html</guid><category>parallelization</category><category>optimization</category><category>gevent</category><category>greenlet</category><category>concurrency</category><category>performance</category></item><item><title>PostgreSQL is Web-Scale (Really :) )</title><link>https://pyvideo.org/europython-2013/postgresql-is-web-scale-really.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I show you how to set up a python and PostgreSQL based
system which is easy to set up and easy to scale, provides ACID
guarantees where they are needed and delays time-consistency between
unrelated objects for scalability and availability where the latter are
deemed more important.&lt;/p&gt;
&lt;p&gt;The best thing is that this kind of scalability work for both OLTP and
OLAP workloads, so with some planning you can have just a single large
“database” which can take almost any type of load.&lt;/p&gt;
&lt;p&gt;Also, if you hate SQL, you can do all the OLTP stuff in a pythonic way
using an automagically generated ORM layer inside the database, near the
data. If you are really masochistic, you can use the same ORM also for
map-reduce type distributed data processing, though on this side the
small effort of learning SQL usually pays off when queries get more
complex. But as I said, everything runs inside the databse, near the
data and thus even the ORM &amp;amp; map-reduce analytics works fast.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hannu Krosing</dc:creator><pubDate>Tue, 02 Jul 2013 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2013-07-02:europython-2013/postgresql-is-web-scale-really.html</guid><category>postgresql</category><category>nosql</category><category>datamining</category><category>parallelization</category><category>distributed</category><category>bigdata</category><category>scalability</category><category>pl/python</category><category>olap</category><category>optimization</category><category>orm</category><category>sql</category><category>performance</category></item><item><title>Uno sguardo agli internal di RestFS</title><link>https://pyvideo.org/europython-2013/uno-sguardo-agli-internal-di-restfs.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fabrizio Manfredi</dc:creator><pubDate>Tue, 02 Jul 2013 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2013-07-02:europython-2013/uno-sguardo-agli-internal-di-restfs.html</guid><category>clustering</category><category>HTTP</category><category>parallelization</category><category>distributed</category><category>twisted</category><category>REST</category><category>optimization</category><category>Algorithms</category><category>scalability</category><category>async</category><category>hpc</category><category>performance</category></item><item><title>Handling ridiculous amounts of data with probabilistic data structures</title><link>https://pyvideo.org/pycon-us-2011/pycon-2011--handling-ridiculous-amounts-of-data-w.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Handling ridiculous amounts of data with probabilistic data structures&lt;/p&gt;
&lt;p&gt;Presented by C. Titus Brown&lt;/p&gt;
&lt;p&gt;Part of my job as a scientist involves playing with rather large amounts
of data (200 gb+). In doing so we stumbled across some neat CS
techniques that scale well, and are easy to understand and trivial to
implement. These techniques allow us to make some or many types of data
analysis map-reducable. I'll talk about interesting implementation
details, fun science, and neat computer science.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;If an extreme talk, I will talk about interesting details/issues in:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Python as the backbone for a non-SciPy scientific software package:
using Python as a frontend to C++ code, esp for parallelization and
testing purposes.&lt;/li&gt;
&lt;li&gt;Implementing probabilistic data structures with one-sided error as
pre-filters for data retrieval and analysis, in ways that are
generally useful.&lt;/li&gt;
&lt;li&gt;Efficiently breaking down certain types of sparse graph problems
using these probabilistic data structures, so that large graphs can
be analyzed straightforwardly. This will be applied to plagiarism
detection and/or duplicate code detection.&lt;/li&gt;
&lt;/ol&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">C. Titus Brown</dc:creator><pubDate>Fri, 11 Mar 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-03-11:pycon-us-2011/pycon-2011--handling-ridiculous-amounts-of-data-w.html</guid><category>bigdata</category><category>parallelization</category><category>pycon</category><category>pycon2011</category><category>testing</category></item></channel></rss>