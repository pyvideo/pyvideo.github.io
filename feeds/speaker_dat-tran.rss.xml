<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 24 Aug 2016 00:00:00 +0000</lastBuildDate><item><title>How you really get your data science models into production the cool way!</title><link>https://pyvideo.org/pydata-san-francisco-2016/how-you-really-get-your-data-science-models-into-production-the-cool-way.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;This talk discusses one of Pivotal Labs’ core principles, API first, and how this can help to overcome the common language problem between data scientists and software engineers. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Tools&lt;/li&gt;
&lt;li&gt;Software engineering methodologies like continuous deployment and TDD&lt;/li&gt;
&lt;li&gt;Microservices&lt;/li&gt;
&lt;li&gt;PaaS and Cloud Native Data Science&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Over the years we have seen many non-tech companies starting to build their own data science teams because they realize that data will eat the world. In an effort to understand the space, these teams have begun to play with their data and create early prototypes. Unfortunately, those prototypes primarily end up in powerpoint and die. Of the ones that move forward, there is a gap in knowledge of their development team in how to release to production.&lt;/p&gt;
&lt;p&gt;From our experience, we’ve seen their data science and development team do not speak a common language. At Pivotal Labs, we found a good way to overcome this language problem. Our solution is to follow an API first approach which is one of our core principles.&lt;/p&gt;
&lt;p&gt;In this talk, I want to to share my experiences of how to put these models into production. I will focus on the tools that we use for this and what data science has to do with microservices. My presentation will contain an end-to-end data science example.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dat Tran</dc:creator><pubDate>Wed, 24 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/how-you-really-get-your-data-science-models-into-production-the-cool-way.html</guid></item><item><title>PySpark in Practice</title><link>https://pyvideo.org/pydata-berlin-2016/pyspark-in-practice.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;In this talk we will share our best practices of using PySpark in numerous customer facing data science engagements. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Unit testing with PySpark&lt;/li&gt;
&lt;li&gt;Integration with SQL on Hadoop engines&lt;/li&gt;
&lt;li&gt;Data pipeline management and workflows&lt;/li&gt;
&lt;li&gt;Data Structures (RDDs vs. Data Frames vs. Data Sets)&lt;/li&gt;
&lt;li&gt;When to use MLlib vs scikit-learn&lt;/li&gt;
&lt;li&gt;Operationalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At Pivotal Labs we have many data science engagements on big data. Typical problems involve real-time data from sensors collected by telecom operators to GPS data produced by vehicle tracking systems. One widespread framework to solve those inherently difficult problems is Apache Spark. In this talk, we want to share our best practices with PySpark, Spark’s Python API, highlighting our experience as well as dos and dont's. In particular, we will focus on the whole data science pipeline from data ingestion, data munging and wrangling to the actual model building.&lt;/p&gt;
&lt;p&gt;Finally, many businesses have started to realise that there is no return on investment from data science if the models do not go into production. At Pivotal Labs, one our core principle is API first. Therefore, we will also talk how we put our models into production, sharing our hands-on knowledge in this field and also how this fits into test-driven development.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ronert Obst</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/pyspark-in-practice.html</guid><category>pyspark</category></item><item><title>PySpark in Practice</title><link>https://pyvideo.org/pydata-london-2016/ronert-obst-dat-tran-pyspark-in-practice.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In this talk we will share our best practices of using PySpark in numerous customer facing data science engagements. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Unit testing with PySpark&lt;/li&gt;
&lt;li&gt;Integration with SQL on Hadoop engines&lt;/li&gt;
&lt;li&gt;Data pipeline management and workflows&lt;/li&gt;
&lt;li&gt;Data Structures (RDDs vs. Data Frames vs. Data Sets)&lt;/li&gt;
&lt;li&gt;When to use MLlib vs scikit-learn&lt;/li&gt;
&lt;li&gt;Operationalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At Pivotal Labs we have many data science engagements on big data. Typical problems involve real-time data from sensors collected by telecom operators to GPS data produced by vehicle tracking systems. One widespread framework to solve those inherently difficult problems is Apache Spark. In this talk, we want to share our best practices with PySpark, Spark’s Python API, highlighting our experience as well as dos and don'ts. In particular, we will focus on the whole data science pipeline from data ingestion, data munging and wrangling to the actual model building.&lt;/p&gt;
&lt;p&gt;Finally, many businesses have started to realise that there is no return on investment from data science if the models do not go into production. At Pivotal Labs, one our core principle is API first. Therefore, we will also talk how we put our models into production, sharing our hands-on knowledge in this field and also how this fits into test-driven development.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://pydata2016.cfapps.io/"&gt;http://pydata2016.cfapps.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/datitran/spark-tdd-example"&gt;https://github.com/datitran/spark-tdd-example&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ronert Obst</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/ronert-obst-dat-tran-pyspark-in-practice.html</guid></item></channel></rss>