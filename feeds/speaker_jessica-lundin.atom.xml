<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_jessica-lundin.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-05-19T00:00:00+00:00</updated><entry><title>Snakes on a Hyperplane: Python Machine Learning in Production</title><link href="https://pyvideo.org/pycon-us-2017/snakes-on-a-hyperplane-python-machine-learning-in-production.html" rel="alternate"></link><published>2017-05-19T00:00:00+00:00</published><updated>2017-05-19T00:00:00+00:00</updated><author><name>Jessica Lundin</name></author><id>tag:pyvideo.org,2017-05-19:pycon-us-2017/snakes-on-a-hyperplane-python-machine-learning-in-production.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Companies with an artificial-intelligence plan have a differentiating
strategy in the intelligence economy; however, implementing robust
machine-learning in production is nontrivial, often requiring a close
collaboration between data scientists and developers, and retooling the
production stack and workflows to develop and maintain accurate models.&lt;/p&gt;
&lt;p&gt;Machine learning in production involves model application, handling
missing data, data artifacts, and data outside of the training
calibration. A rigorous evaluation framework draws upon logging to
determine characteristics of model coverage, model performance,
auditing, and run-time performance. Model coverage includes the number
of times the model produced sensible output relative to number of times
it is called. Model coverage is reduced if the model does not converge
or model criteria are not met. Model performance is evaluated with a
suite of metrics (accuracy, AUC, FPR, TPR, RMSE, MAPE, etc.), which
assist in determining the most appropriate model to use in the
production scenario and the validity of the model training. Regularly
performing manual audits for spot checks is important for debugging and
ensuring the model passes sanity checks. Model performance includes run
times and profiling model pieces, ensuring performance is within
specified requirements and refactoring otherwise.&lt;/p&gt;
&lt;p&gt;In the AI renaissance, where ML is a critical piece of intelligent
products, seamlessly integrating model evaluation into workflows is an
important component of making robust products and building a satisfying
customer experience. Python is a great language to build intelligent
products with its abundance of ML libraries and wrappers contributed as
open-source software in addition to rich full-stack capabilities.&lt;/p&gt;
</summary></entry></feed>