<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Bernhard Schölkopf</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 15 Jul 2024 00:00:00 +0000</lastBuildDate><item><title>Bayesian Online Prediction of Change Points</title><link>https://pyvideo.org/uai-2020/bayesian-online-prediction-of-change-points.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Bayesian Online Prediction of Change Points&lt;/p&gt;
&lt;p&gt;Diego Agudelo-España (MPI for Intelligent Systems, Tübingen)*; Sebastian Gomez-Gonzalez (Max Planck Institute for Intelligent Systems); Stefan Bauer (MPI IS); Bernhard Schölkopf (MPI for Intelligent Systems, Tübingen); Jan Peters (TU Darmstadt + Max Planck Institute for Intelligent Systems)&lt;/p&gt;
&lt;p&gt;Online detection of instantaneous changes in the generative process of a data sequence generally focuses on retrospective inference of such change points without considering their future occurrences. We extend the Bayesian Online Change Point Detection algorithm to also infer the number of time steps until the next change point (i.e., the residual time). This enables to handle observation models which depend on the total segment duration, which is useful to model data sequences with temporal scaling. The resulting inference algorithm for segment detection can be deployed in an online fashion, and we illustrate applications to synthetic and to two medical real-world data sets.&amp;quot;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Diego Agudelo-España</dc:creator><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2020-08-03:/uai-2020/bayesian-online-prediction-of-change-points.html</guid><category>UAI 2020</category></item><item><title>On the design of consequential ranking algorithms</title><link>https://pyvideo.org/uai-2020/on-the-design-of-consequential-ranking-algorithms.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;On the design of consequential ranking algorithms&lt;/p&gt;
&lt;p&gt;Behzad Tabibian (Max Planck Inst. for Intelligent Systems)*; Vicenç Gómez (Universitat Pompeu Fabra); Abir De (IIT Bombay); Bernhard Schölkopf (MPI for Intelligent Systems, Tübingen); Manuel Gomez Rodriguez (MPI-SWS)&lt;/p&gt;
&lt;p&gt;Ranking models are typically designed to optimize some measure of immediate utility to the users. As a result, they have been unable to anticipate an increasing number of undesirable long-term consequences of their proposed rankings, from fueling the spread of misinformation and increasing polarization to degrading social discourse. Can we design ranking models that anticipate the consequences of their proposed rankings and are able to avoid the undesirable ones? In this paper, we first introduce a joint representation of rankings and user dynamics using Markov decision processes. Then, we show that this representation greatly simplifies the construction of consequential ranking models that trade off the
immediate utility and the long-term welfare. In particular, we can obtain optimal consequential rankings by applying weighted sampling on the rankings provided by models that maximize measures of immediate utility. However, in practice, such a strategy may be inefficient and impractical, specially in high dimensional scenarios. To overcome this, we introduce an efficient gradient-based algorithm to learn parameterized consequential ranking models that effectively approximate optimal ones. We illustrate our methodology using synthetic and real data gathered from Reddit and show that our consequential rankings may mitigate the spread of misinformation and improve the civility of online discussions.&amp;quot;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Behzad Tabibian</dc:creator><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2020-08-03:/uai-2020/on-the-design-of-consequential-ranking-algorithms.html</guid><category>UAI 2020</category></item><item><title>Semi-supervised learning, causality, and the conditional cluster assumption</title><link>https://pyvideo.org/uai-2020/semi-supervised-learning-causality-and-the-conditional-cluster-assumption.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Semi-supervised learning, causality, and the conditional cluster assumptionㅇㅇJulius von Kügelgen (MPI for Intelligent Systems, Tübingen &amp;amp; University of Cambridge); Alexander Mey (Delft University of Technology); Marco Loog (Delft University of Technology &amp;amp; University of Copenhagen); Bernhard Schölkopf (MPI for Intelligent Systems, Tübingen)ㅇㅇWhile the success of semi-supervised learning (SSL) is still not fully understood, Sch ̈olkopf et al. (2012) have established a link to the principle of independent causal mechanisms. They conclude that SSL should be impossible when predicting a target variable from its causes, but possible when predicting it from its effects. Since both these cases are restrictive, we extend their work by considering classification using cause and effect features at the same time, such as predicting a disease from both risk factors and symptoms. While standard SSL exploits information contained in the marginal distribution of all inputs (to improve the estimate of the conditional distribution of the target given in-puts), we argue that in our more general setting we should use information in the conditional distribution of effect features given causal features. We explore how this insight generalises the previous understanding, and how it relates to and can be exploited algorithmically for SSL.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Julius von Kügelgen</dc:creator><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2020-08-03:/uai-2020/semi-supervised-learning-causality-and-the-conditional-cluster-assumption.html</guid><category>UAI 2020</category></item><item><title>Testing Goodness of Fit of Conditional Density Models with Kernels</title><link>https://pyvideo.org/uai-2020/testing-goodness-of-fit-of-conditional-density-models-with-kernels.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Testing Goodness of Fit of Conditional Density Models with Kernels&lt;/p&gt;
&lt;p&gt;Wittawat Jitkrittum (Google)*; Heishiro Kanagawa (Gatsby Computational Neuroscience Unit, University College London); Bernhard Schölkopf (MPI for Intelligent Systems, Tübingen)&lt;/p&gt;
&lt;p&gt;We propose two nonparametric statistical tests of goodness of fit for conditional distributions: given a conditional probability density function p(y|x) and a joint sample, decide whether the sample is drawn from p(y|x)q(x) for some density q(x). Our tests, formulated with a Stein operator, can be applied to any differentiable conditional density model, and require no knowledge of the normalizing constant. We show that 1) our tests are consistent against any fixed alternative conditional model; 2) the statistics can be estimated easily, requiring no density estimation as an intermediate step; and 3) our second test offers an interpretable test result providing insight on where the conditional model does not fit well in the domain of the covariate. We demonstrate the interpretability of our test on a task of modeling the distribution of New York City's taxi drop-off location given a pick-up point. To our knowledge, our work is the first to propose such conditional goodness-of-fit tests that simultaneously have all these desirable properties.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Wittawat Jitkrittum</dc:creator><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2020-08-03:/uai-2020/testing-goodness-of-fit-of-conditional-density-models-with-kernels.html</guid><category>UAI 2020</category></item><item><title>UAI 2024 Oral Session 3: Causality</title><link>https://pyvideo.org/uai-2024/uai-2024-oral-session-3-causality.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Towards Bounding Causal Effects under Markov Equivalence
Alexis Bellot
&lt;a class="reference external" href="https://openreview.net/pdf?id=xY33abx46a"&gt;https://openreview.net/pdf?id=xY33abx46a&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Targeted Reduction of Causal Models
Armin Kekić, Bernhard Schölkopf, Michel Besserve
&lt;a class="reference external" href="https://openreview.net/pdf?id=CFHpI53xmb"&gt;https://openreview.net/pdf?id=CFHpI53xmb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Characterising Interventions in Causal Games
Manuj Mishra, James Fox, Michael J. Wooldridge
&lt;a class="reference external" href="https://openreview.net/pdf?id=MXwg8dYBFd"&gt;https://openreview.net/pdf?id=MXwg8dYBFd&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Causally Abstracted Multi-armed Bandits
Fabio Massimo Zennaro, Nicholas George Bishop, Joel Dyer, Yorgos Felekis, Ani Calinescu, Michael J. Wooldridge, Theodoros Damoulas
&lt;a class="reference external" href="https://openreview.net/pdf?id=Uxrxz4X416"&gt;https://openreview.net/pdf?id=Uxrxz4X416&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alexis Bellot</dc:creator><pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-07-15:/uai-2024/uai-2024-oral-session-3-causality.html</guid><category>UAI 2024</category></item></channel></rss>