<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_functional-testing.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-04-22T00:00:00+00:00</updated><entry><title>Integration tests ready to use with pytest-play</title><link href="https://pyvideo.org/pycon-italia-2018/integration-tests-ready-to-use-with-pytest-play.html" rel="alternate"></link><published>2018-04-22T00:00:00+00:00</published><updated>2018-04-22T00:00:00+00:00</updated><author><name>Serena Martinetti</name></author><id>tag:pyvideo.org,2018-04-22:pycon-italia-2018/integration-tests-ready-to-use-with-pytest-play.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In my talk I will introduce a new pytest plugin with which it is very
easy (even for non-technical) go to create and run new integration
testing at any level of IOT complex systems.&lt;/p&gt;
&lt;p&gt;pytest-play is a pytest plugin that allows you to play a JSON file that
describes some actions and assertions. We can use actions like: -
Selenium, driving the browser for the UI test - MQTT messages,
simulating a device - API calls - queries to Cassandra or PostgresSQL
(in the future) - custom commands, thanks to the pluggable architecture
Other advantages: - UI tests more reliable with implicit waits before
interacting with the elements - BDD support to make the scenario more
legible - reusability of steps&lt;/p&gt;
&lt;p&gt;I will show you how easy it is to create a json and execute it on the
fly on a Continuous Integration system. So letâ€™s start having fun in
testing with pytest-play.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;domenica 22 aprile&lt;/strong&gt; at 12:00 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="continuous-integration"></category><category term="open source"></category><category term="Python"></category><category term="testing"></category><category term="selenium"></category><category term="integration"></category><category term="api"></category><category term="pytest"></category><category term="Functional Testing"></category></entry><entry><title>Roboto Framework for Test Code Coverage for cloud services.</title><link href="https://pyvideo.org/pycon-italia-2017/roboto-framework-for-test-code-coverage-for-cloud-services.html" rel="alternate"></link><published>2017-04-09T00:00:00+00:00</published><updated>2017-04-09T00:00:00+00:00</updated><author><name>khushbu parakh</name></author><id>tag:pyvideo.org,2017-04-09:pycon-italia-2017/roboto-framework-for-test-code-coverage-for-cloud-services.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Test case design is an important phase of software testing life cycle
where test cases are identified. Designing test cases for cloud
solutions requires a different approach from traditional application
oriented testing. The quality which is multidimensional will include
more attributes like network resiliency, fault recovery, reliability and
availability when it comes to cloud-based solutions. If such aspects
when not encompassed through test development framework, it will lead to
high chances of defect leakage in production in an agile mode of
delivery. We propose a strategy to design the test cases which traverse
across different levels of testing to find gaps using the Roboto
framework with Python. The solution provides the teams and the product
owners to get a picture of what is getting tested and how much of
coverage is done. The data can be used in team release discussions to
build confidence in the tests that gets executed as part of that
release. &lt;a class="reference external" href="https://github.com/robotframework/robotframework"&gt;https://github.com/robotframework/robotframework&lt;/a&gt; Testing
performed for features in a product is diverse and can be widely
categorized into functional, non-functional like reliability, fault
injections etc. Though developing test cases for functional changes is
achievable, arriving at test cases for non-functional requirements is
challenging in the given two-week sprint cycle of agile, and so leads to
defect leakage. Some vital modules in a project would have been running
in production for years and would have different sets of teams working
at different periods. Quantifying test code coverage and quality only by
a number of test cases per feature does not suffice as testing is
multidimensional and has to span across different levels. Measuring and
improving the test code coverage is the challenge generally faced by
product teams.&lt;/p&gt;
</summary><category term="quality-assurance"></category><category term="Functional Testing"></category><category term="Python"></category><category term="testing"></category><category term="robustness"></category><category term="integration"></category><category term="automation"></category><category term="framework"></category><category term="deployment"></category><category term="google-cloud"></category></entry></feed>