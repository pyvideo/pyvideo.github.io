<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_jonathan-dinu.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2015-07-24T00:00:00+00:00</updated><entry><title>Scalable Pipelines with Luigi or: Iâ€™ll have the Data Engineering, hold the Java!</title><link href="https://pyvideo.org/pydata-seattle-2015/scalable-pipelines-with-luigi-or-ill-have-the-data-engineering-hold-the-java.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Jonathan Dinu</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/scalable-pipelines-with-luigi-or-ill-have-the-data-engineering-hold-the-java.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this workshop you see how (and why) to leverage the PyData ecosystem to build a robust data pipeline. More specifically you will learn how to use the Luigi framework to integrate multiple stages of a model building pipeline (collection, processing, vectorization, training of multiple models, and validation) all in Python!&lt;/p&gt;
&lt;p&gt;As companies scale prototypes and ad hoc analyses into production systems, it is critical to build automated (and repeatable) systems for data collection/processing and model training /evaluation which are fault tolerant enough to adapt to changing constraints. Sustainable software development is often an afterthought for data scientists, especially since the tools for analysis (R, scientific python, etc.) do not naturally lend themselves to building scalable and extensible software abstractions. But now we can have our cake and eat it too... all with Python!&lt;/p&gt;
&lt;p&gt;In this workshop you see how (and why) to leverage the PyData ecosystem to build a robust data pipeline. More specifically you will learn how to use the Luigi framework to integrate multiple stages of a model building pipeline: collection, processing, vectorization, training of multiple models, and validation.&lt;/p&gt;
&lt;p&gt;Outline:
The basic components of a data pipeline (5min)
What and Why Luigi (10min)
Lab: The Smallest (1 stage) pipeline (15min)
Managing dependencies in a pipeline (10min)
Lab: Multi-stage pipeline and introduction to the Luigi Visualizer (15min)
Serialization in a Data Pipeline (10min)
Lab: Integrating your pipeline with HDFS and Postgres (20min)
Scheduling (10min)
Lab: Parallelism and recurring jobs with Luigi (20min)
Wrap up and next steps (5min)&lt;/p&gt;
&lt;p&gt;Materials available here:
Github Repo:  &lt;a class="reference external" href="https://github.com/Jay-Oh-eN/data-engineering-101"&gt;https://github.com/Jay-Oh-eN/data-engineering-101&lt;/a&gt;
Slides:  &lt;a class="reference external" href="http://www.slideshare.net/jonathandinu/presentation-45784222"&gt;http://www.slideshare.net/jonathandinu/presentation-45784222&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>