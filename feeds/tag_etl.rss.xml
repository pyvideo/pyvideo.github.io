<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Thu, 31 Oct 2019 00:00:00 +0000</lastBuildDate><item><title>Big Data Pipeline Design and Tuning in PySpark</title><link>https://pyvideo.org/pycon-se-2019/big-data-pipeline-design-and-tuning-in-pyspark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PySpark is a great tool for doing big data ETL pipeline. While designing a big data pipeline, which is easy to maintain with a holistic view, simple to spot bottleneck is difficult. Not to say enable analytics on ETL pipelines. Rockie Yang will share his experiences on build effective ETL pipeline with PySpark.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rockie Yang</dc:creator><pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-31:pycon-se-2019/big-data-pipeline-design-and-tuning-in-pyspark.html</guid><category>pyspark</category><category>data pipeline</category><category>etl</category></item><item><title>ETL-ing the Israeli Government (and living to tell the tale)</title><link>https://pyvideo.org/pycon-israel-2018/etl-ing-the-israeli-government-and-living-to-tell-the-tale.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In the past 8 years we've been collecting, scraping and processing government data - overcoming technical, legal and other hurdles - to create Israel's richest database of fiscal data (and growing!). In this talk I will talk about how we're doing this (we - at the Public Knowledge Workshop), and present the unique software frameworks and tools we've built to accomplish this goal. In particular, I will talk about the data-package pipelines framework, which combines a suite of standards and tools for 'packaging' data as well as a versatile engine for processing data streams.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Adam Kariv</dc:creator><pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-06-04:pycon-israel-2018/etl-ing-the-israeli-government-and-living-to-tell-the-tale.html</guid><category>etl</category><category>government</category></item><item><title>Simple Data Engineering in python 3.5+ with Bonobo</title><link>https://pyvideo.org/pycon-de-2017/simple-data-engineering-in-python-35-with-bonobo.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Romain Dorgueil&lt;/strong&gt; (&amp;#64;rdorgueil)&lt;/p&gt;
&lt;p&gt;Developer, sysadmin, technical team builder, founder of two companies), advisor.&lt;/p&gt;
&lt;p&gt;Currently helping start-ups to achieve more with less in our acceleration programs in Paris, and in charge of our product development activities.&lt;/p&gt;
&lt;p&gt;Sometimes, I play go and make music, but not at the same time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Simple is better than complex, and that's True for data pipelines, too.&lt;/p&gt;
&lt;p&gt;Bonobo is a python 3.5+ tool used to write and monitor data pipelines. It’s plain, simple, modern, and atomic python.&lt;/p&gt;
&lt;p&gt;This talk is a practical encounter, from zero to a complete data pipeline.&lt;/p&gt;
&lt;p&gt;Spoiler : no «big data» here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Simple is better than complex, right? That’s true for data pipelines too.&lt;/p&gt;
&lt;p&gt;For the last 5 years, I hacked together extract-transform-load (ETL) processes in various different positions (ETL is just a fancy term for «bunch of things that take data somewhere and put it elsewhere, eventually transformed»).&lt;/p&gt;
&lt;p&gt;I did it as a founder, as a consultant, as a technical co-founder, for some side projects, big corporates and small side projects.&lt;/p&gt;
&lt;p&gt;In each case, I felt frustrated with the tools available, and in some serious cases, I had to hack things myself to get the job done. Bonobo is the repackaging of my past experiences for python 3.5+, and grasping the basics should not take more than the length of the presentation.&lt;/p&gt;
&lt;p&gt;Outline (subject to small changes, for the greater good) :&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;INTRO : The ETL market, why a new tool, what it is, what it is not.&lt;/li&gt;
&lt;li&gt;Basics and concepts.&lt;/li&gt;
&lt;li&gt;Simple example.&lt;/li&gt;
&lt;li&gt;Complete data pipeline example, using SQL, RDF and a small Django frontend.&lt;/li&gt;
&lt;li&gt;OUTRO : A glimpse at the future.&lt;/li&gt;
&lt;li&gt;Q&amp;amp;A&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bonobo is the glue you need to tie together regular functions in a transformation graph (think unix pipes). Execution strategies are abstracted so you can focus on the real operations. As a result, you can engineer simple and testable systems, using the same good computer development practices as you use in .&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Romain Dorgueil</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/simple-data-engineering-in-python-35-with-bonobo.html</guid><category>python</category><category>business</category><category>data-engineering</category><category>etl</category><category>simple</category><category>bonobo</category></item><item><title>Dai dati alla visualizzazione: la mia prima data pipeline</title><link>https://pyvideo.org/pycon-italia-2017/dai-dati-alla-visualizzazione-la-mia-prima-data-pipeline.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Costruire una data pipeline non deve essere per forza complicato. Nel
talk vedremo come si può realizzare una semplice data pipeline usando
Luigi per consumare dati da una API e prepararli per fare esplorazione e
visualizzazione con Superset (AKA Caravel).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Riccardo Magliocchetti</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pycon-italia-2017/dai-dati-alla-visualizzazione-la-mia-prima-data-pipeline.html</guid><category>DataExploration</category><category>superset</category><category>DataVisualization</category><category>etl</category><category>luigi</category></item><item><title>Data Integration in the World of Microservices</title><link>https://pyvideo.org/pydata-berlin-2016/data-integration-in-the-world-of-microservices.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Since its launch in 2008, Zalando has grown with tremendous speed. The road from startup to multinational corporation has been full of challenges, especially for Zalando's technology team. Distributed across Berlin, Helsinki, Dublin, Hamburg and Dortmund — with nearly 1000 professionals strong — Zalando Technology still plans to expand by adding 1,000 more developers through the end of 2016.&lt;/p&gt;
&lt;p&gt;This rapid growth has shown us that we need to remain flexible about developing processes and organizational structures, to allow us to continue scaling and experimenting. In March 2015, our team adopted Radical Agility: a tech management approach that emphasizes Autonomy, Purpose, and Mastery, with trust as the glue holding it all together.&lt;/p&gt;
&lt;p&gt;To make autonomy possible, teams can now choose their own technology stacks for the products they own. Microservices, speaking with each other using RESTful APIs, promise to minimize the costs of integration between autonomous teams. In addition, Isolated AWS accounts run on top of our own open-source Platform as a Service (called STUPS.io), give each autonomous team enough hardware to experiment and introduce new features without breaking our entire system.&lt;/p&gt;
&lt;p&gt;One small issue with having microservices isolated in their individual AWS accounts: Our teams keep local data for themselves. In this environment, building an ETL process for data analyses, or integrating data from different services becomes quite challenging.&lt;/p&gt;
&lt;p&gt;PostgreSQL's new logical replication features, however, now make it possible to stream all the data changes from the isolated databases to the data integration system so that it can collect this data, represent it in different forms, and prepare it for analysis.&lt;/p&gt;
&lt;p&gt;In this talk, I will discuss Zalando's open-source data collection prototype, which uses PostgreSQL's logical replication streaming capabilities to collect data from various PostgreSQL databases, and recreate it for different formats and systems (Data Lake, Operational Data Store, KPI calculation systems, automatic process monitoring). The audience will come away with new ideas for how to use Postgres streaming replication in a microservices environment.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Valentine Gogichashvili</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/data-integration-in-the-world-of-microservices.html</guid><category>stups.io</category><category>postgresql</category><category>replication</category><category>etl</category></item></channel></rss>