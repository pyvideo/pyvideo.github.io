<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 04 Sep 2019 00:00:00 +0000</lastBuildDate><item><title>Distributed GPU Computing with Dask</title><link>https://pyvideo.org/euroscipy-2019/distributed-gpu-computing-with-dask.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The need for speed remains important for scientific computing.
Historically, computers were limited to few dozens of processors, but
with modern GPUs, we can have thousands, or even millions of cores
running in parallel on distributed systems.&lt;/p&gt;
&lt;p&gt;However, developing software for distributed GPU systems can be
difficult, both because writing GPU code can be challenging for
non-experts, and because distributed systems are inherently complex. We
can work to address these challenges by using GPU-enabled libraries that
mimic parts of the SciPy ecosystem, such as CuPy, RAPIDS, and Numba,
abstracting GPU programming complexity, combined with Dask to abstract
distributed computing complexity.&lt;/p&gt;
&lt;p&gt;We talk about how Dask has come a long way to support distributed
GPU-enabled systems by leveraging community standards and protocols,
reusing open source libraries for GPU computing, and keeping it simple
and complication-free to build highly-configurable accelerated
distributed software.&lt;/p&gt;
&lt;p&gt;Dask has evolved over the last year to leverage multi-GPU computing
alongside its existing CPU support. We present how this is possible with
the use of NumPy-like libraries and how to get started writing
distributed GPU software.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Peter Andreas Entschev</dc:creator><pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-09-04:euroscipy-2019/distributed-gpu-computing-with-dask.html</guid></item></channel></rss>