<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_maria-navarro.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-13T00:00:00+00:00</updated><entry><title>Quantifying uncertainty in Machine Learning predictions</title><link href="https://pyvideo.org/pydata-london-2019/quantifying-uncertainty-in-machine-learning-predictions.html" rel="alternate"></link><published>2019-07-13T00:00:00+00:00</published><updated>2019-07-13T00:00:00+00:00</updated><author><name>Maria Navarro</name></author><id>tag:pyvideo.org,2019-07-13:pydata-london-2019/quantifying-uncertainty-in-machine-learning-predictions.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;It is common practice to test the performance of ML models, but it is not so common to test the reliability of the predictions. Training a model, test its performance and hoping that it will produce good quality predictions is not the right approach if we are concerned with reliable ML. Hence, in this talk, we will discuss the concept of conformal predictions which quantify quality in predictions.&lt;/p&gt;
</summary></entry></feed>