<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_mai-gimenez.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-08-05T00:00:00+00:00</updated><entry><title>Un vector por tu palabra</title><link href="https://pyvideo.org/europython-2016/un-vector-por-tu-palabra.html" rel="alternate"></link><published>2016-08-05T00:00:00+00:00</published><updated>2016-08-05T00:00:00+00:00</updated><author><name>Mai Giménez</name></author><id>tag:pyvideo.org,2016-08-05:europython-2016/un-vector-por-tu-palabra.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Mai Giménez - Un vector por tu palabra
[EuroPython 2016]
[19 July 2016]
[Bilbao, Euskadi, Spain]
(&lt;a class="reference external" href="https://ep2016.europython.eu//conference/talks/un-vector-por-tu-palabra"&gt;https://ep2016.europython.eu//conference/talks/un-vector-por-tu-palabra&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;El ecosistema científico de python es extraordinario y saca músculo
con las últimas aportaciones de la comunidad científica. Revisaremos
nuevas aproximaciones a la representación de texto. ¡Tus cadenas de
texto merecen algo más que una mísera bolsa de palabras! Veremos cómo
se aplica la representación distribuida (word embeddings) en un caso
práctico de aprendizaje automático, y daremos consejos para hacer
experimentos replicables y obtener datos significativos.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;p&gt;“Dime con quien andas y te diré cómo eres” Este dicho es una de las
ideas más revolucionarias en  PLN. Podemos saber muchas cosas de una
palabra por su contexto. No es lo mismo un adorable gato que un gato
mecánico, pero por el contexto diferenciamos esta palabra polisémica.
Hasta ahora la mayor parte de los modelos representan una frase como
una bolsa de palabras. Por ejemplo, si queremos representar este
conjunto de frases: [“I love Python”, “I love NLP”, “Pyladies are
cool”] tenemos un vocabulario de siete palabras: [“I”, “love”,
“Python”, “NLP”, “Pyladies”, “are”, “cool”] esta representación crea
un vector de tamaño del vocabulario para cada frase, y pone a 1 si la
palabra aparece y a 0 en el caso contrario : [[1,1,1,0,0,0,0],
[1,1,0,1,0,0,],[0,0,0,0,1,1,1]] ¡Pero,se pierde el contexto y los
vectores pueden ser gigantes y con muchísimos 0s!
Recientemente, hemos encontrado una forma mucho mejor de representar
las palabras: La representación distribuida -word2vec, por ejemplo-
En esta charla exploramos esta representación y cómo aplicarla en
problemas de clasificación utilizando textos de redes sociales.
Navegaremos por el rico ecosistema científico en python, veremos cómo
crear gráficas significativas y hablaremos de la importancia de
escribir experimentos bien diseñados, replicables y con código
elegante y por supuesto de la importancia de difundir el conocimiento.
Debemos inspirar a la siguiente generación de científicos y
científicas ¡Seamos extraordinarios!&lt;/p&gt;
</summary></entry></feed>