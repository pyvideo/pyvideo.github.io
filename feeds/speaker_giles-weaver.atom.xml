<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_giles-weaver.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-05-06T00:00:00+00:00</updated><entry><title>Machine learning with ventilator data to improve reporting on critically ill newborn infants</title><link href="https://pyvideo.org/pydata-london-2017/machine-learning-with-ventilator-data-to-improve-reporting-on-critically-ill-newborn-infants.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Ian Ozsvald</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/machine-learning-with-ventilator-data-to-improve-reporting-on-critically-ill-newborn-infants.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description
Mechanical ventilators are widely used in intensive care, they are sophisticated but Doctors do not have time to analyse the copious traces of data in a neonatal unit. We are providing an easy-to-interpret summary of this time-series data using visualisation and machine learning. This is an open source collaboration with the NHS, All results are open.&lt;/p&gt;
&lt;p&gt;Abstract
Mechanical ventilators are widely used in intensive care. Even two decades ago they were be primarily mechanical devices whose &amp;quot;only&amp;quot; task was to inflate the patientâ€™s lung. Recently, however, they have become equipped with powerful computers that provide sophisticated ventilator modes. Data provided by the ventilators are almost never downloaded, stored or analysed. The data is complex, high frequency and requires time-intensive scrutiny to review. Doctors do not have time to analyse these traces in a neonatal unit.&lt;/p&gt;
&lt;p&gt;We are providing a simple and easy-to-interpret summary of 100Hz dual-channel ventilator data to improve the quality of care of young infants by time-poor staff. This involves signal processing, visualisation, building a gold standard and machine learning to segment breaths and summarise a baby's behaviour. This builds on our talk at PyDataLondon Meetup 30 in January 2017. Our goal is to open source the research so that others can benefit from the processes that we develop. We invite feedback from the audience to help improve our methods.&lt;/p&gt;
&lt;p&gt;Anyone interested in time series data, automated labeling, scikit-learn, Bokeh and medical applications will find this talk of interest. Both the highs and lows of our current approaches will be discussed.&lt;/p&gt;
&lt;p&gt;This is a collaboration between Dr Gusztav Belteki (Cambridge University Hopsitals NHS Foundation Turst), Ian Ozsvald (ModelInsight) and Giles Weaver (ModelInsight).&lt;/p&gt;
</summary></entry><entry><title>AlzHack Data Driven Diagnosis of Alzheimer's Disease 1</title><link href="https://pyvideo.org/pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease-1.html" rel="alternate"></link><published>2016-05-11T00:00:00+00:00</published><updated>2016-05-11T00:00:00+00:00</updated><author><name>Frank Kelly</name></author><id>tag:pyvideo.org,2016-05-11:pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease-1.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Alzheimer's disease is a form of dementia that affects over 44 million people globally. Unfortunately the condition is very hard to detect in its early stages. It is usually diagnosed by a simple questionnaire test, an approach that can only detect Alzheimer's disease many years after its onset. The challenge set in this project was earlier detection using Python and data science.&lt;/p&gt;
&lt;p&gt;AlzHack is a collaborative citizen science project undertaken by a small but diverse group of data scientists. We will discuss the challenges encountered in discovering and acquiring suitable data, describe how we cleaned and merged multiple data sources, and how it was possible to extract meaningful features from within.&lt;/p&gt;
&lt;p&gt;We will cover textual feature extraction, examining; amongst other methods, part-of-speech tagging, readability calculations, locality sensitive hashing as well as sentiment analysis, all in Python 3.&lt;/p&gt;
&lt;p&gt;In addition we will show how a variety of machine learning techniques (including text clustering and classification) were used; with the aim of distinguishing diagnosed Alzheimer's sufferers from their healthy peers solely based on samples of their written correspondence.&lt;/p&gt;
&lt;p&gt;This will be followed by a look at changepoint and ramp detection on noisy time series data; deployed to identify subtle changes in signals obtained from correspondence of individuals over time; thus allowing a form of non-medical, 'early warning' style detection of Alzheimer's disease.&lt;/p&gt;
&lt;p&gt;Finally we will address the tough task of scaling up a small, collaborative data science project to become an extremely powerful, widely available self-diagnosis tool.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/FrankKelly3/alz-hack-ii"&gt;http://www.slideshare.net/FrankKelly3/alz-hack-ii&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Statistically Solving Sneezes and Sniffles Step by Step</title><link href="https://pyvideo.org/pydata-london-2016/ian-ozsvald-giles-weaver-statistically-solving-sneezes-and-sniffles-step-by-step.html" rel="alternate"></link><published>2016-05-11T00:00:00+00:00</published><updated>2016-05-11T00:00:00+00:00</updated><author><name>Ian Ozsvald</name></author><id>tag:pyvideo.org,2016-05-11:pydata-london-2016/ian-ozsvald-giles-weaver-statistically-solving-sneezes-and-sniffles-step-by-step.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Since April 2015 our group has studied the Allergic Rhinitis of a subject with the goal of building a machine learned model that predicts the need for antihistamines. Approximately 30% of the world's population suffers from allergies, we aim to provide a methodology for others to identify the drivers of their own symptoms.&lt;/p&gt;
&lt;p&gt;This is a &amp;quot;citizen science&amp;quot; project, currently focused on one individual and a year's worth of self-reported antihistamine usage, sneezing data and geolocated points. We'll discuss the available external data (including the London Air project's pollution readings, weather, diet, exercise and commute data), exploratory data analysis, our approach to feature engineering from time-series and text sources and our modeling progress.&lt;/p&gt;
&lt;p&gt;The data logging iPhone app and data preparation tools are all open sourced. Python tools discussed include scikit-learn, statsmodels, glueviz, textract and t-sne. We'll also review our distributed working practices.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/ianozsvald/statistically-solving-sniffles-step-by-step-a-work-in-progress"&gt;https://speakerdeck.com/ianozsvald/statistically-solving-sniffles-step-by-step-a-work-in-progress&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>AlzHack Data Driven Diagnosis of Alzheimer's Disease</title><link href="https://pyvideo.org/pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease.html" rel="alternate"></link><published>2016-05-09T00:00:00+00:00</published><updated>2016-05-09T00:00:00+00:00</updated><author><name>Frank Kelly</name></author><id>tag:pyvideo.org,2016-05-09:pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Alzheimer's disease is a form of dementia that affects over 44 million people globally. Unfortunately the condition is very hard to detect in its early stages. It is usually diagnosed by a simple questionnaire test, an approach that can only detect Alzheimer's disease many years after its onset. The challenge set in this project was earlier detection using Python and data science.&lt;/p&gt;
&lt;p&gt;AlzHack is a collaborative citizen science project undertaken by a small but diverse group of data scientists. We will discuss the challenges encountered in discovering and acquiring suitable data, describe how we cleaned and merged multiple data sources, and how it was possible to extract meaningful features from within.&lt;/p&gt;
&lt;p&gt;We will cover textual feature extraction, examining; amongst other methods, part-of-speech tagging, readability calculations, locality sensitive hashing as well as sentiment analysis, all in Python 3.&lt;/p&gt;
&lt;p&gt;In addition we will show how a variety of machine learning techniques (including text clustering and classification) were used; with the aim of distinguishing diagnosed Alzheimer's sufferers from their healthy peers solely based on samples of their written correspondence.&lt;/p&gt;
&lt;p&gt;This will be followed by a look at changepoint and ramp detection on noisy time series data; deployed to identify subtle changes in signals obtained from correspondence of individuals over time; thus allowing a form of non-medical, 'early warning' style detection of Alzheimer's disease.&lt;/p&gt;
&lt;p&gt;Finally we will address the tough task of scaling up a small, collaborative data science project to become an extremely powerful, widely available self-diagnosis tool.&lt;/p&gt;
</summary></entry></feed>