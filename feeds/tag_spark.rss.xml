<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Spark</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 17 Apr 2023 00:00:00 +0000</lastBuildDate><item><title>Sparking Pandas: an experiment</title><link>https://pyvideo.org/pycon-italia-2017/sparking-pandas-an-experiment.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a good library to deal with tabular data. What if you need to
manage an amount of data that doesn’t fit into memory? What if you want
to “distribute” your computations among multiple machines?&lt;/p&gt;
&lt;p&gt;Starting from a real scenario, Apache Spark will be presented as the
main tool to read and process collected data. It will be shown how a
Pandas-like syntax will come in handy to run aggregations, filtering and
grouping using a Spark Dataframe.&lt;/p&gt;
&lt;p&gt;A previous knowledge of Docker and Docker Compose will be very useful
while knowing MongoDB (where data will be fetched from) is not
mandatory. Basics of functional programming will help to understand
Spark inner logic.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesco Bruni</dc:creator><pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-07:/pycon-italia-2017/sparking-pandas-an-experiment.html</guid><category>PyCon Italia 2017</category><category>microservices</category><category>Jupyter</category><category>mongodb</category><category>data-visualization</category><category>data-analysis</category><category>spark</category><category>docker</category></item><item><title>Geospatial analysis with Python</title><link>https://pyvideo.org/pycon-italia-2019/geospatial-analysis-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Due to its effectiveness and simplicity, Python is spreading as a choice
for handling geospatial data. From running algorithms capable of
extracting geo- referred insights or processing geo archives, Python
could represent a powerful tool to handle geo-related problems thanks to
an extensive set of libraries. The training will give an overview about
processing geospatial data with Python. An approximate agenda will
include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction&lt;ul&gt;
&lt;li&gt;setting up the the environment&lt;/li&gt;
&lt;li&gt;an introduction to geospatial world&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Working with geospatial data&lt;ul&gt;
&lt;li&gt;playing with geo archives (vector/rasters)&lt;/li&gt;
&lt;li&gt;extracting geo analytics&lt;/li&gt;
&lt;li&gt;Vector tiles big vector data&lt;/li&gt;
&lt;li&gt;Apache Spark for geospatial raster analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Python for Earth observation&lt;ul&gt;
&lt;li&gt;Classifying earth observation images&lt;/li&gt;
&lt;li&gt;Extracting insights from Copernicus products&lt;/li&gt;
&lt;li&gt;Use SNAP from Python&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No tools are required for attending this training. Bring your PC with
Docker installed. Further instructions will be provided.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1674"&gt;https://python.it/feedback-1674&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 10:30 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt; in __on &lt;strong&gt;Saturday 4
May&lt;/strong&gt; at 18:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesco Bruni</dc:creator><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-03:/pycon-italia-2019/geospatial-analysis-with-python.html</guid><category>PyCon Italia 2019</category><category>Jupyter</category><category>pyspark</category><category>geospatial</category><category>geopynotebook</category><category>spark</category><category>docker</category></item><item><title>Building Analytics Workflow using Airflow and Spark</title><link>https://pyvideo.org/pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Yohei had built and operates a data analytics system for global retail logistics operations using Airflow and Spark since the end of last year. In this session, He will talk about how you can build a scalable analytics workflow system based on Airflow (Python) and write extensible job using Python. GCP has provided fully managed Airflow service called Cloud Composer. So he will explain how you can easily build Airflow cluster compared to building your own Airflow cluster on the on-premise server or AWS EC2.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yohei Onishi</dc:creator><pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-02-23:/pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html</guid><category>PyCon Philippines 2019</category><category>airflow</category><category>spark</category><category>analytics</category></item><item><title>The Spark of Big Data - An Introduction to Apache Spark</title><link>https://pyvideo.org/pydata-berlin-2023/the-spark-of-big-data-an-introduction-to-apache-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Get ready to level up your big data processing skills! Join us for an introductory talk on Apache
Spark, the distributed computing system used by tech giants like Netflix and Amazon. We'll
cover PySpark DataFrames and how to use them. Whether you're a Python developer new to
big data or looking to explore new technologies, this talk is for you. You'll gain foundational
knowledge about Apache Spark and its capabilities, and learn how to leverage DataFrames and
SQL APIs to efficiently process large amounts of data. Don't miss out on this opportunity to up
your big data game!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Pasha Finkelshteyn</dc:creator><pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-04-17:/pydata-berlin-2023/the-spark-of-big-data-an-introduction-to-apache-spark.html</guid><category>PyData Berlin 2023</category><category>Spark</category></item><item><title>Use Spark from anywhere - A Spark client in Python powered by Spark Connect</title><link>https://pyvideo.org/pydata-berlin-2023/use-spark-from-anywhere-a-spark-client-in-python-powered-by-spark-connect.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Over the past decade, developers, researchers, and the community have successfully built tens of thousands of data applications using Spark. Since then, use cases and requirements of data applications have evolved: Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data.&lt;/p&gt;
&lt;p&gt;However, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements: there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL.&lt;/p&gt;
&lt;p&gt;Spark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, Notebooks and programming languages.&lt;/p&gt;
&lt;p&gt;This talk highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and give an outlook of how the community can participate in the extension of Spark Connect for new programming languages and frameworks - to bring the power of Spark everywhere.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Grund</dc:creator><pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-04-17:/pydata-berlin-2023/use-spark-from-anywhere-a-spark-client-in-python-powered-by-spark-connect.html</guid><category>PyData Berlin 2023</category><category>Spark</category></item><item><title>Improving PySpark Performance Spark performance beyond the JVM</title><link>https://pyvideo.org/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Holden Karau</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</guid><category>PyData DC 2016</category><category>jvm</category><category>performance</category><category>pyspark</category><category>spark</category></item><item><title>Hassle Free ETL with PySpark</title><link>https://pyvideo.org/pygotham-2016/hassle-free-etl-with-pyspark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While the models of data science get all the press, the real work is in the maze of data preprocessing and pipelines. The goal of this talk is to get a glimpse into how you can use Python and the distributed power of Spark to simplify your (data) life, ditch the ETL boilerplate and get to the insights. We’ll intro PySpark and considerations in ETL jobs with respect to code structure and performance.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rob Howley</dc:creator><pubDate>Sat, 16 Jul 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-07-16:/pygotham-2016/hassle-free-etl-with-pyspark.html</guid><category>PyGotham 2016</category><category>spark</category></item></channel></rss>