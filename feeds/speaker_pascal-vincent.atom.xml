<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Pascal Vincent</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_pascal-vincent.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Stable Policy Optimization via Off-Policy Divergence Regularization</title><link href="https://pyvideo.org/uai-2020/stable-policy-optimization-via-off-policy-divergence-regularization.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Ahmed Touati</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/stable-policy-optimization-via-off-policy-divergence-regularization.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Stable Policy Optimization via Off-Policy Divergence Regularization&lt;/p&gt;
&lt;p&gt;Ahmed Touati (MILA)*; Amy Zhang (McGill, FAIR); Joelle Pineau (McGill / Facebook); Pascal Vincent (Facebook FAIR &amp;amp; MILA Université de Montréal)&lt;/p&gt;
&lt;p&gt;Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) are among the most successful policy gradient approaches in deep reinforcement learning (RL …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Stable Policy Optimization via Off-Policy Divergence Regularization&lt;/p&gt;
&lt;p&gt;Ahmed Touati (MILA)*; Amy Zhang (McGill, FAIR); Joelle Pineau (McGill / Facebook); Pascal Vincent (Facebook FAIR &amp;amp; MILA Université de Montréal)&lt;/p&gt;
&lt;p&gt;Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) are among the most successful policy gradient approaches in deep reinforcement learning (RL). While these methods achieve state-of-the-art performance across a wide range of challenging tasks, there is room for improvement in the stabilization of the policy learning and how the off-policy data are used. In this paper we revisit the theoretical foundations of these algorithms and propose a new algorithm which stabilizes the policy improvement through a proximity term that constrains the discounted state-action visitation distribution induced by consecutive policies to be close to one another. This proximity term, expressed in terms of the divergence between the visitation distributions, is learned in an off-policy and adversarial manner. We empirically show that our proposed method can have a beneficial effect on stability and improve final performance in benchmark high-dimensional control tasks.&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>