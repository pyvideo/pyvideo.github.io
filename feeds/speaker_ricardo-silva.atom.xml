<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Ricardo Silva</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_ricardo-silva.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Learning Joint Nonlinear Effects from Single-variable Interventions in the Presence</title><link href="https://pyvideo.org/uai-2020/learning-joint-nonlinear-effects-from-single-variable-interventions-in-the-presence.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Sorawit Saengkyongam</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/learning-joint-nonlinear-effects-from-single-variable-interventions-in-the-presence.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Learning Joint Nonlinear Effects from Single-variable Interventions in the Presence of Hidden Confounders&lt;/p&gt;
&lt;p&gt;Sorawit  Saengkyongam (University College London)*; Ricardo Silva (University College London)&lt;/p&gt;
&lt;p&gt;We propose an approach to estimate the effect of multiple simultaneous interventions in the presence of hidden confounders. To overcome the problem of hidden confounding, we …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Learning Joint Nonlinear Effects from Single-variable Interventions in the Presence of Hidden Confounders&lt;/p&gt;
&lt;p&gt;Sorawit  Saengkyongam (University College London)*; Ricardo Silva (University College London)&lt;/p&gt;
&lt;p&gt;We propose an approach to estimate the effect of multiple simultaneous interventions in the presence of hidden confounders. To overcome the problem of hidden confounding, we consider the setting where we have access to not only the observational data but also sets of single-variable interventions in which each of the treatment variables is intervened on separately. We prove identifiability under the assumption that the data is generated from a nonlinear continuous structural causal model with additive Gaussian noise. In addition, we propose a simple parameter estimation method by pooling all the data from different regimes and jointly maximizing the combined likelihood. We also conduct comprehensive experiments to verify the identifiability result as well as to compare the performance of our approach against a baseline on both synthetic and real-world data.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry><entry><title>Neural Likelihoods via Cumulative Distribution Functions</title><link href="https://pyvideo.org/uai-2020/neural-likelihoods-via-cumulative-distribution-functions.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Pawel Chilinski</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/neural-likelihoods-via-cumulative-distribution-functions.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Neural Likelihoods via Cumulative Distribution Functions&lt;/p&gt;
&lt;p&gt;Pawel Chilinski (UCL)*; Ricardo Silva (University College London)&lt;/p&gt;
&lt;p&gt;We leverage neural networks as universal approximators of monotonic functions to build a parameterization of conditional cumulative distribution functions (CDFs). By the application of automatic differentiation with respect to response variables and then to parameters …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Neural Likelihoods via Cumulative Distribution Functions&lt;/p&gt;
&lt;p&gt;Pawel Chilinski (UCL)*; Ricardo Silva (University College London)&lt;/p&gt;
&lt;p&gt;We leverage neural networks as universal approximators of monotonic functions to build a parameterization of conditional cumulative distribution functions (CDFs). By the application of automatic differentiation with respect to response variables and then to parameters of this CDF representation, we are able to build black box CDF and density estimators. A suite of families is introduced as alternative constructions for the multivariate case. At one extreme, the simplest construction is a competitive density estimator against state-of-the-art deep learning methods, although it does not provide an easily computable representation of multivariate CDFs. At the other extreme, we have a flexible construction from which multivariate CDF evaluations and marginalizations can be obtained by a simple forward pass in a deep neural net, but where the computation of the likelihood scales exponentially with dimensionality. Alternatives in between the extremes are discussed. We evaluate the different representations empirically on a variety of tasks involving tail area probabilities, tail dependence and (partial) density estimation.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>