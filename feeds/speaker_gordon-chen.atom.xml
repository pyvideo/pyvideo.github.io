<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_gordon-chen.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-11T00:00:00+00:00</updated><entry><title>Safe Handling Instructions for Probabilistic Classification</title><link href="https://pyvideo.org/scipy-2019/safe-handling-instructions-for-probabilistic-classification.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Gordon Chen</name></author><id>tag:pyvideo.org,2019-07-11:scipy-2019/safe-handling-instructions-for-probabilistic-classification.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In machine learning, a common task is to predict whether an unclassified observation belongs to one class or another. However, people are often actually more interested to know the probability of belonging to a class rather than just the most likely class. In such cases, a traditional classification problem becomes a probabilistic classification problem. This distinction is subtle yet crucial. Firstly, the probability-ish outputs of most classifiers are not true probabilities. Moreover, if we use traditional metrics such as the AUC score on probabilistic classification problems, we may end up selecting the wrong models. Fortunately, probability calibration techniques, advanced model stacking/blending methods, and more suitable metrics can fix these issues.&lt;/p&gt;
</summary></entry></feed>