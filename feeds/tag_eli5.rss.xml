<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 23 Feb 2019 00:00:00 +0000</lastBuildDate><item><title>How did you know? Explaining Black Box Model Predictions in Python</title><link>https://pyvideo.org/pycon-philippines-2019/how-did-you-know-explaining-black-box-model-predictions-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As algorithms get more and more complex (i.e. Ensemble models - XGBoost, Random Forest, Neural Networks), it becomes harder to explain the predictions they make. These 'Black Box' models may produce more accurate results but may in fact hard to operationalize in the real world as it gets harder and harder to explain to business decision makers how a model came up with the prediction. In certain cases such as in credit scoring model interpretability is crucial particularly for regulatory compliance. This talk will highlight certain Python tools and libraries such as LIME, ELI5 and Skater, that would allow data scientists to finally be able to explain how their models came up with its predictions.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Suzy Lee</dc:creator><pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-02-23:pycon-philippines-2019/how-did-you-know-explaining-black-box-model-predictions-in-python.html</guid><category>machine learning</category><category>eli5</category><category>lime</category><category>skater</category></item><item><title>“Why Should I Trust You?” - Debugging black-box text classifiers</title><link>https://pyvideo.org/pydata-amsterdam-2018/why-should-i-trust-you-debugging-black-box-text-classifiers.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Classifying text is a common use case for machine learning algorithms. But despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction. We will use eli5 and the LIME algorithm to explain text classifiers.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tobias Sterbak</dc:creator><pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-26:pydata-amsterdam-2018/why-should-i-trust-you-debugging-black-box-text-classifiers.html</guid><category>eli5</category><category>lime</category></item></channel></rss>