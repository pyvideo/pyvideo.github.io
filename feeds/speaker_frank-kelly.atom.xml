<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_frank-kelly.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-09-15T00:00:00+00:00</updated><entry><title>Machine Learning on the Edge</title><link href="https://pyvideo.org/pycon-uk-2019/machine-learning-on-the-edge.html" rel="alternate"></link><published>2019-09-15T00:00:00+00:00</published><updated>2019-09-15T00:00:00+00:00</updated><author><name>Frank Kelly</name></author><id>tag:pyvideo.org,2019-09-15:pycon-uk-2019/machine-learning-on-the-edge.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;An introduction to the Jetson Nano Developer kit by a data scientist&lt;/p&gt;
</summary></entry><entry><title>Hierarchical Data Clustering in Python</title><link href="https://pyvideo.org/pydata-london-2015/hierarchical-data-clustering-in-python.html" rel="alternate"></link><published>2015-06-21T00:00:00+00:00</published><updated>2015-06-21T00:00:00+00:00</updated><author><name>Frank Kelly</name></author><id>tag:pyvideo.org,2015-06-21:pydata-london-2015/hierarchical-data-clustering-in-python.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Clustering of data is an increasingly important task for many data
scientists. This talk will explore the challenge of hierarchical
clustering of text data for summarisation purposes. We'll take a look
at some great solutions now available to Python users including the
relevant Scikit Learn libraries, via Elasticsearch (with the carrot2
plugin), and check out visualisations from both approaches.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Background: methods for clustering text data and the challenge of
data summarisation&lt;/li&gt;
&lt;li&gt;Hierarchical clustering: agglomerative vs divisive&lt;/li&gt;
&lt;li&gt;sklearn.cluster and metrics modules&lt;/li&gt;
&lt;li&gt;Elasticsearch + carrot2 plugin&lt;/li&gt;
&lt;li&gt;Performance comparisons, assessment of ease of scalability and use&lt;/li&gt;
&lt;li&gt;Static visualisation using Matplotlib, interactive using Foamtree&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Is having dementia linked to where you live?</title><link href="https://pyvideo.org/pydata-london-2017/is-having-dementia-linked-to-where-you-live.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Frank Kelly</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/is-having-dementia-linked-to-where-you-live.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
In this talk we will explore the results from a short, collaborative ‘hack’ project with a focus on using Python machine learning tools and open data to find evidence linking increased diesel engine and tyre particulate emissions (PM10, PM2.5 and NO2) in certain residential areas with greater likelihood of diagnosis of dementia amongst the residents of that area.&lt;/p&gt;
&lt;p&gt;Abstract
Overview
Alzheimer’s is a neurodegenerative disease that currently affects over 55 million people worldwide and this is set to increase with trends in population growth and demographics. Meanwhile, particulate emission levels in large cities across Europe and the world have been under scrutiny, as diesel emissions related to a huge uptick in diesel vehicle purchases (due to governments' push to reduce CO2 emission levels) are found to have caused many medical problems in the area of breathing difficulties.&lt;/p&gt;
&lt;p&gt;Breathing function and the lungs may not be the only parts affected; there is now more than one scientific study (see link [1] for an example below) that has found a link between higher incidence of dementia (in particular Alzheimer’s) in those who have lived in highly polluted urban areas for large periods of their lives.&lt;/p&gt;
&lt;p&gt;This talk is about a short, collaborative ‘hack’ project with a focus on using Python machine learning tools and open data to test this hypothesis and look for evidence either confirming or denying the link between increased diesel engine and tyre particulate emissions (PM10, PM2.5 and NO2) in a given residential area with a greater likelihood of diagnosis of dementia amongst the residents of that area.&lt;/p&gt;
&lt;p&gt;The idea behind this endeavour is that this ‘pilot’ study might enable a funded venture to take root that, for example, increases awareness of the true impact of high diesel car pollution in dense conurbations.&lt;/p&gt;
&lt;p&gt;Questions asked
Can we visualize the likelihood of incidence of dementia per person on a heat map for a city or for the UK? And similarly visualize annual particulate exposure per resident? Do they look similar?
Do various sources of data, when placed under the magnifying glass of data science corroborate the evidence gathered by recent studies?
Can a classifier reasonably determine the likelihood of your contracting dementia based on where you have lived mostly during your life?
Where should you live and work that gives you a lower chance of contracting Alzheimer’s later in life?
How can the findings be used to push city officials to improve city air quality? How can we raise awareness?
Challenges encountered on the way
How to interpolate, in order to cover 'gaps' in-between emission monitoring stations, incorporating additional data sources (for example road network structure and traffic intensity levels)?
Sensitive medical data; how to merge open medical datasets securely without revealing personal information?
Which types of machine learning models are best suited to this problem?
Approach
Starting at a high level, it is possible to determine if there is a higher incidence of dementia cases linked with an increase in airborne pollutants (diesel source):&lt;/p&gt;
&lt;p&gt;At a country level (e.g. for the Netherlands or the UK)
At a city level (e.g. for Bristol, Eindhoven and / or London)
At a borough level (e.g. for Camden, London)
At a street level (by postcode)
At individual sufferer level (with de-personalized datasets, based on 1,000 blog posters)
Other research inputs (articles and papers)
[1] &lt;a class="reference external" href="http://www.sciencemag.org/news/2017/01/brain-pollution-evidence-builds-dirty-air-causes-alzheimer-s-dementia"&gt;http://www.sciencemag.org/news/2017/01/brain-pollution-evidence-builds-dirty-air-causes-alzheimer-s-dementia&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>AlzHack Data Driven Diagnosis of Alzheimer's Disease 1</title><link href="https://pyvideo.org/pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease-1.html" rel="alternate"></link><published>2016-05-11T00:00:00+00:00</published><updated>2016-05-11T00:00:00+00:00</updated><author><name>Frank Kelly</name></author><id>tag:pyvideo.org,2016-05-11:pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease-1.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Alzheimer's disease is a form of dementia that affects over 44 million people globally. Unfortunately the condition is very hard to detect in its early stages. It is usually diagnosed by a simple questionnaire test, an approach that can only detect Alzheimer's disease many years after its onset. The challenge set in this project was earlier detection using Python and data science.&lt;/p&gt;
&lt;p&gt;AlzHack is a collaborative citizen science project undertaken by a small but diverse group of data scientists. We will discuss the challenges encountered in discovering and acquiring suitable data, describe how we cleaned and merged multiple data sources, and how it was possible to extract meaningful features from within.&lt;/p&gt;
&lt;p&gt;We will cover textual feature extraction, examining; amongst other methods, part-of-speech tagging, readability calculations, locality sensitive hashing as well as sentiment analysis, all in Python 3.&lt;/p&gt;
&lt;p&gt;In addition we will show how a variety of machine learning techniques (including text clustering and classification) were used; with the aim of distinguishing diagnosed Alzheimer's sufferers from their healthy peers solely based on samples of their written correspondence.&lt;/p&gt;
&lt;p&gt;This will be followed by a look at changepoint and ramp detection on noisy time series data; deployed to identify subtle changes in signals obtained from correspondence of individuals over time; thus allowing a form of non-medical, 'early warning' style detection of Alzheimer's disease.&lt;/p&gt;
&lt;p&gt;Finally we will address the tough task of scaling up a small, collaborative data science project to become an extremely powerful, widely available self-diagnosis tool.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/FrankKelly3/alz-hack-ii"&gt;http://www.slideshare.net/FrankKelly3/alz-hack-ii&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>AlzHack Data Driven Diagnosis of Alzheimer's Disease</title><link href="https://pyvideo.org/pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease.html" rel="alternate"></link><published>2016-05-09T00:00:00+00:00</published><updated>2016-05-09T00:00:00+00:00</updated><author><name>Frank Kelly</name></author><id>tag:pyvideo.org,2016-05-09:pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Alzheimer's disease is a form of dementia that affects over 44 million people globally. Unfortunately the condition is very hard to detect in its early stages. It is usually diagnosed by a simple questionnaire test, an approach that can only detect Alzheimer's disease many years after its onset. The challenge set in this project was earlier detection using Python and data science.&lt;/p&gt;
&lt;p&gt;AlzHack is a collaborative citizen science project undertaken by a small but diverse group of data scientists. We will discuss the challenges encountered in discovering and acquiring suitable data, describe how we cleaned and merged multiple data sources, and how it was possible to extract meaningful features from within.&lt;/p&gt;
&lt;p&gt;We will cover textual feature extraction, examining; amongst other methods, part-of-speech tagging, readability calculations, locality sensitive hashing as well as sentiment analysis, all in Python 3.&lt;/p&gt;
&lt;p&gt;In addition we will show how a variety of machine learning techniques (including text clustering and classification) were used; with the aim of distinguishing diagnosed Alzheimer's sufferers from their healthy peers solely based on samples of their written correspondence.&lt;/p&gt;
&lt;p&gt;This will be followed by a look at changepoint and ramp detection on noisy time series data; deployed to identify subtle changes in signals obtained from correspondence of individuals over time; thus allowing a form of non-medical, 'early warning' style detection of Alzheimer's disease.&lt;/p&gt;
&lt;p&gt;Finally we will address the tough task of scaling up a small, collaborative data science project to become an extremely powerful, widely available self-diagnosis tool.&lt;/p&gt;
</summary></entry></feed>