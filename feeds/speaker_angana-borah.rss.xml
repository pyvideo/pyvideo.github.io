<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Angana Borah</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 22 Apr 2023 00:00:00 +0000</lastBuildDate><item><title>Approaches to Fairness and Bias Mitigation in Natural Language Processing</title><link>https://pyvideo.org/pycon-us-2023/approaches-to-fairness-and-bias-mitigation-in-natural-language-processing.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With the advent of large pre-trained language models like GPT, BERT,
etc., and their usage in almost all natural language understanding and
generation applications, it is important that we evaluate the fairness
and mitigate biases of these models. Since these models are fed with
human-generated data (mostly from the web), they are exposed to human
biases. Hence, they carry forward and also amplify these biases in their
results. In this talk, we will discuss the motivation for fairness and
bias research in NLP and discuss different approaches used to detect and
mitigate biases. We will also explore some available tools to include in
your models to ensure fairness.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Angana Borah</dc:creator><pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-04-22:/pycon-us-2023/approaches-to-fairness-and-bias-mitigation-in-natural-language-processing.html</guid><category>PyCon US 2023</category></item></channel></rss>