<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Alexander Cloninger</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_alexander-cloninger.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Coresets for Estimating Means and Mean Square Error with Limited Greedy Samples</title><link href="https://pyvideo.org/uai-2020/coresets-for-estimating-means-and-mean-square-error-with-limited-greedy-samples.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Saeed Vahidian</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/coresets-for-estimating-means-and-mean-square-error-with-limited-greedy-samples.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Coresets for Estimating Means and Mean Square Error with Limited Greedy Samples&lt;/p&gt;
&lt;p&gt;Saeed Vahidian (University of California San Diego); Baharan Mirzasoleiman (Stanford University); Alexander Cloninger (University of California San Diego)*&lt;/p&gt;
&lt;blockquote&gt;
In a number of situations, collecting a function value for every data point may be prohibitively expensive, and random …&lt;/blockquote&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Coresets for Estimating Means and Mean Square Error with Limited Greedy Samples&lt;/p&gt;
&lt;p&gt;Saeed Vahidian (University of California San Diego); Baharan Mirzasoleiman (Stanford University); Alexander Cloninger (University of California San Diego)*&lt;/p&gt;
&lt;blockquote&gt;
In a number of situations, collecting a function value for every data point may be prohibitively expensive, and random sampling ignores any structure in the underlying data. We introduce a scalable optimization algorithm with no correction steps (in contrast to Frank–Wolfe and its variants), a variant of gradient ascent for coreset selection in graphs, that greedily selects a weighted subset of vertices that are deemed most important to sample. Our algorithm estimates the mean of the function by taking a weighted sum only at these vertices, and we provably bound the estimation error in terms of the location and weights of the selected vertices in the graph. In addition, we consider the case where nodes have different selection costs and provide bounds on the quality of the low-cost selected coresets. We demonstrate the benefits of our algorithm on the semi-supervised node classification of graph convolutional neural network, point clouds and structured graphs, as well as sensor placement where the cost of placing sensors depends on the location of the placement. We also elucidate that the empirical convergence of our proposed method is faster than random selection and various clustering methods while still respecting sensor placement cost. The paper concludes with validation of the developed algorithm on both synthetic and real datasets, demonstrating that it outperforms the current state of the art.&amp;quot;&lt;/blockquote&gt;
</content><category term="UAI 2020"></category></entry></feed>