<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 14 Jul 2014 00:00:00 +0000</lastBuildDate><item><title>Astropy in 2014: What's new, where we are headed</title><link>https://pyvideo.org/scipy-2014/astropy-in-2014-whats-new-where-we-are-headed.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;We report on the progress made on the Astropy Project in the past year
highlighting the new capabilities added as well as the near-term
development plans.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Astropy continues to see significant growth in available software,
developers, and users. A new major release (V0.3) was made in the past
year as well as many minor releases. V0.4 is scheduled for release by
the time of conference and will include support for the VO SAMP
protocol. We will report on the progress made in building and enhancing
the core libraries in the Astropy Project, including a new model and
fitting framework, enhanced units, quantities, and table functionality,
a VO cone search tool, and a new convolution subpackage. We'll review
the current set of tools available, highlighting in particular the new
capabilities present. We will also give an overview of current
activities and development plans for the core and affiliated packages,
as well as adding new resources/tutorials for learning how to use
astropy.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Perry Greenfield</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/astropy-in-2014-whats-new-where-we-are-headed.html</guid><category>astropy</category></item><item><title>Cartopy</title><link>https://pyvideo.org/scipy-2014/cartopy.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Cartopy is a Python package which builds on Proj.4 to define coordinate
reference systems for the transformation and visualisation of geospatial
data. It has a simple matplotlib interface for publication quality
visualisation. This talk will outline some of cartopy's functionality
and demonstrate some practical applications within the realm of
scientific presentation of geospatial data.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The practice of representing geospatial data upon a flat surface is
known as cartography, and the topological implications of projecting
fundamentally 3D data onto a 2 dimensional surface has been the
challenge of map-makers since time immemorial. Geospatial visualisation
software is often implemented without consideration for the 3rd
dimension and this commonly results in problems around the dateline or
at the poles. For small areas these problems are often not apparent and
mostly surmountable, but at a global scale, such as when visualising
output from GCMs (General circulation models), the underlying
representation must be addressed head-on in order to visualise the data
&amp;quot;impact free&amp;quot;.&lt;/p&gt;
&lt;p&gt;Cartopy is a Python package which builds on top of Proj.4 to define
coordinate reference systems for the transformation and visualisation of
geospatial data. As well as the fundamental transformations there is
also a matplotlib interface allowing easy generation of maps with the
same publication quiality expected of matplotlib. Cartopy employs
several techniques to handle geospatial data correctly, including true
spherical interpolation for raster data, and Shapely geometry
interpolate-and-cut transformations for geospatial vector data.&lt;/p&gt;
&lt;p&gt;This talk will outline some of the capabilities of cartopy, and continue
onto its practical application within the realm of scientific
presentation of geospatial data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Richard Hattersley</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/cartopy.html</guid></item><item><title>Deploying Python Tools to GIS Users</title><link>https://pyvideo.org/scipy-2014/deploying-python-tools-to-gis-users.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The geospatial community has coalesced around Python, both in the
commercial and open source spaces. In this talk, I'll show how Python
tools can be shared with users of ArcGIS, a commercial GIS system which
uses Python as its primary development environment. By constructing
small Python wrappers, code can be shared in graphical tools which
enable non-programmers to use what you've built.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Geospatial data is frequently manipulated directly using Python tools,
commonly built on top of powerful libraries such as GDAL, GEOS and
NetCDF. Delivering model results to end users in many instances requires
providing tools in familiar graphical environments, such as desktop GIS
systems, which can permit users without programming knowledge to
integrate models and results into their existing scientific workflows.
This talk discusses how to construct simple wrappers around existing
Python programs to enable their use by ArcGIS, a commonly used
commercial GIS.&lt;/p&gt;
&lt;p&gt;Two separate approaches will be illustrated: creating Python toolboxes,
or collections of tools embeddable in workflows, and creating customized
Python graphical add-ins, which can control the graphical environment
provided within ArcGIS. Building contextual help, interactive widgets,
and leveraging &lt;tt class="docutils literal"&gt;numpy&lt;/tt&gt; for direct data integration will be discussed.
While ArcGIS exposes much of its functionality via the &lt;tt class="docutils literal"&gt;ArcPy&lt;/tt&gt;
package, this talk instead focuses on integrating code from other
environments, and doesn't presume existing ArcGIS expertise.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shaun Walbridge</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/deploying-python-tools-to-gis-users.html</guid><category>gis</category></item><item><title>Enhancements to Ginga: an Astronomical Image Viewer and Toolkit</title><link>https://pyvideo.org/scipy-2014/enhancements-to-ginga-an-astronomical-image-view.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;We describe recent developments in the Ginga package, a open-source
astronomical image viewer and toolkit written in python and hosted on
Github. The package was introduced to the scientific python community at
SciPy 2013 and has received a number of enhancements since then based on
user feedback. The talk includes an image mosaicing demo of a wide-field
camera exposure with 116 4Kx2K CCDs.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Ginga is an open-source astronomical image viewer and toolkit written in
python and &lt;a class="reference external" href="https://github.com/ejeschke/ginga"&gt;hosted on Github&lt;/a&gt;. It
uses and inter-operates with several key scientific python packages:
numpy, scipy, astropy and matplotlib.&lt;/p&gt;
&lt;p&gt;In this talk/poster we describe and illustrate recent enhancements to
the package since the introductory talk at SciPy 2013, including:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;modular/pluggable interfaces for world coordinate systems, image file
I/O and star and image catalogs&lt;/li&gt;
&lt;li&gt;support for rendering into matplotlib figures&lt;/li&gt;
&lt;li&gt;support for image mosaicing&lt;/li&gt;
&lt;li&gt;support for image overlays&lt;/li&gt;
&lt;li&gt;customizable user-interface bindings&lt;/li&gt;
&lt;li&gt;improved documentation&lt;/li&gt;
&lt;li&gt;self contained Mac OS X packages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During the talk we will demonstrate the mosaicing plugin that is being
used with several instruments at Subaru Telescope in Hawaii, including
the new Hyper Suprime-Cam wide-field camera with 116 separate 4Kx2K
CCDs.&lt;/p&gt;
&lt;p&gt;The talk/poster may be of interest to anyone developing code in python
needing to display scientific image (CCD or CMOS) data and astronomers
interested in python-based quick look and analysis tools.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eric Jeschke</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/enhancements-to-ginga-an-astronomical-image-view.html</guid><category>astronomy</category></item><item><title>Epipy: Visualization of Emerging Zoonoses Through Temporal Networks</title><link>https://pyvideo.org/scipy-2014/epipy-visualization-of-emerging-zoonoses-through.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;We introduce two new plots for visualizing infectious disease outbreaks.
Case tree plots depict the emergence and growth of clusters of zoonotic
disease over time. Checkerboard plots also represent temporal case
clusters, but do not construct transmission trees. These plots visualize
outbreak dynamics and allow for analyses like case fatality risk
stratified by generation.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We present two new visualizations, &lt;a class="reference external" href="https://github.com/cmrivers/epipy/blob/master/figs/example_casetree.png"&gt;case tree
plots&lt;/a&gt;
and
&lt;a class="reference external" href="https://github.com/cmrivers/epipy/blob/master/figs/test_checkerboard.png"&gt;checkerboard&lt;/a&gt;
plots, for visualizing emerging zoonoses.&lt;/p&gt;
&lt;p&gt;Zoonoses represent an estimated 58% of all human infectious diseases,
and 73% of emerging infectious diseases. Recent examples of zoonotic
outbreaks include H1N1, SARS and Middle East Respiratory Syndrome, which
have caused thousands of deaths combined. The current toolkit for
visualizing data from these emerging diseases is limited.&lt;/p&gt;
&lt;p&gt;Case tree and checkerboard plots were developed to address that gap. The
visualizations are best suited for diseases like SARS for which there
are a limited number of cases, with data available on human to human
transmission. They a) allow for easy estimation of epidemiological
parameters like basic reproduction number b) indicate the frequency of
introductory events, e.g. spillovers in the case of zoonoses c)
represent patterns of case attributes like patient sex both by
generation and over time.&lt;/p&gt;
&lt;p&gt;Case tree plots depict the emergence and growth of clusters of disease
over time. Each case is represented by a colored node. Nodes that share
an epidemiological link are connected by an edge. The color of the node
varies based on the node attribute; it could represent patient sex,
health status (e.g. alive, dead), or any other categorical attribute.
Node placement along the x-axis corresponds with the date of illness
onset for the case.&lt;/p&gt;
&lt;p&gt;A second visualization, the checkerboard plot, was developed to
complement case tree plots. They can be used in conjunction with case
tree plots, or in situations where representing a hypothetical network
structure is inappropriate.&lt;/p&gt;
&lt;p&gt;The plots are available in the open source package epipy, which is
available on &lt;a class="reference external" href="https://github.com/cmrivers/epipy"&gt;github&lt;/a&gt;. Detailed
documentation and examples are also
&lt;a class="reference external" href="cmrivers.github.io/epipy"&gt;available&lt;/a&gt;. In addition to these
visualizations, epipy includes functions for common epidemiology
calculations like odds ratio and relative risk.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Caitlin Rivers</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/epipy-visualization-of-emerging-zoonoses-through.html</guid><category>epipy</category><category>vis</category></item><item><title>HoloPy: Holograpy and Light Scattering in Python</title><link>https://pyvideo.org/scipy-2014/holopy-holograpy-and-light-scattering-in-python.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Digital holography microscopy is a powerful tool for fast 3D imaging of
soft matter systems. However, making measurements from holograms
requires special computation. HoloPy is a set of tools for
reconstructing and fitting to holograms. It also includes tools for
computing light scattering, setting up inverse problems, and working
with images and metadata.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Digital holographic microscopy is fast and powerful tool for 3D imaging.
Holography captures information about a 3D scene onto a 2D camera using
interference. This means that the speed of holographic imaging is
limited only by camera speed, making holography an ideal tool for
studying fast processes in soft matter systems. However, making use of
this encoded information requires significant computational post
processing. We have developed and released
&lt;a class="reference external" href="http://manoharan.seas.harvard.edu/holopy/"&gt;HoloPy&lt;/a&gt;, a python based
tool for doing these calculations.&lt;/p&gt;
&lt;p&gt;The traditional method for extracting information from holograms is to
optically reconstruct by shining light through a hologram to obtain an
image of the recorded scene. HoloPy implements the digital equivalent of
this, numerical reconstruction, in the form of light propagation by
convolution. This is a fast technique based on fast Fourier transforms,
which effectively allows refocusing a holographic image after it is
taken.&lt;/p&gt;
&lt;p&gt;For systems where a detailed scattering model is available, Lee and
coworkers showed that it is possible to make more precise measurements
by fitting a scattering model to a recorded hologram
[&lt;a class="reference external" href="http://physics.nyu.edu/grierlab/index12c/"&gt;1&lt;/a&gt;]. We have extended
this technique to clusters of spheres
[&lt;a class="reference external" href="http://arxiv.org/pdf/1202.1600"&gt;2&lt;/a&gt;][&lt;a class="reference external" href="http://people.seas.harvard.edu/~vnm/pdf/Perry-Faraday_Discussions-2012.pdf"&gt;3&lt;/a&gt;]
and to non-spherical particles
[&lt;a class="reference external" href="http://arxiv.org/pdf/1310.4517"&gt;4&lt;/a&gt;]. HoloPy implements all of
these fitting techniques such that they can be used with a few lines of
python code. HoloPy also exposes an interface to all of its scattering
models compute light scattering of microscopic particles or clusters of
particles for other purposes.&lt;/p&gt;
&lt;p&gt;HoloPy is open source (GPLv3) and is hosted on
&lt;a class="reference external" href="https://launchpad.net/holopy"&gt;launchpad&lt;/a&gt;. HoloPy uses Numpy for most
of its manipulations, though it calls out to Fortran and
&lt;a class="reference external" href="http://code.google.com/p/a-dda"&gt;C&lt;/a&gt; codes to compute light
scattering. HoloPy also includes matplotlib and mayavi based tools for
visualizing holograms and particles.&lt;/p&gt;
&lt;p&gt;[1] Lee et.al., Optics Express, Vol. 15, Issue 26, pp. 18275-18282
(2007)&lt;/p&gt;
&lt;p&gt;[2] Fung et. al., JQSRT, Vol 113, Issue 18, pp. 2482-2489 (2012)&lt;/p&gt;
&lt;p&gt;[3] Perry et. al., Faraday Discussions, Vol 159, pp. 211-234 (2012)&lt;/p&gt;
&lt;p&gt;[4] Wang et. al. JQSRT, (2014)&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tom Dimiduk</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/holopy-holograpy-and-light-scattering-in-python.html</guid><category>holopy</category></item><item><title>How Interactive Visualization Led to Insights in Digital Holographic Microscopy</title><link>https://pyvideo.org/scipy-2014/how-interactive-visualization-led-to-insights-in.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Digital holographic microscopy is a fast 3D imaging technique. A camera
records a time series of light scattering patterns as standard 2D images
and then post-processing routines extract 3D information. By creating a
GPU-accelerated GUI on top of the Holopy package, we noticed unexpected
discrepancies between the different models used during post-processing.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Digital holographic microscopy is a fast 3D imaging technique ideally
suited to studies of micron-sized objects that diffuse through random
walks via Brownian motion
&lt;a class="reference external" href="http://dx.doi.org/10.1364/OE.15.018275"&gt;[1]&lt;/a&gt;. Microspheres fit this
category and are widely used in biological assays and as ideal test
subjects for experiments in statistical mechanics. Microspheres
suspended in water move too quickly to monitor with confocal microscopy.
With digital holographic microscopy, 2D images encoding 3D volumes can
be recorded at thousands of frames per second
&lt;a class="reference external" href="http://www.nature.com/nmat/journal/v11/n2/abs/nmat3190.html"&gt;[2]&lt;/a&gt;.
The computationally challenging part of digital holographic microscopy
is extracting the 3D information during post-processing.&lt;/p&gt;
&lt;p&gt;The open source &lt;a class="reference external" href="https://launchpad.net/holopy"&gt;Holopy&lt;/a&gt; package which
relies heavily on SciPy and NumPy is used to recover the 3D information
via one of two techniques: reconstruction by numerical back-propagation
of electromagnetic fields or modeling forward light scattering with Mie
theory. The parameter space describing the imaged volume is
multidimensional. Even for simple micron-sized spheres, a hologram
depends on each sphere's radius and index of refraction in addition to
its 3D position. By supplementing Holopy with a &lt;a class="reference external" href="https://github.com/RebeccaWPerry/holography-gpu"&gt;GPU-accelerated
GUI&lt;/a&gt; using PyQt4, we
enabled users to interactively adjust the system parameters and see a
modeled digital hologram change in response.&lt;/p&gt;
&lt;p&gt;Simply adding the capability of interactively manipulating holograms in
a GUI led us to notice unexpected discrepancies between the two modeling
techniques and failures of both, suggesting further experiments. We
observed that the numerical light propagation technique only accurately
characterizes the light within a cone stretching from the extent of the
image back towards the object. Neither model accurately characterizes
the light upstream of the object toward the light source. The GUI was a
natural format to interact with the theory and gain insight because it
showed us the models in an analogous format to how we see the data on
the microscope. Other scientific projects may benefit from tools that
allow experimentalists to interact with theory in the same way they
interact with their experiments.&lt;/p&gt;
&lt;p&gt;[1] Lee et.al., Optics Express, Vol. 15, Issue 26, pp. 18275-18282
(2007) doi: 10.1364/OE.15.018275.&lt;/p&gt;
&lt;p&gt;[2] Kaz et.al., Nature Materials, Vol. 11, pp. 138013142 (2012)
doi:10.1038/nmat3190.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rebecca Perry</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/how-interactive-visualization-led-to-insights-in.html</guid><category>holopy</category></item><item><title>How to Choose a Good Colour Map</title><link>https://pyvideo.org/scipy-2014/how-to-choose-a-good-colour-map.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Representing data through colours is a very common approach to conveying
important information to an audience. We suggest some best practices
scientists should consider when deciding how they should present their
results.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Representing data through colours is a very common approach to conveying
important information to an audience. This is done throughout all fields
in the scientific community and stakes a claim in the commercial and
marketing realm as well. Colour maps and contour maps are the preferred
way for scientists to visualise three-dimensional data in two
dimensions. Research has shown that the choice of colourmap is crucial
since the human brain interpolates hue poorly. We suggest some best
practices scientists should consider when deciding how they should
present their results. Specifically, we look at some examples of
colourmaps that can easily be misinterpreted, making reference to an
in-depth supportive study, and suggest alternative approaches to improve
them. We conclude by listing some open source tools that aid making good
colourmap choices. Kristen Thyng's talk on perception of colourmaps in
matplotlib is an excellent follow-on from this.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Damon McDougall</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/how-to-choose-a-good-colour-map.html</guid><category>vis</category></item><item><title>Light-weight real-time event detection with Python</title><link>https://pyvideo.org/scipy-2014/light-weight-real-time-event-detection-with-pytho.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Real-time feeds of user activity from various apps such as Twitter,
Foursquare, and others are becoming increasingly available. These
'digital footprints' provide new means to understand how individuals
utilize the places and spaces of urban environments. We present a
light-weight framework for real-time event detection in a city based on
existing SciPy libraries and real-time Twitter streams.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this paper, we utilize real-time 'social information sources' to
automatically detect important events at the urban scale. The goal is to
provide city planners and others with information on &lt;em&gt;what&lt;/em&gt; is going on,
and &lt;em&gt;when&lt;/em&gt; and &lt;em&gt;where&lt;/em&gt; it is happening. Traditionally, this type of
analysis would require a large investment in heavy-duty computing
infrastructure, however, we suggest that a focus on real-time analytics
in a lightweight streaming framework is the most logical step forward.&lt;/p&gt;
&lt;p&gt;Using online Latent Semantic Analysis (LSA) from the
&lt;tt class="docutils literal"&gt;`gensim&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="https://github.com/piskvorky/gensim/"&gt;https://github.com/piskvorky/gensim/&lt;/a&gt;&amp;gt;`__ Python package, we
extract 'topics' from tweets in an online training fashion. To maintain
real-time relevance, the topic model is continually updated, and
depending on parameterization, can 'forget' past topics. Based on a set
of learned topics, a grid of spatially located tweets for each
identified topic is generated using standard &lt;tt class="docutils literal"&gt;numpy&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;scipy.spatial&lt;/tt&gt; functionality. Using an efficient streaming algorithm
for approximating 2D kernel density estimation (KDE), locations with the
highest density of tweets on a particular topic are located. Locations
are semantically labeled using the learned topics, based on the
assumption that events can be directly tied to a particularly popular
topic at a particular location.&lt;/p&gt;
&lt;p&gt;To facilitate real time visualization of results, we utilize the
&lt;tt class="docutils literal"&gt;`pico&lt;/tt&gt; &amp;lt;&lt;a class="reference external" href="https://github.com/fergalwalsh/pico"&gt;https://github.com/fergalwalsh/pico&lt;/a&gt;&amp;gt;`__ Python/Javascript
library as a real-time bridge between server-side Python analysis and
client-side Javascript visualization. This enables fast, responsive
interactivity of computationally intensive tasks. Additionally, since
&lt;tt class="docutils literal"&gt;pico&lt;/tt&gt; allows streaming data from Python to Javascript, updates to the
web-interface are sent and consumed as needed, such that only
significant changes in an event's status, or the introduction of a new
event, will cause updates to the visualizations. Finally, because all
models, data structures, and outputs on the server side are pickle-able
Python objects, this entire framework is small enough to be deployed on
almost any server with Python installed.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carson Farmer</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/light-weight-real-time-event-detection-with-pytho.html</guid></item><item><title>Multi Purpose Particle Tracking</title><link>https://pyvideo.org/scipy-2014/multi-purpose-particle-tracking.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;In many scientific contexts it is necessary to identify and track
features in video. Several labs with separate projects and priorities
collaborated to develop a common, novice-accessible package of standard
algorithms. The package manages optional high-performance components,
such as numba, and interactive tools to tackle challenging data, while
prioritizing testing and easy adoption by novices.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Tracking the motion of many particles is an established technique
[&lt;a class="reference external" href="http://dx.doi.org/10.1006/jcis.1996.0217"&gt;Crocker, J.C., Grier,
D.G.&lt;/a&gt;], but many
physicists, biologists, and chemical engineers still (make their
undergraduates) do it by hand.
&lt;a class="reference external" href="https://github.com/soft-matter/trackpy"&gt;Trackpy&lt;/a&gt;, is a flexible,
high-performance implementation of these algorithms in Python using the
scientific stack -- including pandas, numba, the IPython notebook, and
mpld3 -- which scales well to track, filter, and analyze tens of
thousands of feature trajectories. It was developed collaboratively by
research groups at U. Chicago, U. Penn, Johns Hopkins, and others.&lt;/p&gt;
&lt;p&gt;Researchers with very different requirements for performance and
precision collaborate on the same package. Some original &amp;quot;magic&amp;quot; manages
high-performance components, including numba, using them if they are
available and beneficial; however, the package is still fully functional
without these features. Accessibility to new programmers is a high
priority.&lt;/p&gt;
&lt;p&gt;Biological data and video with significant background variation can
confound standard feature identification algorithms, and manual curation
is unavoidable. Here, the high-performance group operations in pandas
and the cutting-edge notebook ecosystem, in particular the interactive
IPython tools and mpld3, enable detailed examination and discrimination.&lt;/p&gt;
&lt;p&gt;The infrastructure developed for this project can be applied to other
work. Large video data sets can be processed frame by frame, out of
core. Image sequences and video are managed through an abstract class
that treats all formats alike through a handy, idiomatic interface in a
companion project dubbed &lt;a class="reference external" href="https://github.com/soft-matter/pims"&gt;PIMS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A suite of over 150 unit tests with automated continuous integration
testing has ensured stability and accuracy during the collaborative
process. In our experience, this is an unusual but worthwhile level of
testing for a niche codebase from an academic lab.&lt;/p&gt;
&lt;p&gt;In general, we have lessons to share from developing shared tools for
researchers with separate priorities and varied levels of programming
skill and interest.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel B. Allan</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/multi-purpose-particle-tracking.html</guid></item><item><title>Perceptions of Matplotlib Colormaps</title><link>https://pyvideo.org/scipy-2014/perceptions-of-matplotlib-colormaps.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;On several issues related to the perception of colormaps...&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The choice of colormap in a scientific figure significantly affects the
way the presented information is perceived by the viewer. This follows
on Damon McDougall's talk on how to choose a colormap for an application
by delving deeper into several important issues and how well many of the
available Matplotlib colormaps stand up against the concerns. For
example, it is known that the human brain is better able to interpret
changes in magnitude of the luminance and saturation of colors in
colormaps instead of the hue. Also, some research has shown that
logarithmic changes in brightness are perceived as linear changes. Next,
being able to print a color plot in black and white from a published
paper is sometimes mandatory and often desirable, and is related to the
grey scale in a colormap. Finally, it is important to account for
various types of color blindness when choosing a divergent colormap for
the plot to be as accessible as possible. All of these concerns have
implications for the design of colormaps, and will be examined in the
context of the properties of the available Matplotlib colormaps in order
to make a best choice for a given application.&lt;/p&gt;
&lt;p&gt;Abstract and slides available &lt;a class="reference external" href="https://github.com/dmcdougall/scipy14-colormaps"&gt;on
Github.&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristen M. Thyng</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/perceptions-of-matplotlib-colormaps.html</guid><category>vis</category></item><item><title>Rasterio: Geospatial Raster Data Access for Programmers and Future Programmers</title><link>https://pyvideo.org/scipy-2014/rasterio-geospatial-raster-data-access-for-progr.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Learn to read, manipulate, and write georeferenced imagery and other
kinds of geospatial raster data using a productive and fun GDAL and
Numpy-based library named Rasterio. It's a new open source project from
the satellite team at Mapbox and is informed by a decade of experience
using Python and GDAL.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Rasterio is a GDAL and Numpy-based Python library guided by lessons
learned over a decade of using GDAL and Python to solve geospatial
problems. Among these lessons: the importance of productivity,
enjoyability, and serendipity.&lt;/p&gt;
&lt;p&gt;I will discuss the motivation for writing Rasterio and explain how and
why it diverges from other GIS software and embraces Python types,
protocols, and idioms. I will also explain why Rasterio adheres to some
GIS paradigms and bends or breaks others.&lt;/p&gt;
&lt;p&gt;Finally, I will show examples of using Rasterio to read, manipulate, and
write georeferenced raster data. Some examples will be familiar to users
of older Python GIS software and will illustrate how Rasterio lets you
get more done with less code and fewer bugs. I will also demonstrate fun
and useful features of Rasterio not found in other geospatial libraries.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sean Gillies</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/rasterio-geospatial-raster-data-access-for-progr.html</guid><category>gis</category></item><item><title>Real time Crunching of Petabytes of Geospatial Data with Google Earth Engine</title><link>https://pyvideo.org/scipy-2014/real-time-crunching-of-petabytes-of-geospatial-da.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Google Earth Engine is a platform designed to enable petabyte-scale
scientific analysis and visualization of geospatial datasets. Earth
Engine provides a consolidated environment including a massive data
catalog co-located with thousands of computers for analysis. This talk
will discuss products that Earth Engine has produced, and how to access
Earth Engine via its Python API.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="section" id="background"&gt;
&lt;h4&gt;Background&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What is &lt;a class="reference external" href="https://earthengine.google.org"&gt;Earth Engine&lt;/a&gt; at a high
level?&lt;/li&gt;
&lt;li&gt;Why did the Earth Engine (EE) project start? &lt;a class="reference external" href="http://blog.google.org/2010/12/introducing-google-earth-engine.html"&gt;To monitor global
deforestation.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;What architecture design decisions were made, and why?&lt;ul&gt;
&lt;li&gt;Just-in-time computation model&lt;/li&gt;
&lt;li&gt;Lazy evaluation for real-time feedback&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="the-earth-engine-python-api"&gt;
&lt;h4&gt;The Earth Engine Python API&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;PyPI package:
&lt;a class="reference external" href="https://pypi.python.org/pypi/earthengine-api"&gt;earthengine-api&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OAuth authentication&lt;/li&gt;
&lt;li&gt;Using IPython Notebooks for algorithm development&lt;ul&gt;
&lt;li&gt;Special display methods for interactive maps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="philosophical-goals-and-how-they-are-manifested"&gt;
&lt;h4&gt;Philosophical goals and how they are manifested&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Organize the world's (geospatial) information and make it universally
accessible and useful&lt;/li&gt;
&lt;li&gt;Facilitate open transparent science&lt;/li&gt;
&lt;li&gt;Speed up science by reducing the effort required to test hypotheses&lt;/li&gt;
&lt;li&gt;Enable collaborative algorithm development&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="selected-results"&gt;
&lt;h4&gt;Selected Results&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Consumer-grade visualizations&lt;ul&gt;
&lt;li&gt;Time-lapse global scale interactive video - &lt;a class="reference external" href="http://googleblog.blogspot.com/2013/05/a-picture-of-earth-through-time.html"&gt;blog
post&lt;/a&gt;,
&lt;a class="reference external" href="https://earthengine.google.org/#timelapse/v=30.27632,-97.74597,10.812,latLng&amp;amp;t=1.67"&gt;interactive viewer (centered on
Austin)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Science-grade Data Products&lt;ul&gt;
&lt;li&gt;High-Resolution Global Maps of 21st-Century Forest Cover Change -
&lt;a class="reference external" href="http://www.sciencemag.org/content/342/6160/850"&gt;Science journal
publication&lt;/a&gt;,
&lt;a class="reference external" href="http://googleresearch.blogspot.com/2013/11/the-first-detailed-maps-of-global.html"&gt;blog
post&lt;/a&gt;,
&lt;a class="reference external" href="http://earthenginepartners.appspot.com/science-2013-global-forest"&gt;interactive
viewer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="the-future"&gt;
&lt;h4&gt;The Future&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Global-scale analysis challenges&lt;/li&gt;
&lt;li&gt;An invitation for developers&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Randy Sargent</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/real-time-crunching-of-petabytes-of-geospatial-da.html</guid></item><item><title>SimpletITK: Advanced Image Analysis for Python</title><link>https://pyvideo.org/scipy-2014/simpletitk-advanced-image-analysis-for-python.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;SimpleITK brings advanced image analysis capabilities to Python. In
particular, it provides support for 2D/3D and multi-components images
with physical. SimpleITK exposes a large collection of image processing
filters from ITK, including image segmentation and registration.
SimpleITK is freely available as an open source package under the Apache
2.0 License.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;SimpleITK provides scientific image analysis, processing, segmentation
and registration for biomedical, microscopy and other scientific fields
by supporting multi-dimensional images with physical locations [1]. It's
is a layer build upon the Insight Segmentation and Registration Toolkit
(ITK) [2].&lt;/p&gt;
&lt;p&gt;While there are many Python packages to process 2D photographic images,
scientific image analysis adds additional requirements. Images
encountered in these domains often have anisotropic pixel spacing, or
spatial orientations, and calculations are best performed in physical
space as opposed to pixel space.&lt;/p&gt;
&lt;p&gt;SimpleITK brings to Python a plethora of capabilities for performing
image analysis. Although SimpleITK was developed by the biomedical
imaging community, it is also used for generic image processing. It
differentiates from OpenCV in offering 3D images and multi-component
images, and it differentiates from scipy by offering the abstraction of
image classes and their associated data structures. This applies to
images modalities such as CT scans, MRI, fMRI, ultrasound, and in
microscopy modalities such as confocal, SEM, TEM, and traditional bright
and dark field.&lt;/p&gt;
&lt;p&gt;Among the key functionalities supported by SimpleITK are over 260
advanced image filtering and segmentation algorithms as well as access
to scientific image file formats, including specialized formats such as
DICOM, Nifti, NRRD, VTK and other formats that preserve 3D metadata.
Example algorithms include Level Sets Segmentation including
multi-phase, Label Maps, Region Growing, Statistical Classification,
Advanced Thresholding, Geometrical Transformations, Deconvolution,
Anti-Aliasing, Edge Detection, Mathematical Morphology on both labels
and grayscale images and Fourier Analysis [4,5].&lt;/p&gt;
&lt;p&gt;SimpleITK is an open source project with an active community, that
builds upon the large amount of image analysis experience of the ITK
community [3] working in biomedical images analysis since 1999, and that
continues to grow year by year, aggregating state of the art algorithms
.&lt;/p&gt;
&lt;p&gt;SimpleITK development is sponsored by the US National Library of
Medicine.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bradley Lowekamp</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/simpletitk-advanced-image-analysis-for-python.html</guid><category>image analysis</category></item><item><title>Simulating X-ray Observations with Python</title><link>https://pyvideo.org/scipy-2014/simulating-x-ray-observations-with-python.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Constructing synthetic X-ray observations from hydrodynamical
simulations and other data sources is made possible with a combination
of the yt analysis toolkit with a number of other astronomical Python
libraries.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;X-ray astronomy is a rapidly expanding field, thanks to the many
observations of existing observatories, such as &lt;em&gt;Chandra&lt;/em&gt; and
&lt;em&gt;XMM-Newton&lt;/em&gt;, and the anticipation of high-resolution spectral data from
upcoming missions such as &lt;em&gt;Astro-H&lt;/em&gt; and &lt;em&gt;Athena+&lt;/em&gt;. Understanding these
observations and connecting them to astrophysical mechanisms requires
not only detailed modeling of the underlying physics but reliable
reproduction of the observed phenomena. I present a method of creating
synthetic X-ray observations from numerical simulations, which leverages
several astronomical Python libraries, including
&lt;a class="reference external" href="http://yt-project.org%20&amp;quot;yt&amp;quot;"&gt;yt&lt;/a&gt;,
&lt;a class="reference external" href="http://www.astropy.org%20&amp;quot;AstroPy&amp;quot;"&gt;AstroPy&lt;/a&gt;, and
&lt;a class="reference external" href="https://heasarc.gsfc.nasa.gov/xanadu/xspec/python/html/%20&amp;quot;PyXspec&amp;quot;"&gt;PyXspec&lt;/a&gt;.
I will describe the method of generating the observations, the Python
packages used, and applications of the method, including connecting
observations of galaxy clusters with MHD simulations and preparing
simulations for observation proposals.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">John ZuHone</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/simulating-x-ray-observations-with-python.html</guid></item><item><title>Software for Panoptes: A Citizen Science Observatory</title><link>https://pyvideo.org/scipy-2014/software-for-panoptes-a-citizen-science-observat.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;In this presentation, we describe the current status of software for
Project Panoptes. Our goal is to build low cost, reliable, robotic
telescopes which can be used to detect transiting exoplanets. Panoptes
is designed from the ground up to be a citizen science project which
will involve the public in all aspects of the science, from data
acquisition to data reduction.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The goal of Project Panoptes (Panoptic Astronomical Networked OPtical
observatory for Transiting Exoplanets Survey, see
&lt;a class="reference external" href="http://projectpanoptes.org/"&gt;http://projectpanoptes.org/&lt;/a&gt;) is to build low cost, reliable, robotic
telescopes which can be used to detect transiting exoplanets. The
hardware is designed to be standardized, using as many commercial off
the shelf components as possible so that a Panoptes &amp;quot;unit&amp;quot; can be
reproduced quickly and easily by students or amateurs. In this way, many
units can be deployed at many different sites to provide continuous and
redundant sky coverage. Panoptes is designed from the ground up to be a
citizen science project which will involve the public in all aspects of
the science, from data acquisition to data reduction.&lt;/p&gt;
&lt;p&gt;In this presentation, we describe the current status of the Panoptes
Observatory Control System (POCS, see &lt;a class="reference external" href="https://github.com/panoptes/POCS"&gt;https://github.com/panoptes/POCS&lt;/a&gt;),
an open source, collaborative, python-based software package. POCS is
designed to be a simple as possible in order to make it accessible to
non-experts. As such, POCS is a state machine which transitions between
a few well defined operating states. We make extensive use of existing
modules (notably astropy and pyephem). The challenge we face in writing
POCS to to balance our desire for simplicity and accessibility against
capability.&lt;/p&gt;
&lt;p&gt;We will also briefly describe the other software challenges of our
project, specifically an algorithm designed to extract accurate
photometry from DSLR images (color images obtained using a Bayer color
filter array) rather than from the more traditional filtered monochrome
CCD image.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Josh Walawender</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/software-for-panoptes-a-citizen-science-observat.html</guid></item><item><title>The History and Design Behind the Python Geophysical Modelling and Interpretation (PyGMI) Package</title><link>https://pyvideo.org/scipy-2014/the-history-and-design-behind-the-python-geophysi.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The development of geophysical software by individual scientists is
achievable through languages such as Python. All goals behind developing
a geophysical potential field interpretation and modelling software have
been achieved to date. The implication of this is that innovation can be
a driving force in projects, rather than waiting for commercial vendors
to provide appropriate scientific tools.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Council for Geoscience (CGS) is the so called &amp;quot;Geological Survey&amp;quot; of
South Africa. Like many similar institutions around the world, financial
restrictions play a significant role in limiting what tools are
available to scientists. It was from this need to stay scientifically
current, while keeping the software inexpensive, that the examination of
Python first started and ultimately ended up in the PyGMI project.&lt;/p&gt;
&lt;p&gt;The origins of PyGMI started with two separate projects. The first was a
joint project where the CGS was responsible for the creation of a
software interface for cluster analysis code, developed by the
University of Potsdam (Paasche et al 2009). The resulting project was
done entirely in Python. Data could be imported, filtered, analyzed and
displayed in graph form using Matplotlib.&lt;/p&gt;
&lt;p&gt;The second project stemmed from the need to perform 3D modelling on
geophysical data. The creation of 3D models can be extremely
time-consuming. Packages available tend to follow either the modelling
of individual 2.5D profiles, which are then joined up into 3D sections,
or modelling fully in three dimensions using polygonal based models. The
initial idea was to use the VTK library as the means to create, display
and interrogate the model, while using the Scipy and Numpy libraries to
perform the actual potential field calculations. It soon became apparent
that editing the resulting mesh quickly became complex and time
consuming. The ability to easily create and change a model is the very
basis of forward modelling and for this reason a new approach was
adopted. The newer 3D modelling package was designed to allow the user
to model simply by drawing the model, in the same way one would draw
views of a house using a paint program. This implies the need to have a
front view, as well as a top view. The model is therefore voxel based
rather than polygonal. The final model can be displayed either within
the PyGMI software, or exported to Google Earth for examination.&lt;/p&gt;
&lt;p&gt;Ultimately these two projects formed the basis of what is now the actual
PyGMI package -- which is a modular collection of various techniques,
including multivariate statistical analysis and potential field
modelling. The interface follows a flow diagram approach and the
individual modules are independent enough to ensure that they do not
interfere with code which has preceded them in previous modules.&lt;/p&gt;
&lt;p&gt;The PyGMI software is available for free download at:
&lt;a class="reference external" href="https://code.google.com/p/pygmi/"&gt;https://code.google.com/p/pygmi/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Patrick Cole</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/the-history-and-design-behind-the-python-geophysi.html</guid></item><item><title>The KBase Narrative Bioinformatics for the 99%</title><link>https://pyvideo.org/scipy-2014/the-kbase-narrative-bioinformatics-for-the-99.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The KBase Narrative builds on the IPython Notebook to provide a
multi-user, virtualized Bioinformatics Laboratory Notebook that brings
Experimental/Wetlab Biologists, students and the bio-curious into the
world of Computational Biology. Tools for genome annotation,
visualization, metabolic modeling and more are made available in a
collaborative and educational web interface.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Computional Biology and Experimental Biology are two specialities that
would deeply benefit from more interaction - computationalists need
access to data, biologists in wetlabs need computational tools. The
KBase Narrative is a computerized laboratory notebook that puts the
power of the KBase predictive biology platform into the hands of
experimentalists and students. KBase provides cluster computation,
analysis and modeling pipelines, large public datasets and a &amp;quot;pluggable&amp;quot;
architecture for future services. The Narrative is an interface enabling
the sharing of data, approaches and workflows on KBase. It also serves
as a teaching tool and publishing platform, allowing other scientists
and students to observe and reproduce the processes that led to the
published result.&lt;/p&gt;
&lt;p&gt;The KBase Narrative is based on the IPython Notebook, extended in the
following ways:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Notebooks are stored in a remote object store that enables
versioning, provenance and sharing&lt;/li&gt;
&lt;li&gt;Support for multiple users has been added, based on OAuth
authentication against a &amp;quot;cloud&amp;quot; authentication service (Globus
Online)&lt;/li&gt;
&lt;li&gt;A framework for dynamically building form inputs for services using
Python introspection and the IPython Traitlets package (a version of
Traits) and displaying the output in JS visualization widgets&lt;/li&gt;
&lt;li&gt;A Docker based provisioning system that builds and tears down
sandboxed IPython Notebook servers on demand, providing a scalable,
reasonable safe and easy to use environment for running hosted
IPython notebooks with much smaller overhead than VM's&lt;/li&gt;
&lt;li&gt;A heavily modified user interface that has been designed to support
computational biology workflows&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The current KBase Narrative was developed over the span of roughly 6
months by a small team of developers and user interface experts - the
short time scale was possible due to the huge amount of functionality
already provided by the IPython Notebook, and taking advantage of the
productivity and power of the Python language.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bill Rihl</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/the-kbase-narrative-bioinformatics-for-the-99.html</guid><category>bioinformatics</category></item><item><title>Transient detection and image analysis pipeline for TOROS project</title><link>https://pyvideo.org/scipy-2014/transient-detection-and-image-analysis-pipeline-f.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;TOROS project will be an astronomical survey of the southern hemisphere
in search of optical transients counterparts for aLIGO. This project
will make extended use of Machine learning techniques to identify
interesting transient candidates to aLIGO alerts. It also uses OpenCV
library for image aligning and some of the subsequent processing.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In preparation for the &lt;a class="reference external" href="http://toros.phys.utb.edu/TOROS/Welcome.html%20&amp;quot;TOROS%20project&amp;quot;"&gt;TOROS
project&lt;/a&gt;
designed to survey the southern hemisphere sky in search for transients,
we develop a pipeline for image analysis and processing based on Python.&lt;/p&gt;
&lt;p&gt;The code makes extended use of the open source image processing library
OpenCV to align the images astrometrically and makes use of other
astronomical specific routines like the Astropy package to deal with
FITS files. The design will involve integration with SciDB,
Astrometry.net, parallelization and other pythonic astronomical tools.&lt;/p&gt;
&lt;p&gt;This automated optical transients discovery tool will be tested with
real (CSTAR and TORITOS telescopes) and simulated data samples as the
input for a machine learning classification tool of light-curves based
on AstroML and Scikit-Learn libraries.&lt;/p&gt;
&lt;p&gt;The project is version controlled using git and we will handle future
collaboration among scientist from different countries using the open
source project manager Trac. It will be available as an open source
project in popular web repositories like github or bitbucket.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Beroiz</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/transient-detection-and-image-analysis-pipeline-f.html</guid></item><item><title>Using PyNIO and MPI for Python to help solve a big data problem for the CESM</title><link>https://pyvideo.org/scipy-2014/using-pynio-and-mpi-for-python-to-help-solve-a-bi.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The Community Earth System Model produces orders of magnitude more data
than earlier models, and the old data handling methods are no longer
adequate. We discuss how PyNIO together with MPI for Python has provided
the most efficient solution yet tested for the task of converting the
raw output of the model to NetCDF files suitable both for archiving and
for convenient use by scientists.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Like most climate models, the CESM (Community Earth System Model) steps
through time as a particular model scenario evolves and, at set
intervals, outputs the state of all the important variables into single
NetCDF files for each component of the model (atmosphere, ocean, land,
and sea ice). Each file contains all the variables for a component at a
single time step. Because the data volume is large, it is impractical to
attempt to handle all the data for a complete model run as a single
aggregation. Therefore, a consensus has evolved to mandate that the data
be reorganized to contain single variables over some convenient time
period. Finding a solution that can take advantage of multi-core
architectures to do the job efficiently has not been easy. Recently, in
an effort to determine the best solution, researchers at NCAR have
conducted a set of benchmark tests to find the best tool for the job.
Contenders included NCO (NetCDF Operators, the current incumbent for the
task); an in-house Fortran code using the parallel I/O library PIO; a
serial Python script using PyNIO; a version of the PyNIO script adapted
to work with mpi4py in a very simple manner; CDO; NCL; and Pagoda.
Surprisingly, PyNIO parallelized with mpi4py generally outperformed the
other contenders by a large margin, and will now be tested as a
replacement for the existing NCO scripts. This talk will look at the
simple mpi4py and PyNIO code that achieves this result, discuss the
reasons why the performance gain varies from case to case, and suggest
ways to improve performance in challenging cases. Along the way, PyNIO's
capabilities and recent improvements will be explained. In addition,
other possible contenders for this role, in particular NetCDF4-Python
coupled with mpi4py in a similar fashion, will be benchmarked using the
same test suite.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Brown</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/using-pynio-and-mpi-for-python-to-help-solve-a-bi.html</guid></item><item><title>yt: volumetric data analysis</title><link>https://pyvideo.org/scipy-2014/yt-volumetric-data-analysis.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;yt started as a tool for visualizing data from astrophysical
simulations, but it has evolved into a method for analyzing generic
volumetric data. In this talk we will present yt 3.0, which includes an
increased focus on inquiry-driven analysis and visualization, a
sympy-powered unit system, a revised user interface, and the ability to
scale to petabyte datasets and tens of thousands of cores.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;ol class="arabic simple"&gt;
&lt;li&gt;What is yt?&lt;ol class="arabic"&gt;
&lt;li&gt;&amp;quot;Lingua-franca for astrophysical simulations&amp;quot;&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://sites.google.com/site/santacruzcomparisonproject/"&gt;AGORA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Some selections from &lt;a class="reference external" href="http://yt-project.org/gallery.html"&gt;the
gallery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;105 &lt;a class="reference external" href="http://adsabs.harvard.edu/cgi-bin/nph-ref_query?bibcode=2011ApJS..192....9T&amp;amp;refs=CITATIONS&amp;amp;db_key=AST"&gt;citations to the yt method
paper&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Massively parallel&lt;ol class="arabic"&gt;
&lt;li&gt;Examples of large-scale calculations and visualizations
performed with yt&lt;/li&gt;
&lt;li&gt;Usage data on XSEDE visualization resources&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Volumetric data analysis beyond astrophysics&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.neurodome.org/"&gt;Neurodome&lt;/a&gt;, Whole-earth seismic
wave data, Weather simulation data, Nuclear engineering, Radio
astronomy&lt;/li&gt;
&lt;li&gt;??? (insert your field here!)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;What's new in yt-3.0?&lt;ol class="arabic"&gt;
&lt;li&gt;Rewrite of data selection, i/o, and field detection and creation&lt;/li&gt;
&lt;li&gt;Octree and particle support (i.e., discrete points)&lt;/li&gt;
&lt;li&gt;Unit conversions and dimensional analysis baked into the codebase&lt;/li&gt;
&lt;li&gt;Rethinking the API, 'rebranding' the project&lt;/li&gt;
&lt;li&gt;Advanced volume rendering&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Growing the Community&lt;ol class="arabic"&gt;
&lt;li&gt;The gallery&lt;/li&gt;
&lt;li&gt;Workshops&lt;/li&gt;
&lt;li&gt;Contributor statistics.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;The future&lt;ol class="arabic"&gt;
&lt;li&gt;New data styles&lt;ol class="arabic"&gt;
&lt;li&gt;Unstructured meshes&lt;/li&gt;
&lt;li&gt;Finite element analysis&lt;/li&gt;
&lt;li&gt;Spectral codes&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;New domain-specific functionality (beyond astrophysics)&lt;/li&gt;
&lt;li&gt;Browser GUIs powered by IPython&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nathan Goldbaum</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-14:scipy-2014/yt-volumetric-data-analysis.html</guid></item><item><title>A Python Framework for 3D Gamma Ray Imaging</title><link>https://pyvideo.org/scipy-2014/a-python-framework-for-3d-gamma-ray-imaging.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;A system capable of imaging gamma rays in 3D in near real time has been
developed. A flexible software framework has been developed using Python
to acquire, analyze, and finally visualize data from multiple sensors,
including novel gamma ray imaging detectors and a Microsoft Kinect.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Introduction and Motivation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Gamma-rays are photons with energies typically thousands to millions of
times greater than the energy of visible light photons. The vastly
higher energies of gamma-rays means that they interact differently with
matter, necessitating new sensors and imaging methods to localize gamma
ray sources. Many sensors and imaging approaches have been developed to
image gamma-rays in 2D, as in a conventional camera, with applications
in astronomy, medical imaging, and nuclear security. We have developed a
mobile gamma-ray imaging system that merges data from both visual and
gamma-ray imaging sensors to generate a visualization of the 3D
gamma-ray distribution in real-time. This creates 3D maps of the
physical environment and correlates that with the objects emitting
gamma-rays. We have used Python to develop a flexible software framework
for acquiring data from the multiple sensors, analyzing and merging data
streams, and finally visualizing the resulting 3D gamma-ray maps.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Methods&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The system consists of a cart that contains a state-of-the art gamma-ray
imaging system, called a Compton Imager, coupled with an RGB-D imaging
system, a Microsoft Kinect. The software package has three main tasks:
gamma-ray acquisition and processing, visual data processing, and
finally the merger of these two streams. The gamma-ray data processing
pipeline involves many computationally intensive tasks, thus a threaded
structure built with multiprocessing forms the basis of the gamma-ray
imaging framework. Furthermore, many other Pythonic tools have been used
to meet our real-time goal; including numexpr, cython, and even the
Python/C API. Several GUI frontends, built with TraitsUI or PySide for
example, are used to monitor and control how the acquired data is
processed in real-time, while a suite of real-time diagnostics are
displayed with matplotlib. The visual pipeline is based on an
open-source implementation of RGBDSLAM (&lt;a class="reference external" href="http://wiki.ros.org/rgbdslam"&gt;http://wiki.ros.org/rgbdslam&lt;/a&gt;),
which is built on the Robot Operating System (ROS) framework. Finally,
these two data streams are sent to a laptop computer via pyzmq, where
the final merger and imaging (by solving a statistical inversion problem
constrained by the visual data) is accomplished. The results are then
displayed as they are produced by the imaging algorithm using mayavi.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Link to Video: &lt;a class="reference external" href="https://www.dropbox.com/s/1w5yrqwepjcbpt1/Moving%20Cart%203D%20scene.mov"&gt;Moving Cart 3D
scene&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This system has been used to demonstrate real-time volumetric gamma ray
imaging for the first time [1]. The results from a typical run are shown
in the above video. The red line indicates the movement of the system
through the environment, while the blue arrows represent an aspect of
the gamma-ray data. The 3D point-cloud provided by RGBDSLAM appear
incrementally as the system traverses the environment. In the end, the
location of a small gamma-ray emitting source is correctly identified
with the hotspot in the image.&lt;/p&gt;
&lt;p&gt;[1]
[&lt;a class="reference external" href="https://www.nss-mic.org/2013/ConferenceRecord/Details.asp?PID=N25-4"&gt;https://www.nss-mic.org/2013/ConferenceRecord/Details.asp?PID=N25-4&lt;/a&gt;]&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrew Haefner</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/a-python-framework-for-3d-gamma-ray-imaging.html</guid></item><item><title>A Success Story in Using Python in a Graduate Chemical Engineering Course</title><link>https://pyvideo.org/scipy-2014/a-success-story-in-using-python-in-a-graduate-che.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;I recently used Python in a new required graduate level chemical
reaction engineering core course. The course was taken by 60 Master's
students with a broad set of educational backgrounds and programming
experience. Several factors contributed to the success of this course,
which I will present and discuss. Based on my experience, it is feasible
to use Python in engineering courses.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Historically, Matlab has been the primary math software tool used in our
courses on Chemical Engineering. Last year, I taught the first course in
the department using Python. In this talk I will present how I did that,
and why it was possible. The first step was demonstrating that Python +
numpy + scipy + matplotlib can solve all the problems we used to solve
with Matlab. This was documented in a project called PYCSE through a
series of over one hundred blog posts and organized in a web site
(&lt;a class="reference external" href="http://kitchingroup.cheme.cmu.edu/pycse"&gt;1&lt;/a&gt;). Second, the
development of Python distributions such as Enthought Canopy made it
possible to students to easily install and use Python. I had to augment
this with some additional functionality with PYCSE
(&lt;a class="reference external" href="http://github.com/jkitchin/pycse"&gt;2&lt;/a&gt;) which adds some statistical
analysis, differential equation solvers, numerical differentiation
functions and a publish function to convert Python scripts to PDF files
with captured output for grading. The only feature of Python missing is
a robust units package; several partial solutions exist, but none solve
all the needs of engineering calculations. Third, Emacs + org-mode
enabled me to write the course notes with integrated Python code and
output. These notes were provided to the students in PDF form, and
annotated during lecture using a tablet PC. Finally, the course was
administered with box.com and a custom python module to automate
assignment collection and return
(&lt;a class="reference external" href="https://github.com/jkitchin/box-course"&gt;3&lt;/a&gt;). An integrated grade
widget in the PDF files that was created when the students published
their assignments was used to aggregate the grades for the gradebook. I
used an innovative homework schedule of one problem every 2-4 days with
rapid feedback to keep students using Python frequently. We used timed
quizzes and online exams to assess their learning. Overall, the course
was successful. Student evaluations of the course were as good as
courses that used other software packages. Based on my experiences, I
will continue to use Python and expand its role in engineering
education.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">John Kitchin</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/a-success-story-in-using-python-in-a-graduate-che.html</guid></item><item><title>Activity Detection from GPS Tracker Data</title><link>https://pyvideo.org/scipy-2014/activity-detection-from-gps-tracker-data.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Data gathered by personal GPS trackers are becoming a major source of
information pertaining to human activity and behaviour. This
presentation will include python work flows which aim to accurately and
efficiently carry out the necessary computation required to process
volunteered GPS trajectories into useful spatial information.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most modern GPS processing techniques started to utilise the time
interval between points as a major indicator for finding POIs in a
user's trajectories. We are taking step back in this process, and only
account for the spatial distribution of measured points in order to
extract POIs. Time is solely used as a secondary indicator and for
ordering purposes.&lt;/p&gt;
&lt;p&gt;Points are first cleaned of any highly inaccurate data and stored in a
PostGIS environment. Using developed python module, we extract the point
data and order them by time into one large trajectory. Then, for each
point, we begin selecting its neighbours (both previous and following)
until we reach one within a specified distance of 50m from the first
point.The number of the selected points is added to the original point
as a new attribute.Continuing with the process the newly calculated
values create an imitation of signal, reflecting a point density.&lt;/p&gt;
&lt;p&gt;Shift in strength of the signal signifies a change in a user's travel
mode which can be used for segmentation of the trajectory into
homogeneous parts. Identification of these parts is currently based on
the decision tree, where individual aspects of the segment (like average
speed, signal strength, time elapsed or centroid) are evaluated and the
segment is categorized.&lt;/p&gt;
&lt;p&gt;Current work seeks to incorporate neural networks into the processing
framework. Since the signal pattern of the each mode of transportation
(car, bus, walk, etc.) is independent from the user behaviour, a
properly trained model should classify more accurately activities for a
broad range of users.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan Vandrol</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/activity-detection-from-gps-tracker-data.html</guid></item><item><title>Campaign for IT literacy through FOSS and Spoken Tutorials</title><link>https://pyvideo.org/scipy-2014/campaign-for-it-literacy-through-foss-and-spoken.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Textbook Companion (TBC) has code for solved problems of textbooks,
coordinated by FOSSEE (&lt;a class="reference external" href="http://fossee.in"&gt;http://fossee.in&lt;/a&gt;). We explain a collaborative
method that helped create 500 Scilab TBCs (&lt;a class="reference external" href="http://cloud.scilab.in"&gt;http://cloud.scilab.in&lt;/a&gt;) and
over 100 Python TBCs (&lt;a class="reference external" href="http://tbc-python.fossee.in/"&gt;http://tbc-python.fossee.in/&lt;/a&gt;). We also explain a
self learning method that trained 250,000 students on FOSS systems using
spoken tutorials (&lt;a class="reference external" href="http://spoken-tutorial.org"&gt;http://spoken-tutorial.org&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The FOSSEE team has been promoting the use of FOSS in educational
institutions in India. It focuses on the following FOSS systems
currently: Scilab, Python, Oscad, OpenFOAM, COIN-OR and OpenFormal
(&lt;a class="reference external" href="http://fossee.in"&gt;http://fossee.in&lt;/a&gt;). On each of these systems, the following three
standardised help are provided:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Support to conduct spoken tutorial based workshops, explained below&lt;/li&gt;
&lt;li&gt;Creation of Textbook Companion (TBC)&lt;/li&gt;
&lt;li&gt;Support to Lab Migration.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A TBC is a collection of code for solved examples of standard textbooks.
We have completed a large number of TBCs on Scilab and made them
available for online use at &lt;a class="reference external" href="http://cloud.scilab.in/"&gt;Scilab&lt;/a&gt; and
offline use at &lt;a class="reference external" href="http://www.scilab.in/Completed_Books"&gt;Completed
Books&lt;/a&gt;. Similarly, one may
access the Python TBC at &lt;a class="reference external" href="http://tbc-python.fossee.in/"&gt;Python TBC&lt;/a&gt;.
These TBCs are created by students and teachers from many colleges and
each creator is paid an honorarium through a project funded by the
Government of India.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://spoken-tutorial.org"&gt;Spoken Tutorial&lt;/a&gt; is a screencast of ten
minute duration on a FOSS topic, created for self learning. Using Spoken
Tutorials, we conduct two hour long workshops on FOSS topics through
volunteers, who need not be experts. We conduct online tests and provide
certificates for all who pass the tests. All of these are done
completely free of cost, thanks to the financial support from the
Government of India. Using this method, we have trained more than 200K
students in the last two years in India
(&lt;a class="reference external" href="http://www.spoken-tutorial.org/statistics"&gt;statistics&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The students love this method, see some testimonials at
&lt;a class="reference external" href="http://www.spoken-tutorial.org/testimonials"&gt;http://www.spoken-tutorial.org/testimonials&lt;/a&gt;. It is being increasingly
accepted by colleges and universities, officially. We expect to conduct
5K to 10K workshops in 2014, training 200K to 500K students. As we dub
the spoken part into all 22 languages of India, the FOSS topics are
accessible also to students who are not fluent in English, thereby
helping us reach out to many students. Spoken Tutorial, which started as
a documentation project for FOSS systems has transformed into a massive
training programme. Our methods are scalable and are available to the
FOSS enthusiasts in the rest of the world.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kannan Moudgalya</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/campaign-for-it-literacy-through-foss-and-spoken.html</guid></item><item><title>Climate &amp; GIS: User Friendly Data Access, Workflows, Manipulation, Analysis and Visualization of Climate Data</title><link>https://pyvideo.org/scipy-2014/climate-gis-user-friendly-data-access-workflo.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Understanding environmental and climate change requires data fusion,
format conversions, processing and visualization to gain insight into
the data. Our open source scientific Python and JavaScript based tools
makes it easy to manipulate geo-spatial and climate data, create and
execute workflows, and produce visualizations over the web for
scientific and decision making tools.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The impact of climate change will resonate through a broad range of
fields including public health, infrastructure, water resources, and
many others. Long-term coordinated planning, funding, and action are
required for climate change adaptation and mitigation. Unfortunately,
widespread use of climate data (simulated and observed) in non-climate
science communities is impeded by factors such as large data size, lack
of adequate metadata, poor documentation, and lack of sufficient
computational and visualization resources. Additionally, working with
climate data in its native format is not ideal for all types of analyses
and use cases often requiring technical skills (and software)
unnecessary to work with other geospatial data formats.&lt;/p&gt;
&lt;p&gt;We present open source tools developed as part of ClimatePipes and
OpenClimateGIS to address many of these challenges by creating an open
source platform that provides state-of-the-art user-friendly data
access, processing, analysis, and visualization for climate and other
relevant geospatial datasets making the climate and other geospatial
data available to non-researchers, decision-makers, and other
stakeholders.&lt;/p&gt;
&lt;p&gt;The overarching goals are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Enable users to explore real-world questions related to environment
and climate change.&lt;/li&gt;
&lt;li&gt;Provide tools for data access, geo-processing, analysis, and
visualization.&lt;/li&gt;
&lt;li&gt;Facilitate collaboration by enabling users to share datasets,
workflows, and visualization.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the key technical features include&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Support for multiprocessing for large datasets using Python-celery
distributed task queuing system&lt;/li&gt;
&lt;li&gt;Generic iterators allowing data to be streamed to arbitrary formats
(relatively) easily (e.g. ESRI Shapefile, CSV, keyed ESRI Shapefile,
CSV, NetCDF)&lt;/li&gt;
&lt;li&gt;NumPy based array computations allowing calculations such as monthly
means or heat indices optionally on temporally grouped data slices&lt;/li&gt;
&lt;li&gt;Decorators to expose existing Python API as a RESTful API&lt;/li&gt;
&lt;li&gt;Simple to use, lightweight Web-framework and JavaScript libraries for
analyzing and visualizing geospatial datasets using D3 and WebGL.&lt;/li&gt;
&lt;/ol&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aashish Chaudhary</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/climate-gis-user-friendly-data-access-workflo.html</guid><category>climate</category></item><item><title>CodaLab: A New Service for Data Exchange, Code Execution, Benchmarks &amp; Reproducible Research</title><link>https://pyvideo.org/scipy-2014/codalab-a-new-service-for-data-exchange-code-ex.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Learn, Share and Collaborate with CodaLab -- A new open source platform
which lets communities create and explore experiments together and
engage in benchmarking and competitions to enable true reproducibility
and advance the state of the art in data-driven research&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;CodaLab is a web-based open source platform that allows researchers to
share and browse code, data, and create experiments in a truly
reproducible manner. CodaLab focuses on accomplishing the following:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Serve as a data repository for data sets including large scale data
sets that could only be hosted in a cloud computing environment&lt;/li&gt;
&lt;li&gt;Serve as an algorithm repository that researchers can use in their
experimentation, to teach and learn from others&lt;/li&gt;
&lt;li&gt;Host the execution of experiments as worksheets, sometimes referred
to as &amp;quot;executable papers&amp;quot;, which are annotated scientific documents
that combine textual process descriptions with live data sets and
functioning code.&lt;/li&gt;
&lt;li&gt;Enable the creation of benchmarks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CodaLab is a community-driven effort led by Percy Liang from Stanford
University who built the precursor of CodaLab, namely, MLComp. From a
development viewpoint CodaLab supports both the Linux and Windows
communities with code in GitHub and Python as one of the main language
used to support the scientific community.&lt;/p&gt;
&lt;p&gt;At SciPi, we invite the community to participate in CodaLab by creating
experiments as executable papers and by sharing them with the rest of
the community at &lt;a class="reference external" href="http://codalab.org"&gt;http://codalab.org&lt;/a&gt;. These worksheets or &amp;quot;executable
papers&amp;quot; can then be freely reproduced, appended, and otherwise modified
to improve productivity and accelerate the pace of discovery and
learning among data-driven scientific professionals.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Christophe Poulain</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/codalab-a-new-service-for-data-exchange-code-ex.html</guid></item><item><title>Fast Algorithms for Binary Spatial Adjacency Measures</title><link>https://pyvideo.org/scipy-2014/fast-algorithms-for-binary-spatial-adjacency-meas.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Spatial weights matrices, &lt;span class="formula"&gt;&lt;i&gt;W&lt;/i&gt;&lt;/span&gt;, represent potential interaction
between all &lt;span class="formula"&gt;&lt;i&gt;i&lt;/i&gt;,&lt;i&gt;j&lt;/i&gt;&lt;/span&gt; in a study area and play an essential role in
many spatial analysis tasks. Commonly applied binary adjacency
algorithms using decomposition and tree representations scale
quadratically and are ill suited for large data sets. We present a
linearly scaling, adjacency algorithm with significantly improved
performance.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Spatial weights matrices, &lt;span class="formula"&gt;&lt;i&gt;W&lt;/i&gt;&lt;/span&gt;, play an essential role in many
spatial analysis tasks including measures of spatial association,
regionalization, and spatial regression. A spatial weight
&lt;span class="formula"&gt;&lt;i&gt;w&lt;/i&gt;&lt;sub&gt;&lt;i&gt;i&lt;/i&gt;,&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;&lt;/span&gt; represents potential interaction between each
&lt;span class="formula"&gt;&lt;i&gt;i&lt;/i&gt;,&lt;i&gt;j&lt;/i&gt;&lt;/span&gt; pair in a set of &lt;span class="formula"&gt;&lt;i&gt;n&lt;/i&gt;&lt;/span&gt; spatial units. The weights are
generally defined as either binary &lt;span class="formula"&gt;&lt;i&gt;w&lt;/i&gt;&lt;sub&gt;&lt;i&gt;i&lt;/i&gt;,&lt;i&gt;j&lt;/i&gt;&lt;/sub&gt;=&lt;br/&gt;1,0&lt;br/&gt;&lt;/span&gt;, depending
on whether or not &lt;span class="formula"&gt;&lt;i&gt;i&lt;/i&gt;&lt;/span&gt; and &lt;span class="formula"&gt;&lt;i&gt;j&lt;/i&gt;&lt;/span&gt; are considered neighbors, or a
continuous value reflecting some general distance relationship between
&lt;span class="formula"&gt;&lt;i&gt;i&lt;/i&gt;&lt;/span&gt; and &lt;span class="formula"&gt;&lt;i&gt;j&lt;/i&gt;&lt;/span&gt;. This work focuses on the case of binary weights
using a contiguity criteria where &lt;span class="formula"&gt;&lt;i&gt;i&lt;/i&gt;&lt;/span&gt; and &lt;span class="formula"&gt;&lt;i&gt;j&lt;/i&gt;&lt;/span&gt; are rook
neighbors when sharing an edge and queen neighbors when sharing a
vertex.&lt;/p&gt;
&lt;p&gt;Population of the &lt;span class="formula"&gt;&lt;i&gt;W&lt;/i&gt;&lt;/span&gt; is computationally expensive, requiring, in
the naive case, &lt;span class="formula"&gt;&lt;i&gt;O&lt;/i&gt;(&lt;i&gt;n&lt;/i&gt;&lt;sup&gt;2&lt;/sup&gt;)&lt;/span&gt; point or edge comparisons. To improve
efficiency data decomposition techniques, in the form of regular grids
and quad-trees, as well as spatial indexing techniques using r-trees
have be utilized to reduce the total number of local point or edge
comparisons. Unfortunately, these algorithms still scale quadratically.
Recent research has also shown that even with the application of
parallel processing techniques, the gridded decomposition method does
not scale as &lt;span class="formula"&gt;&lt;i&gt;n&lt;/i&gt;&lt;/span&gt; increases.&lt;/p&gt;
&lt;p&gt;This work presents the development and testing of a high performance
&lt;a class="reference external" href="https://github.com/jlaura/pysal/blob/weights/pysal/weights/_contW_binning.py#L181"&gt;implementation&lt;/a&gt;,
written in pure Python, using time constant and &lt;span class="formula"&gt;&lt;i&gt;O&lt;/i&gt;(&lt;i&gt;n&lt;/i&gt;)&lt;/span&gt; operations,
by leveraging high performance containers and a vertex comparison
method. The figures below depict results of initial testing using
synthetically generated lattices of triangles, squares, and hexagons
with rook contiguity in black and queen contiguity in gray. These
geometries were selected to control for average neighbour cardinality
and average vertex count per geometry. From these initial tests, we
report a significant speedup over r-tree implementations and a more
modest speedup over gridded decomposition methods. In addition to
scaling linearly, while existing methods scale quadratically, this
method is also more memory efficient. Ongoing work is focusing on
testing using randomly distributed data, and U.S. Census data, the
application of parallelization techniques to test further performance
improvements, and the use of fuzzy operator to account for spatial error
.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="raw times" src="http://github.com/pysal/pPysal/blob/master/weights/figures/rawtime.png?raw=true%20&amp;quot;Initial%20Results%20-%20Raw%20Speed&amp;quot;" /&gt;
&lt;p class="caption"&gt;raw times&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="speedup" src="http://github.com/pysal/pPysal/blob/master/weights/figures/speedup.png?raw=true%20&amp;quot;Initial%20Results%20-%20Speedup&amp;quot;" /&gt;
&lt;p class="caption"&gt;speedup&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="linear speed" src="http://github.com/pysal/pPysal/blob/master/weights/figures/rawl.png?raw=true%20&amp;quot;Initial%20Results%20-%20List%20Raw%20Speed&amp;quot;" /&gt;
&lt;p class="caption"&gt;linear speed&lt;/p&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Lauara</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/fast-algorithms-for-binary-spatial-adjacency-meas.html</guid></item><item><title>GeoPandas: Geospatial Data + Pandas</title><link>https://pyvideo.org/scipy-2014/geopandas-geospatial-data-pandas.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;GeoPandas extends the pandas data analysis library to work with
geographic objects.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/kjordahl/geopandas"&gt;GeoPandas&lt;/a&gt; is a library built
on top of pandas to extend its capabilities to allow spatial
calculations. The two main datatypes are &lt;tt class="docutils literal"&gt;GeoSeries&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;GeoDataFrame&lt;/tt&gt;, extending pandas &lt;tt class="docutils literal"&gt;Series&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;DataFrame&lt;/tt&gt;,
respectively. A &lt;tt class="docutils literal"&gt;GeoSeries&lt;/tt&gt; contains a collection of geometric objects
(such as &lt;tt class="docutils literal"&gt;Point&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;LineString&lt;/tt&gt;, or &lt;tt class="docutils literal"&gt;Polygon&lt;/tt&gt;) and implements
nearly all &lt;tt class="docutils literal"&gt;Shapely&lt;/tt&gt; operations. These include unary operations (e.g.
&lt;tt class="docutils literal"&gt;centroid&lt;/tt&gt;), binary operations (e.g. &lt;tt class="docutils literal"&gt;distance&lt;/tt&gt;, either elementwise
to another &lt;tt class="docutils literal"&gt;GeoSeries&lt;/tt&gt; or to a single geometry), and cumulative
operations (e.g. &lt;tt class="docutils literal"&gt;unary_union&lt;/tt&gt; to combine all items to a single
geometry).&lt;/p&gt;
&lt;p&gt;A &lt;tt class="docutils literal"&gt;GeoDataFrame&lt;/tt&gt; object contains a column of geometries (itself a
&lt;tt class="docutils literal"&gt;GeoSeries&lt;/tt&gt;) that has special meaning. GeoDataFrames can be easily
created from spatial data in other formats, such as shapefiles. Rows in
the &lt;tt class="docutils literal"&gt;GeoDataFrame&lt;/tt&gt; represent features, and columns represent
attributes. Pandas' grouping and aggregation methods are also supported.&lt;/p&gt;
&lt;p&gt;GeoPandas objects can optionally be aware of coordinate reference
systems (by adding a &lt;tt class="docutils literal"&gt;crs&lt;/tt&gt; attribute) and transformed between map
projections. Basic support for plotting is included with GeoPandas.
Other features include geocoding, export to GeoJSON, and retrieving data
from a PostGIS spatial database.&lt;/p&gt;
&lt;p&gt;This talk will describe the main features of GeoPandas and show examples
of its use.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kelsey Jordahl</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/geopandas-geospatial-data-pandas.html</guid><category>gis</category></item><item><title>Geospatial Data and Analysis Stack</title><link>https://pyvideo.org/scipy-2014/geospatial-data-and-analysis-stack.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;There are a growing number of Python packages (fiona, geopandas, pysal,
shapely, etc.) addressing various types of spatial data, as well as the
geoprocessing of that data and its statistical analysis. This session
explore ways to best collaborate between and strengthen these efforts.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Serge Rey</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/geospatial-data-and-analysis-stack.html</guid></item><item><title>IPython-Reveal.js Attacks Again, But Now... It Is Alive!</title><link>https://pyvideo.org/scipy-2014/ipython-revealjs-attacks-again-but-now-it-is.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Recently, the IPython Notebook has began to be used for presentations in
several conferences. But, there is not a full-featured and executable
IPython presentation tool available. So, we developed a new
Reveal.js-powered live slideshow extension, designed specifically to be
used directly from the notebook and to be as executable as the notebook
is, and also powered with great features.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Detailed Abtract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the last years, &lt;strong&gt;IPython&lt;/strong&gt;, a comprehensive environment for
interactive and exploratory computing, has arose as must-have
application to run in the daily scientific &lt;em&gt;work-flow&lt;/em&gt; because provides,
not only an enhanced interactive &lt;strong&gt;Python&lt;/strong&gt; shell (terminal or
qt-based), but also a very popular interactive browser-based
&lt;strong&gt;notebook&lt;/strong&gt; with an unimaginable scope.&lt;/p&gt;
&lt;p&gt;The presentation of our research results and the teaching of our
knownledge are very important steps in the scientific research
work-flow, and recently, the &lt;strong&gt;IPython Notebook&lt;/strong&gt; has began to be used
for all kind of oral communications in several conferences, curses,
classes and bootcamps.&lt;/p&gt;
&lt;p&gt;Despite the fact that we can present our talks with the IPython notebook
itself or through a static &lt;strong&gt;Reveal.js&lt;/strong&gt;-based slideshow powered by
IPython.nbconvert (a tool we presented at SciPy 2013), there is not a
&lt;strong&gt;full-featured&lt;/strong&gt; and &lt;strong&gt;executable&lt;/strong&gt; (live) IPython presentation tool
available. So, we developed a new &lt;strong&gt;IPython&lt;/strong&gt;-&lt;strong&gt;Reveal.js&lt;/strong&gt;-powered
&lt;strong&gt;live&lt;/strong&gt; slideshow extension, designed specifically to be used
&lt;strong&gt;directly&lt;/strong&gt; from the IPython notebook and to be as &lt;strong&gt;executable&lt;/strong&gt; as
the notebook is (because deep inside, it is the notebook itself but
rendered with another face), and also powered with a lot of &lt;em&gt;features&lt;/em&gt;
to address the most common tasks performed during the oral presentation
and spreading of our scientific work, such as: main slides and nested
slides, fragments views, transitions, themes and speaker notes.&lt;/p&gt;
&lt;p&gt;To conclude, we have developed a new visualization tool for the
&lt;strong&gt;IPython Notebook&lt;/strong&gt;, suited for the final steps of our scientific
research &lt;em&gt;work-flow&lt;/em&gt;, providing us with, not only an enhanced and new
experience in the oral presentation and communication of our results,
but also with a super-powerful tool at teaching time, helping us to
easily tranfer our concepts and spread our knowledge.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can see the extension in action in the following video link:
&lt;a class="reference external" href="https://www.youtube.com/watch?v=Pc-1FS0l2vg"&gt;https://www.youtube.com/watch?v=Pc-1FS0l2vg&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And you can also see the source code of the extension at the following
repository: &lt;a class="reference external" href="http://github.com/damianavila/live_reveal"&gt;http://github.com/damianavila/live_reveal&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Damian Avila</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/ipython-revealjs-attacks-again-but-now-it-is.html</guid></item><item><title>Lightning Talks | SciPy 2014 | July 10th, 2014</title><link>https://pyvideo.org/scipy-2014/lightning-talks-scipy-2014-july-10th-2014.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Various speakers</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/lightning-talks-scipy-2014-july-10th-2014.html</guid><category>lightning talks</category></item><item><title>Mapping Networks of Violence</title><link>https://pyvideo.org/scipy-2014/mapping-networks-of-violence.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;A novel approach to both violence prevention and the measurement of
propensity to violence is presented. The work is part of the evaluation
of Cure Violence's (Ransford, Kane and Slutkin 2009; Slutkin 2012)
implementation in NYC. Python libraries such as IPython, PySAL, Numpy,
Basemap, Fiona, Shapely, Matplotlib, bNetworkX, Pandas and scikit-learn
feature prominently in the work.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Violence remains a significant problem in New York City's poor
neighborhoods. There were more than 9,000 gun homicides in 2008 (FBI,
2009) and the CDC (2012) reports that there were more than 71K non-fatal
wounds in the US. One novel approach to the problem of violence is the
Cure Violence Model (Ransford, Kane and Slutkin 2009; Slutkin 2012).
Cure Violence treats violence as a disease passed between people in a
social network. The program tries to use the same network to change how
people who are prone to and have been the victims of violence react to
stress and conflict. Cure Violence is viewed as having been successful
in Chicago and shown promising in other cities (Skogin 2009, Wilson
2010, Webster 2009). All of these studies have used reported incidents
of violence before and after the program to assess the efficacy. The NYC
Council and Robert Wood Johnson Foundation have commited significant
resoures to this approach. Both have retained the CUNY John Jay Research
&amp;amp; Evaluation center to evaluate the efficacy. Our research adds to the
literature by being the first to attempt to measure the change in the
propensity to violence of people in the community. Novel preliminary
research is presented on network cliques of respondents and the
demographic, education, victimization experiences that constitute
greatest risk. All of the analysis was conducted in Python libraries
including IPython, PySAL, Numpy, Basemap, Fiona, Shapely, Matplotlib,
bNetworkX, Pandas and scikit-learn.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Evan Misshula</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/mapping-networks-of-violence.html</guid></item><item><title>MASA: A Tool for the Verification of Scientific Software</title><link>https://pyvideo.org/scipy-2014/masa-a-tool-for-the-verification-of-scientific-s.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Numerical simulations have a broad range of application, from aircraft
design to drug discovery. However, any prediction from a computer must
be tested to ensure its reliability. Verification ensures the outputs of
a computation accurately reflect the underlying model. This talks covers
MASA, a tool for the verification of software used in a large class of
problems in applied mathematics.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Numerical simulations have an incredibly broad range of applicability,
from computer aided aircraft design to drug discovery. However, any
prediction arising from a computer model must be rigorously tested to
ensure its reliability. Verification is a process that ensures that the
outputs of a computation accurately reflect the solution of the
mathematical models.&lt;/p&gt;
&lt;p&gt;This talk will first provide an introduction to the method of
manufactured solutions (MMS), which is a powerful approach for
verification of model problems in which a solution cannot be determined
analytically. However, verifying computational science software using
manufactured&lt;/p&gt;
&lt;p&gt;solutions requires the generation of the solutions with associated
forcing terms and their reliable implementation in software. There are
several issues that arise in generating solutions, including ensuring
that they are meaningful, and the algebraic complexity of the forcing
terms. After briefly discussing these issues, the talk will introduce
MASA, the Manufactured Analytical Solution Abstraction library. MASA is
an open-source library written in C++ (with python interfaces) which is
designed for the veri?cation of software used for solving a large class
of problems stemming from numerical methods in applied mathematics
including nonlinear equations, systems of algebraic equations, and
ordinary and partial differential equations.&lt;/p&gt;
&lt;p&gt;Example formulations in MASA include the Heat Equation, Laplace's
Equation, and the Navier-Stokes Equations, but in principle MASA
supports instantiating any model that can be written down
mathematically. This talk will end with details on two methods to import
manufactured solutions into the library, either by generate the source
terms, or using the automatic differentiation capabilities provided in
MASA.&lt;/p&gt;
&lt;p&gt;The library is available at:
&lt;a class="reference external" href="https://github.com/manufactured-solutions/MASA"&gt;https://github.com/manufactured-solutions/MASA&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicholas Malaya</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/masa-a-tool-for-the-verification-of-scientific-s.html</guid></item><item><title>Measuring Rainshafts: Bringing Python to Bear on Remote Sensing Data</title><link>https://pyvideo.org/scipy-2014/measuring-rainshafts-bringing-python-to-bear-on.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This presentation details how Python is being used to extract
geophysical insight from active remote sensing data, namely Radars. By
using a common data model our work bridges the gap between the domains
of radar engineering and image analysis.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Remote sensing data is complicated, very complicated! It is not only
geospatially tricky but also indirect as the sensor measures the
interaction of the media with the probing radiation, not the geophysics.
However the problem is made tractable by the large number of algorithms
available in the Scientific Python community, what is needed is a common
data model for active remote sensing data that can act as a layer
between highly specialized file formats and the cloud of scientific
software in Python. This presentation motivates this work by asking: How
big is a rainshaft? What is the natural dimensionality of rainfall
patterns and how well is this represented in fine scale atmospheric
models. Rather than being specific to the domain of meteorology we will
break down how we approach this problem in terms what tools across
numerous packages we used to read, correct, map and reduce the data to
forms able to answer our science questions. This is a &amp;quot;how&amp;quot;
presentation, covering signal processing using linear programming
methods, mapping using KD Trees, and image analysis using ndimage and,
of course graphics using Matplotlib.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Scott Collis</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/measuring-rainshafts-bringing-python-to-bear-on.html</guid></item><item><title>Ocean Model Assessment for Everyone</title><link>https://pyvideo.org/scipy-2014/ocean-model-assessment-for-everyone.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;An end-to-end workflow for assessing storm-driven water levels predicted
by coastal ocean models will be discussed which uses &lt;tt class="docutils literal"&gt;OWSLib&lt;/tt&gt; for CSW
Catalog access, &lt;tt class="docutils literal"&gt;Iris&lt;/tt&gt; for ocean model access and &lt;tt class="docutils literal"&gt;pyoos&lt;/tt&gt; for Sensor
Observation Service data access. Analysis and visualization is done with
&lt;tt class="docutils literal"&gt;Pandas&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;Cartopy&lt;/tt&gt;, and the entire workflow is shared as in
IPython Notebook with custom environment in Wakari.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;To assess how well ocean models are performing, the model products need
to be compared with data. Finding what models and data exist has
historically been challenging because this information is held and
distributed by numerous providers. Accessing data has been challenging
because ocean models produce gigabytes or terabytes of information, is
usually stored in scientific data formats like HDF or NetCDF, while
ocean observations are often stored in scientific data formats or in
databases.&lt;/p&gt;
&lt;p&gt;To solve this problem, the Integrated Ocean Observing System (IOOS) has
been building a distributed information system based on standard web
services for discovery and access. IOOS is now embarking on a nationwide
system-test using python to formulate queries, process responses, and
analyze and visualize the data. An end-to-end
(search-access-analyze-visualize) workflow for assessing storm-driven
water levels predicted by coastal ocean models will be discussed, which
uses &lt;tt class="docutils literal"&gt;OWSLib&lt;/tt&gt; for OGC CSW service catalog access, &lt;tt class="docutils literal"&gt;Iris&lt;/tt&gt; for ocean
model access and &lt;tt class="docutils literal"&gt;pyoos&lt;/tt&gt; (which wraps OWSLib) for Sensor Observation
Service data access. Analysis and visualization is done with &lt;tt class="docutils literal"&gt;Pandas&lt;/tt&gt;
and &lt;tt class="docutils literal"&gt;Cartopy&lt;/tt&gt;, and the entire end-to-end workflow is shared as in
IPython Notebook with custom environment in Wakari.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Richard Signell</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/ocean-model-assessment-for-everyone.html</guid></item><item><title>Plasticity in OOF</title><link>https://pyvideo.org/scipy-2014/plasticity-in-oof.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;We discuss recent advances in the Object Oriented Finite-Element project
at NIST, a Python and C++ tool designed to bring sophisticated numerical
modeling capabilities to users in the field of Materials Science.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We discuss recent advances in the Object Oriented Finite-Element project
at NIST (also called &lt;a class="reference external" href="http://www.ctcms.nist.gov/oof/oof2"&gt;OOF&lt;/a&gt;), a
Python and C++ tool designed to bring sophisticated numerical modeling
capabilities to users in the field of Materials Science.&lt;/p&gt;
&lt;p&gt;As part of the effort to expand the solid-mechanics capabilities of the
code, the solver has been extended to include the ability to handle
history-dependent properties, such as occur in viscoplastic systems, and
inequality constraints, which are present in conventional isotropic
plasticity, as well as surface interactions.&lt;/p&gt;
&lt;p&gt;This software provides numerous tools for constructing finite-element
meshes from microstructural images, and for implementing material
properties from a very broad class which includes elasticity, chemical
and thermal diffusion, and electrostatics.&lt;/p&gt;
&lt;p&gt;The code is a hybrid of Python and C++ code, with the high level user
interface and control code in Python, and the heavy numeric work being
done in C++. Numerous tools are provided for constructing finite-element
meshes from microstructural images, and for implementing material
properties from a very broad class which includes elasticity, chemical
and thermal diffusion, and electrostatics. The software can be operated
either as an interactive, GUI-driven application, as a scripted
command-line tool, or as a supporting library, providing useful access
to users of varying levels of expertise. At every level, the
user-interface objects are intended to be familiar to the
materials-science user.&lt;/p&gt;
&lt;p&gt;The modular object-oriented design of the code, and the strategy of
separating the finite-element infrastructure from the material
constitutive rules proved itself in implementing the new solid-mechanics
capabilities.&lt;/p&gt;
&lt;p&gt;Development on a fully-3D version of the code has also made significant
progress, overcoming several challenges associated with user-interface
issues. A nontrivial, solved 3D problem will be presented.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrew Reid</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/plasticity-in-oof.html</guid></item><item><title>Prototyping a Geophysical Algorithm in Python</title><link>https://pyvideo.org/scipy-2014/prototyping-a-geophysical-algorithm-in-python.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Spitz' paper on FX pattern recognition contains a long paragraph that
describes a model, an algorithm, and the results of applying the
algorithm to the model. The algorithm requires Fourier transforms,
convolutional filtering, matrix multiplication, and solving linear
equations. I describe how to use numpy, scipy, and mapplotlib to
prototype the algorithm and display the processed model.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A geophysics paper by Spitz has a long paragraph that describes a model,
an algorithm, and the results of applying the algorithm to the model. I
wanted to implement and test the algorithm to ensure I fully understood
the method. This is a good illustration of Python for geophysics because
the implementation requires:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Fourier transforms provided by numpy.fft&lt;/li&gt;
&lt;li&gt;Setting up linear equations using numpy.array and numpy.matrix&lt;/li&gt;
&lt;li&gt;solving the linear equations using scipy.linalg.solve&lt;/li&gt;
&lt;li&gt;Applying convolutional filters using scipy.signal.lfilter&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A bandlimited flat event model is created using array slicing in numpy
and is bandlimited in the frequency domain. Another component of the
model is created by convolving a short derivative filter on a similar
flat event model. After Fourier transform, linear equations are set up
to compute a prediction filter in the FX domain. These equations are
created using data slicing, conjugate transpose, matrix multiple (all
available in numpy). Scipy.linalg.solve is used to solve for the
prediction error filter. A final filter is computed using the recursive
filter capability in scipy.signal.lfilter. Results are displayed using
matplotlib.&lt;/p&gt;
&lt;p&gt;This is quite a tour of scipy and numpy to implement an algorithm
described in a single paragraph. Many operations commonly used in
geophysics are illustrated in the program. The resulting program is less
than 200 lines of code. I will describe the algorithm and share the
prototype code.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;Spitz, S. (1999). Pattern recognition, spatial predictability, and
subtraction of multiple events. The Leading Edge, 18(1), 55-58. &lt;a class="reference external" href="http://dx.doi.org/10.1190/1.1438154"&gt;doi:
10.1190/1.1438154&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Karl Schleicher</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/prototyping-a-geophysical-algorithm-in-python.html</guid></item><item><title>Putting the v in IPython: vim-ipython and ipython-vimception</title><link>https://pyvideo.org/scipy-2014/putting-the-v-in-ipython-vim-ipython-and-ipython.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This talk will explain how to intimately integrate IPython with your
favorite text editor, as well as how to customize the IPython Notebook
interface to behave in a way that makes sense to &lt;em&gt;you&lt;/em&gt;. Though the
concrete examples are centered around the world-view of a particular
text editor, the content will be valuable to anyone wishing to extend
and customize IPython for their own purposes.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will cover two projects:
&lt;a class="reference external" href="https://github.com/ivanov/vim-ipython"&gt;vim-ipython&lt;/a&gt; (1) and
&lt;a class="reference external" href="https://github.com/ivanov/ipython-vimception"&gt;ipython-vimception&lt;/a&gt;
(2)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Most people think of IPython as an application - but much of it
is written as a library, making it possible to integrate with other
tools.&lt;/p&gt;
&lt;p&gt;vim-ipython is a Vim plugin that was first written during the sprints at
SciPy 2011 as a two-way interface between the Vim text editor and a
running IPython kernel. It turns vim into a frontend for IPython
kernels, like the qtconsole and the notebook interface. It allows you to
send lines or whole files for IPython to execute, and also get back
object introspection and word completions in Vim, like what you get
with: object?&lt;tt class="docutils literal"&gt;&amp;lt;enter&amp;gt;&lt;/tt&gt; and object.&lt;tt class="docutils literal"&gt;&amp;lt;tab&amp;gt;&lt;/tt&gt; in IPython. It
currently has over 430 star gazers on GitHub. Because vim-ipython simply
leverages much of existing IPython machinery, it allows users to
interact with non-Python kernels (such as IJulia and IHaskell) in the
same manner from the convenience of their favorite text editor. More
recently, vim-ipython has gained the ability to conveniently view and
edit IPython notebooks (.ipynb files) without a running an IPython
Notebook server.&lt;/p&gt;
&lt;p&gt;vim-ipython has a small and accessible code base (13 people have
contributed patches to the project), which has frequently made it &lt;em&gt;the&lt;/em&gt;
reference example for how to implement and utilize the IPython messaging
protocol that allows for the language-independent communication between
frontends and kernels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; The IPython Notebook user interface has become highly
customizable, and authoring code and content in the Notebook can be more
pleasant and productive experience if you take the time to make it
yours.&lt;/p&gt;
&lt;p&gt;IPython 2.0 brings a modal notion to the Notebook interface. There are
two modes: edit and mode command mode. In command mode, many single-key
keyboard shortcuts are available. For example, &lt;tt class="docutils literal"&gt;m&lt;/tt&gt; changes the current
cell type to Markdown, &lt;tt class="docutils literal"&gt;a&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;b&lt;/tt&gt; will insert a new cell above and
below the current one, and so on. Edit mode removes these single key
shortcuts so that new code and text can be typed in, but still retains a
few familiar shortcuts, such as &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;Ctrl-Enter&lt;/span&gt;&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;Alt-Enter&lt;/span&gt;&lt;/tt&gt;, and
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;Shift-Enter&lt;/span&gt;&lt;/tt&gt; for cell execution (with some nuanced differences).&lt;/p&gt;
&lt;p&gt;Part of the motivation behind the introduction of this modal interface
was that performing operations on notebook cells became a tedious and
awkward, as most operations required &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;Ctrl-m&lt;/span&gt;&lt;/tt&gt; to be typed too many
times. For example, inserting 3 cells involved
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;Ctrl-m&lt;/span&gt; a &lt;span class="pre"&gt;Ctrl-m&lt;/span&gt; a &lt;span class="pre"&gt;Ctrl-m&lt;/span&gt; a&lt;/tt&gt;, whereas now it's just &lt;tt class="docutils literal"&gt;aaa&lt;/tt&gt; in Command
mode. But the other major reason for the modal refactor was to make it
possible to add and remove shortcuts. For example, a user who finds it
annoying that &lt;tt class="docutils literal"&gt;a&lt;/tt&gt; stands for &amp;quot;insert above&amp;quot; and &lt;tt class="docutils literal"&gt;b&lt;/tt&gt; for &amp;quot;insert
below&amp;quot; and thinks that &lt;tt class="docutils literal"&gt;a&lt;/tt&gt; for &amp;quot;insert after&amp;quot; and &lt;tt class="docutils literal"&gt;b&lt;/tt&gt; for &amp;quot;insert
before&amp;quot; makes more sense will now be able to make that change for
herself.&lt;/p&gt;
&lt;p&gt;Some of the keyboard shortcuts in command mode are already vi-like
(&lt;tt class="docutils literal"&gt;j&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;k&lt;/tt&gt; to move up and down between cells) but many are not,
and a few are confusingly placed. ipython-vimception aims to be a
reference implementation for how to perform shortcut and user interface
customization in the notebook. In particular, along with vim-ipython's
new ability to edit .ipynb files, ipython-vimception addresses the
concerns of many die-hard vim aficionados. Many of them have otherwise
shied away form the notebook interface as it offends their sensibilities
for how text editing and document manipulation should be done. However,
with the new customizable shortcut system in IPython, along with a vim
emulation mode in cell text input areas, they finally will have a way to
stay productive without having to change their ways.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Paul Ivanov</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/putting-the-v-in-ipython-vim-ipython-and-ipython.html</guid></item><item><title>pySI A Python Framework for Spatial Interaction Modelling</title><link>https://pyvideo.org/scipy-2014/pysi-a-python-framework-for-spatial-interaction.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Spatial Interaction Modelling is a method for calibrating parameters for
components within a system of flows, such as migration or trade, and
then using those parameters to estimate new flows. Despite their
popularity, a unified Python framework to employ them does not exist. In
response, pySI was created as a coherent tool for calibrating models and
simulating flows for a variety of models.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Functions from libraries such as scipy.optimize, scipy.spatial,
statsmodels, and numdifftools comprise the core of the pySI.calibrate
routines, which are automatically constructed depending upon the
specified model inputs. As a result, the user can focus on identifying
different flow systems and understanding the associated spatial
processes, rather than the algorithmic divergences which emerge between
different models. After calibration is completed, the estimated
parameters and their diagnostic statistics can be reported in a uniform
fashion. Using functions within pySI.simulate, the parameter estimates
can act as inputs in order to predict new flows. More recently developed
models, which do not require input parameters, are also made available,
allowing comparisons amongst results from differing conceptual
formulations. Finally, results may be visualized with plots and networks
via matplotlib, igraph, and networkx. Overall, the pySI framework will
increase the accessibility of spatial interaction modelling while also
serving as a tool which can help new users understand the associated
methodological intricacies.&lt;/p&gt;
&lt;p&gt;Within this presentation, the concept of spatial interaction and a few
key modelling terms will first be introduced, along with several example
applications. Next, two traditional techniques for calibrating spatial
interaction models, Poisson generalized linear regression and direct
maximum likelihood estimation will be contrasted. It will then be
demonstrated how this new framework will allow users to execute either
form of calibration using identical input variables, which are based
upon a pandas DataFrame specification, without any significant
mathematical or statistical training. Results from two different
conceptual models will be compared to illustrate how pySI can be used to
explore different methods and models of spatial interaction.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taylor Oshan</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/pysi-a-python-framework-for-spatial-interaction.html</guid></item><item><title>Python Beyond CPython: Adventures in Software Distribution</title><link>https://pyvideo.org/scipy-2014/python-beyond-cpython-adventures-in-software-dis.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nick Coghlan</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/python-beyond-cpython-adventures-in-software-dis.html</guid></item><item><title>Python for economists (and other social scientists!)</title><link>https://pyvideo.org/scipy-2014/python-for-economists-and-other-social-scientist.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;I have developed a curriculum for a three part, graduate level course on
computational methods designed to increase the exposure of graduate
students and researchers to basic techniques used in computational
modeling and simulation using the Python programming language.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Together with theory and experimentation, computational modeling and
simulation has become a &amp;quot;third pillar&amp;quot; of scientific enquiry. I am
developing a curriculum for a three part, graduate level course on
computational methods designed to increase the exposure of graduate
students and researchers in the College of Humanities and Social
Sciences at the University of Edinburgh to basic techniques used in
computational modeling and simulation using the Python programming
language. My course requires no prior knowledge or experience with
computer programming or software development and all current and future
course materials will be made freely available online via GitHub.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Pugh</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/python-for-economists-and-other-social-scientist.html</guid></item><item><title>Reflexive Data Science on SciPy Communities</title><link>https://pyvideo.org/scipy-2014/reflexive-data-science-on-scipy-communities.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;I present tools for collecting data generated by Scientific Python
community development infrastructure (mailing list archives, pull
requests, issue trackers) and analyzing it with Pandas and NetworkX.
Showing preliminery results using social network analysis and complex
systems modeling, I demonstrate using reflexive data science to enrich
our understanding of open source development.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="section" id="background-motivation"&gt;
&lt;h4&gt;Background/Motivation&lt;/h4&gt;
&lt;p&gt;The Scientific Python community's contributions to greater scientific
understanding have been underappreciated by academic institutions. One
reason for this is that software engineering is widely misunderstood and
not recognized as research work in its own right, as opposed to paper
publication and patents. A better understanding of the open source
software development process itself will help academic institutions
recognize the contributions of open source developers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="methods"&gt;
&lt;h4&gt;Methods&lt;/h4&gt;
&lt;p&gt;I collect historical data from development of Scientific Python projects
and render these into formats suitable for analysis using SciPy tools.
To demonstrate the potential of this work, I will show two ways of
analyzing this data scientifically: as a self-excited Hawkes process
exibiting shock behavior, and as information diffusion over a social
network.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h4&gt;Results&lt;/h4&gt;
&lt;p&gt;The purpose of this talk is twofold.&lt;/p&gt;
&lt;p&gt;First, to introduce tools and techniques for turning data from open
source software production into scientific data suitable for analysis.
This talk proposes that there's an opportunity for SciPy to engage in
&lt;em&gt;reflexive data science&lt;/em&gt;, using its own data to learn more about how it
functions and how to operate more efficiently.&lt;/p&gt;
&lt;p&gt;Second, this talk will present visualizations of the data based on
complex systems research and social network analysis. Building on prior
work, these results will focus on the role of productive bursts in
communications. Drawing on social network analysis and prior work on
roles in Usenet communities and open source communities, this talk will
provide historical insight into the interaction between SciPy
communities.&lt;/p&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sebastian Benthall</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/reflexive-data-science-on-scipy-communities.html</guid></item><item><title>Scientific Knowledge Management with Web of Trails</title><link>https://pyvideo.org/scipy-2014/scientific-knowledge-management-with-web-of-trail.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Do you hate repeating yourself? Want to know when your publication is
repeating someone else? The Web of Trails project is a solution to
knowledge management that empowers users to quickly find repetition of
key phrases. Using syntactic indexing, as opposed to lexical techniques,
this approach is capable of representing the literature using less space
while providing high value results.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Web of Trails (WOT) is an open source project that uses context-free
grammars (CFG's) as the basic building block for search. Current search
technology relies upon the presence of words on a page, sometimes
augmented with statistical correlations among words. Even with these
restrictions, maintenance of an index requires storage much greater than
the input size (a polynomial function of it). CFG's have been used for
decades in compilation and language tools, and more recently in data
compression.&lt;/p&gt;
&lt;p&gt;The primary advantage of this CFG approach, based upon the Sequitur
algorithm, is that it indexes content in linear-space, not
polynomial-space. The secondary advantage is that combined with research
in inference, grammars can express human concepts and connections rather
than just correlations. This project uses grammar and syntactic analysis
to replace lexical and word-based approaches to the problem of searching
collections of digital artifacts. Benchmarking in web content indexing
will be shown relative to popular alternatives such Apache Lucene and
Amazon Cloud Search.&lt;/p&gt;
&lt;p&gt;In addition to implementing content indexing with Sequitur, this project
will enable domain-specific extensions of WOT. Once complete, we will
research novel techniques for generalizing the grammars inferred by
Sequitur. As this fundamental research develops, it will inform later
framework development and increase search precision. This is a big leap
in the state of the art, as text artifacts are no longer represented as
bags of words, but as bags on non-terminals in a growing and adapting
grammar.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jon Riehl</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/scientific-knowledge-management-with-web-of-trail.html</guid></item><item><title>Spatial-Temporal Prediction of Climate Change Impacts using pyimpute, scikit learn and GDAL</title><link>https://pyvideo.org/scipy-2014/spatial-temporal-prediction-of-climate-change-imp.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;In this talk, I019ll show how we apply climate change models to predict
shifts in agricultural zones across the western US. I will outline the
use of the pyimpute, GDAL and scikit-klearn to perform supervised
classification; training a model using current climatic conditions to
predict spatially-explicit zones under future climate scenarios.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As the field of climate modeling continues to mature, we must anticipate
the practical implications of the climatic shifts predicted by these
models. In this talk, I'll show how we apply the results of climate
change models to predict shifts in agricultural zones across the western
US. I will outline the use of the Geospatial Data Abstraction Library
(&lt;a class="reference external" href="http://www.gdal.org/"&gt;GDAL&lt;/a&gt;) and Scikit-Learn
(&lt;a class="reference external" href="http://scikit-learn.org/"&gt;sklearn&lt;/a&gt;) to perform supervised
classification, training the model using current climatic conditions and
predicting the zones as spatially-explicit raster surfaces across a
range of future climate scenarios. Finally, I'll present a python module
(&lt;a class="reference external" href="https://github.com/perrygeo/pyimpute"&gt;pyimpute&lt;/a&gt;) which provides an
API to optimize and streamline the process of spatial classification and
regression problems.&lt;/p&gt;
&lt;div class="section" id="outline"&gt;
&lt;h4&gt;Outline&lt;/h4&gt;
&lt;p&gt;This talk will consist of four parts:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A brief overview of climate data and the concept of agro-ecological
zones&lt;/li&gt;
&lt;li&gt;The theory and intuition behind bioclimatic envelope modeling using
supervised classification&lt;/li&gt;
&lt;li&gt;Visualization and interpretation of our results&lt;/li&gt;
&lt;li&gt;Detailed demonstration of the pyimpute/GDAL/sklearn workflow&lt;ul&gt;
&lt;li&gt;Loading spatial data into numpy arrays&lt;/li&gt;
&lt;li&gt;Random stratified sampling&lt;/li&gt;
&lt;li&gt;Training, assessing and selecting the sklearn classifier&lt;/li&gt;
&lt;li&gt;Prediction of zones given future climate data as explanatory
variables&lt;/li&gt;
&lt;li&gt;Quantifying and interpreting uncertainty&lt;/li&gt;
&lt;li&gt;Writing results to spatial data formats&lt;/li&gt;
&lt;li&gt;Discussion of performance and memory limitations&lt;/li&gt;
&lt;li&gt;Visualizing and interacting with the results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matthew Perry</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/spatial-temporal-prediction-of-climate-change-imp.html</guid><category>Tech</category></item><item><title>Synthesis and analysis of circuits in the IPython notebook</title><link>https://pyvideo.org/scipy-2014/synthesis-and-analysis-of-circuits-in-the-ipython.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Building on the new IPython 2.0 widget model and the jsPlumb package we
create a schematic capture tool that allows graphically editing a
circuit as well as its components' parameters and then instantly
updating a domain specific modeling backend. This allows for an
integrated circuit modeling workflow and to extend widget-based user
interfaces for engineering and research projects.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Circuits, i.e., a network of interconnected components with ports, have
found application in various scientific and engineering domains, ranging
from applications close to the physical implementation, such as
electrical circuits, photonic circuits for optical information
processing, superconducting quantum circuits for quantum information
applications to more abstract circuit representations of dynamical
systems, biological processes or even software algorithms.&lt;/p&gt;
&lt;p&gt;This has already led to the development of quite general
domain-independent circuit modeling toolkits such as
&lt;a class="reference external" href="https://www.modelica.org/"&gt;Modelica&lt;/a&gt;, but to date, there exist very
few open source graphical general circuit editing environments that can
be tightly integrated with custom, domain-specific implementation
simulation or analysis backends as well as
&lt;a class="reference external" href="http://ipython.org"&gt;IPython&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we present our first attempt at creating such a tool as well as
some applications from our own research on nano-photonic quantum circuit
models. Our existing &lt;a class="reference external" href="http://mabuchilab.github.io/QNET/"&gt;QNET&lt;/a&gt;
software package allows to model these circuits in a purely symbolic
fashion and interfaces with various codes for numerical simulation.&lt;/p&gt;
&lt;p&gt;We demonstrate that the extension of our package with a visual circuit
editor leads to a rich integrated simulation and analysis workflow in
which an engineer or researcher can receive very fast feedback when
making changes to his model.&lt;/p&gt;
&lt;p&gt;As a consequence, it is much easier to build intuition for the
particular kinds of circuit models and find novel and creative solutions
to an engineering task.&lt;/p&gt;
&lt;p&gt;Finally, given the broad range of applications for circuit models and
representations, we outline how our visual circuit editor can be adapted
to export a circuit for interfacing with other domain specific software
such as Modelica.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nikolas Tezak</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/synthesis-and-analysis-of-circuits-in-the-ipython.html</guid></item><item><title>The PlaceIQ Location Based Analytic Platform</title><link>https://pyvideo.org/scipy-2014/the-placeiq-location-based-analytic-platform.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;PlaceIQ's patented platform analyzes half a trillion diverse data points
about location, time, and real-world behavior to define human audiences
and allow businesses to understand consumers at scale. It ingests large
volumes of mobile activity data and geographic data, calling for
creative use machine learning techniques to enable the high-fidelity
abstractions insightful to businesses.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The surge in mobile device adoption and the subsequent abundance of
time-stamped location data have given rise to possibilities and interest
in understanding movement-based human behavior. The PlaceIQ analytic
platform is a large-scale data analysis system that addresses this
demand, providing a large-scale, flexible, and reliable platform created
around the concepts of location and audience. The platform's data
processing piece ingests large volumes of mobile activity data daily and
overlays them onto geospatial data. These data include: the
discretization of the U.S. into 1 billion 100 meter by 100 m tiles; more
than 400,000 proprietary polygons delineating the shapes of properties
and businesses; business listings and census data; and terabytes of
mobile activity data. We discuss here the methodologies - namely DBSCAN
clustering and kd-trees - used to de-dupe disparate geodata sources and
evaluate the quality of noisy activity data at scale. The resulting
overlays of people, places, and time create high-fidelity abstractions
and manipulations insightful to businesses, particularly in the mobile
advertising domain.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eliza Chang</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/the-placeiq-location-based-analytic-platform.html</guid></item><item><title>TracPy: Wrapping the FORTRAN Lagrangian trajectory model TRACMASS</title><link>https://pyvideo.org/scipy-2014/tracpy-wrapping-the-fortran-lagrangian-trajector.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;An example of a Python wrapper of a FORTRAN code, applications of a
Lagrangian trajectory model, and lessons learned about code development.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Numerical Lagrangian tracking is a way to follow parcels of fluid as
they are advected by a numerical circulation model. This is a natural
method to investigate transport in a system and understand the physics
on the wide range of length scales that are actually experienced by a
drifter. TRACMASS is a tool for Lagrangian trajectory modeling that has
been developed over the past two decades. It has been used to better
understand physics and its applications to real-world problems in many
areas around the world, in both atmospheric and oceanic settings.
TRACMASS is written in FORTRAN, which is great for speed but not as
great for ease of use. This code has been wrapped in Python to run
batches of simulations and improve accessibility --- and dubbed TracPy.&lt;/p&gt;
&lt;p&gt;In this talk, I will outline some of the interesting features of the
TRACMASS algorithm and several applications, then discuss the layout of
the TracPy code. The code setup and organization have been a learning
process and I will also share some of my hard-earned lessons.&lt;/p&gt;
&lt;p&gt;TracPy is continually in development and is available &lt;a class="reference external" href="https://github.com/kthyng/tracpy"&gt;on
GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristen Thyng</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-13:scipy-2014/tracpy-wrapping-the-fortran-lagrangian-trajector.html</guid></item><item><title>Advanced 3D Seismic Visualizations in Python</title><link>https://pyvideo.org/scipy-2014/advanced-3d-seismic-visualizations-in-python.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;3D reflection seismic data collected as a part of the NanTroSEIZE
project revealed complex interactions between active sedimentation and
tectonics in the Nankai Trough, Japan. We implemented co-rendering of
multiple attributes and stratal slicing in python to better visualize
the structural and stratigraphic relationships within the piggyback
slope basins of the accretionary prism.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;3D reflection seismic data acquired &lt;a class="reference external" href="http://www.geology.wisc.edu/~jkington/scipy2014/LocationMap.png"&gt;offshore of southeast
Japan&lt;/a&gt;
as part of the Nankai Trough Seismogenic Zone Experiment (NanTroSEIZE)
provides a unique opportunity to study active accretionary prism
processes. The 3D seismic volume revealed complex interactions between
active sedimentation and tectonics within &lt;a class="reference external" href="http://www.geology.wisc.edu/~jkington/scipy2014/inline_2695_w_interp_brown_seismic.png"&gt;multiple slope
basins&lt;/a&gt;
above the accretionary prism. However, our ability to understand these
interactions was hindered without access to expensive specialized
software packages.&lt;/p&gt;
&lt;p&gt;We implemented stratal slicing of the 3D volume and co-rendering of
multiple attributes in python to better visualize our results. Stratal
slicing allows volumetric attributes to be displayed &lt;a class="reference external" href="http://www.geology.wisc.edu/~jkington/scipy2014/stratal_slicing_animation.gif"&gt;in map view along
an arbitrary geologic
timeline&lt;/a&gt;(~30MB
animated gif) by interpolating between interpreted geologic surfaces.
This enhances the visibility of subtle changes in stratigraphic
architecture through time. Co-rendering coherence on top of seismic
amplitudes facilitates fault interpretation in both cross section and
map view. This technique allowed us to &lt;a class="reference external" href="http://www.geology.wisc.edu/~jkington/scipy2014/ContemporaneousStrikeSlipAndNormalFaults.png"&gt;confidently interpret
faults&lt;/a&gt;
near the limit of seismic resolution.&lt;/p&gt;
&lt;p&gt;The scientific python ecosystem proved to be an effective platform both
for making publication-quality cross sections and for rapidly
implementing state-of-the-art seismic visualization techniques. We
created &lt;a class="reference external" href="http://www.geology.wisc.edu/~jkington/scipy2014/Basin_uplift.png"&gt;publication quality cross
sections&lt;/a&gt;
(some annotations added in Inkscape) and interactive 2D visualizations
in &lt;tt class="docutils literal"&gt;matplotlib&lt;/tt&gt;. For 3D display of seismic volumes we used &lt;tt class="docutils literal"&gt;mayavi&lt;/tt&gt;
to easily create interactive scenes. &lt;tt class="docutils literal"&gt;scipy.ndimage&lt;/tt&gt; provided most of
the underlying image processing capability and allowed us to preform
memory-efficient operations on &amp;gt;10GB arrays.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joe Kington</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/advanced-3d-seismic-visualizations-in-python.html</guid></item><item><title>Behind the Scenes of the University and Supplier Relationship</title><link>https://pyvideo.org/scipy-2014/behind-the-scenes-of-the-university-and-supplier.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The University of California, Berkeley and San Francisco combined are
one of the largest buyers in the Bay Area. Historically, it has been a
time-consuming process to analyze suppliers' proposed price files and
ensure the University is not paying more than contracted. Through the
use of Pandas and Python, this once tedious and manual process can
routinely be done in a matter of a few seconds.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For the University of California, Berkeley and San Francisco, a routine
management process of supplier price files used to be a time-consuming
process. It is essential to analyze the sometimes tens of thousands of
items a supplier offers to make sure the University doesn't accept
larger price increases than is in compliance with a contract. A
historical figure of past purchases is matched against the current and
proposed catalogs and then analyzed to ultimately find out the
percentage increase and number of products removed. Each Universities'
motivation is to not accept a file that has larger price increases than
contracted nor a file with several previously purchased products
removed.&lt;/p&gt;
&lt;p&gt;To combat the tedious and time-consuming process of manually analyzing
the previous spend with the current and proposed files, a Python script
was written. This heavily uses Pandas as well as Numpy for computations.
The code uploads all three files as a dataframes and creates a common
variable to compare similar products. It matches what was previously
purchased to the identical products in the current and proposed
catalogs. After filtering any 'bad' input that would skew the results,
several values are computed and the code outputs the necessary figures
to determine if a supplier's price file is acceptable. The code even
documents each catalog result automatically so the historical changes
are organized and noted in a csv.&lt;/p&gt;
&lt;p&gt;This code is an exponential improvement to the manual process that was
historically done. The end numbers are known in a matter of seconds as
opposed to hours of Excel or Access analysis. For some suppliers, Excel
is even incapable of uploading the entire catalog, thus making any
analysis nearly impossible. Python and Pandas have not only made the
analysts time more efficient but have opened the door for several
possibilities.&lt;/p&gt;
&lt;p&gt;Although this code has greatly improved this continuous analysis, more
advanced techniques could potentially improve the process. The
department soon hopes to use a forecasted spend figure rather than a
historical snapshot to project spend against the proposed catalog.
Moving forward, having the analysts armed with Python knowledge,
Strategic Sourcing hopes to yield more meaning through the daily flow of
spend data through machine learning techniques.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alexis Perez</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/behind-the-scenes-of-the-university-and-supplier.html</guid></item><item><title>Blaze: Building a Foundation for Array-Oriented Computing in Python</title><link>https://pyvideo.org/scipy-2014/blaze-building-a-foundation-for-array-oriented-c.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The Blaze project is a collection of libraries being built towards the
goal of generalizing NumPy's data model and working on distributed data.
This talk covers each of these libraries, and how they work together to
accomplish this goal.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python's scientific computing and data analysis ecosystem, built around
NumPy, SciPy, Matplotlib, Pandas, and a host of other libraries, is a
tremendous success. NumPy provides an array object, the array-oriented
ufunc primitive, and standard practices for exposing and writing
numerical libraries to Python all of which have assisted in making it a
solid foundation for the community. Over time, however, it has become
clear that there are some limitations of NumPy that are difficult to
address via evolution from within. Notably, the way NumPy arrays are
restricted to data with regularly strided memory structure on a single
machine is not easy to change.&lt;/p&gt;
&lt;p&gt;Blaze is a project being built with the goal of addressing these
limitations, and becoming a foundation to grow Python's success in
array-oriented computing long into the future. It consists of a small
collection of libraries being built to generalize NumPy's notions of
array, dtype, and ufuncs to be more extensible, and to represent data
and computation that is distributed or does not fit in main memory.&lt;/p&gt;
&lt;p&gt;Datashape is the array type system that describes the structure of data,
including a specification of a grammar and set of basic types, and a
library for working with them. LibDyND is an in-memory array programming
library, written in C++ and exposed to Python to provide the local
representation of memory supporting the datashape array types. BLZ is a
chunked column-oriented persistence storage format for storing Blaze
data, well-suited for out of core computations. Finally, the Blaze
library ties these components together with a deferred execution graph
and execution engine, which can analyze desired computations together
with the location and size of input data, and carry out an execution
plan in memory, out of core, or in a distributed fashion as is needed.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mark Wiebe</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/blaze-building-a-foundation-for-array-oriented-c.html</guid></item><item><title>Building petabyte-scale comparative genomics pipelines</title><link>https://pyvideo.org/scipy-2014/building-petabyte-scale-comparative-genomics-pipe.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This talk will educate the audience about Python tools and best
practices for creating reproducible petabyte-scale pipelines. This is
done within the context of demonstrating a new grammar-based approach to
comparative genomics. The genome grammars are produced using public data
from the National Institutes of Health, streamed over a high-throughput
Internet2 connection to Amazon Web Services.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We introduce a high-performance, open-source application written in
Python that models genomic data with a context-free grammar (CFG), a
construct from formal language theory. This approach is intended to
advance fundamental science by delivering a more extensive model of the
genetic interaction of diseases. Current comparative models treat
genomic sequences as strings, and recent advances are little more than
optimizations of the &amp;quot;grep approach&amp;quot;. However a genome is a grammar: it
is parsed, follows rules, and has an inherent hierarchical structure.
Understanding the structure and rules of this implied grammar are
essential for mapping loci to diseases when those loci are distributed
across genomic regions.&lt;/p&gt;
&lt;p&gt;To produce the CFGs, we have implemented the Sequitur algorithm to run
on the AWS Elastic MapReduce platform. This application is written in
Python and uses the following packages: MRjob, boto, and pandas. This is
a petascale computing pipeline that is successful because it uses
inherently scalable services and is able to take advantage of the 100G
Internet2 connection between Amazon Web Services and the National
Institutes of Health (NIH). This architecture delivers unprecedented
transfer speeds and relatively low latency.&lt;/p&gt;
&lt;p&gt;We discuss the advantages of this architecture, especially for groups
without comparable local resources. In reviewing the results of our
computation, we not only look at methods to measure the utility of our
CFG models, but also the computational advantages of this approach. Just
like the fastest alignment algorithms, this complex approach still
operates within linear-space. In addition, future pairwise comparisons
are faster because our CFGs act as a compressed representation of the
raw sequence data. Our hope is that this CFG approach is further tested
as a replacement for raw sequence analysis. In addition, we hope that
our bioinformatics pipeline serves as an example for the SciPy community
on how to perform large computations across the many petabytes made
available by NIH.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Cope</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/building-petabyte-scale-comparative-genomics-pipe.html</guid></item><item><title>Clustering of high content images to discover off target phenotypes</title><link>https://pyvideo.org/scipy-2014/clustering-of-high-content-images-to-discover-off.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;In high content imaging screens, cells are subjected to various
treatments (usually shutting down specific genes) in high throughput,
imaged, and a phenotype of interest measured. We argue that there is a
wealth of information to be found in off-target phenotypes, and present
an image clustering approach to discover these and infer gene function.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In the decade between 1999 and 2008, more newly-approved, first-in-class
drugs were found by phenotypic screens than by molecular target-based
approaches. This is despite far more resources being invested in the
latter, and highlights the rising importance of screens in biomedical
research. (&lt;a class="reference external" href="http://www.nature.com/nrd/journal/v10/n7/full/nrd3480.html"&gt;Swinney and Anthony, Nat Rev Drug Discov,
2011&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Despite this success, the data from phenotypic screens is vastly
underutilized. A typical analysis takes millions of images, obtained at
a cost of, say, $250,000, and reduces each to a single number, a
quantification of the phenotype of interest. The images are then ranked
by that value and the top-ranked images are flagged for further
investigation. (&lt;a class="reference external" href="https://www.cell.com/trends/biotechnology/abstract/S0167-7799(10)00035-1"&gt;Zanella et al, Trends Biotech,
2010&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The images, however, contain a lot more information than just a single
phenotypic number. For one, usually only the mean phenotype of all the
cells in the image is reported, with no information about variability,
even though the distribution of cell shapes in a single image is highly
informative (&lt;a class="reference external" href="http://www.nature.com/ncb/journal/v15/n7/full/ncb2764.html"&gt;Yin et al, Nat Cell Biol,
2013&lt;/a&gt;).
Additionally, cells display a variety of off-target phenotypes,
independently of the target, that can provide biological insight and new
research avenues.&lt;/p&gt;
&lt;p&gt;We are developing an unsupervised clustering pipeline, tentatively named
high-content-screen unsupervised sample clustering
(&lt;a class="reference external" href="http://github.com/jni/husc"&gt;HUSC&lt;/a&gt;), that leverages the scientific
Python stack, particularly &lt;tt class="docutils literal"&gt;scipy.stats&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;pandas&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;scikit-image&lt;/span&gt;&lt;/tt&gt;, and &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;scikit-learn&lt;/span&gt;&lt;/tt&gt;, to summarize images with feature
vectors, cluster them, and infer the functions of genes corresponding to
each cluster. The library includes functions for preprocessing images,
computing an array of features designed specifically for microscopy
images, and accessing a MongoDB database containing sample data. Its API
allows easy extensibility by placing screen-specific functions under the
&lt;tt class="docutils literal"&gt;screens&lt;/tt&gt; sub-package. An example IPython notebook with a preliminary
analysis can be found
&lt;a class="reference external" href="http://jni.github.io/notebooks/hcs_nb.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We plan to use this library to develop a flexible web interface for
flexible and extensible analysis of high-content screens, and relish the
opportunity to enlist the help and expertise of the SciPy crowd.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/clustering-of-high-content-images-to-discover-off.html</guid></item><item><title>Lightning Talks | SciPy 2014 | July 9, 2014</title><link>https://pyvideo.org/scipy-2014/lightning-talks-scipy-2014-july-9-2014.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Various speakers</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/lightning-talks-scipy-2014-july-9-2014.html</guid><category>lightning talks</category></item><item><title>scikit-bio: core bioinformatics data structures and algorithms in Python</title><link>https://pyvideo.org/scipy-2014/scikit-bio-core-bioinformatics-data-structures-a.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;We present scikit-bio, a library based on the Python scientific
computing stack implementing core bioinformatics data structures,
algorithms and parsers. scikit-bio is useful for students in
bioinformatics, who can learn topics such as iterative progressive
multiple sequence alignment from the source code and accompanying
documentation, and for real-world bioinformatics applications
developers.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python is widely used in computational biology, with many high profile
bioinformatics software projects, such as
&lt;a class="reference external" href="http://galaxyproject.org/"&gt;Galaxy&lt;/a&gt;,
&lt;a class="reference external" href="http://khmer.readthedocs.org/en/latest/"&gt;Khmer&lt;/a&gt; and
&lt;a class="reference external" href="http://www.qiime.org"&gt;QIIME&lt;/a&gt;, being largely or entirely written in
Python. We present &lt;a class="reference external" href="http://www.scikit-bio.org"&gt;scikit-bio&lt;/a&gt;, a new
library based on the standard Python scientific computing stack (e.g.,
numpy, scipy, and matplotlib) implementing core bioinformatics data
structures, algorithms, parsers, and formatters. scikit-bio is the first
bioinformatics-centric &lt;a class="reference external" href="https://scikits.appspot.com/"&gt;scikit&lt;/a&gt;, and
arises from over ten years of development efforts on
&lt;a class="reference external" href="http://www.pycogent.org"&gt;PyCogent&lt;/a&gt; and
&lt;a class="reference external" href="http://www.qiime.org"&gt;QIIME&lt;/a&gt;, representing an effort to update the
functionality provided by these extensively used tools, and to make that
functionality more accessible. scikit-bio is intended to be useful both
as a resource for students, who can learn topics such as heuristic-based
sequence database searching or iterative progressive multiple sequence
alignment from the source code and accompanying documentation, and as a
powerful library for 'real-world' bioinformatics developers. To achieve
these goals, scikit-bio development is centered around test-driven,
peer-reviewed software development; C/Cython integration for
computationally expensive algorithms; extensive API documentation and
doc-testing based on the &lt;a class="reference external" href="https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt"&gt;numpy docstring
standards&lt;/a&gt;;
user documentation and &lt;a class="reference external" href="http://caporasolab.us/An-Introduction-To-Applied-Bioinformatics/"&gt;theoretical discussion of topics in IPython
Notebooks&lt;/a&gt;;
adherence to PEP8; and continuous integration testing. scikit-bio is
available free of charge under the BSD license.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J Gregory Caporaso</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/scikit-bio-core-bioinformatics-data-structures-a.html</guid></item><item><title>Software Carpentry: Lessons Learned</title><link>https://pyvideo.org/scipy-2014/software-carpentry-lessons-learned-0.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr. Greg Wilson</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/software-carpentry-lessons-learned-0.html</guid></item><item><title>Teaching Python to undergraduate students</title><link>https://pyvideo.org/scipy-2014/teaching-python-to-undergraduate-students.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Teaching undergraduate students in programming is interesting and
challenging at the same time, because one has to deal mostly with two
types: Those who have already experience and those who have not. I will
present two models from Bonn University for Physics students with now
much more responsibility for the tutors and would like to initiate
discussions about different systems all over the world.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Teaching undergraduate students in programming languages like Python is
interesting and challenging at the same time. You have to deal mostly
with two types of students: Those who have already some experience in
programming (not neccessarily Python), e.g. from high school, and those
who have not. At Bonn University we have recently changed the structure
of such a course for Bachelor of Science in Physics students. First, we
include the Python tutorial in a lecture in the first term instead of a
voluntary course in the lecture-free time before the fourth semester,
where the &amp;quot;Numerical Methods for Physicists&amp;quot; course takes place. Second,
instead of a weekly lecture, in which the topics are explained in
detail, and a 2 hours exercise class, the new system provides only one
introduction lecture per topic but a 3 hours exercise class per week. So
especially the tutors are much more responsible for the success of the
students in the final report of this course. Furthermore, as a student
representative and also tutor for both courses, I have been heavily
involved in this process.&lt;/p&gt;
&lt;p&gt;I like to initiate a larger discussion about how to teach programming to
undergraduate students, especially because in the last decades
programming got more and more important in science and due to e.g. the
Bologna reform in Europe, it should be easier to change between
universities after e.g. the Bachelor program.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dominik Klaes</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/teaching-python-to-undergraduate-students.html</guid></item><item><title>The Berkeley Institute for Data Science a place for people like us</title><link>https://pyvideo.org/scipy-2014/the-berkeley-institute-for-data-science-a-place-f.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;I will describe the new Berkeley Institute for Data Science (BIDS), part
of a collaboration with UW and NYU funded by the Moore and Sloan
Foundations. It will be a space for the open and interdisciplinary work
that is typical of the SciPy community. In the creation of BIDS, the
role of open source scientific tools for Data Science, and specifically
the SciPy ecosystem, played an important role.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In 2013, the Gordon and Betty Moore and the Alfred P. Sloan foundations
&lt;a class="reference external" href="http://www.moore.org/programs/science/data-driven-discovery/data-science-environments"&gt;awarded&lt;/a&gt;
UC Berkeley, U. Washington and NYU for a collaborative, $38M in support
of a 5-year initiative to create novel environments for Data Science.
This project was driven by the recognition that computing and data
analysis have now become the backbone of all scientific research, and
yet the teams, collaborations and individuals that make this possible
typically encounter significant barriers in today's academic
environments.&lt;/p&gt;
&lt;p&gt;The SciPy community is one of the poster children of this issue: many of
our members live &amp;quot;officially&amp;quot; in traditional, discipline-oriented
scientific research, and yet we have committed time and effort to
creating an open ecosystem of tools for research. As we all know, this
is often done with little support from the standard incentive structures
of science, be it publication venues, funding agencies or hiring, tenure
and promotion committees.&lt;/p&gt;
&lt;p&gt;The launch of this initiative is an important moment, as it signals the
recognition of this problem by important and well-respected foundations
in science. At UC Berkeley, we took this opportunity to create the new
&lt;a class="reference external" href="http://vcresearch.berkeley.edu/datascience/bids-launch-dec-12"&gt;Berkeley Institute for Data
Science&lt;/a&gt;.
In this effort, the open source tools of the SciPy community will play a
central role.&lt;/p&gt;
&lt;p&gt;In this talk, I will describe the larger context in which this
initiative has been created, as well as the scientific scope of our
team, our goals, and the opportunities that we will try to provide with
this space. We expect that this new institute, together with our
partners at UW and NYU, will play an important role in support of the
great work of the SciPy ecosystem.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fernando Prez</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/the-berkeley-institute-for-data-science-a-place-f.html</guid></item><item><title>The Road to Modelr: Building a Commercial Web Application on an Open Source Foundation</title><link>https://pyvideo.org/scipy-2014/the-road-to-modelr-building-a-commercial-web-app.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Lessons learned along the bumpy road from Python noob to an open source
geophysics web application, with a commercial web service front end.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Software for applied geoscientists in the petroleum industry is usually
expensive, hard to use, Windows or Linux only, and slow to evolve.
Furthermore, it is almost always stridently proprietary and therefore
black-box. Open source software is rare. There are few developers
working outside of seismic processing and enterprise database
development, and consequently there is very little in the web and mobile
domain. Reconciling a commitment to open source with a desire to earn a
good living is one of the great conundrums of software engineering. We
have chosen a hybrid approach of open core (like OpendTect, which has
proprietary add-ons) and software-as-a-service (like WordPress.org vs
WordPress.com).&lt;/p&gt;
&lt;div class="section" id="open-source-back-end"&gt;
&lt;h4&gt;Open source back-end&lt;/h4&gt;
&lt;p&gt;Our open core is a Python web app for producing synthetic seismic
models, in much the same way that the now-deprecated &lt;a class="reference external" href="https://developers.google.com/chart/image/"&gt;Google Image
Charts API&lt;/a&gt; used to work:
the user provides a URL, which contains all the relevant data, and a
JPEG image generated by matplotlib is returned. Along with the image, we
return some computed data about the model, such as the elastic
properties of the rocks involved. The mode of the tool is described by
&amp;quot;scripts&amp;quot;, which for now reside on the server, but which we plan to
allow users to provide as part of the API. Scripts have various
parameters, such as the P-wave and S-wave velocities, and the bulk
density of the rocks in the model, and it is these parameters that make
up most of the data in the API call. Other parameters include the type
and frequency of wavelet to use, and the computation method for the
reflectivity (for example the Zoeppritz equations, or the Aki013Richards
approximation). The app has no user interface to speak of, only a web
API. It is licensed under the Apache 2 license and can be found &lt;a class="reference external" href="https://github.com/agile-geoscience/modelr"&gt;on
GitHub&lt;/a&gt;. We are running
an instance of our app on a &amp;quot;T1.micro&amp;quot; &lt;a class="reference external" href="http://aws.amazon.com/ec2/"&gt;Amazon EC2
instance&lt;/a&gt; running Ubuntu.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="proprietary-front-end"&gt;
&lt;h4&gt;Proprietary front-end&lt;/h4&gt;
&lt;p&gt;The commercial, proprietary front end is a Python web app that lives in
the &lt;a class="reference external" href="https://developers.google.com/appengine/"&gt;Google App Engine&lt;/a&gt;
walled garden. This app, which uses the &lt;a class="reference external" href="http://getbootstrap.com/"&gt;Twitter Bootstrap
framework&lt;/a&gt;, is serving at
&lt;a class="reference external" href="https://www.modelr.io/"&gt;modelr.io&lt;/a&gt; and provides a user object in
which a geoscientist can save rocks and scenarios consisting of a script
and all its parameters. We chose App Engine for its strong
infrastructure, good track record, and the easy availability of tools
like the datastore, memcache, login, and so on. We also host support
channels and materials through this front end, which has a very
lightweight &amp;quot;demo&amp;quot; mode, and otherwise requires a $9/month subscription
to use, handled by &lt;a class="reference external" href="https://stripe.com/ca"&gt;Stripe&lt;/a&gt;. This necessitated
serving both the front and back ends over HTTPS, something we wanted to
do anyway, because of industry mistrust of the cloud.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="summary"&gt;
&lt;h4&gt;Summary&lt;/h4&gt;
&lt;p&gt;Some of the things we picked up along the way:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;We started with a strong need of our own, so had clear milestones
from day 1.&lt;/li&gt;
&lt;li&gt;We left the project alone for months, but good documentation and
GitHub meant this was not a problem.&lt;/li&gt;
&lt;li&gt;Sprinting with a professional developer at the start meant less
thrashing later.&lt;/li&gt;
&lt;li&gt;The cloud landscape is exciting, but it's easy to be distracted by
all the APIs. Keeping it simple is a constant struggle.&lt;/li&gt;
&lt;li&gt;Pushing through Xeno's paradox to get to a live, public-facing app
took stamina and focus.&lt;/li&gt;
&lt;li&gt;There's nothing like having other users to get you to up your coding
game.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We hope that by telling this story of the early days of a commercial
scientific web application, built by a bunch of consulting scientists in
Nova Scotia, not a tech startup in San Francisco, we can speed others
along the path to creating a rich ecosystem of new geoscience tools and
web APIs.&lt;/p&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matt Hall</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/the-road-to-modelr-building-a-commercial-web-app.html</guid></item><item><title>Time Series Analysis for Network Security</title><link>https://pyvideo.org/scipy-2014/time-series-analysis-network-security.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Endgame seeks to develop products that allow customers to gain
visibility into their networks and discover anomalies. I will describe
how Endgame brings together various Python packages (scipy, pandas,
statsmodels, kairos, etc...) in order to collect, record, and then
analyze time series that are collected from network security data feeds.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, I will describe how Endgame has brought together many
different Python tools in order to solve the problem of detecting
outliers in network security data.&lt;/p&gt;
&lt;p&gt;The first step in this process is collecting and storing the metrics
that will form a time series. Here, I will describe how Endgame plugs
into the flow of network data and then stores that data. (Python
packages: elasticsearch, pyspark, kairos)&lt;/p&gt;
&lt;p&gt;The next step is applying a Fourier transform in order to classify time
series that exhibit daily and weekly patterns. This information is
especially useful in deciding how to characterize a time series's past
behavior and thus judge how unusual new data is. (Python package: numpy)&lt;/p&gt;
&lt;p&gt;Finally, exponentially weighted moving averages and standard deviations
are calculated in different ways depending on how the time series was
classified. For example, if strong daily patterns are present, the data
is stacked by daily time bin and moving averages are calculated within
each time bin. Corrections for weekend and weekday behavior are also
applied if necessary. Autoregressive moving average models are also used
and the performance of each algorithm is gauged and compared (Python
packages: pandas, scikits.statsmodels).&lt;/p&gt;
&lt;p&gt;The final result of this process is a list of outliers and their
severity. Further algorithms will judge what outliers are serious enough
to present to users.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Phil Roth</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/time-series-analysis-network-security.html</guid></item><item><title>Zero Dependency Python</title><link>https://pyvideo.org/scipy-2014/zero-dependency-python.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;We present a new method for distributing and using Python that requires
no dependencies beyond the Google Chrome web browser based on Portable
Native Client (PNaCl). We will demonstrate an IPython notebook run
completely client side with no out-of-browser components, backed by
Google Drive, an HTML5 File System, and able to pass numpy arrays as
typed arrays without serialization as JSON.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We present a new method for distributing and using Python that requires
no dependencies beyond the Google Chrome web browser. By combining the
static linking methodology of traditional supercomputer-style
deployments of Python with the technology Portable Native Client (PNaCl)
we have constructed a method for building, deploying, and sharing
fully-sandboxed scientific python stacks that require no client-side
installation: the entire IPython notebook and scientific python stack,
in a website, at native speeds. We will present this technology, along
with some of its potential applications, describing its shortcomings and
future extensibility. We will conclude by demonstrating an IPython
notebook run completely client side with no out-of-browser components,
backed by Google Drive and an HTML5 File System, and able to pass numpy
arrays as typed arrays into the browser without serialization as JSON.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;We will begin by briefly describing the problems with deploying
scientific python as a stack, particularly the dependency graph,
installation time, and so on.&lt;/li&gt;
&lt;li&gt;We'll describe the PNaCl technology and build system for scientific
python, including how individuals can create their own .pexes with
their own application stack&lt;/li&gt;
&lt;li&gt;We'll describe potential applications, such as bundling safe,
sandboxed executables with scripts and lessons&lt;/li&gt;
&lt;li&gt;We will demonstrate a complete system for running the IPython
notebook in a sandboxed, Google Chrome window&lt;/li&gt;
&lt;li&gt;We'll conclude by describing methods that this system could be
extended to run sandboxed python executables on any system,
independent of the Chrome web browser, such as supercomputers and
non-virtualized hosting providers&lt;/li&gt;
&lt;/ol&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kester Tong</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-10:scipy-2014/zero-dependency-python.html</guid></item><item><title>A Common Scientific Compute Environment for Research and Education</title><link>https://pyvideo.org/scipy-2014/a-common-scientific-compute-environment-for-resea.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;I provide an overview of the challenges weve tackled at UC Berkeley
deploying scientific compute environments in both educational and
research contexts. After a discussion of how these needs can be served
by devops tools like Docker and Ansible, I argue that a coherent,
easy-to-understand philosophy around reproducible compute environments
is fundamental.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As the line between developer and researcher becomes ever more blurred,
the challenge of sharing your compute environment with students and
colleagues becomes ever more complex. Large, private organizations have
been grappling with this issue for a while, spawning a great deal of
enthusiasm around tools like Docker, Puppet, Vagrant, and Packer. And
lets not forget notable python-based upstarts, Ansible and Salt! These
tools can generate immense enthusiasm, followed by the question, Why
are we doing this?&lt;/p&gt;
&lt;p&gt;The problem is that researcher / developers can become overwhelmed by
the complexity and variety inherent in devops tools - all the while
losing sight of the real reason for using these tools: a philosophy of
documenting your research compute environments in a reproducible
fashion, with a focus on scripting as much as is reasonable.&lt;/p&gt;
&lt;p&gt;At UC Berkeley, members of the D-Lab, the Statistical Compute Facility,
Computer Science and Research IT have organized a project to develop the
Berkeley Common Environment (BCE). Ill provide an overview of the
challenges weve tackled in both educational and research contexts, and
the needs served by the above-mentioned devops tools. In the end, I
argue that a coherent, easy-to-understand philosophy around scientific
compute environments is fundamental - the tools are just a way to make
your collaboration architecture a little easier for the people building
these environments a few times a year. What we should focus on, though,
is end-user experience and research community buy-in.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dav Clark</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/a-common-scientific-compute-environment-for-resea.html</guid><category>devops</category><category>reproducible research</category></item><item><title>Airspeed Velocity: Tracking Performance of Python Projects Over Their Lifetime</title><link>https://pyvideo.org/scipy-2014/airspeed-velocity-tracking-performance-of-python.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Presenting &amp;quot;airspeed velocity&amp;quot;, a new tool for benchmarking Python
software projects over their lifetime.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As software projects mature and become more robust against bugs, they
may also lose some of their runtime performance and memory efficiency.
Airspeed velocity (asv) is a new tool to help find those performance
degradations before they get out to end users. It automatically runs a
benchmark suite over a range of commits in a project's repository, as
well as in a matrix of configurations of Python versions and other
dependencies. The results, possibly from multiple machines, are then
collated and published in a web-based report.&lt;/p&gt;
&lt;p&gt;While filling a similar role as projects such as &amp;quot;codespeed&amp;quot; and
&amp;quot;vbench&amp;quot;, airspeed velocity is designed to be easier to set up and
deploy, since it uses only a DVCS repository as its database and the
report is deployable to any static web server.&lt;/p&gt;
&lt;p&gt;Airspeed velocity provides an easy way to write benchmarks, inspired by
&amp;quot;nosetests&amp;quot; and &amp;quot;py.test&amp;quot;. It is possible to benchmark runtime, memory
usage, or any user-defined metric.&lt;/p&gt;
&lt;p&gt;Other features either implemented or in the planning stages include:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;tight integration with existing profiling tools, such as RunSnakeRun&lt;/li&gt;
&lt;li&gt;parameterized benchmarks to investigate how an algorithm scales with
data size&lt;/li&gt;
&lt;li&gt;automatic search for degrading commits&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The presentation will provide a demo of airspeed velocity, and discuss
its early usage for benchmarking the astropy project.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Michael Droettboom</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/airspeed-velocity-tracking-performance-of-python.html</guid><category>benchmarking</category><category>performance</category></item><item><title>Anatomy of Matplotlib - Part 1</title><link>https://pyvideo.org/scipy-2014/anatomy-of-matplotlib-part-1.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This tutorial will be the introduction to matplotlib. Users will learn
the types of plots and experiment with them. Then the fundamental
concepts and terminologies of matplotlib are introduced. Next, we will
learn how to change the &amp;quot;look and feel&amp;quot; of their plots. Finally, users
will be introduced to other toolkits that extends matplotlib.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Introduction&lt;ul&gt;
&lt;li&gt;Purpose of matplotlib&lt;/li&gt;
&lt;li&gt;Online Documentation&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="http://matplotlib.org"&gt;matplotlib.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mailing Lists and StackOverflow&lt;/li&gt;
&lt;li&gt;Github Repository&lt;/li&gt;
&lt;li&gt;Bug Reports &amp;amp; Feature Requests&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What is this &amp;quot;backend&amp;quot; thing I keep hearing about?&lt;ul&gt;
&lt;li&gt;Interactive versus non-interactive&lt;/li&gt;
&lt;li&gt;Agg&lt;/li&gt;
&lt;li&gt;Tk, Qt, GTK, MacOSX, Wx, Cairo&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Plotting Functions&lt;ul&gt;
&lt;li&gt;Graphs (plot, scatter, bar, stem, etc.)&lt;/li&gt;
&lt;li&gt;Images (imshow, pcolor, pcolormesh, contour[f], etc.)&lt;/li&gt;
&lt;li&gt;Lesser Knowns: (pie, acorr, hexbin, streamplot, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What goes in a Figure?&lt;ul&gt;
&lt;li&gt;Axes&lt;/li&gt;
&lt;li&gt;Axis&lt;/li&gt;
&lt;li&gt;ticks (and ticklines and ticklabels) (both major &amp;amp; minor)&lt;/li&gt;
&lt;li&gt;axis labels&lt;/li&gt;
&lt;li&gt;axes title&lt;/li&gt;
&lt;li&gt;figure subtitle&lt;/li&gt;
&lt;li&gt;axis spines&lt;/li&gt;
&lt;li&gt;colorbars (and the oddities thereof)&lt;/li&gt;
&lt;li&gt;axis scale&lt;/li&gt;
&lt;li&gt;axis gridlines&lt;/li&gt;
&lt;li&gt;legend&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Manipulating the &amp;quot;Look-and-Feel&amp;quot;&lt;ul&gt;
&lt;li&gt;Introducing matplotlibrc&lt;/li&gt;
&lt;li&gt;Properties&lt;ul&gt;
&lt;li&gt;color (and edgecolor, linecolor, facecolor, etc...)&lt;/li&gt;
&lt;li&gt;linewidth and edgewidth and markeredgewidth (and the oddity that happens in errorbar())&lt;/li&gt;
&lt;li&gt;linestyle&lt;/li&gt;
&lt;li&gt;fonts&lt;/li&gt;
&lt;li&gt;zorder&lt;/li&gt;
&lt;li&gt;visible&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What are toolkits?&lt;ul&gt;
&lt;li&gt;axes_grid1&lt;/li&gt;
&lt;li&gt;mplot3d&lt;/li&gt;
&lt;li&gt;basemap&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Benjamin Root</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/anatomy-of-matplotlib-part-1.html</guid><category>matplotlib</category></item><item><title>Anatomy of Matplotlib - Part 2</title><link>https://pyvideo.org/scipy-2014/anatomy-of-matplotlib-part-2.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This tutorial will be the introduction to matplotlib. Users will learn
the types of plots and experiment with them. Then the fundamental
concepts and terminologies of matplotlib are introduced. Next, we will
learn how to change the &amp;quot;look and feel&amp;quot; of their plots. Finally, users
will be introduced to other toolkits that extends matplotlib.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Introduction&lt;ul&gt;
&lt;li&gt;Purpose of matplotlib&lt;/li&gt;
&lt;li&gt;Online Documentation&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="http://matplotlib.org"&gt;matplotlib.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mailing Lists and StackOverflow&lt;/li&gt;
&lt;li&gt;Github Repository&lt;/li&gt;
&lt;li&gt;Bug Reports &amp;amp; Feature Requests&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What is this &amp;quot;backend&amp;quot; thing I keep hearing about?&lt;ul&gt;
&lt;li&gt;Interactive versus non-interactive&lt;/li&gt;
&lt;li&gt;Agg&lt;/li&gt;
&lt;li&gt;Tk, Qt, GTK, MacOSX, Wx, Cairo&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Plotting Functions&lt;ul&gt;
&lt;li&gt;Graphs (plot, scatter, bar, stem, etc.)&lt;/li&gt;
&lt;li&gt;Images (imshow, pcolor, pcolormesh, contour[f], etc.)&lt;/li&gt;
&lt;li&gt;Lesser Knowns: (pie, acorr, hexbin, streamplot, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What goes in a Figure?&lt;ul&gt;
&lt;li&gt;Axes&lt;/li&gt;
&lt;li&gt;Axis&lt;/li&gt;
&lt;li&gt;ticks (and ticklines and ticklabels) (both major &amp;amp; minor)&lt;/li&gt;
&lt;li&gt;axis labels&lt;/li&gt;
&lt;li&gt;axes title&lt;/li&gt;
&lt;li&gt;figure subtitle&lt;/li&gt;
&lt;li&gt;axis spines&lt;/li&gt;
&lt;li&gt;colorbars (and the oddities thereof)&lt;/li&gt;
&lt;li&gt;axis scale&lt;/li&gt;
&lt;li&gt;axis gridlines&lt;/li&gt;
&lt;li&gt;legend&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Manipulating the &amp;quot;Look-and-Feel&amp;quot;&lt;ul&gt;
&lt;li&gt;Introducing matplotlibrc&lt;/li&gt;
&lt;li&gt;Properties&lt;ul&gt;
&lt;li&gt;color (and edgecolor, linecolor, facecolor, etc...)&lt;/li&gt;
&lt;li&gt;linewidth and edgewidth and markeredgewidth (and the oddity that happens in errorbar())&lt;/li&gt;
&lt;li&gt;linestyle&lt;/li&gt;
&lt;li&gt;fonts&lt;/li&gt;
&lt;li&gt;zorder&lt;/li&gt;
&lt;li&gt;visible&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What are toolkits?&lt;ul&gt;
&lt;li&gt;axes_grid1&lt;/li&gt;
&lt;li&gt;mplot3d&lt;/li&gt;
&lt;li&gt;basemap&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Benjamin Root</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/anatomy-of-matplotlib-part-2.html</guid><category>matplotlib</category></item><item><title>Anatomy of Matplotlib - Part 3</title><link>https://pyvideo.org/scipy-2014/anatomy-of-matplotlib-part-3.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This tutorial will be the introduction to matplotlib. Users will learn
the types of plots and experiment with them. Then the fundamental
concepts and terminologies of matplotlib are introduced. Next, we will
learn how to change the &amp;quot;look and feel&amp;quot; of their plots. Finally, users
will be introduced to other toolkits that extends matplotlib.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Introduction&lt;ul&gt;
&lt;li&gt;Purpose of matplotlib&lt;/li&gt;
&lt;li&gt;Online Documentation&lt;ul&gt;
&lt;li&gt;&lt;a class="reference external" href="http://matplotlib.org"&gt;matplotlib.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mailing Lists and StackOverflow&lt;/li&gt;
&lt;li&gt;Github Repository&lt;/li&gt;
&lt;li&gt;Bug Reports &amp;amp; Feature Requests&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What is this &amp;quot;backend&amp;quot; thing I keep hearing about?&lt;ul&gt;
&lt;li&gt;Interactive versus non-interactive&lt;/li&gt;
&lt;li&gt;Agg&lt;/li&gt;
&lt;li&gt;Tk, Qt, GTK, MacOSX, Wx, Cairo&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Plotting Functions&lt;ul&gt;
&lt;li&gt;Graphs (plot, scatter, bar, stem, etc.)&lt;/li&gt;
&lt;li&gt;Images (imshow, pcolor, pcolormesh, contour[f], etc.)&lt;/li&gt;
&lt;li&gt;Lesser Knowns: (pie, acorr, hexbin, streamplot, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What goes in a Figure?&lt;ul&gt;
&lt;li&gt;Axes&lt;/li&gt;
&lt;li&gt;Axis&lt;/li&gt;
&lt;li&gt;ticks (and ticklines and ticklabels) (both major &amp;amp; minor)&lt;/li&gt;
&lt;li&gt;axis labels&lt;/li&gt;
&lt;li&gt;axes title&lt;/li&gt;
&lt;li&gt;figure subtitle&lt;/li&gt;
&lt;li&gt;axis spines&lt;/li&gt;
&lt;li&gt;colorbars (and the oddities thereof)&lt;/li&gt;
&lt;li&gt;axis scale&lt;/li&gt;
&lt;li&gt;axis gridlines&lt;/li&gt;
&lt;li&gt;legend&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Manipulating the &amp;quot;Look-and-Feel&amp;quot;&lt;ul&gt;
&lt;li&gt;Introducing matplotlibrc&lt;/li&gt;
&lt;li&gt;Properties&lt;ul&gt;
&lt;li&gt;color (and edgecolor, linecolor, facecolor, etc...)&lt;/li&gt;
&lt;li&gt;linewidth and edgewidth and markeredgewidth (and the oddity that happens in errorbar())&lt;/li&gt;
&lt;li&gt;linestyle&lt;/li&gt;
&lt;li&gt;fonts&lt;/li&gt;
&lt;li&gt;zorder&lt;/li&gt;
&lt;li&gt;visible&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What are toolkits?&lt;ul&gt;
&lt;li&gt;axes_grid1&lt;/li&gt;
&lt;li&gt;mplot3d&lt;/li&gt;
&lt;li&gt;basemap&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Benjamin Root</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/anatomy-of-matplotlib-part-3.html</guid><category>matplotlib</category></item><item><title>Astropy and astronomical tools Part I</title><link>https://pyvideo.org/scipy-2014/astropy-and-astronomical-tools-part-i.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The introductory session will start with an overview of the astropy
project and the goals of the tutorial, followed by the basics on
accessing astronomical data and the associated attributes of such data,
including the units and coordinates. Also covered is how to use the new
and powerful quantities facility, which allows physical quantities to be
explicitly bound to the units they are defined in.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Outline&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Overview of astropy (15 minutes) [Greenfield]&lt;ul&gt;
&lt;li&gt;Exercise: Import astropy, demonstrate that tools are present and
echo simple examples given. (15 minutes: allowing for typical
start-up problems)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Units/quantities (15 minutes) [Droettboom]&lt;ul&gt;
&lt;li&gt;Exercise: Solve problems using standard units; define new unit;
use unit equivalencies; define blackbody function using
quantities/units (15 minutes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tables (20 minutes) [Aldcroft]&lt;ul&gt;
&lt;li&gt;Exercise: read in provided table files and apply requested table
manipulations (20 minutes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Break (15 minutes)&lt;/li&gt;
&lt;li&gt;Accessing and updating data&lt;ul&gt;
&lt;li&gt;FITS (30 minutes) [Bray]&lt;ul&gt;
&lt;li&gt;Exercise: Open supplied data files; manipulate header
information; manipulate data; write results; update and append
to existing file (30 minutes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ascii tables (15 minutes) [Aldcroft]&lt;ul&gt;
&lt;li&gt;Exercise: Open supplied ascii files, modify and convert into
csv files (15 minutes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;coordinates (sky/time) (15 minutes) [Robitaille]&lt;ul&gt;
&lt;li&gt;Exercises: solve coordinate/time conversion problems; read in
various string representations for coordinates/times; print
alternate string representations (15 minutes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Erik Bray</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/astropy-and-astronomical-tools-part-i.html</guid><category>astropy</category></item><item><title>Bayesian Statistical Analysis using Python - Part 3</title><link>https://pyvideo.org/scipy-2014/bayesian-statistical-analysis-using-python-part.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This hands-on tutorial will introduce statistical analysis in Python
using Bayesian methods. Bayesian statistics offer a flexible &amp;amp; powerful
way of analyzing data, but are computationally-intensive, for which
Python is ideal. As a gentle introduction, we will solve simple problems
using NumPy and SciPy, before moving on to Markov chain Monte Carlo
methods to build more complex models using PyMC.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The aim of this course is to introduce new users to the Bayesian
approach of statistical modeling and analysis, so that they can use
Python packages such as NumPy, SciPy and
&lt;a class="reference external" href="https://github.com/pymc-devs/pymc"&gt;PyMC&lt;/a&gt; effectively to analyze
their own data. It is designed to get users quickly up and running with
Bayesian methods, incorporating just enough statistical background to
allow users to understand, in general terms, what they are implementing.
The tutorial will be example-driven, with illustrative case studies
using real data. Selected methods will include approximation methods,
importance sampling, Markov chain Monte Carlo (MCMC) methods such as
Metropolis-Hastings and Slice sampling. In addition to model fitting,
the tutorial will address important techniques for model checking, model
comparison, and steps for preparing data and processing model output.
Tutorial content will be derived from the instructor's book &lt;em&gt;Bayesian
Statistical Computing using Python&lt;/em&gt;, to be published by Springer in late
2014.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="PyMC forest plot" src="http://d.pr/i/pqWT+" /&gt;
&lt;p class="caption"&gt;PyMC forest plot&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="DAG" src="http://d.pr/i/AHZV+" /&gt;
&lt;p class="caption"&gt;DAG&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;All course content will be available as a GitHub repository, including
IPython notebooks and example data.&lt;/p&gt;
&lt;div class="section" id="tutorial-outline"&gt;
&lt;h4&gt;Tutorial Outline&lt;/h4&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Overview of Bayesian statistics.&lt;/li&gt;
&lt;li&gt;Bayesian Inference with NumPy and SciPy&lt;/li&gt;
&lt;li&gt;Markov chain Monte Carlo (MCMC)&lt;/li&gt;
&lt;li&gt;The Essentials of PyMC&lt;/li&gt;
&lt;li&gt;Fitting Linear Regression Models&lt;/li&gt;
&lt;li&gt;Hierarchical Modeling&lt;/li&gt;
&lt;li&gt;Model Checking and Validation&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="installation-instructions"&gt;
&lt;h4&gt;Installation Instructions&lt;/h4&gt;
&lt;p&gt;The easiest way to install the Python packages required for this
tutorial is via
&lt;a class="reference external" href="https://store.continuum.io/cshop/anaconda/"&gt;Anaconda&lt;/a&gt;, a scientific
Python distribution offered by Continuum analytics. Several other
tutorials will be recommending a similar setup.&lt;/p&gt;
&lt;p&gt;One of the key features of Anaconda is a command line utility called
&lt;tt class="docutils literal"&gt;conda&lt;/tt&gt; that can be used to manage third party packages. We have built
a PyMC package for &lt;tt class="docutils literal"&gt;conda&lt;/tt&gt; that can be installed from your terminal
via the following command:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
conda install -c https://conda.binstar.org/pymc pymc
&lt;/pre&gt;
&lt;p&gt;This should install any prerequisite packages that are required to run
PyMC.&lt;/p&gt;
&lt;p&gt;One caveat is that conda does not yet have a build of PyMC for &lt;strong&gt;Python
3&lt;/strong&gt;. Therefore, you would have to build it yourself via pip:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip install git+git://github.com/pymc-devs/pymc.git&amp;#64;2.3
&lt;/pre&gt;
&lt;p&gt;For those of you on Mac OS X that are already using the
&lt;a class="reference external" href="http://brew.sh"&gt;Homebrew&lt;/a&gt; package manager, I have prepared a script
that will install the entire Python scientific stack, including PyMC
2.3. You can download the script
&lt;a class="reference external" href="https://gist.github.com/fonnesbeck/7de008b05e670d919b71"&gt;here&lt;/a&gt; and
run it via:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sh install_superpack_brew.sh
&lt;/pre&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Fonnesbeck</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/bayesian-statistical-analysis-using-python-part.html</guid><category>bayesian</category><category>statistics</category></item><item><title>Bayesian Statistical Analysis using Python - Part 2</title><link>https://pyvideo.org/scipy-2014/bayesian-statistical-analysis-using-python-part-0.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This hands-on tutorial will introduce statistical analysis in Python
using Bayesian methods. Bayesian statistics offer a flexible &amp;amp; powerful
way of analyzing data, but are computationally-intensive, for which
Python is ideal. As a gentle introduction, we will solve simple problems
using NumPy and SciPy, before moving on to Markov chain Monte Carlo
methods to build more complex models using PyMC.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The aim of this course is to introduce new users to the Bayesian
approach of statistical modeling and analysis, so that they can use
Python packages such as NumPy, SciPy and
&lt;a class="reference external" href="https://github.com/pymc-devs/pymc"&gt;PyMC&lt;/a&gt; effectively to analyze
their own data. It is designed to get users quickly up and running with
Bayesian methods, incorporating just enough statistical background to
allow users to understand, in general terms, what they are implementing.
The tutorial will be example-driven, with illustrative case studies
using real data. Selected methods will include approximation methods,
importance sampling, Markov chain Monte Carlo (MCMC) methods such as
Metropolis-Hastings and Slice sampling. In addition to model fitting,
the tutorial will address important techniques for model checking, model
comparison, and steps for preparing data and processing model output.
Tutorial content will be derived from the instructor's book &lt;em&gt;Bayesian
Statistical Computing using Python&lt;/em&gt;, to be published by Springer in late
2014.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="PyMC forest plot" src="http://d.pr/i/pqWT+" /&gt;
&lt;p class="caption"&gt;PyMC forest plot&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="DAG" src="http://d.pr/i/AHZV+" /&gt;
&lt;p class="caption"&gt;DAG&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;All course content will be available as a GitHub repository, including
IPython notebooks and example data.&lt;/p&gt;
&lt;div class="section" id="tutorial-outline"&gt;
&lt;h4&gt;Tutorial Outline&lt;/h4&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Overview of Bayesian statistics.&lt;/li&gt;
&lt;li&gt;Bayesian Inference with NumPy and SciPy&lt;/li&gt;
&lt;li&gt;Markov chain Monte Carlo (MCMC)&lt;/li&gt;
&lt;li&gt;The Essentials of PyMC&lt;/li&gt;
&lt;li&gt;Fitting Linear Regression Models&lt;/li&gt;
&lt;li&gt;Hierarchical Modeling&lt;/li&gt;
&lt;li&gt;Model Checking and Validation&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="installation-instructions"&gt;
&lt;h4&gt;Installation Instructions&lt;/h4&gt;
&lt;p&gt;The easiest way to install the Python packages required for this
tutorial is via
&lt;a class="reference external" href="https://store.continuum.io/cshop/anaconda/"&gt;Anaconda&lt;/a&gt;, a scientific
Python distribution offered by Continuum analytics. Several other
tutorials will be recommending a similar setup.&lt;/p&gt;
&lt;p&gt;One of the key features of Anaconda is a command line utility called
&lt;tt class="docutils literal"&gt;conda&lt;/tt&gt; that can be used to manage third party packages. We have built
a PyMC package for &lt;tt class="docutils literal"&gt;conda&lt;/tt&gt; that can be installed from your terminal
via the following command:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
conda install -c https://conda.binstar.org/pymc pymc
&lt;/pre&gt;
&lt;p&gt;This should install any prerequisite packages that are required to run
PyMC.&lt;/p&gt;
&lt;p&gt;One caveat is that conda does not yet have a build of PyMC for &lt;strong&gt;Python
3&lt;/strong&gt;. Therefore, you would have to build it yourself via pip:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip install git+git://github.com/pymc-devs/pymc.git&amp;#64;2.3
&lt;/pre&gt;
&lt;p&gt;For those of you on Mac OS X that are already using the
&lt;a class="reference external" href="http://brew.sh"&gt;Homebrew&lt;/a&gt; package manager, I have prepared a script
that will install the entire Python scientific stack, including PyMC
2.3. You can download the script
&lt;a class="reference external" href="https://gist.github.com/fonnesbeck/7de008b05e670d919b71"&gt;here&lt;/a&gt; and
run it via:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sh install_superpack_brew.sh
&lt;/pre&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Fonnesbeck</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/bayesian-statistical-analysis-using-python-part-0.html</guid><category>bayesian</category><category>statistics</category></item><item><title>Bayesian Statistical Analysis using Python - Part 1</title><link>https://pyvideo.org/scipy-2014/bayesian-statistical-analysis-using-python-part-1.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This hands-on tutorial will introduce statistical analysis in Python
using Bayesian methods. Bayesian statistics offer a flexible &amp;amp; powerful
way of analyzing data, but are computationally-intensive, for which
Python is ideal. As a gentle introduction, we will solve simple problems
using NumPy and SciPy, before moving on to Markov chain Monte Carlo
methods to build more complex models using PyMC.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The aim of this course is to introduce new users to the Bayesian
approach of statistical modeling and analysis, so that they can use
Python packages such as NumPy, SciPy and
&lt;a class="reference external" href="https://github.com/pymc-devs/pymc"&gt;PyMC&lt;/a&gt; effectively to analyze
their own data. It is designed to get users quickly up and running with
Bayesian methods, incorporating just enough statistical background to
allow users to understand, in general terms, what they are implementing.
The tutorial will be example-driven, with illustrative case studies
using real data. Selected methods will include approximation methods,
importance sampling, Markov chain Monte Carlo (MCMC) methods such as
Metropolis-Hastings and Slice sampling. In addition to model fitting,
the tutorial will address important techniques for model checking, model
comparison, and steps for preparing data and processing model output.
Tutorial content will be derived from the instructor's book &lt;em&gt;Bayesian
Statistical Computing using Python&lt;/em&gt;, to be published by Springer in late
2014.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="PyMC forest plot" src="http://d.pr/i/pqWT+" /&gt;
&lt;p class="caption"&gt;PyMC forest plot&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="DAG" src="http://d.pr/i/AHZV+" /&gt;
&lt;p class="caption"&gt;DAG&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;All course content will be available as a GitHub repository, including
IPython notebooks and example data.&lt;/p&gt;
&lt;div class="section" id="tutorial-outline"&gt;
&lt;h4&gt;Tutorial Outline&lt;/h4&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Overview of Bayesian statistics.&lt;/li&gt;
&lt;li&gt;Bayesian Inference with NumPy and SciPy&lt;/li&gt;
&lt;li&gt;Markov chain Monte Carlo (MCMC)&lt;/li&gt;
&lt;li&gt;The Essentials of PyMC&lt;/li&gt;
&lt;li&gt;Fitting Linear Regression Models&lt;/li&gt;
&lt;li&gt;Hierarchical Modeling&lt;/li&gt;
&lt;li&gt;Model Checking and Validation&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="installation-instructions"&gt;
&lt;h4&gt;Installation Instructions&lt;/h4&gt;
&lt;p&gt;The easiest way to install the Python packages required for this
tutorial is via
&lt;a class="reference external" href="https://store.continuum.io/cshop/anaconda/"&gt;Anaconda&lt;/a&gt;, a scientific
Python distribution offered by Continuum analytics. Several other
tutorials will be recommending a similar setup.&lt;/p&gt;
&lt;p&gt;One of the key features of Anaconda is a command line utility called
&lt;tt class="docutils literal"&gt;conda&lt;/tt&gt; that can be used to manage third party packages. We have built
a PyMC package for &lt;tt class="docutils literal"&gt;conda&lt;/tt&gt; that can be installed from your terminal
via the following command:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
conda install -c https://conda.binstar.org/pymc pymc
&lt;/pre&gt;
&lt;p&gt;This should install any prerequisite packages that are required to run
PyMC.&lt;/p&gt;
&lt;p&gt;One caveat is that conda does not yet have a build of PyMC for &lt;strong&gt;Python
3&lt;/strong&gt;. Therefore, you would have to build it yourself via pip:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
pip install git+git://github.com/pymc-devs/pymc.git&amp;#64;2.3
&lt;/pre&gt;
&lt;p&gt;For those of you on Mac OS X that are already using the
&lt;a class="reference external" href="http://brew.sh"&gt;Homebrew&lt;/a&gt; package manager, I have prepared a script
that will install the entire Python scientific stack, including PyMC
2.3. You can download the script
&lt;a class="reference external" href="https://gist.github.com/fonnesbeck/7de008b05e670d919b71"&gt;here&lt;/a&gt; and
run it via:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sh install_superpack_brew.sh
&lt;/pre&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Fonnesbeck</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/bayesian-statistical-analysis-using-python-part-1.html</guid><category>bayesian</category><category>statistics</category></item><item><title>Bokeh: Interactive Visualizations in the Browser</title><link>https://pyvideo.org/scipy-2014/bokeh-interactive-visualizations-in-the-browser.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Bokeh is a Python visualization library for large datasets that natively
uses the latest web technologies. Its goal is to provide concise
construction of novel graphics, while delivering high-performance
interactivity over large data to thin clients. This talk will cover the
motivation and architecture behind Bokeh, demonstrate interesting uses
and capability, and discuss future plans.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With support from the DARPA XDATA Initiative, and contributions from
community members, the Bokeh visualization library
(&lt;a class="reference external" href="http://bokeh.pydata.org"&gt;http://bokeh.pydata.org&lt;/a&gt;) has grown into a large, successful open source
project with heavy interest and following on GitHub
(&lt;a class="reference external" href="https://github.com/ContinuumIO/bokeh"&gt;https://github.com/ContinuumIO/bokeh&lt;/a&gt;). The principal goals of Bokeh are
to provide capability to developers and domain experts:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;easily create novel and powerful visualizations&lt;/li&gt;
&lt;li&gt;that extract insight from remote, possibly large data sets&lt;/li&gt;
&lt;li&gt;published to the web for others to explore and interact&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This talk will describe how the architecture of Bokeh enables these
goals, and demonstrate how it can be leveraged by anyone using python
for analysis to visualize and present their work. We will talk about
current development and future plans, including a brief discussion of
Joseph Cottam's exciting academic work on abstract rendering for large
data sets that is going into Bokeh
(&lt;a class="reference external" href="https://github.com/JosephCottam/AbstractRendering"&gt;https://github.com/JosephCottam/AbstractRendering&lt;/a&gt;).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bryan Van de Ven</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/bokeh-interactive-visualizations-in-the-browser.html</guid><category>bokeh</category><category>vis</category></item><item><title>Conda: A Cross Platform Package Manager for any Binary Distribution</title><link>https://pyvideo.org/scipy-2014/conda-a-cross-platform-package-manager-for-any-b.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Conda is an open source package manager, which can be used to manage
binary packages and virtual environments on any platform. It is the
package manager of the Anaconda Python distribution, although it can be
used independently of Anaconda. We will look at how conda solves many of
the problems that have plagued Python packaging in the past, followed by
a demonstration of its features.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will look at the issues that have plagued packaging in the Python
ecosystem in the past, and discuss how Conda solves these problems. We
will show how to use conda to manage multiple environments. Finally, we
will look at how to build your own conda packages.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What is the packaging problem? We will briefly look at the history of
the problem and the various solutions to it. There are two sides to
the packaging problem: the problem of installing existing packages
and the problem of building packages to be installed. We look at the
history of distutils, setuptools, distribute, and pip, the some of
the problems they solved, and issues that arose, particularly for the
scientific Python community.&lt;/li&gt;
&lt;li&gt;We will look at the conda package format, the design decisions that
guided the format, and the implications of those decisions. A conda
package is a bz2 compressed tarfile of all the files installed in a
prefix, along with a metadata directory for the package. A conda
package is typically installed by hard linking these files into the
install prefix. Conda packages should be relocatable, so that they
can be installed into any prefix. This allows conda packages to be
installed into many virtual environments at once. A conda package is
not Python specific.&lt;/li&gt;
&lt;li&gt;We will look at how basic commands for installation and environment
management. Conda uses a SAT solver to solve package dependency
constraints, which is a simple, rigorous, and modern way to ensure
that the set of packages that are installed are consistent with one
another.&lt;/li&gt;
&lt;li&gt;Conda has an extensive build framework which allows anybody to build
their own conda packages. We will show how to use these tools and how
to upload them to Binstar, a free packaging hosting service.&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aaron Meurer</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/conda-a-cross-platform-package-manager-for-any-b.html</guid><category>conda</category><category>packaging</category></item><item><title>Creating a browser based virtual computer lab for classroom instruction</title><link>https://pyvideo.org/scipy-2014/creating-a-browser-based-virtual-computer-lab-for.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;With laptops and tablets becoming more powerful and more ubiquitous in
the classroom, traditional computer labs with rows of expensive desktops
are beginning to lose their relevance. This presentation will discuss
browser-based virtual computer labs for teaching Python, using a
notebook interface, as an alternative approach to classroom instruction.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;One of the difficulties in using Python for scientific applications is
that one needs a fairly complete set of Python data processing and
visualization packages to be installed, beyond the standard Python
distribution. Freely available scientific Python distributions like
Enthought Canopy and Anaconda address this problem. A typical approach
to teaching Python is to use a dedicated computer lab, where one of
these distributions is installed on a set of machines with identical
computing environments for use by students. With laptop computers
becoming cheap and ubiquitous, an alternative approach is to allow
students to use their own computers, where they install one of the
scientific Python distributions by themselves. This approach requires
more set-up time, because the software often requires some minor
tweaking for each software platform, but requires no dedicated hardware
and has the advantage of allowing students to easily run programs after
class on their own computers. This presentation discusses a third
approach that involves creating a software environment for Python using
cloud computing. There are already commercial products available that
provide well-supported Python computing environments in the cloud. This
presentation focuses on alternative roll your own solutions using
open-source software that are specifically targeted for use in an
interactive classroom instruction setting.&lt;/p&gt;
&lt;p&gt;Creating a virtual computing lab usually involves instantiating a server
using a cloud infrastructure provider, such as Amazon Web Services. A
new server can be set-up within minutes, with a scientific Python
distribution automatically installed during set-up. Students can then
login to their own accounts on the server using a browser-based
interface to execute Python programs and visualize graphical output.
Typically, each student would use a notebook interface to work on
lessons.&lt;/p&gt;
&lt;p&gt;Different approaches can be used to create separate accounts for
multiple users. The simplest would be to create different user accounts
on a Linux virtual machine. If greater isolation is required,
lightweight linux containers can be created on-demand for each user.
Although IPython Notebook can currently be run as a public server to
work with multiple notebooks simultaneously, true multi-user support is
expected to be implemented further down the road. However, there are a
few open-source projects, such as JiffyLab, that already support a
multi-user IPython Notebook environment. Another option is to use the
open-source GraphTerm server, which supports a multi-user graphical
terminal environment with a notebook interface. The pros and cons of
these different approaches to building a virtual computer lab will be
discussed.&lt;/p&gt;
&lt;p&gt;Also discussed will be additional features that could be useful in a
virtual computing lab such as the capability for the instructor to chat
with the students and monitor their individual progress using a
dashboard. Allowing students to collaborate in groups, with ability to
view and edit each others code, can help promote classroom interaction.
Enhancements to the notebook interface, such as fill in the blanks
notebooks, can facilitate more structured instruction. The
implementation of some of these features in the GraphTerm server will be
discussed.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/ptone/jiffylab"&gt;JiffyLab source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/mitotic/graphterm"&gt;GraphTerm source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://conference.scipy.org/proceedings/scipy2013/pdfs/saravanan.pdf"&gt;GraphTerm talk from SciPy
2013&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ramalingam Saravanan</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/creating-a-browser-based-virtual-computer-lab-for.html</guid></item><item><title>Frequentism and Bayesianism: What's the Big Deal?</title><link>https://pyvideo.org/scipy-2014/frequentism-and-bayesianism-whats-the-big-deal.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Statistical analysis comes in two main flavors: frequentist and
Bayesian. The subtle differences between the two can lead to widely
divergent approaches to common data analysis tasks. After a brief
discussion of the philosophical distinctions between the views, Ill
utilize well-known Python libraries to demonstrate how this philosophy
affects practical approaches to several common analysis tasks.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In scientific data mining and machine learning, a fundamental division
is that of the frequentist and Bayesian approaches to statistics. Often
the fodder for impassioned debate among statisticians and other
practitioners, the subtle philosophical differences between the two
camps can lead to surprisingly different practical approaches to the
analysis of scientific data.&lt;/p&gt;
&lt;p&gt;In this talk I will delve into both the philosophical and practical
aspects of Bayesian and frequentist approaches, drawing from a &lt;a class="reference external" href="http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/"&gt;series
of
posts&lt;/a&gt;
from my blog.&lt;/p&gt;
&lt;p&gt;I'll start by addressing the philosophical differences between
frequentism and Bayesianism, which boil down to different definitions of
probability. I'll next move briefly into the mathematical details behind
the two approaches, at a level which will be informative to a general
scientific audience. I'll then show some examples of the two approaches
applied to some increasingly more complicated problems using standard
Python packages, namely: &lt;a class="reference external" href="http://numpy.org"&gt;NumPy&lt;/a&gt;,
&lt;a class="reference external" href="http://scipy.org"&gt;SciPy&lt;/a&gt;, &lt;a class="reference external" href="http://matplotlib.org"&gt;Matplotlib&lt;/a&gt;,
and &lt;a class="reference external" href="http://dan.iel.fm/emcee/"&gt;emcee&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With this combination of philosophy and practical examples, the audience
should walk away with a much better understanding of the differences
between frequentist and Bayesian approaches to statistical analysis, and
especially how the philosophy of each approach affects the practical
aspects of computation in data-intensive scientific research.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jake VanderPlas</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/frequentism-and-bayesianism-whats-the-big-deal.html</guid><category>statistics</category></item><item><title>Fundamentals of the IPython Display Architecture+Interactive Widgets</title><link>https://pyvideo.org/scipy-2014/fundamentals-of-the-ipython-display-architecture.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;In this tutorial, attendees will learn how to use the IPython Notebooks
display architecture and interactive widgets. As we cover these topics,
attendees will learn about the underlying architecture, how to use
IPythons existing APIs, and how to extend them for their own purposes.
This tutorial will not cover the basics of the IPython Notebook.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;IPython provides an architecture for interactive computing. The IPython
Notebook is a web-based interactive computing environment for
exploratory and reproducible computing. With the IPython Notebook, users
create documents, called notebooks, that contain formatted text,
figures, equations, programming code, and code output.&lt;/p&gt;
&lt;p&gt;The IPython Notebook generalizes the notion of output to include images,
LaTeX, video, HTML, JavaScript, PDF, etc. These output formats are
displayed in the Notebook using IPythons display architecture, embedded
in notebook documents and rendered on the IPython Notebook Viewer. By
taking advantage of these rich output formats users can build notebooks
that include rich representations and visualizations of data and other
content. In this tutorial, we will describe the display architecture,
existing Python APIs and libraries that already use it (mpld3, vincent,
polotly, etc.), and how users can define custom display logic for their
own Python objects.&lt;/p&gt;
&lt;p&gt;As of version 2.0, the IPython Notebook also includes interactive
JavaScript widgets. These widgets provide a way for users to interact
with UI controls in the browser that are tied to Python code in running
in the kernel. We will begin by covering the highest-level API for these
widgets, interact, which automatically builds a user interface for
exploring a Python function. Next we will describe the lower-level
widget objects that are included with IPython: sliders, text boxes,
buttons, etc. However, the full potential of the widget framework lies
with its extensibility. Users can create their own custom widgets using
Python, JavaScript, HTML and CSS. We will conclude with a detailed look
at custom widget creation.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Brian Granger</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/fundamentals-of-the-ipython-display-architecture.html</guid><category>ipython</category></item><item><title>Geospatial data in Python: Database, Desktop, and the Web part 1</title><link>https://pyvideo.org/scipy-2014/geospatial-data-in-python-database-desktop-and.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Using the wide range of tools and libraries available for working with
geospatial data, it is now possible to transport geospatial data from a
database to a web-interface in only a few lines of code. In this
tutorial, we explore some of these libraries and work through examples
which showcase the power of Python for geospatial data.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Tools and libraries for working with geospatial data in Python are
currently undergoing rapid development and expansion. Libraries such as
shapely, fiona, rasterio, geopandas, and others now provide Pythonic
ways of reading, writing, editing, and manipulating geographic data. In
this tutorial, participants will be exposed to a number of new and
legacy geospatial libraries in Python, with a focus on simple and rapid
interaction with geospatial data.&lt;/p&gt;
&lt;p&gt;We will utilize Python to interact with geographic data from a database
to a web interface, all the while showcasing how Python can be used to
access data from online resources, query spatially enabled databases,
perform coordinate transformations and geoprocessing functions, and
export geospatial data to web-enabled formats for visualizing and
sharing with others. Time permitting, we will also briefly explore
Python plugin development for the QGIS Desktop GIS environment.&lt;/p&gt;
&lt;p&gt;This tutorial should be accessible to anyone who has basic Python
knowledge (though familiarity with Pandas, NumPy, matplotlib, etc. will
be helpful) as well as familiarity with IPython Notebook. We will take
some time at the start of the tutorial to go over installation
strategies for geospatial libraries (GDAL/OGR, Proj.4, GEOS) and their
Python bindings (Shapely, Fiona, GeoPandas) on Windows, Mac, and Linux.
Some knowledge of geospatial concepts such as map projections and GIS
data formats will also be helpful.&lt;/p&gt;
&lt;div class="section" id="outline"&gt;
&lt;h4&gt;Outline&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to geospatial data&lt;ul&gt;
&lt;li&gt;Map projections, data formats, and looking at maps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Introduction to geospatial libraries&lt;ul&gt;
&lt;li&gt;GDAL/OGR (Fiona); Shapely (GEOS); PostGIS; GeoPandas; and more&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GeoPandas&lt;ul&gt;
&lt;li&gt;Reading data from various sources&lt;/li&gt;
&lt;li&gt;Data manipulation and plotting&lt;/li&gt;
&lt;li&gt;Writing data to various sources&lt;/li&gt;
&lt;li&gt;Getting data from the web&lt;/li&gt;
&lt;li&gt;Pushing data to the web (for maps)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Putting it all together&lt;ul&gt;
&lt;li&gt;Quick example: From database to web&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Introduction to QGIS Desktop GIS (time permitting)&lt;ul&gt;
&lt;li&gt;Python interface (PyQGIS)&lt;/li&gt;
&lt;li&gt;Building a simple plugin&lt;/li&gt;
&lt;li&gt;Plugin deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carson Farmer</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/geospatial-data-in-python-database-desktop-and.html</guid><category>geospatial</category><category>gis</category><category>tutorial</category></item><item><title>Geospatial data in Python: Database, Desktop, and the Web part 2</title><link>https://pyvideo.org/scipy-2014/geospatial-data-in-python-database-desktop-and-0.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Using the wide range of tools and libraries available for working with
geospatial data, it is now possible to transport geospatial data from a
database to a web-interface in only a few lines of code. In this
tutorial, we explore some of these libraries and work through examples
which showcase the power of Python for geospatial data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carson Farmer</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/geospatial-data-in-python-database-desktop-and-0.html</guid><category>geospatial</category><category>gis</category><category>tutorial</category></item><item><title>Geospatial data in Python: Database, Desktop and the Web - Part 3</title><link>https://pyvideo.org/scipy-2014/geospatial-data-in-python-database-desktop-and-1.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Using the wide range of tools and libraries available for working with
geospatial data, it is now possible to transport geospatial data from a
database to a web-interface in only a few lines of code. In this
tutorial, we explore some of these libraries and work through examples
which showcase the power of Python for geospatial data.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Tools and libraries for working with geospatial data in Python are
currently undergoing rapid development and expansion. Libraries such as
shapely, fiona, rasterio, geopandas, and others now provide Pythonic
ways of reading, writing, editing, and manipulating geographic data. In
this tutorial, participants will be exposed to a number of new and
legacy geospatial libraries in Python, with a focus on simple and rapid
interaction with geospatial data.&lt;/p&gt;
&lt;p&gt;We will utilize Python to interact with geographic data from a database
to a web interface, all the while showcasing how Python can be used to
access data from online resources, query spatially enabled databases,
perform coordinate transformations and geoprocessing functions, and
export geospatial data to web-enabled formats for visualizing and
sharing with others. Time permitting, we will also briefly explore
Python plugin development for the QGIS Desktop GIS environment.&lt;/p&gt;
&lt;p&gt;This tutorial should be accessible to anyone who has basic Python
knowledge (though familiarity with Pandas, NumPy, matplotlib, etc. will
be helpful) as well as familiarity with IPython Notebook. We will take
some time at the start of the tutorial to go over installation
strategies for geospatial libraries (GDAL/OGR, Proj.4, GEOS) and their
Python bindings (Shapely, Fiona, GeoPandas) on Windows, Mac, and Linux.
Some knowledge of geospatial concepts such as map projections and GIS
data formats will also be helpful.&lt;/p&gt;
&lt;div class="section" id="outline"&gt;
&lt;h4&gt;Outline&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to geospatial data&lt;ul&gt;
&lt;li&gt;Map projections, data formats, and looking at maps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Introduction to geospatial libraries&lt;ul&gt;
&lt;li&gt;GDAL/OGR (Fiona); Shapely (GEOS); PostGIS; GeoPandas; and more&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GeoPandas&lt;ul&gt;
&lt;li&gt;Reading data from various sources&lt;/li&gt;
&lt;li&gt;Data manipulation and plotting&lt;/li&gt;
&lt;li&gt;Writing data to various sources&lt;/li&gt;
&lt;li&gt;Getting data from the web&lt;/li&gt;
&lt;li&gt;Pushing data to the web (for maps)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Putting it all together&lt;ul&gt;
&lt;li&gt;Quick example: From database to web&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Introduction to QGIS Desktop GIS (time permitting)&lt;ul&gt;
&lt;li&gt;Python interface (PyQGIS)&lt;/li&gt;
&lt;li&gt;Building a simple plugin&lt;/li&gt;
&lt;li&gt;Plugin deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carson Farmer</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/geospatial-data-in-python-database-desktop-and-1.html</guid><category>geospatial</category><category>gis</category><category>tutorial</category></item><item><title>HDF5 is for Lovers part 2</title><link>https://pyvideo.org/scipy-2014/hdf5-is-for-lovers-part-2.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;HDF5 is a hierarchical, binary database format that has become the de
facto standard for scientific computing. While the spec may be used in a
relatively simple way it also supports several high-level features that
prove invaluable. HDF5 bindings exist for almost every language -
including two Python libraries (PyTables and h5py). This tutorial will
cover HDF5 through the lens of PyTables.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="section" id="description"&gt;
&lt;h4&gt;Description&lt;/h4&gt;
&lt;p&gt;HDF5 is a hierarchical, binary database format that has become the de
facto standard for scientific computing. While the specification may be
used in a relatively simple way (persistence of static arrays) it also
supports several high-level features that prove invaluable. These
include chunking, ragged data, extensible data, parallel I/O,
compression, complex selection, and in-core calculations. Moreover, HDF5
bindings exist for almost every language - including two Python
libraries (PyTables and h5py). This tutorial will cover HDF5 itself
through the lens of PyTables.&lt;/p&gt;
&lt;p&gt;This tutorial will discuss tools, strategies, and hacks for really
squeezing every ounce of performance out of HDF5 in new or existing
projects. It will also go over fundamental limitations in the
specification and provide creative and subtle strategies for getting
around them. Overall, this tutorial will show how HDF5 plays nicely with
all parts of an application making the code and data both faster and
smaller. With such powerful features at the developer's disposal, what
is not to love?!&lt;/p&gt;
&lt;p&gt;Knowledge of Python, NumPy, C or C++, and basic HDF5 is recommended but
not required.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="outline"&gt;
&lt;h4&gt;Outline&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Meaning in layout (20 min)&lt;ul&gt;
&lt;li&gt;Tips for choosing your hierarchy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advanced datatypes (20 min)&lt;ul&gt;
&lt;li&gt;Tables&lt;/li&gt;
&lt;li&gt;Nested types&lt;/li&gt;
&lt;li&gt;Tricks with malloc() and byte-counting&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exercise on above topics&lt;/strong&gt; (20 min)&lt;/li&gt;
&lt;li&gt;Chunking (20 min)&lt;ul&gt;
&lt;li&gt;How it works&lt;/li&gt;
&lt;li&gt;How to properly select your chunksize&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Queries and Selections (20 min)&lt;ul&gt;
&lt;li&gt;In-core vs Out-of-core calculations&lt;/li&gt;
&lt;li&gt;PyTables.where()&lt;/li&gt;
&lt;li&gt;Datasets vs Dataspaces&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exercise on above topics&lt;/strong&gt; (20 min)&lt;/li&gt;
&lt;li&gt;The Starving CPU Problem (1 hr)&lt;ul&gt;
&lt;li&gt;Why you should always use compression&lt;/li&gt;
&lt;li&gt;Compression algorithms available&lt;/li&gt;
&lt;li&gt;Choosing the correct one&lt;/li&gt;
&lt;li&gt;Exercise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Integration with other databases (1 hr)&lt;ul&gt;
&lt;li&gt;Migrating to/from SQL&lt;/li&gt;
&lt;li&gt;HDF5 in other databases (JSON example)&lt;/li&gt;
&lt;li&gt;Other Databases in HDF5 (JSON example)&lt;/li&gt;
&lt;li&gt;Exercise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anthony Scopatz</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/hdf5-is-for-lovers-part-2.html</guid><category>HDF5</category><category>PyTables</category><category>tutorial</category></item><item><title>HDF5 is for Lovers, Tutorial part 1</title><link>https://pyvideo.org/scipy-2014/hdf5-is-for-lovers-tutorial-part-1.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;HDF5 is a hierarchical, binary database format that has become the de
facto standard for scientific computing. While the spec may be used in a
relatively simple way it also supports several high-level features that
prove invaluable. HDF5 bindings exist for almost every language -
including two Python libraries (PyTables and h5py). This tutorial will
cover HDF5 through the lens of PyTables.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="section" id="description"&gt;
&lt;h4&gt;Description&lt;/h4&gt;
&lt;p&gt;HDF5 is a hierarchical, binary database format that has become the de
facto standard for scientific computing. While the specification may be
used in a relatively simple way (persistence of static arrays) it also
supports several high-level features that prove invaluable. These
include chunking, ragged data, extensible data, parallel I/O,
compression, complex selection, and in-core calculations. Moreover, HDF5
bindings exist for almost every language - including two Python
libraries (PyTables and h5py). This tutorial will cover HDF5 itself
through the lens of PyTables.&lt;/p&gt;
&lt;p&gt;This tutorial will discuss tools, strategies, and hacks for really
squeezing every ounce of performance out of HDF5 in new or existing
projects. It will also go over fundamental limitations in the
specification and provide creative and subtle strategies for getting
around them. Overall, this tutorial will show how HDF5 plays nicely with
all parts of an application making the code and data both faster and
smaller. With such powerful features at the developer's disposal, what
is not to love?!&lt;/p&gt;
&lt;p&gt;Knowledge of Python, NumPy, C or C++, and basic HDF5 is recommended but
not required.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="outline"&gt;
&lt;h4&gt;Outline&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Meaning in layout (20 min)&lt;ul&gt;
&lt;li&gt;Tips for choosing your hierarchy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advanced datatypes (20 min)&lt;ul&gt;
&lt;li&gt;Tables&lt;/li&gt;
&lt;li&gt;Nested types&lt;/li&gt;
&lt;li&gt;Tricks with malloc() and byte-counting&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exercise on above topics&lt;/strong&gt; (20 min)&lt;/li&gt;
&lt;li&gt;Chunking (20 min)&lt;ul&gt;
&lt;li&gt;How it works&lt;/li&gt;
&lt;li&gt;How to properly select your chunksize&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Queries and Selections (20 min)&lt;ul&gt;
&lt;li&gt;In-core vs Out-of-core calculations&lt;/li&gt;
&lt;li&gt;PyTables.where()&lt;/li&gt;
&lt;li&gt;Datasets vs Dataspaces&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exercise on above topics&lt;/strong&gt; (20 min)&lt;/li&gt;
&lt;li&gt;The Starving CPU Problem (1 hr)&lt;ul&gt;
&lt;li&gt;Why you should always use compression&lt;/li&gt;
&lt;li&gt;Compression algorithms available&lt;/li&gt;
&lt;li&gt;Choosing the correct one&lt;/li&gt;
&lt;li&gt;Exercise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Integration with other databases (1 hr)&lt;ul&gt;
&lt;li&gt;Migrating to/from SQL&lt;/li&gt;
&lt;li&gt;HDF5 in other databases (JSON example)&lt;/li&gt;
&lt;li&gt;Other Databases in HDF5 (JSON example)&lt;/li&gt;
&lt;li&gt;Exercise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anthony Scopatz</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/hdf5-is-for-lovers-tutorial-part-1.html</guid><category>HDF5</category><category>PyTables</category><category>tutorial</category></item><item><title>Image analysis in Python with scipy and scikit image 4</title><link>https://pyvideo.org/scipy-2014/image-analysis-in-python-with-scipy-and-scikit-im.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;From telescopes to satellite cameras to electron microscopes, scientists
are producing more images than they can manually inspect. This tutorial
will introduce automated image analysis using the &amp;quot;images as numpy
arrays&amp;quot; abstraction, run through various fundamental image analysis
operations (filters, morphology, segmentation), and finally complete one
or two more advanced real-world examples.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Image analysis is central to a boggling number of scientific endeavors.
Google needs it for their self-driving cars and to match satellite
imagery and mapping data. Neuroscientists need it to understand the
brain. NASA needs it to &lt;a class="reference external" href="http://www.bbc.co.uk/news/technology-26528516"&gt;map
asteroids&lt;/a&gt; and save
the human race. It is, however, a relatively underdeveloped area of
scientific computing. Attendees will leave this tutorial confident of
their ability to extract information from their images in Python.&lt;/p&gt;
&lt;p&gt;Attendees will need a working knowledge of numpy arrays, but no further
knowledge of images or voxels or other doodads. After a brief
introduction to the idea that images are just arrays and vice versa, we
will introduce fundamental image analysis operations: filters, which can
be used to extract features such as edges, corners, and spots in an
image; morphology, inferring shape properties by modifying the image
through local operations; and segmentation, the division of an image
into meaningful regions.&lt;/p&gt;
&lt;p&gt;We will then combine all these concepts and apply them to several
real-world examples of scientific image analysis: given an image of a
pothole, measure its size in pixels compare the fluorescence intensity
of a protein of interest in the centromeres vs the rest of the
chromosome. observe the distribution of cells invading a wound site&lt;/p&gt;
&lt;p&gt;Attendees will also be encouraged to bring their own image analysis
problems to the session for guidance, and, if time allows, we will cover
more advanced topics such as image registration and stitching.&lt;/p&gt;
&lt;p&gt;The entire tutorial will be coordinated with the IPython notebook, with
various code cells left blank for attendees to fill in as exercises.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/image-analysis-in-python-with-scipy-and-scikit-im.html</guid><category>scikit</category></item><item><title>Image analysis in Python with scipy and scikit image, Part 1</title><link>https://pyvideo.org/scipy-2014/image-analysis-with-scikit-image-part-1.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;From telescopes to satellite cameras to electron microscopes, scientists
are producing more images than they can manually inspect. This tutorial
will introduce automated image analysis using the &amp;quot;images as numpy
arrays&amp;quot; abstraction, run through various fundamental image analysis
operations (filters, morphology, segmentation), and finally complete one
or two more advanced real-world examples.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Image analysis is central to a boggling number of scientific endeavors.
Google needs it for their self-driving cars and to match satellite
imagery and mapping data. Neuroscientists need it to understand the
brain. NASA needs it to &lt;a class="reference external" href="http://www.bbc.co.uk/news/technology-26528516"&gt;map
asteroids&lt;/a&gt; and save
the human race. It is, however, a relatively underdeveloped area of
scientific computing. Attendees will leave this tutorial confident of
their ability to extract information from their images in Python.&lt;/p&gt;
&lt;p&gt;Attendees will need a working knowledge of numpy arrays, but no further
knowledge of images or voxels or other doodads. After a brief
introduction to the idea that images are just arrays and vice versa, we
will introduce fundamental image analysis operations: filters, which can
be used to extract features such as edges, corners, and spots in an
image; morphology, inferring shape properties by modifying the image
through local operations; and segmentation, the division of an image
into meaningful regions.&lt;/p&gt;
&lt;p&gt;We will then combine all these concepts and apply them to several
real-world examples of scientific image analysis: given an image of a
pothole, measure its size in pixels compare the fluorescence intensity
of a protein of interest in the centromeres vs the rest of the
chromosome. observe the distribution of cells invading a wound site&lt;/p&gt;
&lt;p&gt;Attendees will also be encouraged to bring their own image analysis
problems to the session for guidance, and, if time allows, we will cover
more advanced topics such as image registration and stitching.&lt;/p&gt;
&lt;p&gt;The entire tutorial will be coordinated with the IPython notebook, with
various code cells left blank for attendees to fill in as exercises.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/image-analysis-with-scikit-image-part-1.html</guid><category>scikit</category></item><item><title>Image analysis in Python with scipy and scikit image, Part 2</title><link>https://pyvideo.org/scipy-2014/image-analysis-with-scikit-image-part-2.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;From telescopes to satellite cameras to electron microscopes, scientists
are producing more images than they can manually inspect. This tutorial
will introduce automated image analysis using the &amp;quot;images as numpy
arrays&amp;quot; abstraction, run through various fundamental image analysis
operations (filters, morphology, segmentation), and finally complete one
or two more advanced real-world examples.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Image analysis is central to a boggling number of scientific endeavors.
Google needs it for their self-driving cars and to match satellite
imagery and mapping data. Neuroscientists need it to understand the
brain. NASA needs it to &lt;a class="reference external" href="http://www.bbc.co.uk/news/technology-26528516"&gt;map
asteroids&lt;/a&gt; and save
the human race. It is, however, a relatively underdeveloped area of
scientific computing. Attendees will leave this tutorial confident of
their ability to extract information from their images in Python.&lt;/p&gt;
&lt;p&gt;Attendees will need a working knowledge of numpy arrays, but no further
knowledge of images or voxels or other doodads. After a brief
introduction to the idea that images are just arrays and vice versa, we
will introduce fundamental image analysis operations: filters, which can
be used to extract features such as edges, corners, and spots in an
image; morphology, inferring shape properties by modifying the image
through local operations; and segmentation, the division of an image
into meaningful regions.&lt;/p&gt;
&lt;p&gt;We will then combine all these concepts and apply them to several
real-world examples of scientific image analysis: given an image of a
pothole, measure its size in pixels compare the fluorescence intensity
of a protein of interest in the centromeres vs the rest of the
chromosome. observe the distribution of cells invading a wound site&lt;/p&gt;
&lt;p&gt;Attendees will also be encouraged to bring their own image analysis
problems to the session for guidance, and, if time allows, we will cover
more advanced topics such as image registration and stitching.&lt;/p&gt;
&lt;p&gt;The entire tutorial will be coordinated with the IPython notebook, with
various code cells left blank for attendees to fill in as exercises.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/image-analysis-with-scikit-image-part-2.html</guid><category>scikit</category></item><item><title>Image analysis in Python with scipy and scikit image, Part 3</title><link>https://pyvideo.org/scipy-2014/image-analysis-with-scikit-image-part-3.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;From telescopes to satellite cameras to electron microscopes, scientists
are producing more images than they can manually inspect. This tutorial
will introduce automated image analysis using the &amp;quot;images as numpy
arrays&amp;quot; abstraction, run through various fundamental image analysis
operations (filters, morphology, segmentation), and finally complete one
or two more advanced real-world examples.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Image analysis is central to a boggling number of scientific endeavors.
Google needs it for their self-driving cars and to match satellite
imagery and mapping data. Neuroscientists need it to understand the
brain. NASA needs it to &lt;a class="reference external" href="http://www.bbc.co.uk/news/technology-26528516"&gt;map
asteroids&lt;/a&gt; and save
the human race. It is, however, a relatively underdeveloped area of
scientific computing. Attendees will leave this tutorial confident of
their ability to extract information from their images in Python.&lt;/p&gt;
&lt;p&gt;Attendees will need a working knowledge of numpy arrays, but no further
knowledge of images or voxels or other doodads. After a brief
introduction to the idea that images are just arrays and vice versa, we
will introduce fundamental image analysis operations: filters, which can
be used to extract features such as edges, corners, and spots in an
image; morphology, inferring shape properties by modifying the image
through local operations; and segmentation, the division of an image
into meaningful regions.&lt;/p&gt;
&lt;p&gt;We will then combine all these concepts and apply them to several
real-world examples of scientific image analysis: given an image of a
pothole, measure its size in pixels compare the fluorescence intensity
of a protein of interest in the centromeres vs the rest of the
chromosome. observe the distribution of cells invading a wound site&lt;/p&gt;
&lt;p&gt;Attendees will also be encouraged to bring their own image analysis
problems to the session for guidance, and, if time allows, we will cover
more advanced topics such as image registration and stitching.&lt;/p&gt;
&lt;p&gt;The entire tutorial will be coordinated with the IPython notebook, with
various code cells left blank for attendees to fill in as exercises.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/image-analysis-with-scikit-image-part-3.html</guid><category>scikit</category></item><item><title>Integrating Python and C++ with Boost Python part 2</title><link>https://pyvideo.org/scipy-2014/integrating-python-and-c-with-boost-python-part.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Python and C++ can be powerful complements to one another. C++ is great
for performance-critical calculations, while Python is great for
everything else. In this tutorial well look at how to integrate Python
and C++ using the Boost.Python library. Youll learn techniques for
easily developing hybrid systems that use the right language for the
right task, resulting in better software.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python and C++ are both popular languages that each bring a lot to the
table. The languages also complement one another well: Python is
high-level, dynamic, and easy to use while C++ is at-the-metal, static,
and (in)famously tricky. There are times when there are real advantages
to combining these disparate natures, and Pythons C API provides a
strong interface for doing just that. Boost.Python is a C++ library that
builds upon and improves Pythons C API to give users a simpler, more
intuitive, and safer means to integrate Python and C++.&lt;/p&gt;
&lt;p&gt;In this tutorial well look at how to use Boost.Python to effectively
bridge the Python/C++ boundary. Well start by briefly looking at the
fundamentals of the Python C API since that defines the ground rules;
this includes things like reference counting, the basic object model,
and so forth. Well then quickly look at the Boost.Python API and show
how it provides the same functionality as the underlying C API, but does
so in a way that doesnt obscure the real semantics of the Python
language.&lt;/p&gt;
&lt;p&gt;After this introduction, the rest of the tutorial will involve writing
code to explore various elements of Boost.Python. Well focus on
techniques for extending Python with C++, that is, writing Python
modules in C++. Boost.Python can be used for embedding (i.e. invoking
Python code from C++), but that involves a different set of techniques,
and in practice most scientific Python developers are more interested in
developing extensions.&lt;/p&gt;
&lt;p&gt;The syllabus for the four-hour tutorial will be like this:&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;Introduction: C-API and Boost.Python basics&lt;/p&gt;
&lt;p&gt;Note that this can be reduced or eliminated of participants are
already comfortable with the topics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Hello World: Exposing a basic function&lt;/p&gt;
&lt;p&gt;In this section well get a minimal Boost.Python module working. This
will not only introduce students to the infrastructure of
Boost.Python, but it will also give us a chance to make sure that
everyones build environment is working.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Exposing functions&lt;/p&gt;
&lt;p&gt;In this section well look at the details of exposing C++ functions
to Python. The topics well cover will include overloading (including
Boost.Pythons auto-overload feature), default argument values, and a
brief look at call policies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Exposing classes&lt;/p&gt;
&lt;p&gt;Here well look at how to expose C++ classes to Python. Topics will
include the basic &lt;tt class="docutils literal"&gt;class_&amp;lt;T&amp;gt;&lt;/tt&gt; template, member functions, data
members, properties, inheritance, and virtual functions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt; class is Boost.Pythons primary
interface to Pythons &lt;tt class="docutils literal"&gt;PyObject&lt;/tt&gt; structure. Understanding how to
work with this class is a key building-block for developing Python
modules with Boost.Python. Well explore its API and features,
including areas like attribute access, reference counting, and
converting between Python and C++ objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Derived object types&lt;/p&gt;
&lt;p&gt;Boost.Python provides a number of &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt;
subclasses for important Python classes like &lt;tt class="docutils literal"&gt;list&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt;, and
&lt;tt class="docutils literal"&gt;tuple&lt;/tt&gt;. In this section well look at these subclasses and how to
use them in Boost.Python modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Enums&lt;/p&gt;
&lt;p&gt;Boost.Python provides &lt;tt class="docutils literal"&gt;enum_&amp;lt;T&amp;gt;&lt;/tt&gt; for exposing C++ enums to Python.
Python doesnt have a notion of enums &lt;em&gt;per se&lt;/em&gt;, but in this section
well explore how this template makes it straightforward to use C++
enums in Python in a simple and intuitive way.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Type conversion&lt;/p&gt;
&lt;p&gt;In this section well look at Boost.Pythons support for doing
automatic type-conversion across the Python/C++ boundary. Well see
how you can register type-converters with Boost.Python which will be
invoked whenever Boost.Python needs to convert a Python object to a
C++ object or vice versa.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a fairly ambitious set of topics, and its possible that we
wont be able to cover them all. The topics are roughly in
most-often-used to least-often-used order, however, so students will be
sure to be exposed to the most important and relevant elements of the
course.&lt;/p&gt;
&lt;p&gt;Likewise, the four-hour format of the course means that we wont be able
to go into great depth on many topics. The main goal of the course,
then, is to give students enough orientation and hands-on experience
with Boost.Python that they can continue to learn on their own.
Inter-language integration - especially between languages as dissimilar
as C++ and Python - can be quite complex, but this tutorial will give
students the grounding they need to successfully apply Boost.Python to
their problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Austin Bingham</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/integrating-python-and-c-with-boost-python-part.html</guid><category>boost</category><category>c++</category></item><item><title>Integrating Python and C++ with Boost Python part 1</title><link>https://pyvideo.org/scipy-2014/integrating-python-and-c-with-boost-python-part-0.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Python and C++ can be powerful complements to one another. C++ is great
for performance-critical calculations, while Python is great for
everything else. In this tutorial well look at how to integrate Python
and C++ using the Boost.Python library. Youll learn techniques for
easily developing hybrid systems that use the right language for the
right task, resulting in better software.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python and C++ are both popular languages that each bring a lot to the
table. The languages also complement one another well: Python is
high-level, dynamic, and easy to use while C++ is at-the-metal, static,
and (in)famously tricky. There are times when there are real advantages
to combining these disparate natures, and Pythons C API provides a
strong interface for doing just that. Boost.Python is a C++ library that
builds upon and improves Pythons C API to give users a simpler, more
intuitive, and safer means to integrate Python and C++.&lt;/p&gt;
&lt;p&gt;In this tutorial well look at how to use Boost.Python to effectively
bridge the Python/C++ boundary. Well start by briefly looking at the
fundamentals of the Python C API since that defines the ground rules;
this includes things like reference counting, the basic object model,
and so forth. Well then quickly look at the Boost.Python API and show
how it provides the same functionality as the underlying C API, but does
so in a way that doesnt obscure the real semantics of the Python
language.&lt;/p&gt;
&lt;p&gt;After this introduction, the rest of the tutorial will involve writing
code to explore various elements of Boost.Python. Well focus on
techniques for extending Python with C++, that is, writing Python
modules in C++. Boost.Python can be used for embedding (i.e. invoking
Python code from C++), but that involves a different set of techniques,
and in practice most scientific Python developers are more interested in
developing extensions.&lt;/p&gt;
&lt;p&gt;The syllabus for the four-hour tutorial will be like this:&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;Introduction: C-API and Boost.Python basics&lt;/p&gt;
&lt;p&gt;Note that this can be reduced or eliminated of participants are
already comfortable with the topics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Hello World: Exposing a basic function&lt;/p&gt;
&lt;p&gt;In this section well get a minimal Boost.Python module working. This
will not only introduce students to the infrastructure of
Boost.Python, but it will also give us a chance to make sure that
everyones build environment is working.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Exposing functions&lt;/p&gt;
&lt;p&gt;In this section well look at the details of exposing C++ functions
to Python. The topics well cover will include overloading (including
Boost.Pythons auto-overload feature), default argument values, and a
brief look at call policies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Exposing classes&lt;/p&gt;
&lt;p&gt;Here well look at how to expose C++ classes to Python. Topics will
include the basic &lt;tt class="docutils literal"&gt;class_&amp;lt;T&amp;gt;&lt;/tt&gt; template, member functions, data
members, properties, inheritance, and virtual functions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt; class is Boost.Pythons primary
interface to Pythons &lt;tt class="docutils literal"&gt;PyObject&lt;/tt&gt; structure. Understanding how to
work with this class is a key building-block for developing Python
modules with Boost.Python. Well explore its API and features,
including areas like attribute access, reference counting, and
converting between Python and C++ objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Derived object types&lt;/p&gt;
&lt;p&gt;Boost.Python provides a number of &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt;
subclasses for important Python classes like &lt;tt class="docutils literal"&gt;list&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt;, and
&lt;tt class="docutils literal"&gt;tuple&lt;/tt&gt;. In this section well look at these subclasses and how to
use them in Boost.Python modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Enums&lt;/p&gt;
&lt;p&gt;Boost.Python provides &lt;tt class="docutils literal"&gt;enum_&amp;lt;T&amp;gt;&lt;/tt&gt; for exposing C++ enums to Python.
Python doesnt have a notion of enums &lt;em&gt;per se&lt;/em&gt;, but in this section
well explore how this template makes it straightforward to use C++
enums in Python in a simple and intuitive way.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Type conversion&lt;/p&gt;
&lt;p&gt;In this section well look at Boost.Pythons support for doing
automatic type-conversion across the Python/C++ boundary. Well see
how you can register type-converters with Boost.Python which will be
invoked whenever Boost.Python needs to convert a Python object to a
C++ object or vice versa.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a fairly ambitious set of topics, and its possible that we
wont be able to cover them all. The topics are roughly in
most-often-used to least-often-used order, however, so students will be
sure to be exposed to the most important and relevant elements of the
course.&lt;/p&gt;
&lt;p&gt;Likewise, the four-hour format of the course means that we wont be able
to go into great depth on many topics. The main goal of the course,
then, is to give students enough orientation and hands-on experience
with Boost.Python that they can continue to learn on their own.
Inter-language integration - especially between languages as dissimilar
as C++ and Python - can be quite complex, but this tutorial will give
students the grounding they need to successfully apply Boost.Python to
their problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Austin Bingham</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/integrating-python-and-c-with-boost-python-part-0.html</guid><category>boost</category><category>c++</category></item><item><title>Integrating Python and C++ with Boost Python part 3</title><link>https://pyvideo.org/scipy-2014/integrating-python-and-c-with-boost-python-part-1.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Python and C++ can be powerful complements to one another. C++ is great
for performance-critical calculations, while Python is great for
everything else. In this tutorial well look at how to integrate Python
and C++ using the Boost.Python library. Youll learn techniques for
easily developing hybrid systems that use the right language for the
right task, resulting in better software.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python and C++ are both popular languages that each bring a lot to the
table. The languages also complement one another well: Python is
high-level, dynamic, and easy to use while C++ is at-the-metal, static,
and (in)famously tricky. There are times when there are real advantages
to combining these disparate natures, and Pythons C API provides a
strong interface for doing just that. Boost.Python is a C++ library that
builds upon and improves Pythons C API to give users a simpler, more
intuitive, and safer means to integrate Python and C++.&lt;/p&gt;
&lt;p&gt;In this tutorial well look at how to use Boost.Python to effectively
bridge the Python/C++ boundary. Well start by briefly looking at the
fundamentals of the Python C API since that defines the ground rules;
this includes things like reference counting, the basic object model,
and so forth. Well then quickly look at the Boost.Python API and show
how it provides the same functionality as the underlying C API, but does
so in a way that doesnt obscure the real semantics of the Python
language.&lt;/p&gt;
&lt;p&gt;After this introduction, the rest of the tutorial will involve writing
code to explore various elements of Boost.Python. Well focus on
techniques for extending Python with C++, that is, writing Python
modules in C++. Boost.Python can be used for embedding (i.e. invoking
Python code from C++), but that involves a different set of techniques,
and in practice most scientific Python developers are more interested in
developing extensions.&lt;/p&gt;
&lt;p&gt;The syllabus for the four-hour tutorial will be like this:&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;Introduction: C-API and Boost.Python basics&lt;/p&gt;
&lt;p&gt;Note that this can be reduced or eliminated of participants are
already comfortable with the topics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Hello World: Exposing a basic function&lt;/p&gt;
&lt;p&gt;In this section well get a minimal Boost.Python module working. This
will not only introduce students to the infrastructure of
Boost.Python, but it will also give us a chance to make sure that
everyones build environment is working.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Exposing functions&lt;/p&gt;
&lt;p&gt;In this section well look at the details of exposing C++ functions
to Python. The topics well cover will include overloading (including
Boost.Pythons auto-overload feature), default argument values, and a
brief look at call policies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Exposing classes&lt;/p&gt;
&lt;p&gt;Here well look at how to expose C++ classes to Python. Topics will
include the basic &lt;tt class="docutils literal"&gt;class_&amp;lt;T&amp;gt;&lt;/tt&gt; template, member functions, data
members, properties, inheritance, and virtual functions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt; class is Boost.Pythons primary
interface to Pythons &lt;tt class="docutils literal"&gt;PyObject&lt;/tt&gt; structure. Understanding how to
work with this class is a key building-block for developing Python
modules with Boost.Python. Well explore its API and features,
including areas like attribute access, reference counting, and
converting between Python and C++ objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Derived object types&lt;/p&gt;
&lt;p&gt;Boost.Python provides a number of &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt;
subclasses for important Python classes like &lt;tt class="docutils literal"&gt;list&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt;, and
&lt;tt class="docutils literal"&gt;tuple&lt;/tt&gt;. In this section well look at these subclasses and how to
use them in Boost.Python modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Enums&lt;/p&gt;
&lt;p&gt;Boost.Python provides &lt;tt class="docutils literal"&gt;enum_&amp;lt;T&amp;gt;&lt;/tt&gt; for exposing C++ enums to Python.
Python doesnt have a notion of enums &lt;em&gt;per se&lt;/em&gt;, but in this section
well explore how this template makes it straightforward to use C++
enums in Python in a simple and intuitive way.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Type conversion&lt;/p&gt;
&lt;p&gt;In this section well look at Boost.Pythons support for doing
automatic type-conversion across the Python/C++ boundary. Well see
how you can register type-converters with Boost.Python which will be
invoked whenever Boost.Python needs to convert a Python object to a
C++ object or vice versa.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a fairly ambitious set of topics, and its possible that we
wont be able to cover them all. The topics are roughly in
most-often-used to least-often-used order, however, so students will be
sure to be exposed to the most important and relevant elements of the
course.&lt;/p&gt;
&lt;p&gt;Likewise, the four-hour format of the course means that we wont be able
to go into great depth on many topics. The main goal of the course,
then, is to give students enough orientation and hands-on experience
with Boost.Python that they can continue to learn on their own.
Inter-language integration - especially between languages as dissimilar
as C++ and Python - can be quite complex, but this tutorial will give
students the grounding they need to successfully apply Boost.Python to
their problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Austin Bingham</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/integrating-python-and-c-with-boost-python-part-1.html</guid><category>boost</category><category>c++</category></item><item><title>Integrating Python and C++ with Boost Python part 4</title><link>https://pyvideo.org/scipy-2014/integrating-python-and-c-with-boost-python-part-2.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Python and C++ can be powerful complements to one another. C++ is great
for performance-critical calculations, while Python is great for
everything else. In this tutorial well look at how to integrate Python
and C++ using the Boost.Python library. Youll learn techniques for
easily developing hybrid systems that use the right language for the
right task, resulting in better software.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python and C++ are both popular languages that each bring a lot to the
table. The languages also complement one another well: Python is
high-level, dynamic, and easy to use while C++ is at-the-metal, static,
and (in)famously tricky. There are times when there are real advantages
to combining these disparate natures, and Pythons C API provides a
strong interface for doing just that. Boost.Python is a C++ library that
builds upon and improves Pythons C API to give users a simpler, more
intuitive, and safer means to integrate Python and C++.&lt;/p&gt;
&lt;p&gt;In this tutorial well look at how to use Boost.Python to effectively
bridge the Python/C++ boundary. Well start by briefly looking at the
fundamentals of the Python C API since that defines the ground rules;
this includes things like reference counting, the basic object model,
and so forth. Well then quickly look at the Boost.Python API and show
how it provides the same functionality as the underlying C API, but does
so in a way that doesnt obscure the real semantics of the Python
language.&lt;/p&gt;
&lt;p&gt;After this introduction, the rest of the tutorial will involve writing
code to explore various elements of Boost.Python. Well focus on
techniques for extending Python with C++, that is, writing Python
modules in C++. Boost.Python can be used for embedding (i.e. invoking
Python code from C++), but that involves a different set of techniques,
and in practice most scientific Python developers are more interested in
developing extensions.&lt;/p&gt;
&lt;p&gt;The syllabus for the four-hour tutorial will be like this:&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;Introduction: C-API and Boost.Python basics&lt;/p&gt;
&lt;p&gt;Note that this can be reduced or eliminated of participants are
already comfortable with the topics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Hello World: Exposing a basic function&lt;/p&gt;
&lt;p&gt;In this section well get a minimal Boost.Python module working. This
will not only introduce students to the infrastructure of
Boost.Python, but it will also give us a chance to make sure that
everyones build environment is working.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Exposing functions&lt;/p&gt;
&lt;p&gt;In this section well look at the details of exposing C++ functions
to Python. The topics well cover will include overloading (including
Boost.Pythons auto-overload feature), default argument values, and a
brief look at call policies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Exposing classes&lt;/p&gt;
&lt;p&gt;Here well look at how to expose C++ classes to Python. Topics will
include the basic &lt;tt class="docutils literal"&gt;class_&amp;lt;T&amp;gt;&lt;/tt&gt; template, member functions, data
members, properties, inheritance, and virtual functions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt; class is Boost.Pythons primary
interface to Pythons &lt;tt class="docutils literal"&gt;PyObject&lt;/tt&gt; structure. Understanding how to
work with this class is a key building-block for developing Python
modules with Boost.Python. Well explore its API and features,
including areas like attribute access, reference counting, and
converting between Python and C++ objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Derived object types&lt;/p&gt;
&lt;p&gt;Boost.Python provides a number of &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;boost::python::object&lt;/span&gt;&lt;/tt&gt;
subclasses for important Python classes like &lt;tt class="docutils literal"&gt;list&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt;, and
&lt;tt class="docutils literal"&gt;tuple&lt;/tt&gt;. In this section well look at these subclasses and how to
use them in Boost.Python modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Enums&lt;/p&gt;
&lt;p&gt;Boost.Python provides &lt;tt class="docutils literal"&gt;enum_&amp;lt;T&amp;gt;&lt;/tt&gt; for exposing C++ enums to Python.
Python doesnt have a notion of enums &lt;em&gt;per se&lt;/em&gt;, but in this section
well explore how this template makes it straightforward to use C++
enums in Python in a simple and intuitive way.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Type conversion&lt;/p&gt;
&lt;p&gt;In this section well look at Boost.Pythons support for doing
automatic type-conversion across the Python/C++ boundary. Well see
how you can register type-converters with Boost.Python which will be
invoked whenever Boost.Python needs to convert a Python object to a
C++ object or vice versa.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a fairly ambitious set of topics, and its possible that we
wont be able to cover them all. The topics are roughly in
most-often-used to least-often-used order, however, so students will be
sure to be exposed to the most important and relevant elements of the
course.&lt;/p&gt;
&lt;p&gt;Likewise, the four-hour format of the course means that we wont be able
to go into great depth on many topics. The main goal of the course,
then, is to give students enough orientation and hands-on experience
with Boost.Python that they can continue to learn on their own.
Inter-language integration - especially between languages as dissimilar
as C++ and Python - can be quite complex, but this tutorial will give
students the grounding they need to successfully apply Boost.Python to
their problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Austin Bingham</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/integrating-python-and-c-with-boost-python-part-2.html</guid><category>boost</category><category>c++</category></item><item><title>Interactive Parallel Computing with IPython Part 1</title><link>https://pyvideo.org/scipy-2014/interactive-parallel-computing-with-ipython-part.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Learn about interactive parallel computing in IPython.parallel, with
examples including parallel image processing, machine learning, and
physical simulations. IPython provides an easy way to interact with your
multicore laptop or compute cluster.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;IPython provides tools for interactive exploration of code and data.
IPython.parallel is the part of IPython that enables an interactive
model for parallel execution, and aims to make distributing your work on
a multicore computer, local clusters or cloud services such as AWS or MS
Azure simple and straightforward. The tutorial will cover how to do
interactive and asynchronous parallel computing with IPython, and how to
get the most out of your IPython cluster. Some of IPythons novel
interactive features will be demonstrated, such as automatically
parallelizing code with magics in the IPython Notebook and interactive
debugging of remote execution. Examples covered will include parallel
image processing, machine learning, and physical simulations, with
exercises to solve along the way.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to IPython.parallel&lt;/li&gt;
&lt;li&gt;Deploying IPython&lt;/li&gt;
&lt;li&gt;Using DirectViews and LoadBalancedViews&lt;/li&gt;
&lt;li&gt;The basic model for execution&lt;/li&gt;
&lt;li&gt;Getting to know your IPython cluster:&lt;/li&gt;
&lt;li&gt;Working with remote namespaces&lt;/li&gt;
&lt;li&gt;AsyncResult: the API for asynchronous execution&lt;/li&gt;
&lt;li&gt;Interacting with incomplete results. Remember, its about
interactivity&lt;/li&gt;
&lt;li&gt;Interactive parallel plotting&lt;/li&gt;
&lt;li&gt;More advanced topics:&lt;/li&gt;
&lt;li&gt;Using IPython.parallel with traditional (MPI) parallel programs&lt;/li&gt;
&lt;li&gt;Debugging parallel code&lt;/li&gt;
&lt;li&gt;Minimizing data movement&lt;/li&gt;
&lt;li&gt;Task dependencies&lt;/li&gt;
&lt;li&gt;Caveats and tuning tips for IPython.parallel&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fernando Prez</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/interactive-parallel-computing-with-ipython-part.html</guid><category>ipython</category></item><item><title>Interactive Parallel Computing with IPython Part 2</title><link>https://pyvideo.org/scipy-2014/interactive-parallel-computing-with-ipython-part-0.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Learn about interactive parallel computing in IPython.parallel, with
examples including parallel image processing, machine learning, and
physical simulations. IPython provides an easy way to interact with your
multicore laptop or compute cluster.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;IPython provides tools for interactive exploration of code and data.
IPython.parallel is the part of IPython that enables an interactive
model for parallel execution, and aims to make distributing your work on
a multicore computer, local clusters or cloud services such as AWS or MS
Azure simple and straightforward. The tutorial will cover how to do
interactive and asynchronous parallel computing with IPython, and how to
get the most out of your IPython cluster. Some of IPythons novel
interactive features will be demonstrated, such as automatically
parallelizing code with magics in the IPython Notebook and interactive
debugging of remote execution. Examples covered will include parallel
image processing, machine learning, and physical simulations, with
exercises to solve along the way.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to IPython.parallel&lt;/li&gt;
&lt;li&gt;Deploying IPython&lt;/li&gt;
&lt;li&gt;Using DirectViews and LoadBalancedViews&lt;/li&gt;
&lt;li&gt;The basic model for execution&lt;/li&gt;
&lt;li&gt;Getting to know your IPython cluster:&lt;/li&gt;
&lt;li&gt;Working with remote namespaces&lt;/li&gt;
&lt;li&gt;AsyncResult: the API for asynchronous execution&lt;/li&gt;
&lt;li&gt;Interacting with incomplete results. Remember, its about
interactivity&lt;/li&gt;
&lt;li&gt;Interactive parallel plotting&lt;/li&gt;
&lt;li&gt;More advanced topics:&lt;/li&gt;
&lt;li&gt;Using IPython.parallel with traditional (MPI) parallel programs&lt;/li&gt;
&lt;li&gt;Debugging parallel code&lt;/li&gt;
&lt;li&gt;Minimizing data movement&lt;/li&gt;
&lt;li&gt;Task dependencies&lt;/li&gt;
&lt;li&gt;Caveats and tuning tips for IPython.parallel&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fernando Prez</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/interactive-parallel-computing-with-ipython-part-0.html</guid><category>ipython</category></item><item><title>Interactive Parallel Computing with IPython Part 3</title><link>https://pyvideo.org/scipy-2014/interactive-parallel-computing-with-ipython-part-1.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Learn about interactive parallel computing in IPython.parallel, with
examples including parallel image processing, machine learning, and
physical simulations. IPython provides an easy way to interact with your
multicore laptop or compute cluster.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;IPython provides tools for interactive exploration of code and data.
IPython.parallel is the part of IPython that enables an interactive
model for parallel execution, and aims to make distributing your work on
a multicore computer, local clusters or cloud services such as AWS or MS
Azure simple and straightforward. The tutorial will cover how to do
interactive and asynchronous parallel computing with IPython, and how to
get the most out of your IPython cluster. Some of IPythons novel
interactive features will be demonstrated, such as automatically
parallelizing code with magics in the IPython Notebook and interactive
debugging of remote execution. Examples covered will include parallel
image processing, machine learning, and physical simulations, with
exercises to solve along the way.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to IPython.parallel&lt;/li&gt;
&lt;li&gt;Deploying IPython&lt;/li&gt;
&lt;li&gt;Using DirectViews and LoadBalancedViews&lt;/li&gt;
&lt;li&gt;The basic model for execution&lt;/li&gt;
&lt;li&gt;Getting to know your IPython cluster:&lt;/li&gt;
&lt;li&gt;Working with remote namespaces&lt;/li&gt;
&lt;li&gt;AsyncResult: the API for asynchronous execution&lt;/li&gt;
&lt;li&gt;Interacting with incomplete results. Remember, its about
interactivity&lt;/li&gt;
&lt;li&gt;Interactive parallel plotting&lt;/li&gt;
&lt;li&gt;More advanced topics:&lt;/li&gt;
&lt;li&gt;Using IPython.parallel with traditional (MPI) parallel programs&lt;/li&gt;
&lt;li&gt;Debugging parallel code&lt;/li&gt;
&lt;li&gt;Minimizing data movement&lt;/li&gt;
&lt;li&gt;Task dependencies&lt;/li&gt;
&lt;li&gt;Caveats and tuning tips for IPython.parallel&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fernando Prez</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/interactive-parallel-computing-with-ipython-part-1.html</guid><category>ipython</category></item><item><title>Intergrating Pylearn2 and Hyperopt: Taking Deep Learning Further with Hyperparamter Optimization</title><link>https://pyvideo.org/scipy-2014/intergrating-pylearn2-and-hyperopt-taking-deep-l.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This talk/poster will outline and present recent work in integrating
Hyperopt, a package for the optimization of the hyperparameters of
machine learning algorithms, with Pylearn2, a machine learning research
and prototyping framework focused on &amp;quot;deep learning&amp;quot; algorithms, the
technical challenges we faced and how we addressed them.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Deep learning algorithms have recently garnered much attention for their
successes in solving very difficult industrial machine perception
problems. However, for many practical purposes, these algorithms are
unwieldy due to the rapid proliferation of &amp;quot;hyperparameters&amp;quot; in their
specification -- architectural and optimization constants which
ordinarily must be specified a priori by the practitioner. There is a
growing interest within the machine learning community, and acutely so
amongst deep learning researchers, in intelligently automating the
selection of hyperparameters for machine learning algorithms by through
the use of sequential model-based optimization techniques.
[Hyperopt][&lt;a class="reference external" href="http://hyperopt.github.io/hyperopt/"&gt;http://hyperopt.github.io/hyperopt/&lt;/a&gt;] is software package
designed for this purpose, architected as a general framework for
hyperparameter optimization algorithms with support for complicated,
awkward hyperparameter spaces that, e.g., involve many hyperparameters
that are only meaningful in the context of certain values of other
hyperparameters.&lt;/p&gt;
&lt;p&gt;[Pylearn2][&lt;a class="reference external" href="http://deeplearning.net/software/pylearn2"&gt;http://deeplearning.net/software/pylearn2&lt;/a&gt;] is a framework for
machine learning developed by the LISA laboratory at Universit de
Montral; it is a research and prototyping library aimed primarily at
machine learning researchers, with a focus on &amp;quot;deep learning&amp;quot;
algorithms. Despite being far from a stable release, it has had
considerable impact and developed a very active user community outside
of the laboratory that birthed it.&lt;/p&gt;
&lt;p&gt;This talk will deecribe recent efforts in building a flexible,
user-friendly bridge between Pylearn2 and Hyperopt for the purpose of
optimizing the hyperparameters of deep learning algorithms. Briefly, it
will outline the relevant problem domain and the two packages, the
technical challenges we've met in adapting the two for use with one
another and our solutions to them, in particular the development of a
novel common deferred evaluation/call-graph description language based
on &lt;tt class="docutils literal"&gt;functools.partial&lt;/tt&gt;, which we hope to make available in the near
future as a standalone package.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Warde-Farley</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/intergrating-pylearn2-and-hyperopt-taking-deep-l.html</guid><category>machine learning</category></item><item><title>Introduction to Julia - Part 1</title><link>https://pyvideo.org/scipy-2014/introduction-to-julia-part-1.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;An introduction to the new Julia language from scratch, emphasising
similarities and differences with scientific Python.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Julia is a new, up-and-coming language that has many similarities to
Python, but some differences. One of its main advantages is the speed
gain obtained by automatically compiling all code (in a somewhat similar
way to &lt;tt class="docutils literal"&gt;PyPy&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;Cython&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;numba&lt;/tt&gt;, etc.), despite having an
interactive interface very similar to that of Python.&lt;/p&gt;
&lt;p&gt;This will be a tutorial on the basic features of Julia from scratch,
given by a user (rather than a developer) of the language, emphasising
those features which are similar to Python (and hence do not require
much explanation) and those features which are rather different.&lt;/p&gt;
&lt;p&gt;The idea of the tutorial is to give an idea of why there is suddenly
such a buzz around Julia and why it can be useful for certain projects.&lt;/p&gt;
&lt;p&gt;This tutorial is aimed at people who are already familiar with the basic
scientific Python packages; it is not aimed at beginners in scientific
programming.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David P. Sanders</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/introduction-to-julia-part-1.html</guid><category>Tech</category></item><item><title>Introduction to Julia - Part 2</title><link>https://pyvideo.org/scipy-2014/introduction-to-julia-part-2.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;An introduction to the new Julia language from scratch, emphasising
similarities and differences with scientific Python.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Julia is a new, up-and-coming language that has many similarities to
Python, but some differences. One of its main advantages is the speed
gain obtained by automatically compiling all code (in a somewhat similar
way to &lt;tt class="docutils literal"&gt;PyPy&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;Cython&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;numba&lt;/tt&gt;, etc.), despite having an
interactive interface very similar to that of Python.&lt;/p&gt;
&lt;p&gt;This will be a tutorial on the basic features of Julia from scratch,
given by a user (rather than a developer) of the language, emphasising
those features which are similar to Python (and hence do not require
much explanation) and those features which are rather different.&lt;/p&gt;
&lt;p&gt;The idea of the tutorial is to give an idea of why there is suddenly
such a buzz around Julia and why it can be useful for certain projects.&lt;/p&gt;
&lt;p&gt;This tutorial is aimed at people who are already familiar with the basic
scientific Python packages; it is not aimed at beginners in scientific
programming.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David P. Sanders</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/introduction-to-julia-part-2.html</guid><category>julia</category></item><item><title>Keynote: Computational Thinking is Computational Learning</title><link>https://pyvideo.org/scipy-2014/keynote-lorena-barba.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lorena Barba</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/keynote-lorena-barba.html</guid></item><item><title>Lightning Talks Tuesday July 8 2014</title><link>https://pyvideo.org/scipy-2014/lightning-talks-tuesday-july-8-2014.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Various speakers</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/lightning-talks-tuesday-july-8-2014.html</guid><category>lightning talks</category></item><item><title>Multibody Dynamics and Control with Python part 1</title><link>https://pyvideo.org/scipy-2014/multibody-dynamics-and-control-with-python-part-1.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;In this tutorial, attendees will learn how to derive, simulate, and
visualize the motion of a multibody dynamic system with Python tools.
These methods and techniques play an important role in the design and
understanding of robots, vehicles, spacecraft, manufacturing machines,
human motion, etc. Attendees will develop code to simulate the motion of
a human or humanoid robot.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this tutorial, attendees will learn how to derive, simulate, and
visualize the motion of a multibody dynamic system with Python tools.
The tutorial will demonstrate an advanced symbolic and numeric pipeline
for a typical multibody simulation problem. These methods and techniques
play an important role in the design and understanding of robots,
vehicles, spacecraft, manufacturing machines, human motion, etc. At the
end, the attendees will have developed code to simulate the uncontrolled
and controlled motion of a human or humanoid robot.&lt;/p&gt;
&lt;p&gt;We will highlight the derivation of realistic models of motion with the
SymPy Mechanics package. We will walk through the derivation of the
equations of motion of a multibody system (i.e. the model or the plant),
simulating and visualizing the free motion of the system, and finally we
will addfeedback controllers to control the plants that we derive.&lt;/p&gt;
&lt;p&gt;It is best if the attendees have some background with calculus-based
college level physics. They should also be familiar with the SciPy
Stack, in particular IPython, SymPy, NumPy, and SciPy. Our goal is that
attendees will come away with the ability to model basic multibody
systems, simulate and visualize the motion, and apply feedback
controllers all in a Python framework.&lt;/p&gt;
&lt;p&gt;The tutorial materials including an outline can be viewed here:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/pydy/pydy-tutorial-pycon-2014"&gt;https://github.com/pydy/pydy-tutorial-pycon-2014&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason K. Moore</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/multibody-dynamics-and-control-with-python-part-1.html</guid><category>robots</category><category>simulation</category><category>sympy</category></item><item><title>Multibody Dynamics and Control with Python part 2</title><link>https://pyvideo.org/scipy-2014/multibody-dynamics-and-control-with-python-part-2.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;In this tutorial, attendees will learn how to derive, simulate, and
visualize the motion of a multibody dynamic system with Python tools.
These methods and techniques play an important role in the design and
understanding of robots, vehicles, spacecraft, manufacturing machines,
human motion, etc. Attendees will develop code to simulate the motion of
a human or humanoid robot.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this tutorial, attendees will learn how to derive, simulate, and
visualize the motion of a multibody dynamic system with Python tools.
The tutorial will demonstrate an advanced symbolic and numeric pipeline
for a typical multibody simulation problem. These methods and techniques
play an important role in the design and understanding of robots,
vehicles, spacecraft, manufacturing machines, human motion, etc. At the
end, the attendees will have developed code to simulate the uncontrolled
and controlled motion of a human or humanoid robot.&lt;/p&gt;
&lt;p&gt;We will highlight the derivation of realistic models of motion with the
SymPy Mechanics package. We will walk through the derivation of the
equations of motion of a multibody system (i.e. the model or the plant),
simulating and visualizing the free motion of the system, and finally we
will addfeedback controllers to control the plants that we derive.&lt;/p&gt;
&lt;p&gt;It is best if the attendees have some background with calculus-based
college level physics. They should also be familiar with the SciPy
Stack, in particular IPython, SymPy, NumPy, and SciPy. Our goal is that
attendees will come away with the ability to model basic multibody
systems, simulate and visualize the motion, and apply feedback
controllers all in a Python framework.&lt;/p&gt;
&lt;p&gt;The tutorial materials including an outline can be viewed here:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/pydy/pydy-tutorial-pycon-2014"&gt;https://github.com/pydy/pydy-tutorial-pycon-2014&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason K. Moore</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/multibody-dynamics-and-control-with-python-part-2.html</guid><category>robots</category><category>simulation</category><category>sympy</category></item><item><title>Object oriented Programming with NumPy using CPython &amp; PyPy</title><link>https://pyvideo.org/scipy-2014/object-oriented-programming-with-numpy-using-cpyt.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;In the paper we compare object-oriented implementations of an advection
algorithm written in Python, C++ and modern FORTRAN. The main angles of
comparison are code brevity and syntax clarity (and hence
maintainability and auditability) as well as performance. A notable
performance gain when switching from CPython to PyPy will be
exemplified, and the reasons for it will be briefly explained.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In the paper we compare object-oriented implementations of an advection
algorithm written in Python, C++ and modern FORTRAN. The MPDATA
advection algorithm (Multidimensional Positive-Definite Advective
Transport Algorithm) used as a core of weather, ocean and climate
modelling systems serves as an example.&lt;/p&gt;
&lt;p&gt;In the context of scientific programming, employment of object-oriented
programming (OOP) techniques may help to improve code readability, and
hence its auditability and maintainability. OOP offers, in particular,
the possibility to reproduce in the program code the mathematical
&amp;quot;blackboard abstractions&amp;quot; used in the literature. We compare how the
choice of a particular language influences syntax clarity, code length
and the performance: CPU time and memory usage.&lt;/p&gt;
&lt;p&gt;The Python implementation of MPDATA is based on NumPy. Its performance
is compared with C++/Blitz++ and FORTRAN implementations. A notable
performance gain when switching from the standard CPython to PyPy will
be exemplified, and the reasons for it will be briefly explained.
Discussion of other selected solutions for improving the NumPys
relatively poor performance will be also presented.&lt;/p&gt;
&lt;p&gt;This talk will describe and extend on the key findings presented in
&lt;a class="reference external" href="http://arxiv.org/abs/1301.1334"&gt;http://arxiv.org/abs/1301.1334&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dorota Jarecka</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/object-oriented-programming-with-numpy-using-cpyt.html</guid><category>pypy</category></item><item><title>Practical Experience in Teaching Numerical Methods with IPython Notebooks</title><link>https://pyvideo.org/scipy-2014/practical-experience-in-teaching-numerical-method.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;New tools like the IPython notebook can enhance both lectures and
textbooks, by making class time and individual study more interactive
through the inclusion of executable code and animations. I will demo
some materials and activities I've provided for students using the
IPython notebook. I will focus on practical issues I've faced that are
particular to this teaching approach.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Traditional university teaching is based on the use of lectures in class
and textbooks out of class. The medium (lecture or book) discourages the
natural curiosity that can lead to deeper understanding by investigating
the result of changing a parameter, or looking at the full results of a
time-dependent simulation.&lt;/p&gt;
&lt;p&gt;The IPython notebook provides a single medium in which mathematics,
explanations, executable code, and animated or interactive visualization
can be combined. Notebooks that combine all of these components can
enable new modes of student-led inquiry: the student can experiment with
modifications to the code and see the results, all without stepping away
from the mathematical explanations themselves. When notebooks are used
by students in the classroom, students can quickly share and discuss
results with the instructor or other class members. The instructor can
facilitate deeper learning by posing questions that students may answer
through writing appropriate code, during class time.&lt;/p&gt;
&lt;p&gt;For the past four years, I have taught a graduate numerical analysis
course using SAGE worksheets and IPython notebooks. I will show examples
of the notebooks I've developed and successfully used in this course. I
will describe some practical aspects of my experience, such as:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Tradeoffs between using IPython and SAGE&lt;/li&gt;
&lt;li&gt;Experiences with use of cloud computing platforms&lt;/li&gt;
&lt;li&gt;Dealing with students' installation issues&lt;/li&gt;
&lt;li&gt;Quickly getting students up to speed with the Python language and
packages&lt;/li&gt;
&lt;li&gt;Testing and evaluating homework in a math course that is
programming-intensive&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David I. Ketcheson</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-07-09:scipy-2014/practical-experience-in-teaching-numerical-method.html</guid><category>education</category><category>ipynb</category></item></channel></rss>