<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Adrian Boguszewski</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_adrian-boguszewski.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2022-05-13T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Optimize your network inference time with OpenVINO</title><link href="https://pyvideo.org/pycon-de-2022/optimize-your-network-inference-time-with-openvino.html" rel="alternate"></link><published>2022-05-13T00:00:00+00:00</published><updated>2022-05-13T00:00:00+00:00</updated><author><name>Adrian Boguszewski</name></author><id>tag:pyvideo.org,2022-05-13:/pycon-de-2022/optimize-your-network-inference-time-with-openvino.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker:: Adrian Boguszewski&lt;/p&gt;
&lt;p&gt;Track: PyData: Deep Learning
During the talk, I'll present the OpenVINO™ Toolkit. You'll learn how to automatically convert the model using Model Optimizer and how to run the inference with OpenVINO Runtime to infer your model with low latency on the CPU and iGPU you already …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker:: Adrian Boguszewski&lt;/p&gt;
&lt;p&gt;Track: PyData: Deep Learning
During the talk, I'll present the OpenVINO™ Toolkit. You'll learn how to automatically convert the model using Model Optimizer and how to run the inference with OpenVINO Runtime to infer your model with low latency on the CPU and iGPU you already have. The magic with only a few lines of code.&lt;/p&gt;
&lt;p&gt;Recorded at the PyConDE &amp;amp; PyData Berlin 2022 conference, April 11-13 2022.
&lt;a class="reference external" href="https://2022.pycon.de"&gt;https://2022.pycon.de&lt;/a&gt;
More details at the conference page: &lt;a class="reference external" href="https://2022.pycon.de/program/PKERX8"&gt;https://2022.pycon.de/program/PKERX8&lt;/a&gt;
Twitter: &lt;a class="reference external" href="https://twitter.com/pydataberlin"&gt;https://twitter.com/pydataberlin&lt;/a&gt;
Twitter: &lt;a class="reference external" href="https://twitter.com/pyconde"&gt;https://twitter.com/pyconde&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon DE 2022"></category><category term="PyCon"></category><category term="PyConDE"></category><category term="pyconde2022"></category><category term="pydata"></category><category term="PyDataBerlin"></category><category term="pydataberlin2022"></category></entry><entry><title>In the service of the history. AI in archivistics.</title><link href="https://pyvideo.org/pydata-warsaw-2019/in-the-service-of-the-history-ai-in-archivistics.html" rel="alternate"></link><published>2019-12-12T00:00:00+00:00</published><updated>2019-12-12T00:00:00+00:00</updated><author><name>Adrian Boguszewski</name></author><id>tag:pyvideo.org,2019-12-12:/pydata-warsaw-2019/in-the-service-of-the-history-ai-in-archivistics.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A trillions of old photos are a valuable source of information about the
past. Unfortunately most of them are not described sufficiently or at
all. Imagine Artificial Intelligence as a friend of the archivist of the
21st century. This is the end of unlabeled photos epoch.&lt;/p&gt;
&lt;p&gt;1839 is a …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A trillions of old photos are a valuable source of information about the
past. Unfortunately most of them are not described sufficiently or at
all. Imagine Artificial Intelligence as a friend of the archivist of the
21st century. This is the end of unlabeled photos epoch.&lt;/p&gt;
&lt;p&gt;1839 is a data generally accepted as the birth year of practical
photography. Since then mankind produced about 10 quadrillions of photos
including 1 quadrillion only last year. This huge amount of unlabeled an
undescribed data is a problem, if we want to obtain important
information quickly and efficiently. Old photos are extremely valuable,
because they contain a lot of data about the past. However some
expertise and experience is needed to properly describe such images.
What if we include all this knowledge into neural networks? Can AI
become a friend of the 21st century archivist? Let’s talk about
automatic image tagging and faces recognition in old photos.&lt;/p&gt;
</content><category term="PyData Warsaw 2019"></category></entry></feed>