<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-seattle-2015.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2015-07-26T00:00:00+00:00</updated><entry><title>An example of Predictive Analytics: Building a Recommendation Engine using Python</title><link href="https://pyvideo.org/pydata-seattle-2015/an-example-of-predictive-analytics-building-a-recommendation-engine-using-python.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Anusua Trivedi</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/an-example-of-predictive-analytics-building-a-recommendation-engine-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recommendation systems are needed to locate appropriate data on same topic or on similar topics of interest. In this talk, I will describe a recommender system framework for PubMed articles. I will present the background and motivation for these recommendation systems, and discuss the implementations of this PubMed recommendation engine with codes and examples.&lt;/p&gt;
&lt;p&gt;One of the most popular features of Big Data is predictive analytics. Predictive analytics is a form of business intelligence gathering. Far from the latest business buzzword, predictive analytics is a set of techniques that have become fundamental to the business strategies. In this tutorial, we will cover an example of predictive analytics through implementing a recommendation engine using python. A recommendation engine (sometimes referred to as a recommender system) is a tool that lets algorithm developers predict what a user may or may not like among a list of given items.&lt;/p&gt;
&lt;p&gt;It is very common now a day to have access to large amount of data on similar or related topics. Recommendation systems are needed to locate appropriate data on same topic or on similar topics of interest. In this talk, I will describe a recommender system framework for PubMed articles. PubMed is a free search engine that primarily accesses the MEDLINE database of references and abstracts on life-sciences and biomedical topics. The proposed recommender system produces two types of recommendations – i) content-based recommendation and (ii) recommendations based on similarities with other users' search profiles. The first type of recommendation, viz., content-based recommendation, can efficiently search for material that is similar in context or topic to the input publication. The second mechanism generates recommendations using the search history of users whose search profiles match the current user. In the talk I will present the background and motivation for these recommendation systems, and discuss the implementations of this PubMed recommendation system using python.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/an-example-of-predictive-analytics-building-a-recommendation-engine-using-pythonanusua-trivedi"&gt;http://www.slideshare.net/PyData/an-example-of-predictive-analytics-building-a-recommendation-engine-using-pythonanusua-trivedi&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Bayesian inference with PyMC 3</title><link href="https://pyvideo.org/pydata-seattle-2015/bayesian-inference-with-pymc-3.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>John Salvatier</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/bayesian-inference-with-pymc-3.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyMC 3 (&lt;a class="reference external" href="https://github.com/pymc-devs/pymc3"&gt;https://github.com/pymc-devs/pymc3&lt;/a&gt;), a total rewrite of PyMC 2, provides a powerful yet easy-to-use language for specifying statistical models and provides powerful yet easy-to-use gradient-based techniques for fitting them. New advances in sampling techniques have made it possible to fit large and complex Bayesian models much more easily than ever before and PyMC 3 is the easiest way to use them.&lt;/p&gt;
&lt;p&gt;Bayesian inference is a powerful and flexible way to learn from data, that is easy to understand. Unfortunately larger problems are often computationally intractable. Markov Chain Monte Carlo sampling techniques help, but are still computationally limited. New gradient-based methods like the No U-Turn Sampler (NUTS) dramatically increase performance on hard problems.&lt;/p&gt;
&lt;p&gt;PyMC 3 provides a easy and concise way to specify models and provides powerful yet easy to use samplers like NUTS. This enables users easily fit large and complex models with thousands of parameters. PyMC 3 is a complete rewrite of PyMC 2 based on Theano. PyMC expands its powerful NumPy-like syntax, and is now easier to extend and automatically optimized by Theano.&lt;/p&gt;
&lt;p&gt;We first introduce Bayesian inference and then give several examples of using PyMC 3 to show off the ease of model building and model fitting even for difficult models.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/probabilistic-programming-in-python-with-pymc3-john-salvatier"&gt;http://www.slideshare.net/PyData/probabilistic-programming-in-python-with-pymc3-john-salvatier&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Big Data Analytics- The Best of the Worst: AntiPatterns &amp; Antidotes</title><link href="https://pyvideo.org/pydata-seattle-2015/big-data-analytics-the-best-of-the-worst-antipatterns-antidotes.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Krishna Sankar</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/big-data-analytics-the-best-of-the-worst-antipatterns-antidotes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let us explore some of the anti-patterns that organizations end up with in their voyage to the Big Data Nirvana. &amp;quot;Data swamp&amp;quot;, &amp;quot;Technology Stampede&amp;quot; and &amp;quot;Big data to nowhere&amp;quot; are all the waypoints one should avoid. Of course, we will also look at the antidotes and folk wisdom in data science as well as data management and ways to extract oneself out of the sandtraps.&lt;/p&gt;
&lt;p&gt;The Big Data Analytics Pipeline viz. Collect-store-Transform-Model-Reason-Visualize-Predict-Recommend-Explore
What could go wrong ? The anti-patterns
Folk wisdom - Data Management &amp;amp; Data Science&lt;/p&gt;
</summary></entry><entry><title>Bokeh Dashboard Capability Use Case Demo</title><link href="https://pyvideo.org/pydata-seattle-2015/bokeh-dashboard-capability-use-case-demo.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Casey Clements</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/bokeh-dashboard-capability-use-case-demo.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- Continuum Analytics&lt;/p&gt;
</summary></entry><entry><title>Bot or Not</title><link href="https://pyvideo.org/pydata-seattle-2015/bot-or-not.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Erin Shellman</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/bot-or-not.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Twitter makes money by selling ads, but they’ve got an insidious infestation eroding their advertising credibility: bots. These bots are automatons living in the Twittersphere, ranging wildly in capability. In Bot or Not I’ll discuss how to identify bots with a classification algorithm created in scikit-learn and provide some tips on how to account for them when analyzing social media experiments.&lt;/p&gt;
&lt;p&gt;Like many Internet giants Twitter makes money by selling ads, but they’ve got an insidious infestation eroding their advertising credibility: bots. More than 23 million of them. Twitter bots are automatons living in the Twittersphere and ranging wildly in capability. In their simplest form, they follow you maybe fav-ing or retweeting your statuses. At their most complex, they troll and ironically, troll trolls using speech patterns that can, at times, fool humans. But when advertisers pay for engagement, they aren’t interested in a four-hour flame war between a gamergate bot and a Kanye bot. When advertisers analyze social data they want to be sure their findings are the result of human activity. In Bot or Not I’ll discuss the taxonomy of Twitter bots, segmenting them based on “physical features” such as profile configuration, and on behavioral features: tweeting, retweeting and fav-ing. We’ll also see how to identify bots with a classification algorithm created in scikit-learn. Finally, I’ll provide some tips to advertisers on how to account for bot behavior when analyzing and interpreting results from social media experiments.&lt;/p&gt;
&lt;p&gt;I’ll outline my experimental design, including the process of buying bots to create the training set. Technical details I’ll discuss include using the python-twitter library to connect to the Twitter API and retrieve data, development of a bot taxonomy, and subsequent classification algorithm with pandas and scikit-learn.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/ErinShellman/bot-or-not"&gt;http://www.slideshare.net/ErinShellman/bot-or-not&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Counterfactual evaluation of machine learning models</title><link href="https://pyvideo.org/pydata-seattle-2015/counterfactual-evaluation-of-machine-learning-models.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Michael Manapat</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/counterfactual-evaluation-of-machine-learning-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Machine learning models often result in actions: search results are reordered, fraudulent transactions are blocked, etc. But how do you evaluate model performance when you are altering the distribution of outcomes? I'll describe how injecting randomness in production allows you to evaluate current models correctly and generate unbiased training data for new models.&lt;/p&gt;
&lt;p&gt;Stripe processes billions of dollars in payments a year and uses machine learning to detect and stop fraudulent transactions. Like models used for ad and search ranking, Stripe's models don't just score---they dictate actions that directly change outcomes. High-scoring transactions are blocked before they can ever get refunded or disputed by the card holder. Deploying an initial model that successfully blocks a substantial amount of fraud is a great first step, but since your model is altering outcomes, subsequent parts of the modeling process become more difficult:&lt;/p&gt;
&lt;p&gt;How do you evaluate the model? You can't observe the eventual outcomes of the transactions you block (would they have been refunded or disputed?) or the ads you didn't show (would they have been clicked?) In general, how do you quantify the difference between the world with the model and the world without it?&lt;/p&gt;
&lt;p&gt;How do you train new models? If your current model is blocking a lot of transactions, you have substantially fewer samples of fraud for your new training set. Furthermore, if your current model detects and blocks some types of fraud more than others, any new model you train will be biased towards detecting that residual fraud. Ideally, new models would be trained on the &amp;quot;unconditional&amp;quot; distribution that exists in the absence of the original model.&lt;/p&gt;
&lt;p&gt;In this talk, I'll describe how injecting a small amount of randomness in the production scoring environment allows you to answer these questions. We'll see how to obtain estimates of precision and recall (standard measures of model performance) from production data and how to approximate the distribution of samples that would exist in a world without the original model so that new models can be trained soundly.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/MichaelManapat/counterfactual-evaluation-of-machine-learning-models"&gt;http://www.slideshare.net/MichaelManapat/counterfactual-evaluation-of-machine-learning-models&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Creating an intelligent world at Dato</title><link href="https://pyvideo.org/pydata-seattle-2015/creating-an-intelligent-world-at-dato.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Shawn Scully</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/creating-an-intelligent-world-at-dato.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- Dato
The future is one full of intelligence. Thanks to the practical application of machine learning, apps can now drive product recommendations, predict machine failures, forecast airfare, social match-make, identify fraud, predict disease outbreaks, and repurpose pharmaceuticals. These applications output real-time predictions and recommendations in response to user and machine input to directly create intelligent services and amazing experiences which result in tremendous business value. I’ll share our mission at Dato to accelerate the development of intelligent applications. There will be code and demos.&lt;/p&gt;
</summary></entry><entry><title>High Throughput Processing of Space Debris Data</title><link href="https://pyvideo.org/pydata-seattle-2015/high-throughput-processing-of-space-debris-data.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Andreas Schreiber</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/high-throughput-processing-of-space-debris-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Space Debris are defunct objects in space, including old space vehicles (such as satellites or rocket stages) or fragments from collisions. Space debris can cause great damage to functional space ships and satellites. Thus detection of space debris and prediction of their orbital paths are essential for today's operation of space missions. The talk shows the Python based infrastructures BACARDI for gathering and storing space debris data from sensors and Skynet for high-throughput data processing and orbital collision detection.&lt;/p&gt;
</summary></entry><entry><title>Integration with the Vernacular</title><link href="https://pyvideo.org/pydata-seattle-2015/integration-with-the-vernacular.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/integration-with-the-vernacular.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The numpy model of computation in Python has proven to be one of the most successful ways to integrate high-performance computational code into an application. This talk offers a foundational conceptualization for this approach and discusses its strengths and limitations.&lt;/p&gt;
</summary></entry><entry><title>Jupyter Notebooks and ML Model Operationalization</title><link href="https://pyvideo.org/pydata-seattle-2015/jupyter-notebooks-and-ml-model-operationalization.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Dino Viehland</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/jupyter-notebooks-and-ml-model-operationalization.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- Microsoft
Jupyter (formerly IPython) notebooks provide a convenient canvas for exploring and visualizing data. Jupyter has quickly become the preferred “IDE” for many Data Science and Technical Computing scenarios. Azure ML Studio is an easy to use, drag/drop IDE that provides the ability to build and deploy web services that expose Machine Learning models as RESTful APIs (a.k.a. Operationalization). We’re happy to announce the we’ve incorporated Jupyter with the Azure ML Studio.&lt;/p&gt;
&lt;p&gt;Beyond the integration, with the release of the AzureML APIs in the Python SDK, we are bringing the ease of publishing high scale predictive web services that run on Azure to Jupyter and other IDE’s. Using this SDK, you can now do the data manipulation and feature engineering using the REPL experience that Jupyter provides and then publish the final model as an Azure ML web service. The published web services provide a RESTful interface that can then be called from variety of platforms and clients such as Excel, .NET, Java, Python, R, etc.&lt;/p&gt;
</summary></entry><entry><title>Keynote - A Systems View of Machine Learning</title><link href="https://pyvideo.org/pydata-seattle-2015/keynote-a-systems-view-of-machine-learning.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Josh Bloom</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/keynote-a-systems-view-of-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Despite the growing abundance of powerful tools, building and deploying machine-learning frameworks into production continues to be major challenge, in both science and industry. I'll present some particular pain points and cautions for practitioners as well as recent work addressing some of the nagging issues. I advocate for a systems view, which, when expanded beyond the algorithms and codes to the organizational ecosystem, places some interesting constraints on the teams tasked with development and stewardship of ML products.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/JoshuaBloom/a-systems-view-of-machine-learnipydata-2015"&gt;http://www.slideshare.net/JoshuaBloom/a-systems-view-of-machine-learnipydata-2015&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Keynote - Data driven Education and the Quantified Student</title><link href="https://pyvideo.org/pydata-seattle-2015/keynote-data-driven-education-and-the-quantified-student.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Lorena Barba</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/keynote-data-driven-education-and-the-quantified-student.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Education has seen the rise of a new trend in the last few years: Learning Analytics. This talk will weave through the complex interacting issues and concerns involving learning analytics, at a high level. The goal is to whet the appetite and motivate reflection on how data scientists can work with educators and learning scientists in this swelling field.&lt;/p&gt;
&lt;p&gt;Higher education has used analytics for a long time to guide administrative decisions. Universities are already adept at developing data-driven admissions strategies and increasingly they are using analytics in fund-raising. Learning analytics is a newer trend. Its core goal is to improve teaching, learning and student success through data. This is very appealing, but it's also fraught with complex interactions among many concerns and with disciplinary gaps between the various players.&lt;/p&gt;
&lt;p&gt;Faculty have always collected data on students' performance on assessments and responses on surveys for the purposes of grading and complying with accreditation, sometimes also for improving teaching methods and more rarely for research on how students learn. To call it Learning Analytics, though, requires scale and some form of systemic effort.&lt;/p&gt;
&lt;p&gt;Some early university efforts in analytics developed predictive models to identify at-risk first-year students, aiming to improve freshman retention (e.g., Purdue's &amp;quot;Signals&amp;quot; project). Others built alert systems in support of student advising, with the goal of increasing graduation rates (e.g., Arizona State University's &amp;quot;eAdvisor&amp;quot; system). Experts now segregate these efforts out of learning analytics, proper, because retention and graduation are not the same as learning. The goal, in that case, is to improve the function of the educational system, while learning analytics should be guided by educational research and be aimed at enhancing learning.&lt;/p&gt;
&lt;p&gt;To elucidate what is learning analytics, it looks like we first need to answer: what is learning? What is knowledge? And can more data lead to better learning? That is perhaps the zeroth assumption of learning analytics—and it needs to be tested. There are assumptions behind any data system that go as far back as selecting what to track, where it will be tracked, how it will be collected, stored and delivered.&lt;/p&gt;
&lt;p&gt;Most analytics is based on log data in the Learning Management System (LMS). This &amp;quot;learning in a box&amp;quot; model is inadequate, but the diverse ecosystem of apps and services used by faculty and students poses a huge interoperability problem. The billion-dollar education industry of LMS platforms, textbook publishers and testing companies all want a part in the prospect of &amp;quot;changing education&amp;quot; through analytics. They're all marketing their dazzling dashboards in a worrying wave of ed-tech solutionism. Meanwhile, students' every move gets tracked and logged, often without their knowledge or consent, adding ethical and legal issues of privacy for the quantified student.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://figshare.com/articles/Data_driven_Education_and_the_Quantified_Student/1495511"&gt;http://figshare.com/articles/Data_driven_Education_and_the_Quantified_Student/1495511&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Low Friction NLP with Gensim</title><link href="https://pyvideo.org/pydata-seattle-2015/low-friction-nlp-with-gensim.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Trent Hauck</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/low-friction-nlp-with-gensim.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Gensim is fairly popular NLP library available in Python. In addition to having implementations of several popular algorithms, it has a utilities that make working with the corpus itself easier.&lt;/p&gt;
&lt;p&gt;In this talk I'd like to give an overview of Gensim, and then two examples. One will illustrate an LDA example, then I'll show a somewhat novel use of Word2Vec to understand user preferences.&lt;/p&gt;
&lt;p&gt;Overview:
The overview will follow the general arc of an NLP project.
Reading the corpus, here this is done with gensim's streaming API.
Transformations, often a transformation to BOW is done, and potentially something like TFIDF.
Training the model from the corpus.
Working with the result for analysis or otherwise.
Examples:
This will be a straight forward application: topic discovery on a corpus and then analyzing the resulting topics to look for patterns.
Next I'll cover how to use Gensim's Word2Vec implementation to better understand customer preferences.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://blog.trenthauck.com/portfolio/presentation.pdf"&gt;http://blog.trenthauck.com/portfolio/presentation.pdf&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Mistakes I've Made</title><link href="https://pyvideo.org/pydata-seattle-2015/mistakes-ive-made.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Cameron Davidson Pilon</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/mistakes-ive-made.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this humbling talk, I'll describe some mistakes I've made in working in statistics and machine learning. I'll describe my original intentions, symptoms, how I eventually discovered the mistake, and possibly even a solution. The topics include mistakes in A/B testing, Kaggle competitions, data collection, and other fields.&lt;/p&gt;
&lt;p&gt;In this humbling talk, I'll describe some mistakes I've made in working in statistics and machine learning. I'll describe my original intentions, symptoms, how I eventually discovered the mistake, and possibly even a solution. The topics include mistakes in A/B testing, Kaggle competitions, data collection, and other fields. I'll also introduce some interesting statistical and machine learning counterexamples: examples where our original intuition fails, and solutions to these examples.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/mistakes-ive-made-cam-davidsonpilon"&gt;http://www.slideshare.net/PyData/mistakes-ive-made-cam-davidsonpilon&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>NLP and text analytics at scale with PySpark and notebooks</title><link href="https://pyvideo.org/pydata-seattle-2015/nlp-and-text-analytics-at-scale-with-pyspark-and-notebooks.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Paco Nathan</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/nlp-and-text-analytics-at-scale-with-pyspark-and-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Who's who in a developer community and what do they discuss? And with whom? This project, based on Apache Spark, provides Python pipelines for scraping, parsing, and analyzing discussion forums for a given Apache developer community -- along with analysis of related meetup events and conference talks.&lt;/p&gt;
&lt;p&gt;Messages get parsed with NLTK and TextBlob, then represented as JSON. Analytics pipelines, organized as notebooks, produce leaderboards with Spark SQL, predictive models using MLlib, and visualizations in Seaborn, while storing the data with Parquet. Code is available on GitHub.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/pacoid/microservices-containers-and-machine-learning-50862677"&gt;http://www.slideshare.net/pacoid/microservices-containers-and-machine-learning-50862677&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Numba: Flexible analytics written in Python with machine code speeds and avoiding</title><link href="https://pyvideo.org/pydata-seattle-2015/numba-flexible-analytics-written-in-python-with-machine-code-speeds-and-avoiding.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Travis Oliphant</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/numba-flexible-analytics-written-in-python-with-machine-code-speeds-and-avoiding.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will discuss the current state of Numba and how it can be used to accelerate performance and build analytics in Python with machine-code speeds and on multiple threads. Numba has improved dramatically over the past year and we will discuss what it can do and features coming in the future.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/numba-flexible-analytics-written-in-python-with-machinecode-speeds-and-avoiding-the-gil-travis-oliphant"&gt;http://www.slideshare.net/PyData/numba-flexible-analytics-written-in-python-with-machinecode-speeds-and-avoiding-the-gil-travis-oliphant&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Pandas Under The Hood: Peeking behind the scenes of a high performance data analys</title><link href="https://pyvideo.org/pydata-seattle-2015/pandas-under-the-hood-peeking-behind-the-scenes-of-a-high-performance-data-analys.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Jeffrey Tratner</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/pandas-under-the-hood-peeking-behind-the-scenes-of-a-high-performance-data-analys.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will give a broad, accessible overview of pandas’ internal structure and help explain how pandas works behind the scenes, including the libraries it relies on, its internal data structures (NDFrame, BlockManager, Index, etc), and how they all tie together to provide a flexible and performant API. I’ll also explore how you can use this background to build up a better intuition about how to use pandas effectively.&lt;/p&gt;
&lt;p&gt;Materials available here:
Interactive slides: &lt;a class="reference external" href="http://bit.ly/1M5ISBn"&gt;http://bit.ly/1M5ISBn&lt;/a&gt;
PDF slides: &lt;a class="reference external" href="http://bit.ly/1hkGJGu"&gt;http://bit.ly/1hkGJGu&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Python Data Bikeshed</title><link href="https://pyvideo.org/pydata-seattle-2015/python-data-bikeshed.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Rob Story</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/python-data-bikeshed.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The PyData ecosystem is growing rapidly, with existing tools maturing and exciting new tools appearing on a regular basis. This talk will examine the crowded PyData ecosystem and bring some clarity to which Python data tool is the right one to reach for on any given analysis. It will focus on use-cases for pure python, toolz, Numpy, Pandas, Blaze, xray, bcolz, Dask, and Spark.&lt;/p&gt;
&lt;p&gt;The PyData ecosystem can be a bit confusing for those new to Python, or even experienced programmers moving to Python for its excellent data analysis capabilities. How do you know which tool to reach for on any given project? What tools work best for my data of size FooBar in data store FizzBuzz?&lt;/p&gt;
&lt;p&gt;This talk will explore the Python data toolchain from bottom to top, with a focus on what tools work best based on both data locality and analysis velocity. Think of your data pipeline and storage as a city, and your data tools as a shed full of bikes. What bike works best for which trip? When should you use pure Python (the fixie) to perform your analysis? How do Pandas (the geared commuter) and Blaze (the tandem) work together? Where does Spark (the fat tire bike) fit into all of this?&lt;/p&gt;
&lt;p&gt;This talk seeks to use questionable bike analogies to provide less-questionable look at the crowded PyData ecosystem and bring some clarity to which Python data tool is the right one to reach for on any given analysis. It will touch on pure python, toolz, Numpy, Pandas, Blaze, xray, bcolz, Dask, and Spark, with a focus on the use-cases for each one.&lt;/p&gt;
&lt;p&gt;Finally, we’ll talk about which library you should use to paint the bikeshed.&lt;/p&gt;
&lt;p&gt;Materials available here:  &lt;a class="reference external" href="https://github.com/wrobstory/pydataseattle2015"&gt;https://github.com/wrobstory/pydataseattle2015&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Sequoia: Point Cloud Processing and Meshing</title><link href="https://pyvideo.org/pydata-seattle-2015/sequoia-point-cloud-processing-and-meshing.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Mark Wiebe</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/sequoia-point-cloud-processing-and-meshing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sequoia (&lt;a class="reference external" href="http://sequoia.thinkboxsoftware.com/"&gt;http://sequoia.thinkboxsoftware.com/&lt;/a&gt;) is a product, currently in beta, from Thinkbox Software, for processing and meshing large point clouds. This talk dives into some technical details of what it takes to create a scalable point cloud system.&lt;/p&gt;
&lt;p&gt;This talk will cover some of the details of how to create a scalable point cloud system, including how floating-point Morton order can be used to spatially organize a point cloud within a globally defined octree.&lt;/p&gt;
</summary></entry><entry><title>Social Media Brand Positioning Workflow</title><link href="https://pyvideo.org/pydata-seattle-2015/social-media-brand-positioning-workflow.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>David Gerson</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/social-media-brand-positioning-workflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Social Media is becoming ever pervasive in modern culture. One of the simplest use cases of social data is as a &amp;quot;temperature check&amp;quot; for how a brand is performing. This talk offers a simple walk-through of how python can be used to take Social Data from its raw form and transform it into a usable visualization to help understand the market a company exists in.&lt;/p&gt;
&lt;p&gt;This project focus on how social media data extracted about the fast food industry can simply and repeatably be turned into a system for analyzing market position. This market position can by proxy be used to derive strategy.&lt;/p&gt;
&lt;p&gt;Data will be extracted from Twitter for a variety of brands that operate in the same market space.
That data will be split into its individual words and using nltk it will be cast to its root word.
A TFIDF (sci-kit or raw code) analysis will be used to identify which words are uniquely used to describe the industry.
The data will then be topic modeled in gensim to further reduce the number of dimensions and to reduce the number of feature vectors that need to be tracked.
Finally the data will be reduced into a 4-plot using Correspondence Analysis or MDS.
While these are all independently simple the analysis itself is extensible enough to solve this problem in a variety of circumstances.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/social-media-brand-positioning-workflow-david-gerson"&gt;http://www.slideshare.net/PyData/social-media-brand-positioning-workflow-david-gerson&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>State of the Library: matplotlib</title><link href="https://pyvideo.org/pydata-seattle-2015/state-of-the-library-matplotlib.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Thomas Caswell</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/state-of-the-library-matplotlib.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Materials available here: &lt;a class="reference external" href="http://nbviewer.ipython.org/gist/tacaswell/e95cb2d57ca3783ffc40"&gt;http://nbviewer.ipython.org/gist/tacaswell/e95cb2d57ca3783ffc40&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Statistical learning of human brain structure</title><link href="https://pyvideo.org/pydata-seattle-2015/statistical-learning-of-human-brain-structure.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Ariel Rokem</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/statistical-learning-of-human-brain-structure.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Statistical learning provides a set of powerful principles and tools to interpret data from many different domains. This enables insights about a range of phenomena through the construction of accurate models of the data. The application of these principles and tools to data from a specific scientific domain often presents challenges, because it requires an understanding of both the phenomena measured, as well as the properties of the measurement. In this talk, I will explore these challenges, by focusing on data from measurements of the living human brain with MRI.&lt;/p&gt;
&lt;p&gt;Diffusion MRI (dMRI) measures water diffusion in the brain, and because tissue compartments form boundaries to free diffusion, these measurements allow us to probe the structure of the tissue, and delineate the trajectories of bundles of nerve cell projections (axons) connecting different parts of brain. Therefore, it can be used to make inferences about brain structure and connectivity, and about the tissue properties of different parts of the brain, as well as their relation to health and to cognitive abilities.&lt;/p&gt;
&lt;p&gt;Here, I focus on the use of cross-validation to compare different models of the dMRI signal. I will discuss the cross-validation API that we developed in the open-source Dipy project (&lt;a class="reference external" href="http://dipy.org"&gt;http://dipy.org&lt;/a&gt;). This API was designed to match specific features of the data and the measurement, but also to generalize across different models. The data contains information at multiple size scales, and cross-validation can be applied at different levels, to evaluate and validate models of the microscopic distribution of fiber directions in small regions of the brain as well as long-range connections between distant brain regions.&lt;/p&gt;
&lt;p&gt;Materials available here: &lt;a class="reference external" href="http://arokem.github.io/2015-pydatanw/#/"&gt;http://arokem.github.io/2015-pydatanw/#/&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>The Possibilities Of Plotting With pandas and IPython</title><link href="https://pyvideo.org/pydata-seattle-2015/the-possibilities-of-plotting-with-pandas-and-ipython.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Matthew Sundquist</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/the-possibilities-of-plotting-with-pandas-and-ipython.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A wave of complimentary new tools allow developers to quickly access, analyze, and plot data. IPython Notebooks let you harness these libraries and code in a web-based, language agnostic Notebook. Pandas lets you wrangle your data. And matplotlib, ggplot for Python, Plotly, bokeh, and Seaborn let you make beautiful, interactive plots. This talk shows how to use and deploy these tools together.&lt;/p&gt;
&lt;p&gt;A new wave of Python libraries enable interactive, scientific figures, and web shareability. From Python we can access and manipulate data with pandas, make 2D, 3D, and live-streaming graphs inside IPython Notebooks, and translate plots from static images into apps we can deploy with WebGL and D3.js.&lt;/p&gt;
&lt;p&gt;A number of libraries support--to various degrees--plotting directly from pandas inside IPython Notebooks--matplotlib, ggplot for Python, Plotly, bokeh, cufflinks, mpld3, and Seaborn. Some of these plots can also be turned into interactive web-based plots. This talk will show the volume of plotting options available for Python users from within these libraries, and highlight relevant technologies along the way.&lt;/p&gt;
&lt;p&gt;Materials available here: &lt;a class="reference external" href="https://plot.ly/python/"&gt;https://plot.ly/python/&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Top 5 uses of Redis as a Database</title><link href="https://pyvideo.org/pydata-seattle-2015/top-5-uses-of-redis-as-a-database.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Dave Nielsen</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/top-5-uses-of-redis-as-a-database.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- Redis&lt;/p&gt;
</summary></entry><entry><title>Why "data informed" beats "data driven</title><link href="https://pyvideo.org/pydata-seattle-2015/why-data-informed-beats-data-driven.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Greg Reda</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/why-data-informed-beats-data-driven.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Companies can't stop gushing about how &amp;quot;data-driven&amp;quot; they are - how they're using &amp;quot;big data&amp;quot; and &amp;quot;data science&amp;quot; to synergize and streamline all the things. But being driven by data alone is a flawed approach. Instead, companies should seek to be &amp;quot;data-informed&amp;quot; - interweaving designers, UXers, and data scientists so that each side is able to perfectly complement the one another.&lt;/p&gt;
&lt;p&gt;This talk will discuss the importance of allowing data and user research to complement one another, in addition to the pitfalls of being driven by data alone (for instance, the cons of A/B testing).&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/gjreda/pydata2015sea"&gt;https://github.com/gjreda/pydata2015sea&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Accelerate data analytics and Python performance with Intel® software</title><link href="https://pyvideo.org/pydata-seattle-2015/accelerate-data-analytics-and-python-performance-with-intelr-software.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Sergey Maidanov</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/accelerate-data-analytics-and-python-performance-with-intelr-software.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- Intel
This talk presents an overview of some of the tools, technologies and programs being developed and offered by Intel® for the Data community and Python Developers. We'll highlight the new Intel® Data Analytics Acceleration Library (Intel® DAAL), a prototype Python profiler, a Python Distribution for scientific computing &amp;amp; data analysis, Intel® MKL, Intel® IPP and the free software tools program.&lt;/p&gt;
&lt;p&gt;This talk presents an overview of some of the tools, technologies and programs being developed and offered by Intel® for the Data community and Python Developers. The Intel® Data Analytics Acceleration Library (Intel® DAAL) is a C++ and Java software solution for data analytics. The library provides a set of optimized building blocks that can be used in all stages of the data analytics workflow. A prototype Python profiler will be featured, with details on the highlights and advantages of this tool. Another project in the works is a Python Distribution for scientific computing and data analysis, that leverages the power of Intel® Math Kernel library (Intel® MKL) to deliver faster performance. A quick spotlight on the latest features and additions to the Intel ® Performance Libraries – Intel® MKL and Intel® Integrated Performance Primitives (Intel® IPP). And finally, a fresh look at the Intel free tools software program, to discover some of the free offerings for academic researchers, educators, students and open source contributors.&lt;/p&gt;
</summary></entry><entry><title>Accelerating the Random Forest algorithm for commodity parallel hardware</title><link href="https://pyvideo.org/pydata-seattle-2015/accelerating-the-random-forest-algorithm-for-commodity-parallel-hardware.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Mark Seligman</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/accelerating-the-random-forest-algorithm-for-commodity-parallel-hardware.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Arborist is an open-source implementation of the Random Forest algorithm designed for acceleration on a wide variety of hardware platforms, including multicore, multinode and GPGPU. We examine the challenges of parallelization and present recent work extending the implementation to Python.&lt;/p&gt;
&lt;p&gt;The Random Forest algorithm serves as a sort of Swiss Army Knife for data modeling. It is among a handful of commonly-applied predictive techniques, and is well known both for its robustness and its predictive power. Performance is a common drawback of the method, however, owing to a slow training phase. The Arborist is an open-source implementation of the algorithm intended to overcome key performance bottlenecks.&lt;/p&gt;
&lt;p&gt;The Arborist's design goals include minimization of costly data movement, parallelization across a wide range of commodity hardware, language-agnostic implementation and ready internalization of common workflows. An R-language front-end has been available for several months, and a Python version is being made available for the general machine-learning community. Internalized workflows include quantile regression, as well as nonparametric resampling.&lt;/p&gt;
&lt;p&gt;Popular approaches to accelerating the algorithm have included distribution of training across computational nodes using such tools as MPI or Hadoop. The Arborist augments these approaches by parallelizing training within individual trees as well across blocks, and by employing either - or both - multicore and GPGPU hardware, effecting a hierarchical parallel structure. The ability to do this is imparted by an innovative recasting of the algorithm focused on identifying, and conserving, data locality.&lt;/p&gt;
&lt;p&gt;We outline the algorithmic structure of the Arborist and discuss some of the challenges posed in accelerating this popular technique.&lt;/p&gt;
&lt;p&gt;Slides here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/accelerating-the-random-forest-algorithm-for-commodity-parallel-mark-seligman"&gt;http://www.slideshare.net/PyData/accelerating-the-random-forest-algorithm-for-commodity-parallel-mark-seligman&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>An Intuitive Introduction to the Fourier Transform and FFT</title><link href="https://pyvideo.org/pydata-seattle-2015/an-intuitive-introduction-to-the-fourier-transform-and-fft.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>William Cox</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/an-intuitive-introduction-to-the-fourier-transform-and-fft.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The “fast fourier transform” (FFT) algorithm is a powerful tool for looking at time-based measurements in an interesting way, but do you understand what it does? This talk will start from basic geometry and explain what the fourier transform is, how to understand it, why it’s useful and show examples.&lt;/p&gt;
&lt;p&gt;If you’re collecting time-series data (e.g. heart rate, stock prices, server usage, temperature) the fourier transform can be a useful tool for analyzing the underlying periodic nature of the data. But, what is it actually doing? In this talk we’ll start from the foundation of basic geometry and explain what the transform is doing. The talk will feature lots of animated graphics to take the mystery out of this powerful method … and to keep you from reading Twitter during the talk. We’ll look at example applications and example code on how to use it in practice, along with practical tips, like choosing the number of bins and what in the world “windowing” functions are.&lt;/p&gt;
&lt;p&gt;Materials available here:  &lt;a class="reference external" href="https://github.com/gallamine/fft_oscon/"&gt;https://github.com/gallamine/fft_oscon/&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Anaconda Cluster Use Case</title><link href="https://pyvideo.org/pydata-seattle-2015/anaconda-cluster-use-case.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Peter Steinberg</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/anaconda-cluster-use-case.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- Continuum Analytics
Materials available here:
Github: &lt;a class="reference external" href="https://github.com/ContinuumIO/image-analyzer"&gt;https://github.com/ContinuumIO/image-analyzer&lt;/a&gt;
iPython Notebook: &lt;a class="reference external" href="http://nbviewer.ipython.org/github/continuumio/image-analyzer/blob/master/Explore_Spark_Results.ipynb"&gt;http://nbviewer.ipython.org/github/continuumio/image-analyzer/blob/master/Explore_Spark_Results.ipynb&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Blaze and Odo</title><link href="https://pyvideo.org/pydata-seattle-2015/blaze-and-odo.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Phillip Cloud</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/blaze-and-odo.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Blaze and Odo are designed to help domain experts answer questions more quickly. They work together by providing a symbolic computation layer (blaze) alongside a graph of data converters (odo) that enables users to move seamlessly between formats in the most performant way. We discuss both libraries in the context of PyData and emerging data analytics technologies.&lt;/p&gt;
&lt;p&gt;Blaze separates expressions from computation. Odo moves complex data resources from point A to point B. Together Blaze and Odo smooth over many of the complexities of computing with large data warehouse technologies like Redshift, Impala and HDFS. Because we designed Blaze and Odo with PyData in mind they also integrate well with pandas, numpy, and a host of other foundational libraries. We show examples of both Blaze and Odo in action and discuss the design behind each library.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://slides.com/phillipcloud/odo-5"&gt;http://slides.com/phillipcloud/odo-5&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Brains &amp; Brawn: the Logic and Implementation of a Redesigned Adver</title><link href="https://pyvideo.org/pydata-seattle-2015/brains-brawn-the-logic-and-implementation-of-a-redesigned-adver.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Stephanie Tzeng</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/brains-brawn-the-logic-and-implementation-of-a-redesigned-adver.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- AppNexus
In this session, the AppNexus team will discuss how their algorithm, called the Offer Quick Test, has improved the online advertising marketplace. Using fine-tuned processes based on existing data and mathematical parameters, this algorithm allows for website owners to quickly and precisely test out each advertising offer and increase the ROI for all players in the marketplace.&lt;/p&gt;
&lt;p&gt;In today’s online advertising marketplaces, how can websites determine which advertisers will be successful? One of the ways to monetize a website is by implementing Pay Per Click or Conversion advertising; however, this causes website owners to take on risk and potentially lose money.&lt;/p&gt;
&lt;p&gt;In this session, the AppNexus team will discuss how to solve this challenge by quickly and effectively matching the advertisers and websites using an algorithm, called the Offer Quick Test. This algorithm involves fine-tuned processes based on existing data and mathematical parameters, making it an interesting data science as well as implementation problem.&lt;/p&gt;
&lt;p&gt;Join Data Scientist, Stephanie Tzeng and Software Engineer, Sal Rinchiera as they explore these challenges to showcase how Offer Quick Test can increase ROI to the benefit of both advertisers and publishers. Learn how their in-house distributed work queue weaves data through a complex pipeline.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/brains-brawn-the-logic-and-implementation-of-a-redesigned-advertising-marketplace-sponsor-talk"&gt;http://www.slideshare.net/PyData/brains-brawn-the-logic-and-implementation-of-a-redesigned-advertising-marketplace-sponsor-talk&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Building a JIT for Python</title><link href="https://pyvideo.org/pydata-seattle-2015/building-a-jit-for-python.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Dino Viehland</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/building-a-jit-for-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's make Python faster! In a past life I spent a bunch of time working to make a fast Python implementation built on top of .NET which leveraged the CLRs JIT for improved performance. In this talk I'll look at taking the CLR's JIT which has now been open sourced and bringing it together with the standard CPython implementation.&lt;/p&gt;
&lt;p&gt;Implementation Strategy Translating CPython bytecode to .NET IL Examples of what the translation looks like Look at what subset of IL is used What a standardized JIT interface for CPython could look like Challenges Places where the CLR JIT could better support Python like semantics Places where CPython makes it difficult Performance Results A look at various benchmarks with and without a JIT and against competing Python implementations Unsupported features Functionality which currently isn't or can't be supported Future Directions Future possibilities to improve performance&lt;/p&gt;
</summary></entry><entry><title>Building TaxBrain: Numba enabled Financial Computing on the Web</title><link href="https://pyvideo.org/pydata-seattle-2015/building-taxbrain-numba-enabled-financial-computing-on-the-web.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>T.J.  Alumbaugh</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/building-taxbrain-numba-enabled-financial-computing-on-the-web.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Open Source Policy Center maintains a Python package (“Tax Calculator”) that uses Numba to model the federal income tax code for policy analysis. In this talk, we describe the construction of TaxBrain, a web app deployed on Heroku that allows non-programmers to use this package. We discuss the particulars of handling computationally intensive workloads with compiled code on a cloud platform.&lt;/p&gt;
&lt;p&gt;The Open-Source Policy Center (OSPC) seeks to make policy analysis more transparent, trustworthy, and collaborative by harnessing open-source methods to build cutting-edge economic models. Our first package for release is the Tax Calculator. This Python package encodes current federal tax law and can be used to assess how policy reforms will affect government revenue and the distribution of the tax burden across income groups. In order to make this resource available to a large audience, we have created TaxBrain, a web application that allows users to specify Tax Calculator computations through a browser. The results are displayed in the browser as a number of tables, downloadable as CSV files. In this talk, we discuss the architecture of this web app, and its deployment on the Heroku platform. Tax Brain is a unique combination of web-enabled and traditional “scientific stack” Python code. We discuss our lessons learned, and give advice for those who wish to deploy numerical calculation codes in web-accessible environments.&lt;/p&gt;
</summary></entry><entry><title>Deep Learning with Python: getting started &amp; getting from ideas to insights in mins</title><link href="https://pyvideo.org/pydata-seattle-2015/deep-learning-with-python-getting-started-getting-from-ideas-to-insights-in-mins.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Alex Korbonits</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/deep-learning-with-python-getting-started-getting-from-ideas-to-insights-in-mins.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What is deep learning? How do I start? Can I use Python? This survey talk will include a brief overview of deep learning before addressing how you can get started doing deep learning in a matter of minutes using your laptop and your favorite PyData libraries. There will be a live demo of a few famous trained DNNs, and we'll touch on a few jumping-off points for real-world applications.&lt;/p&gt;
&lt;p&gt;What is deep learning? How do I start? Can I use Python? (short answer: yes!) This survey talk will include a brief overview of the past, present, and future of deep learning (what is a perceptron, what is backpropagation, and (briefly) what are some of the newer methods and their applications (drop out, convolutional nets, recurrent nets, Hinton's theory of capsules)) before addressing how you (yes, you!) can get started doing deep learning against your GPU(s) in a matter of minutes using your laptop and your favorite PyData libraries. We’ll navigate the landscape of existing tools for doing deep learning with Python (Caffe, Theano, PyLearn, Graphlab-Create, etc.), and discuss some of the advantages and pitfalls of each. There will be a live demo including a few famous multi-layer neural networks (LeNet, AlexNet, and an unsupervised example such as QuocNet) trained using Python and open datasets (MNIST, ImageNet, etc.). Visualization will also be discussed. Finally, we’ll touch on a few ideas that can be used as jumping-off points for real-world applications using these basic building blocks (image classification, music classification, natural language processing, automatic speech recognition, time series modeling, video game AI (via reinforcement learning), etc.).&lt;/p&gt;
&lt;p&gt;Slides can be found here: &lt;a class="reference external" href="http://www.slideshare.net/AlexanderKorbonits/deep-learning-with-python-pydata-seattle-2015"&gt;http://www.slideshare.net/AlexanderKorbonits/deep-learning-with-python-pydata-seattle-2015&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Democratizing Data Science</title><link href="https://pyvideo.org/pydata-seattle-2015/democratizing-data-science.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Benjamin Mako Hill</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/democratizing-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What if programming and data science was something everybody learned? As part of the Community Data Science Workshops, more than 50 volunteers have taught than 200 complete beginners the basics of Python and data analysis in a series of 4-day workshops. We'll talk about our approach and describe the successes and challenges of approaching Python and data science as basic literacies.&lt;/p&gt;
&lt;p&gt;The Community Data Science Workshops (CDSW) are a series of project-based workshops for anyone interested in learning how to use programming and data science tools to ask and answer questions about online communities like Wikipedia, Twitter, free and open source software, and civic media. The workshops are for people with no previous programming experience. The workshops bring together researchers, academics, and participants and leaders in online communities. Run three times in 2014 and 2015, the workshops have all been free of charge and are open to the public.&lt;/p&gt;
&lt;p&gt;The sessions are scheduled for one Friday evening and three Saturdays all day. Each session involves a period for lecture and technical demonstrations in the morning. The rest of the day consists of self-directed work on programming and data science projects supported by more experienced mentors.&lt;/p&gt;
&lt;p&gt;Our goal is that, after the three workshops, participants will be able to use data to produce numbers, hypothesis tests, tables, and graphical visualizations to answer questions like: Are new contributors to an article in Wikipedia sticking around longer or contributing more than people who joined last year? Who are the most active or influential users of a particular Twitter hashtag? Are people who participated in a Wikipedia outreach event staying involved? How do they compare to people that joined the project outside of the event?&lt;/p&gt;
&lt;p&gt;Our very first workshops was originally modeled after the Boston Python Workshops but most our curriculum is new and has been developed and modified by the mentors and with feedback from the participants. The CDSW curriculum, now being taught outside Seattle by others inspired by our model, is entirely based on Python. Our most recent round of workshops in Spring 2015 was taught entirely using Python 3.&lt;/p&gt;
&lt;p&gt;Teaching data science over only four days to people who begin without any familiarity with concepts like the command line or variables is a major departure from traditional data science curricula that assume at least some familiarity with programming and statistics.&lt;/p&gt;
&lt;p&gt;This talk will describe the approach we have taken to refine our material over the three times we have run the workshops and will share details of our experience. CDSW's organizers are professional programmers and data scientists and several of us have experience teaching data science in more traditional university and corporate settings. Our talk will describe how &amp;quot;democratized&amp;quot; data science is similar to — and sometimes extremely different from — these more traditional approaches. We will talk about some of the challenges we have faced and highlight some of our most inspirational successes.&lt;/p&gt;
</summary></entry><entry><title>From DataFrames to Interactive Web Applications in 10 minutes</title><link href="https://pyvideo.org/pydata-seattle-2015/from-dataframes-to-interactive-web-applications-in-10-minutes.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Adam Hajari</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/from-dataframes-to-interactive-web-applications-in-10-minutes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Any data driven projects can benefit greatly from a simple, interactive, and easily accessible user interface. Whether your project is in the prototyping stage or you just want a way to quickly get your ideas and research to an audience unfamiliar with the command line, this talk will show you how to quickly turn your python code into interactive web applications.&lt;/p&gt;
&lt;p&gt;As an engineer, analyst, or scientist, sharing your work with someone outside of your immediate team can be a challenge. End-users embody many roles with a wide range of technical skill and often times no familiarity with Python or the command line. Findings, key results, and models are frequently boiled down to static graphs, tables, and figures presented in short reports or slideshow presentations. However, engaging research and data analysis is interactive, anticipating the users’ questions and giving them the tools to answer those questions with a simple and intuitive user interface.&lt;/p&gt;
&lt;p&gt;Browser based applications are an ideal vehicle for delivering these types of interactive tools, but building a web app requires setting up backend applications to serve up content and creating a UI with languages like HTML, CSS, and JavaScript. This is a non-trivial task even for web-developers and can be completely overwhelming for anyone not familiar with web stack basics.&lt;/p&gt;
&lt;p&gt;Spyre (&lt;a class="reference external" href="https://github.com/adamhajari/spyre"&gt;https://github.com/adamhajari/spyre&lt;/a&gt;) is a web application framework for the python developer who may have little knowledge of how web applications works, much less how to build them. Spyre takes care of setting up both the front and back-end of your web application. It uses CherryPy to handle HTTP request logic and Jinja2 to auto-generate all of the client-side nuts and bolts, allowing developers to quickly move the inputs and outputs of their python modules into a browser based application. Inputs, controls, outputs, and the relationships between all of these components are specified in a python dictionary. The developer need only define this dictionary and override the methods needed to generate content (text, tables, and plots).&lt;/p&gt;
&lt;p&gt;While Spyre apps are launched on CherryPy’s production-ready server, Spyre’s primary goal is to provide a development path for simple light-weight apps without the need for a designer or front-end engineer. For example, Spyre can be used for:&lt;/p&gt;
&lt;p&gt;rapid prototyping and building MVPs
data exploration
developing educational resources
building monitoring tools
presenting interactive scientific or analytical results to a non-technical audience
just to name a few.&lt;/p&gt;
&lt;p&gt;Materials available here: &lt;a class="reference external" href="http://bit.ly/pydata2015_spyre"&gt;http://bit.ly/pydata2015_spyre&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Hack the Derivative</title><link href="https://pyvideo.org/pydata-seattle-2015/hack-the-derivative.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Erik Taubeneck</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/hack-the-derivative.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Numerical estimates of the derivative of a function are typically done using an approximation called &amp;quot;Finite Difference.&amp;quot; However, the accuracy of this method is computationally bounded. Using Complex Analysis, we're able to take the step in the imaginary direction on the complex plane and achieve near perfect estimation of the derivative in only three lines of code!&lt;/p&gt;
&lt;p&gt;Numerical estimates of the derivative of a function are typically done using an approximation called &amp;quot;Finite Difference.&amp;quot; In a nutshell, the derivative is a function that tells you the slope of a function at a specific point. The Finite Difference approximation takes the rise over run formula (which works exactly for linear equations), and applies it to a non-linear with a very small step size. The derivative is defined as the limit of this process, as the step size goes to zero. Computationally, however, this problem is ill-formed: floating point numbers have gaps between them, and eventually our step size becomes smaller than those gaps. This puts a hard limit on the potential accuracy of finite difference approach. Fear not! Using some fancy mathematics (the Cauchy-Riemann equations, the crown jewel of Complex Analysis) we're able to take the step in the imaginary direction on the complex plane, reducing the problem to one evaluation of the target function and a division. This results in a near perfect estimation of the derivative in only three lines of code (with a few interesting caveats)!&lt;/p&gt;
&lt;p&gt;Timeline:
(5 min) The Derivative and Finite Differences
(5 min) Scipy Implementation, with benchmarks
(8 min) The Cauchy Riemann Equations Overview
(5 min) Use the Cauchy Riemann Equations with Finite Difference
(2 min) Implementing the Complex Step Derivative in Three Lines of Code!
(5 min) Examples with benchmarks
(5 min) Caveats and Fixes
(5 min) Q&amp;amp;A&lt;/p&gt;
</summary></entry><entry><title>Investigating User Experience with Natural Language Analysis</title><link href="https://pyvideo.org/pydata-seattle-2015/investigating-user-experience-with-natural-language-analysis.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Stephanie Kim</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/investigating-user-experience-with-natural-language-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk focuses on the methodology and intent of studying feedback form data using Python tools and libraries for natural language processing and machine learning analysis. I will discuss potential trouble areas of starting such a project from scratch from a developer’s perspective. This will include the type of analysis that might be helpful in discerning user experience and what analysis that you run, but might end up tossing out at the end due to lack of insight on your data. For instance, what is the value of running a K-Means cluster analysis and does it offer substantial actionable insights for textual content? And how much data do you need to pre-label for a training set for a Naïve Bayes Classification in order for it to be accurate? My intent is that people will walk away learning the basics of textual analysis and become motivated to help their users succeed in whatever tasks they are trying to accomplish through finding potential points of friction and even issues that spring up from changes in the design. This talk will be for developers or marketers who don’t have a lot or any experience in data analysis or machine learning.&lt;/p&gt;
</summary></entry><entry><title>Jupyter for Education: Beyond Gutenberg and Erasmus</title><link href="https://pyvideo.org/pydata-seattle-2015/jupyter-for-education-beyond-gutenberg-and-erasmus.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Paco Nathan</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/jupyter-for-education-beyond-gutenberg-and-erasmus.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;O'Reilly Learning focuses on the evolution of learning experiences for our audience, across O'Reilly Media. One key part leverages containerized notebooks as a radically different approach to publishing. This talk provides a deep-dive into the tech stack, its impact on rubric, the resulting author workflow, and overall how this moves beyond Books, beyond Kindle, beyond MOOCs.&lt;/p&gt;
&lt;p&gt;O'Reilly Learning is a new business unit focused on the (rapid) evolution of learning experiences for our audience, across O'Reilly Media.&lt;/p&gt;
&lt;p&gt;One key part of our work leverages cloud-based containerized notebooks as a radically different approach to publishing. This is in close collaboration with Project Jupyter. See Jupyter at O'Reilly for examples.&lt;/p&gt;
&lt;p&gt;This talk reviews how O'Reilly Media leverages Jupiter's interactive reproducible computing environments for patterns of &amp;quot;code-as-media&amp;quot; -- delivered across a spectrum of technology and business education. Moving forward, key concepts include: learning paths, continuous learning, inverted classroom, computational thinking, learner segmentation, computational narratives, social context.&lt;/p&gt;
&lt;p&gt;Jupyter helps O'Reilly provide on-demand environments for learning and analytics. It is also important to consider how it integrates classroom and remote learning environments, and the spectrum of learning modes engaged.&lt;/p&gt;
&lt;p&gt;Moving beyond Books, beyond Kindle, beyond MOOCs ... this talk looks at the required tech stack for a learning architecture at scale, as well as some of the impact of this technology on rubric.&lt;/p&gt;
&lt;p&gt;We show our curated list of excellent examples of notebook usage throughout academia and industry, as well as compare/contrast Jupyter with other notebook technologies.&lt;/p&gt;
&lt;p&gt;We also show how this work from the author workflow perspective, leveraging two key components: Atlas (editing/publishing platform) and the open source project Thebe (notebook media player).&lt;/p&gt;
&lt;p&gt;The speaker has taught for the past two years using mixed media integrated with notebooks as the central focus, and now leads the O'Reilly Learning team.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/pacoid/jupyter-for-education-beyond-gutenberg-and-erasmus"&gt;http://www.slideshare.net/pacoid/jupyter-for-education-beyond-gutenberg-and-erasmus&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Keynote- Clouded Intelligence</title><link href="https://pyvideo.org/pydata-seattle-2015/keynote-clouded-intelligence.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Joseph Sirosh</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/keynote-clouded-intelligence.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Seattle 2015
Several exciting trends are driving the birth of the intelligent cloud. The vast majority of world’s data is now connected data resident in the cloud. The majority of world’s new software is now connected software, also resident in or using the cloud. New cloud based Machine Learning as a Service platforms help transform data into intelligence and build cloud-hosted intelligent APIs for connected software applications. Face analysis, computer vision, text analysis, speech recognition, and more traditional analytics such as churn prediction, recommendations, anomaly detection, forecasting, and clustering are all available now as cloud APIs, and far more are being created at a rapid pace. Cloud hosted marketplaces for crowdsourcing intelligent APIs have been launched. In this talk I will review what these trends mean for the future of data science and show examples of revolutionary applications that you can build using cloud platforms.&lt;/p&gt;
</summary></entry><entry><title>Keynote - Computation at the edges</title><link href="https://pyvideo.org/pydata-seattle-2015/keynote-computation-at-the-edges.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Van Lindberg</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/keynote-computation-at-the-edges.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For most of the history of computing, we have focused on centralized forms of computing. It has always been faster and cheaper to do our processing with a powerful centralized processor than a group of lower-powered processors at the edges. This is changing. The last couple years have brought a new focus on scale-out and distributed architectures, but that is not enough. We are getting to the place where vastly more data, and vastly more compute power, will be available at the edges of our networks. How should this change how we approach the the future?&lt;/p&gt;
</summary></entry><entry><title>Learning Data Science Using Functional Python</title><link href="https://pyvideo.org/pydata-seattle-2015/learning-data-science-using-functional-python.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Joel Grus</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/learning-data-science-using-functional-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Seattle 2015
Everyone has an opinion on the best way to learn data science. Some people start with statistics or machine learning theory, some use R, and some use libraries like scikit-learn. I'll use several examples to contrast these with a simpler approach using functional programming techniques in Python. In addition, I'll show how even advanced data scientists can benefit from thinking more functionally.&lt;/p&gt;
&lt;p&gt;Materials available here:
Github: &lt;a class="reference external" href="https://github.com/joelgrus/stupid-itertools-tricks-pydata"&gt;https://github.com/joelgrus/stupid-itertools-tricks-pydata&lt;/a&gt;
Slides: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1eI60SL3UxtWfr9ktrv48-pcIkk4S7JiDmeXGCyyGhCs/edit#slide=id.p"&gt;https://docs.google.com/presentation/d/1eI60SL3UxtWfr9ktrv48-pcIkk4S7JiDmeXGCyyGhCs/edit#slide=id.p&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Memex - Mining the Dark Web</title><link href="https://pyvideo.org/pydata-seattle-2015/memex-mining-the-dark-web.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Katrina Riehl</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/memex-mining-the-dark-web.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Slides available here:  &lt;a class="reference external" href="http://www.slideshare.net/continuumio/memex-pydata-seattle"&gt;http://www.slideshare.net/continuumio/memex-pydata-seattle&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Mixed language Python &amp; C++ debugging with Python Tools for Visual Studio</title><link href="https://pyvideo.org/pydata-seattle-2015/mixed-language-python-c-debugging-with-python-tools-for-visual-studio.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Pavel Minaev</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/mixed-language-python-c-debugging-with-python-tools-for-visual-studio.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- Microsoft
A well-recognized strength of Python is the ease with which it can be extended with libraries written in C. At the same time, the debugging story for such extensions has been notoriously poor, with no ability to seamlessly debug both languages at the same time. This talk showcases an implementation of such seamless debugging in Python Tools for Visual Studio.&lt;/p&gt;
&lt;p&gt;Problems with existing Python and C++ debuggers when mixing two code types: no ability to step between the two, no way to peek at underlying native data structures under the hood when in Python. Seamless mixed-mode debugging of both types in a single debug session is necessary to solve these problems, including transparent stepping from Python to native code and from native to Python, breakpoints in both types of code side by side, and the ability to inspect how the objects look from the other side.&lt;/p&gt;
&lt;p&gt;The talk showcases the overall mixed-mode experience in Python Tools for Visual Studio, and then goes over all the specific features listed above in detail, and the limitations imposed by the mixed-language model.&lt;/p&gt;
&lt;p&gt;Slides are available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/mixedlanguage-pythonc-debugging-with-python-tools-for-visual-studio-pavel-minaev"&gt;http://www.slideshare.net/PyData/mixedlanguage-pythonc-debugging-with-python-tools-for-visual-studio-pavel-minaev&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Panel: Using, contributing to, and integrating open source in large corporate environments</title><link href="https://pyvideo.org/pydata-seattle-2015/panel-using-contributing-to-and-integrating-open-source-in-large-corporate-environments.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/panel-using-contributing-to-and-integrating-open-source-in-large-corporate-environments.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary></entry><entry><title>Saving Lives with Data: Python and Global Health</title><link href="https://pyvideo.org/pydata-seattle-2015/saving-lives-with-data-python-and-global-health.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Kyle Foreman</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/saving-lives-with-data-python-and-global-health.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At the University of Washington's Institute for Health Metrics and Evaluation we combine a massive collection of global health data with cutting edge statistics to inform decision making that potentially affects the health of billions of people. I'll explain how Python is an integral part of our stack for managing petabytes of data and introduce several novel statistical tools we've developed.&lt;/p&gt;
&lt;p&gt;The Institute for Health Metrics and Evaluation has been pushing the science of global health forward by introducing cutting edge statistical and computational techniques to a rapidly growing collection of health data from around the world. I will demonstrate this through several examples of how Python fits into our large data analysis stack:&lt;/p&gt;
&lt;p&gt;PyMB is a model building tool that uses algorithmic differentiation to allow us to optimize large statistical models. Previous Bayesian modeling frameworks were unable to fit such large models quickly (or sometimes not at all), so this has enabled us to greatly enhance the quality of our models.
V1 uses IPython magic on top of rpy2 to abstract away the complexities of writing TMB models.
V2 is in progress and uses PyCppAD to create a Pythonic interface for generating highly efficient C++ models that uses numpy for data I/O.
DisMod is a disease modeling package that uses PyMC to fit compartmental models to &amp;quot;messy&amp;quot; data.
CODEm is an ensemble modeling framework that can test thousands of hypothetical models and generate optimal combinations using crossvalidation.
V1 used Python to glue together a lot of Stata code that had previously been exceptionally difficult to run on our 17k core cluster.
V2 is entirely rewritten in Python and uses multithreading and Theano to speed up the previous Stata implementation by over 100x.
We have begun a new project to forecast the entire Global Burden of Disease to 2040 and enable policymakers and funders to decide how to best ensure the health of the world in the future.
This project uses PySpark to manage simulations outputting over 3 petabytes of data each time they're run.
We have built a tool to enable us to create directed acyclic graphs from SymPy expressions that can be executed seamlessly on backends ranging from single threaded numpy packages to large Hadoop clusters.
Finally, I'll touch on how we use web tools such as GBD Compare in our work to both share results and enable collaborators around the world to easily run sophisticated models on our cluster using simple GUIs. These tools are primarily javascript on the front end, but most leverage Django on the backend to allow our developers to quickly prototype and integrate with our statistical software.&lt;/p&gt;
</summary></entry><entry><title>SFrame and SGraph</title><link href="https://pyvideo.org/pydata-seattle-2015/sframe-and-sgraph.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Jay (Haijie) Gu</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/sframe-and-sgraph.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A good machine learning platform requires not just robust implementations of statistical models and algorithms; it also relies on having the right data structures for efficient and scalable feature engineering and data cleaning. In this talk, we discuss SFrame and SGraph, two scalable data structures designed with machine learning tasks in mind. These external memory structures make efficient use of disk and utilize a whole bag of tricks for speed. On a single machine, SFrame supports real time interactive query on terabytes of data. When used in a distributed setting, SGraph supports iterative graph analytics tasks at unparalleled speed. On a graph with 100 billions of edges, SGraph computes Pagerank at 30secs/iter with only16 EC2 machines. We walk through the architectural design and discuss tricks for scale and speed. SFrame and SGraph are the backbone of a new Python machine learning platform called GraphLab Create. Both are available for download as open source projects, or as part of the GraphLab Create binary.&lt;/p&gt;
</summary></entry><entry><title>Sparkling Pandas- Letting Pandas Roam on Spark DataFrames</title><link href="https://pyvideo.org/pydata-seattle-2015/sparkling-pandas-letting-pandas-roam-on-spark-dataframes.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/sparkling-pandas-letting-pandas-roam-on-spark-dataframes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a fast and expressive library for data analysis that doesn’t naturally scale to more data than can fit in memory. PySpark is the Python API for Apache Spark that is designed to scale to huge amounts of data but lacks the natural expressiveness of Pandas. This talk introduces Sparkling Pandas, a library that brings together the best features of Pandas and PySpark.&lt;/p&gt;
&lt;p&gt;Pandas is a fast and expressive library for data analysis that doesn’t naturally scale to more data than can fit in memory. PySpark is the Python API for Apache Spark that is designed to scale to huge amounts of data but lacks the natural expressiveness of Pandas. This talk introduces Sparkling Pandas, a library that brings together the best features of Pandas and PySpark; Expressiveness, speed, and scalability.&lt;/p&gt;
&lt;p&gt;While both Spark 1.3 and Pandas have classes named ‘DataFrame’ the Pandas DataFrame API is broader and not fully covered by the ‘DataFrame’ class in Spark. This talk will explore some of the differences between Spark’s DataFrames and Panda’s DataFrames and then examine some of the work done to implement Panda’s like DataFrames on top of Spark. In some cases, providing Pandas like functionality is computationally expensive in a distributed environment, and we will explore some techniques to minimize this cost.&lt;/p&gt;
&lt;p&gt;At the end of this talk you should have a better understanding of both Sparkling Pandas and Spark’s own DataFrames. Whether you end up using Sparkling Pandas or Spark directly, you will have a greater understanding of how to work with structured data in a distributed context using Apache Spark and familiar DataFrame APIs.&lt;/p&gt;
&lt;p&gt;Materials available here:
Slides: &lt;a class="reference external" href="http://www.slideshare.net/hkarau/sparkling-pandas-electric-bugaloo-py-data-seattle-2015"&gt;http://www.slideshare.net/hkarau/sparkling-pandas-electric-bugaloo-py-data-seattle-2015&lt;/a&gt;
Project github: &lt;a class="reference external" href="https://github.com/sparklingpandas/sparklingpandas"&gt;https://github.com/sparklingpandas/sparklingpandas&lt;/a&gt;
Project website: &lt;a class="reference external" href="http://sparklingpandas.com/"&gt;http://sparklingpandas.com/&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Straight, White Males Should Advocate for Diversity</title><link href="https://pyvideo.org/pydata-seattle-2015/straight-white-males-should-advocate-for-diversity.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Tony Wieczorek</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/straight-white-males-should-advocate-for-diversity.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This is a talk for anyone who wants a more diverse engineering culture at work. If you've ever been frustrated by the sameness of your engineering peers, you'll hear practical advice you can use immediately. Diverse engineering teams recruit the best talent, are more innovative, better reflect the needs of their users and make for incredibly fun places to work.&lt;/p&gt;
&lt;p&gt;Anyone can advocate for their engineering department to be more inclusive of diverse genders, races and backgrounds. It's not a &amp;quot;woman problem&amp;quot;, a &amp;quot;gay problem&amp;quot; or a &amp;quot;minority problem&amp;quot; - it's a community problem.&lt;/p&gt;
&lt;p&gt;Measure - You need data to measure progress, and the first step is knowing how diverse your group really is.
Fund - Getting the funds and providing logistical support to host meetups for diverse groups in your city
Raise - Raise your hand and let your company's management know that the issue is important to you and to your colleagues.
Call Out - When you hear something misogynistic, homophobic or racist, say so. Don't let the responsibility fall solely on the group of people targeted.
Recruit - Actively recruit for diversity in your department.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/straight-white-malebeing-an-ally-in-diversity-tony-wieczorek"&gt;http://www.slideshare.net/PyData/straight-white-malebeing-an-ally-in-diversity-tony-wieczorek&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Supernova Cosmology with python</title><link href="https://pyvideo.org/pydata-seattle-2015/supernova-cosmology-with-python.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Rahul Biswas</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/supernova-cosmology-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Certain types of exploding stars, Supernovae Type Ia have been used to infer that cosmic expansion is accelerating. Large surveys like LSST will potentially find numerous such supernovae probing the physics and phenomenology underlying the acceleration. We discuss a flexible, modular suite of python scientific software for simulation and analysis of such survey data with different algorithms.&lt;/p&gt;
&lt;p&gt;Scientific Background
Supernovae Type Ia which are exploding stars of a certain class have been considered to be emperical standardizable candles: astrophysical objects whose intrinsic luminosity can be inferred from other observable characteristics of their light after explosion. This property was used in the discovery that the expansion of the universe is accelerating.&lt;/p&gt;
&lt;p&gt;Extracting this information from the data requires a number of analysis steps; it is important to try out different methods in many of these steps to discover the optimal methods, thereby requiring a certain amount of modularity in the cod e. The information extracted also depends on the observational strategy used in the survey, and thus exploring the best observational strategy is also important.&lt;/p&gt;
&lt;p&gt;Implementation
The readily available and growing library of python scientific software for data analysis provides a good environment for quickly and easily implementing new algorithms, along with the ability to call programs in other languages. Our suite uses a python package SNCosmo to provide basic functionality for several tasks related to supernovae analysis. We also use products from the suite of LSST software to characterize the observational strategy. Finally, we use a workflow management tool Tigres to build larger tasks out of the primitives, and parallelize on certain systems.&lt;/p&gt;
</summary></entry><entry><title>Swarm Intelligence Optimization using Python</title><link href="https://pyvideo.org/pydata-seattle-2015/swarm-intelligence-optimization-using-python.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>James McCaffrey</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/swarm-intelligence-optimization-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Swarm intelligence (SI) algorithms mimic the collective behavior of groups such as flocks of birds and schools of fish. This session describes in detail three major SI algorithms: amoeba method optimization, particle swam optimization, and simulated bee colony optimization. Attendees will receive Python source code for each algorithm.&lt;/p&gt;
&lt;p&gt;Although SI algorithms have been studied for years, there is little practical implementation guidance available. This session describes the scenarios when SI algorithms are useful (and scenarios when SI algorithms are not useful), carefully explains how three major SI algorithms work, and presents a production quality, working demo, coded using Python, of each algorithm. Attendees will leave this session with a clear understanding of exactly what SI algorithms are, and have the knowledge needed to apply them immediately.&lt;/p&gt;
&lt;p&gt;This session assumes attendees have intermediate or higher level coding ability with Python, but does not assume any knowledge of swarm intelligence.&lt;/p&gt;
</summary></entry><entry><title>Testing for Data Scientists</title><link href="https://pyvideo.org/pydata-seattle-2015/testing-for-data-scientists.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Trey Causey</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/testing-for-data-scientists.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data scientists often don't have experience working as software developers and never learned how to write tests for their code. Things get even more complicated when your code has non-deterministic outcomes as is the case with probabalistic models. In this talk, I'll introduce the concept of unit testing for data scientists and discuss how to make testing a part of their normal workflow.&lt;/p&gt;
&lt;p&gt;Data scientists often don't have experience working as software developers and never learned how to write unit tests for their code. They may be used to writing and executing code interactively or in an ad hoc fashion; they may be unused to writing code that runs without supervision or as part of a larger pipeline. Things get even more complicated when code has non-deterministic outcomes as is the case with probabalistic models. In this talk, I'll introduce the concept of unit testing, describe how to write good tests, and discuss how to make testing a part of a normal data science workflow. Attendees should be already familiar with the PyData stack but might not be writing production code.&lt;/p&gt;
</summary></entry><entry><title>The past, present, and future of Jupyter and IPython</title><link href="https://pyvideo.org/pydata-seattle-2015/the-past-present-and-future-of-jupyter-and-ipython.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Jonathan Frederic</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/the-past-present-and-future-of-jupyter-and-ipython.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Seattle 2015
Learn about IPython, Jupyter, and the larger Jupyter universe. A brief history of the project will be covered, including the evolution of Jupyter from IPython. Some of the lesser known Jupyter projects will be described and demoed. Lastly, the future of the project will be discussed, including grant deliverables.&lt;/p&gt;
&lt;p&gt;People often confuse Jupyter and IPython, or are unaware that Jupyter exists and is related to IPython. The true relationship of the two will be described, and a brief history about why both exist will be given.&lt;/p&gt;
&lt;p&gt;The talk will describe the current state of Jupyter, including the notebook and some of the other projects, like JupyterHub, nbgrader, nbconvert, nbviewer, etc. Things that can be demoed live, like JupyterHub, will be demoed live.&lt;/p&gt;
&lt;p&gt;The Jupyter project recently received funding, some of the deliverables on that grant will be described in addition to other changes that the team has been working on.&lt;/p&gt;
</summary></entry><entry><title>Trend Estimation in Time Series Signals</title><link href="https://pyvideo.org/pydata-seattle-2015/trend-estimation-in-time-series-signals.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Bugra Akyildiz</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/trend-estimation-in-time-series-signals.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Trend estimation is a family of methods to be able to detect and predict tendencies and regularities in time series signals without knowing any information a priori about the signal. Trend estimation is not only useful for trends but also could yield seasonality(cycles) of data as well. I will introduce various ways to detect trends in time series signals.&lt;/p&gt;
&lt;p&gt;With more and more sensors readily available and collection of data becomes more ubiquitous and enables machine to machine communication(a.k.a internet of things), time series signals play more and more important role in both data collection process and also naturally in the data analysis. Data aggregation from different sources and from many people make time-series analysis crucially important in these settings.
Detecting trends and patterns in time-series signals enable people to respond these changes and take actions intelligibly. Historically, trend estimation has been useful in macroeconomics, financial time series analysis, revenue management and many more fields to reveal underlying trends from the time series signals.&lt;/p&gt;
&lt;p&gt;Trend estimation is a family of methods to be able to detect and predict tendencies and regularities in time series signals without knowing any information a priori about the signal. Trend estimation is not only useful for trends but also could yield seasonality(cycles) of data as well. Robust estimation of increasing and decreasing trends not only infer useful information from the signal but also prepares us to take actions accordingly and more intelligibly where the time of response and to action is important.&lt;/p&gt;
&lt;p&gt;In this talk, I will introduce following trend estimation methods and compare them in real-world datasets comparing their advantages and disadvantages of each algorithm:
- Moving average filtering
- Exponential smoothing,
- Median filtering,
- Bandpass filtering,
- Hodrick Prescott Filter,
- Gradient Boosting Regressor,
- l_1 trend filtering(my own library)&lt;/p&gt;
&lt;p&gt;Materials Available
Slides: &lt;a class="reference external" href="http://bugra.github.io/pages/deck/2015-07-25/#/"&gt;http://bugra.github.io/pages/deck/2015-07-25/#/&lt;/a&gt;
Github Repo: &lt;a class="reference external" href="https://github.com/bugra/pydata-seattle-2015"&gt;https://github.com/bugra/pydata-seattle-2015&lt;/a&gt;
Notebook Link: &lt;a class="reference external" href="https://github.com/bugra/pydata-seattle-2015/blob/master/notebooks/Trend%20Estimation%20Methods.ipynb"&gt;https://github.com/bugra/pydata-seattle-2015/blob/master/notebooks/Trend%20Estimation%20Methods.ipynb&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>University of Washington eScience Institute</title><link href="https://pyvideo.org/pydata-seattle-2015/university-of-washington-escience-institute.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Jake VanderPlas</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/university-of-washington-escience-institute.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- University of Washington eScience Institute&lt;/p&gt;
&lt;p&gt;Slides available here:  &lt;a class="reference external" href="https://speakerdeck.com/jakevdp/the-escience-institute-data-science-at-uw"&gt;https://speakerdeck.com/jakevdp/the-escience-institute-data-science-at-uw&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Using Python and Azure Machine Learning</title><link href="https://pyvideo.org/pydata-seattle-2015/using-python-and-azure-machine-learning.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Chris Wilcox</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/using-python-and-azure-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Talk- Microsoft&lt;/p&gt;
</summary></entry><entry><title>What's coming in Python 3.5 and why you should be excited</title><link href="https://pyvideo.org/pydata-seattle-2015/whats-coming-in-python-35-and-why-you-should-be-excited.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Steve Dower</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/whats-coming-in-python-35-and-why-you-should-be-excited.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Overview of the newest additions to Python 3.5, being released later this year.&lt;/p&gt;
&lt;p&gt;Python 3.5, the latest installment of the language and library, is just around the corner-https://www.python.org/dev/peps/pep-0478/ (though you can try out the beta now-https://www.python.org/downloads/release/python-350b2/). This session will cover some of the new syntax and library additions that should have people excited to start using it.&lt;/p&gt;
&lt;p&gt;As a teaser (come to the session for all the details), we'll look at better asynchronous programming, simpler mathematics, easier installation, better package management, formalized type hints, flexible function calls, and more!&lt;/p&gt;
</summary></entry><entry><title>When is it good to be bad? How do hockey penalties affect the outcome of the game?</title><link href="https://pyvideo.org/pydata-seattle-2015/when-is-it-good-to-be-bad-how-do-hockey-penalties-affect-the-outcome-of-the-game.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Wendy Grus</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/when-is-it-good-to-be-bad-how-do-hockey-penalties-affect-the-outcome-of-the-game.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;On Jan. 20, Philadelphia Flyers forward Zac Rinaldo was ejected from a game after boarding Penguins defenseman Kris Letang. The Flyers came back to win. After the game, Rinaldo said he &amp;quot;changed the game&amp;quot; (for which he was suspended 8 games). Using Python for webscraping and data analysis, I explore data from 10 NHL seasons to investigate how hockey penalties affect the outcome of the game.&lt;/p&gt;
&lt;p&gt;When is Good to be Bad? How do hockey penalties affect the outcome of the game?&lt;/p&gt;
&lt;p&gt;This talk will focus more on the process of getting the penalty and goal data and less on data analysis. Although in the end, I will address the question of &amp;quot;When is it good to be bad?&amp;quot;&lt;/p&gt;
&lt;p&gt;LOOKING AT THE SOURCE DATA TO UNDERSTAND HOW TO PARSE IT WITH BEAUTIFULSOUP
Beginning with the 2002-2003 season, I explored 10 seasons worth of penalty data from NHL.com. During this time, NHL.com had at least 3 formats for their play-by-play game recaps, including some that were not valid HTML formats recognized by BeautifulSoup. With BeautifulSoup, I was able to build parser that could scrape the penalty information.&lt;/p&gt;
&lt;p&gt;USING TRY/EXCEPT TO IDENTIFY AND ACCOMODATE OF EDGE CASES
Many edge cases had to be accounted for in the parser. For example, how do I account for a shootout goal versus a regulation/overtime goal? And, how do I account for goals scored before the game starts?&lt;/p&gt;
&lt;p&gt;USING NUMPY AND SCIPY FOR EXPLORATORY DATA ANALYSIS
Once all the data was put together, I had to classify penalties. Some classifications I took from the NHL rule book (eg, Physical Fouls, Stick Infractions) and other classifications I came up with based on logic (eg, Physical Altercations, Bench, Double Minor). Penalties could belong to more than one classification. Not all seasons gave the same penalties in the pla-by-play, and rules changed between seasons. I then divided the penalties by the team score when the penalty occurred (tied, ahead, or behind), and the outcome after the penalty and calculated the odds ratio for different penalty types based on different starting points. I did a similar analysis but used &amp;quot;team to score the next goal&amp;quot; instead of outcome in case the effect of the penalty is fleeting. All analyses are preliminary, and I need to figure out how to take into account that you need at least one member of each team for a physical altercation penalty.&lt;/p&gt;
</summary></entry><entry><title>Who needs users? Just simulate them!</title><link href="https://pyvideo.org/pydata-seattle-2015/who-needs-users-just-simulate-them.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Chris Harland</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/who-needs-users-just-simulate-them.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How do I build a robust, trustworthy, and scalable experimentation system when my product doesn't have the millions of users I expect it to have in the future? Simulation of course! In this talk, we will explore components of an experimentation platform through modular simulation units that provide a shockingly realistic picture user logs and wade through the pitfalls of A/B testing.&lt;/p&gt;
&lt;p&gt;Experimentation (A/B testing) is a hot topic in online services. While there is ample discussion around why you should test and the benefits it might bring there is a surprising lack of discussion around actually implementing or building an experimentation platform. From an engineering perspective, how do I go from a blog post on A/B testing to a full fledged platform that provides my company robust, trustworthy, and scalable experimentation?&lt;/p&gt;
&lt;p&gt;In this talk, I will demonstrate the power of simulating user interaction logs (written in python) as building blocks for a test driven approach to constructing various parts of an experimental platform. I will mainly focus on the aggregation of such logs into interpretable scorecards fit for non-technical consumption. I will demonstrate the accuracy and flexibility of simulated logs in both reproducing real world outcomes as well as providing methods for testing unseen scenarios. I will also touch on user randomization, the pitfalls of incorrect aggregations, and provide an abstract way to think about experimentation in general. Finally, I will comment on a few of the (very public) ways that exeperimentation has been shown to fail and, with the use of simulated users, provide possible explanations for the failures.&lt;/p&gt;
&lt;p&gt;The goal of this talk is to provide evidence of the usefulness of log simulation, demonstrate the very simple concepts behind building the non-operational components of an experimentation system, and hopefully impart some experimental intuitions to attendees not deeply familiar with online experimentation.&lt;/p&gt;
</summary></entry><entry><title>A brief introduction to Distributed Computing with PySpark</title><link href="https://pyvideo.org/pydata-seattle-2015/a-brief-introduction-to-distributed-computing-with-pyspark.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/a-brief-introduction-to-distributed-computing-with-pyspark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Spark is a fast and general engine for distributed computing &amp;amp; big data processing with APIs in Scala, Java, Python, and R. This tutorial will briefly introduce PySpark (the Python API for Spark) with some hands-on-exercises combined with a quick introduction to Spark's core concepts. We will cover the obligatory wordcount example which comes in with every big-data tutorial, as well as discuss Spark's unique methods for handling node failure and other relevant internals. Then we will briefly look at how to access some of Spark's libraries (like Spark SQL &amp;amp; Spark ML) from Python. While Spark is available in a variety of languages this workshop will be focused on using Spark and Python together.&lt;/p&gt;
&lt;p&gt;This tutorial is intended for people new to Spark/PySpark, please install Spark (1.3.1 or later) from &lt;a class="reference external" href="http://spark.apache.org/downloads.html"&gt;http://spark.apache.org/downloads.html&lt;/a&gt; before class (we are working to have cluster resources available but having a local install is sufficient for the workshop and a good backup in case the WiFi isn't cooperating).&lt;/p&gt;
&lt;p&gt;Materials available here:
Slides: &lt;a class="reference external" href="http://www.slideshare.net/hkarau/a-really-really-fast-introduction-to-py-spark-lightning-fast-cluster-computing-with-python-1"&gt;http://www.slideshare.net/hkarau/a-really-really-fast-introduction-to-py-spark-lightning-fast-cluster-computing-with-python-1&lt;/a&gt;
Notebook:  &lt;a class="reference external" href="https://github.com/holdenk/intro-to-pyspark-demos/blob/master/ipython/Super-Fast-PySpark-Intro-PyData-Seattle-2015.ipynb"&gt;https://github.com/holdenk/intro-to-pyspark-demos/blob/master/ipython/Super-Fast-PySpark-Intro-PyData-Seattle-2015.ipynb&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Beautiful Interactive Visualizations in the Browser with Bokeh</title><link href="https://pyvideo.org/pydata-seattle-2015/beautiful-interactive-visualizations-in-the-browser-with-bokeh.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Bryan Van de Ven</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/beautiful-interactive-visualizations-in-the-browser-with-bokeh.html</id><summary type="html"></summary></entry><entry><title>Dask: out of core arrays with task scheduling</title><link href="https://pyvideo.org/pydata-seattle-2015/dask-out-of-core-arrays-with-task-scheduling.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/dask-out-of-core-arrays-with-task-scheduling.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask Array implements the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores.&lt;/p&gt;
&lt;p&gt;We describe dask, dask.array, dask.dataframe, as well as task scheduling generally.&lt;/p&gt;
&lt;p&gt;NumPy and Pandas provide excellent in-memory containers and computation for the Scientific Python ecosystem. As we extend to larger-than-memory datasets these containers fail, leaving scientists with less productive options that mesh less well with the existing ecosystem.&lt;/p&gt;
&lt;p&gt;A common solution to this problem is blocking algorithms and task scheduling. Blocking algorithms define macro-scale operations on the full dataset as a network of smaller operations on in-memory blocks of the dataset. Task scheduling allows many parallel workers to execute these tasks in a way consistent to their data dependencies.&lt;/p&gt;
&lt;p&gt;We introduce dask, a task scheduling specification, and dask.array a high-level abstraction that implements a large subset of the NumPy API with blocked algorithms. In many cases dask.array provides a drop-in replacement for NumPy for out-of-core datasets with parallel execution. We discuss the design choices behind dask, dask.array, and related projects and show performance both quantitatively with benchmarks and also in usability by demonstrating integration into the larger ecosystem.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/ContinuumIO/dask-tutorial"&gt;https://github.com/ContinuumIO/dask-tutorial&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Keynote - Computer Science: America's Untapped Opportunity</title><link href="https://pyvideo.org/pydata-seattle-2015/keynote-computer-science-americas-untapped-opportunity.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Hadi Partovi</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/keynote-computer-science-americas-untapped-opportunity.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Software and computers are everywhere, revolutionizing every field around us. But the majority of schools don't teach computer science. Code.org believes every student should have the opportunity to shape the 21st-century and wants to turn this problem around. Code.org has already helped over 100 million students try computer science for the first time with one Hour of Code, partnered with 70+ school districts to bring courses to schools, released a free online learning platform with 10% of students in K-8 US schools enrolled. We've helped change policy in 16 states to better support computer science. This is just the beginning of a bold vision to bring this foundational field to every K-12 public school by 2020.&lt;/p&gt;
</summary></entry><entry><title>Machine Learning with Scikit Learn</title><link href="https://pyvideo.org/pydata-seattle-2015/machine-learning-with-scikit-learn.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Jake VanderPlas</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/machine-learning-with-scikit-learn.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial will offer an introduction to the core concepts of machine learning and the Scikit-Learn package. We will introduce the scikit-learn API, and use it to explore the basic categories of machine learning problems and related topics such as feature selection and model validation, and practice applying these tools to real-world data sets.&lt;/p&gt;
&lt;p&gt;Machine learning is the branch of computer science concerned with the development of algorithms which can be trained by previously-seen data in order to make predictions about future data. It has become an important aspect of work in a variety of applications: from optimization of web searches, to financial forecasts, to studies of the nature of the Universe.&lt;/p&gt;
&lt;p&gt;This tutorial will explore machine learning with a hands-on introduction to the scikit-learn package. Beginning from the broad categories of supervised and unsupervised learning problems, we will dive into the fundamental areas of classification, regression, clustering, and dimensionality reduction. In each section, we will introduce aspects of the Scikit-learn API and explore practical examples of some of the most popular and useful methods from the machine learning literature.&lt;/p&gt;
&lt;p&gt;The strengths of scikit-learn lie in its uniform and well-document interface, and its efficient implementations of a large number of the most important machine learning algorithms. Those present at this tutorial will gain a basic practical background in machine learning and the use of scikit-learn, and will be well poised to begin applying these tools in many areas, whether for work, for research, for Kaggle-style competitions, or for their own pet projects.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/jakevdp/sklearn_pydata2015"&gt;https://github.com/jakevdp/sklearn_pydata2015&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Pandas: .head() to .tail()</title><link href="https://pyvideo.org/pydata-seattle-2015/pandas-head-to-tail.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Tom Augspurger</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/pandas-head-to-tail.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is an extremely powerful library for data analysis. With that power comes complexity. This tutorial will focus on the core features of pandas, which handle most data munging tasks. The emphasis will be on practical applications, illustrating solutions to common problems using real-world data.&lt;/p&gt;
&lt;p&gt;The motivation of this tutorial mirrors that of pandas itself: practicality. A brief discussion on the problems pandas tries to solve will help frame the rest of the tutorial. We'll aim for an intuitive understanding of each new method and data structure. This will help keep us from getting overwhelmed by the options available as we expand our data munging toolkit. The start of the talk will focus on the core operations of&lt;/p&gt;
&lt;p&gt;Selecting and Indexing
Reshaping and Tidy Data
Summarization
Grouped operations
Merging and Joining
These operations can be combined into &amp;quot;pandastic&amp;quot; method chains that flow seamlessly from data IO to analysis.&lt;/p&gt;
&lt;p&gt;Time permitting we'll look at some of the more specialized areas of pandas including Categoricals, time-series analysis, Hierarchical Indexes, chunked / out of core processing, and data pipelines.&lt;/p&gt;
&lt;p&gt;Learning to use a library the size of pandas is a huge commitment. What's more, your goal is rarely achieved just with pandas. Rather, pandas gets you to the point where you can begin your interesting analysis. We'll build the foundation to quickly get you past the data munging, to the analysis.&lt;/p&gt;
&lt;p&gt;Materials:
- slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/pandas-head-to-tail-slidestom-augspurger"&gt;http://www.slideshare.net/PyData/pandas-head-to-tail-slidestom-augspurger&lt;/a&gt;
- Github repo: &lt;a class="reference external" href="https://github.com/tomaugspurger/pydataseattle"&gt;https://github.com/tomaugspurger/pydataseattle&lt;/a&gt;
- nbviewer link to notebooks: &lt;a class="reference external" href="http://nbviewer.ipython.org/github/TomAugspurger/PyDataSeattle/tree/master/notebooks/"&gt;http://nbviewer.ipython.org/github/TomAugspurger/PyDataSeattle/tree/master/notebooks/&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>PySnpTools - A New Open Source Library for Reading &amp; Manipulating Matrix Data</title><link href="https://pyvideo.org/pydata-seattle-2015/pysnptools-a-new-open-source-library-for-reading-manipulating-matrix-data.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Carl Kadie</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/pysnptools-a-new-open-source-library-for-reading-manipulating-matrix-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Anyone who uses fast numeric NumPy arrays but would like a simpler-than-Pandas ability to slice-and-dice, read-and-write will find PySnpTools useful. I'll describe PySnpTools and also tell how it fits into our Machine Learning research group's long-term move from C++/VB to C# to Python. I'll also show how we use PySnpTools in FaST-LMM to do state-of-the-art Genome Wide Association Studies.&lt;/p&gt;
&lt;p&gt;The tutorial will cover:
PstReader: Full NumPy-meets-Pandas-like slicing and subsetting of matrix data before (and after) reading from disk. (For genomics, it includes support for the PLINK Bed and phenotype formats. It also includes low-memory, high-speed methods for common operations such as standardization and kernel-creation.)&lt;/p&gt;
&lt;p&gt;Utilities: One line intersecting and re-ordering of data for machine learning and statistics. Faster-than-NumPy extraction of a subarray from a NumPy array.&lt;/p&gt;
&lt;p&gt;IntRangeSet: Manipulate from zero to billions of integers as sets with very little memory.&lt;/p&gt;
&lt;p&gt;Python Trade Offs We Observe:
Our industrial research group focuses on Machine Learning. Over 15 years, we have moved from C++/VB to C# to Python. I'll talk about why we choose Python and what tradeoffs we see.&lt;/p&gt;
&lt;p&gt;Application:
PySnpTools spun out of FaST-LMM. FaST-LMM is an Open Source, Python-based state-of-the-art system for doing Genome Wide Association Studies (GWAS). It is described in publications in Nature Methods, Nature Genetics, and Bioinfomatics.&lt;/p&gt;
</summary></entry><entry><title>Python for Data Science: A Rapid On ramp Primer</title><link href="https://pyvideo.org/pydata-seattle-2015/python-for-data-science-a-rapid-on-ramp-primer.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Joe McCarthy</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/python-for-data-science-a-rapid-on-ramp-primer.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The goal of this tutorial is to provide efficient and sufficient scaffolding for people with no prior knowledge of Python – but with some knowledge of programming – to effectively utilize Python-based tools for data science research and development, such as the pandas and scikit-learn open source libraries, or the Atigeo xPatterns analytics framework.&lt;/p&gt;
&lt;p&gt;The first part of the tutorial will cover basic data science concepts and use code and data examples relevant to data science (drawn from the UCI mushroom dataset). Basic Python programming concepts will include data structures (strings, lists, tuples, dictionaries), control structures (conditionals &amp;amp; loops), file I/O, and defining and calling functions.&lt;/p&gt;
&lt;p&gt;The second part of the tutorial will focus on constructing a simple decision tree based on the ID3 algorithm and using it to classify instances from the UCI mushroom dataset. This portion will also include the use of recursion, Python classes (object-oriented programming) and the use of Python scripts with arguments from the command line.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/gumption/Python_for_Data_Science"&gt;https://github.com/gumption/Python_for_Data_Science&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Real Time Change Detection on Streaming Data</title><link href="https://pyvideo.org/pydata-seattle-2015/real-time-change-detection-on-streaming-data.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Cody Rioux</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/real-time-change-detection-on-streaming-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sponsor Tutorial- Netflix
This tutorial offers an introduction to how we perform change detection on data streams at Netflix using Python. We will develop a framework for running and evaluating change detection algorithms and then experiment with various techniques on real-world data gathered at Netflix.&lt;/p&gt;
&lt;p&gt;This tutorial offers an introduction to how we perform change detection on data streams at Netflix using Python. We will introduce the problem and a framework for solving it, as well as a method for evaluating the effectiveness of different techniques. Once we've established the problem and a framework we will dive into some real-world data collected at Netflix about device call volume in our call centre, and attempt to detect a meaningful increase in call volume related to a device.&lt;/p&gt;
&lt;p&gt;The associated notebook will scaffold out the framework, then introduce simple techniques, slowly building in complexity. Time permitting we will discuss these techniques and potentially the usefulness of an ensemble of techniques in a production environment to provide robustness against different behavior patterns of data.&lt;/p&gt;
&lt;p&gt;Topics Covered: - Change deteciton problem definition and framework - Simple change detection techniques - Synthetic data generation - Real-world data evaluation&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/codyrioux/pydata2015seattle"&gt;https://github.com/codyrioux/pydata2015seattle&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Scalable Pipelines with Luigi or: I’ll have the Data Engineering, hold the Java!</title><link href="https://pyvideo.org/pydata-seattle-2015/scalable-pipelines-with-luigi-or-ill-have-the-data-engineering-hold-the-java.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Jonathan Dinu</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/scalable-pipelines-with-luigi-or-ill-have-the-data-engineering-hold-the-java.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this workshop you see how (and why) to leverage the PyData ecosystem to build a robust data pipeline. More specifically you will learn how to use the Luigi framework to integrate multiple stages of a model building pipeline (collection, processing, vectorization, training of multiple models, and validation) all in Python!&lt;/p&gt;
&lt;p&gt;As companies scale prototypes and ad hoc analyses into production systems, it is critical to build automated (and repeatable) systems for data collection/processing and model training /evaluation which are fault tolerant enough to adapt to changing constraints. Sustainable software development is often an afterthought for data scientists, especially since the tools for analysis (R, scientific python, etc.) do not naturally lend themselves to building scalable and extensible software abstractions. But now we can have our cake and eat it too... all with Python!&lt;/p&gt;
&lt;p&gt;In this workshop you see how (and why) to leverage the PyData ecosystem to build a robust data pipeline. More specifically you will learn how to use the Luigi framework to integrate multiple stages of a model building pipeline: collection, processing, vectorization, training of multiple models, and validation.&lt;/p&gt;
&lt;p&gt;Outline:
The basic components of a data pipeline (5min)
What and Why Luigi (10min)
Lab: The Smallest (1 stage) pipeline (15min)
Managing dependencies in a pipeline (10min)
Lab: Multi-stage pipeline and introduction to the Luigi Visualizer (15min)
Serialization in a Data Pipeline (10min)
Lab: Integrating your pipeline with HDFS and Postgres (20min)
Scheduling (10min)
Lab: Parallelism and recurring jobs with Luigi (20min)
Wrap up and next steps (5min)&lt;/p&gt;
&lt;p&gt;Materials available here:
Github Repo:  &lt;a class="reference external" href="https://github.com/Jay-Oh-eN/data-engineering-101"&gt;https://github.com/Jay-Oh-eN/data-engineering-101&lt;/a&gt;
Slides:  &lt;a class="reference external" href="http://www.slideshare.net/jonathandinu/presentation-45784222"&gt;http://www.slideshare.net/jonathandinu/presentation-45784222&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Simplified statistics through simulation</title><link href="https://pyvideo.org/pydata-seattle-2015/simplified-statistics-through-simulation.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Justin Bozonier</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/simplified-statistics-through-simulation.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will learn how to make valid statistical inferences using only Python/Numpy in a way that is easy to understand. You shouldn't need to put blind faith in your local statistical expert. This tutorial will show you how to use less theory and validate your methods concretely using simulation.&lt;/p&gt;
&lt;p&gt;When I started learning more about statistics I became very frustrated with the numerous specialized tests and statistical measures. They all have their place but they tend to make an already intimidating field even more intimidating. As a result, I began to figure out methods of validating what I was doing that were based more on repeatable simulation rather than theory. I was then able to show working code to fellow analysts that illustrated why I was doing the statistical analyses I was doing and they could replicate and tweak my simulations to explore the possibilities rather than debate. At it's heart, this is a tutorial about how to have more constructive conversations about statistical inference with your peers.&lt;/p&gt;
&lt;p&gt;I will walk attendees through the topic via an Python notebook and all charting will be done in front of them as well using either Seaborn or Matplotlib. The core of the topic revolves around using distributions of data in order to drive all of our inference. I will take a few minutes in the beginning to prove out (via simulation) that these simulations on the data distributions are perfectly equivalent to their probability distributions. In other words, a beta distribution can be modeled using a distribution of 0's and 1's, a normal distribution can be developed sampling means randomly from observed static data, etc.&lt;/p&gt;
&lt;p&gt;I will cover the following topics:
Simulations and Monte Carlo methods
Parametric vs. non-parametric statistical tests
Bootstrapping
Solve probability puzzles with simulation
Answer &amp;quot;How many samples does this experiment need?&amp;quot;
Find split test conversion lift using simulation
From here to bayesian statistics (quick shout out to PyMC)&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/jcbozonier/PyData2015"&gt;https://github.com/jcbozonier/PyData2015&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Using Python for Linguistic Data Analysis</title><link href="https://pyvideo.org/pydata-seattle-2015/using-python-for-linguistic-data-analysis.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Rutu Mulkar-Mehta</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/using-python-for-linguistic-data-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial is an introduction to Natural Language Processing using Python, to rapidly build your own NLP module. We will start with the very basics of NLP - Lemmatization, Stemming, POS tagging, Parsing, Language Models, to the more complex pieces of NLP involving probabilities, statistics and word co-occurrences and finally deep learning approaches to NLP and word vectorization techniques.&lt;/p&gt;
&lt;p&gt;As the amount of Unstructured Linguistic Data is increasing each day, it is becoming important to develop tools to analyze this data automatically. In this tutorial I will take you through the basics of linguistic data analytics and then build up to come more complicated pieces of NLP. We will start with basic linguistic techniques - such as Lemmatization, Part of Speech Tagging, Parsing etc, and write some code to implement some these using NLTK. Next, I will talk about how probabilities and statistics are used with Linguistic Data Processing to develop Language Models, and finally we will talk about more complicated techniques such as Deep Learning. If we have time, we will go over two use of NLP - search engines, and sentiment analysis of customer reviews&lt;/p&gt;
&lt;p&gt;Slides can be found here: &lt;a class="reference external" href="https://github.com/rutum/pynlp/raw/master/NLP-using-python.pptx"&gt;https://github.com/rutum/pynlp/raw/master/NLP-using-python.pptx&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>