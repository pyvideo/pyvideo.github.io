<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Horace He</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 18 Sep 2024 00:00:00 +0000</lastBuildDate><item><title>Accelerating Generative AI</title><link>https://pyvideo.org/pytorch-conference-2023/accelerating-generative-ai.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a Cambrian explosion of performant and efficient methods to train and serve generative AI models within the community. The PyTorch team will present optimizations to transformer based Generative AI models, using pure, native PyTorch. In this talk we aim to cover both new techniques in PyTorch for driving efficiency gains, as well as showcasing how they can be composed on popular Generative AI models. Highlights will include methods spanning torch compile, quantization, sparsity, memory efficient attention, reducing padding.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Christian Puhrsch</dc:creator><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/accelerating-generative-ai.html</guid><category>PyTorch Conference 2023</category></item><item><title>FlexAttention - The Flexibility of PyTorch + The Performance of FlashAttention</title><link>https://pyvideo.org/pytorch-conference-2024/flexattention-the-flexibility-of-pytorch-the-performance-of-flashattention.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Introducing a novel abstraction leveraging the PyTorch compiler stack to enable custom, user-defined attention mechanisms. This new API supports dynamic modifications to attention scores within SDPA, providing both runtime and memory efficiency through kernel fusion with the FlashAttention algorithm.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yanbo Liang</dc:creator><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/flexattention-the-flexibility-of-pytorch-the-performance-of-flashattention.html</guid><category>PyTorch Conference 2024</category><category>Lightning Talk</category></item><item><title>New Activation Checkpointing APIs in PyTorch</title><link>https://pyvideo.org/pytorch-conference-2024/new-activation-checkpointing-apis-in-pytorch.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Activation checkpointing is a commonly used technique to reduce memory usage during model training by reducing the number of activations saved for backward. Instead of keeping tensors needed for backward alive until they are used in gradient computation during backward, those tensors are recomputed during the backward pass. This talk will introduce new activation checkpoint APIs that can help achieve a better trade off between memory savings and compute overhead that recomputing introduces.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jeffrey Wan</dc:creator><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/new-activation-checkpointing-apis-in-pytorch.html</guid><category>PyTorch Conference 2024</category><category>Lightning Talk</category></item></channel></rss>