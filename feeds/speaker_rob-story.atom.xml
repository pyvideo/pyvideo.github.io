<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_rob-story.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-07-06T00:00:00+00:00</updated><entry><title>Machine Learning Infrastructure at Stripe: Bridging from Python JVM</title><link href="https://pyvideo.org/pydata-seattle-2017/machine-learning-infrastructure-at-stripe-bridging-from-python-jvm.html" rel="alternate"></link><published>2017-07-06T00:00:00+00:00</published><updated>2017-07-06T00:00:00+00:00</updated><author><name>Rob Story</name></author><id>tag:pyvideo.org,2017-07-06:pydata-seattle-2017/machine-learning-infrastructure-at-stripe-bridging-from-python-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Machine learning at Stripe has a foundation built on Python and the PyData stack, with scikit-learn and pandas continuing to be core components of an ML pipeline that feeds a production system written in Scala. This talk will cover the ML Infra team’s work to bridge the serialization and scoring gap between Python and the JVM, as well as how ML Engineers ship models to production.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Stripe Machine Learning Infrastructure team exists to help engineers, data scientists, and analysts at Stripe develop and ship models to production. They own and operate the primary service that provides an API for scoring models for applications such as fraud and NLP, and are always looking for ways to help internal Stripe customers ship ML for new applications or model types.&lt;/p&gt;
&lt;p&gt;ML models at Stripe are trained and evaluated in Python, with scikit-learn as an integral piece in our pipeline. However, the primary scoring service is written in Scala, which presents us with a problem: how do we serialize and export models from Python to the JVM? This talk will discuss our serialization framework for serializing and packaging machine learning components; by the end you will learn how we export models, transformers, encoders, and pipelines from the world of scikit to that of our Scala service.&lt;/p&gt;
&lt;p&gt;We'll then cover what happens after the model has been loaded by our Scala service, namely how we name models uniquely and use metadata we call &amp;quot;tags&amp;quot; to keep track of what model is currently running in production, history of production models, etc. We'll discuss how we score candidate models in parallel to the production model to evaluate them for promotion to production.&lt;/p&gt;
&lt;p&gt;By the end of the talk you should have a clear idea of how we serialize, package, promote, and evaluate candidate models across the entire machine learning infra stack, from the start of training in Python to the final scoring in Scala.&lt;/p&gt;
</summary></entry><entry><title>Python Data Bikeshed</title><link href="https://pyvideo.org/pydata-seattle-2015/python-data-bikeshed.html" rel="alternate"></link><published>2015-07-26T00:00:00+00:00</published><updated>2015-07-26T00:00:00+00:00</updated><author><name>Rob Story</name></author><id>tag:pyvideo.org,2015-07-26:pydata-seattle-2015/python-data-bikeshed.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The PyData ecosystem is growing rapidly, with existing tools maturing and exciting new tools appearing on a regular basis. This talk will examine the crowded PyData ecosystem and bring some clarity to which Python data tool is the right one to reach for on any given analysis. It will focus on use-cases for pure python, toolz, Numpy, Pandas, Blaze, xray, bcolz, Dask, and Spark.&lt;/p&gt;
&lt;p&gt;The PyData ecosystem can be a bit confusing for those new to Python, or even experienced programmers moving to Python for its excellent data analysis capabilities. How do you know which tool to reach for on any given project? What tools work best for my data of size FooBar in data store FizzBuzz?&lt;/p&gt;
&lt;p&gt;This talk will explore the Python data toolchain from bottom to top, with a focus on what tools work best based on both data locality and analysis velocity. Think of your data pipeline and storage as a city, and your data tools as a shed full of bikes. What bike works best for which trip? When should you use pure Python (the fixie) to perform your analysis? How do Pandas (the geared commuter) and Blaze (the tandem) work together? Where does Spark (the fat tire bike) fit into all of this?&lt;/p&gt;
&lt;p&gt;This talk seeks to use questionable bike analogies to provide less-questionable look at the crowded PyData ecosystem and bring some clarity to which Python data tool is the right one to reach for on any given analysis. It will touch on pure python, toolz, Numpy, Pandas, Blaze, xray, bcolz, Dask, and Spark, with a focus on the use-cases for each one.&lt;/p&gt;
&lt;p&gt;Finally, we’ll talk about which library you should use to paint the bikeshed.&lt;/p&gt;
&lt;p&gt;Materials available here:  &lt;a class="reference external" href="https://github.com/wrobstory/pydataseattle2015"&gt;https://github.com/wrobstory/pydataseattle2015&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Data Engineering Architecture at Simple</title><link href="https://pyvideo.org/pydata-chicago-2016/data-engineering-architecture-at-simple.html" rel="alternate"></link><published>2016-08-27T00:00:00+00:00</published><updated>2016-08-27T00:00:00+00:00</updated><author><name>Rob Story</name></author><id>tag:pyvideo.org,2016-08-27:pydata-chicago-2016/data-engineering-architecture-at-simple.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;A walk through Simple's Data Engineering stack, including lessons learned and why we chose certain tools and languages for different parts of our infrastructure.&lt;/p&gt;
</summary><category term="architecture"></category><category term="Data"></category><category term="engineering"></category></entry></feed>