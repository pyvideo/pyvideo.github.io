<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 20 May 2017 12:45:00 +0200</lastBuildDate><item><title>Feature Importance and Ensemble Methods : a new perspective</title><link>https://pyvideo.org/pydata-barcelona-2017/feature-importance-and-ensemble-methods-a-new-perspective.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Ensemble methods are extremely performant in terms of prediction, but lack easy interpretation. Feature importance is not only counting up how many times a feature has been used in a weak learner, but also by how much this feature contributes to the result. Detailed example and implementation are provided in a jupyter notebook in python for the library &amp;quot;xgboost&amp;quot; of extreme gradient boosting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I - Feature importance in ensemble algorithms - state of the art&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Feature importance in sklearn/xgboost: basically counts the occurrences of a feature in all the weak learners&lt;/li&gt;
&lt;li&gt;Construction of the trees in xgboost: if the trees are deep enough, every feature is going to be used&lt;/li&gt;
&lt;li&gt;Global feature importance is a misleading: a given feature might be critical for a given subpopulation but completely irrelevant for another (ex : multi-class classification)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;II - Xgboost real feature importance&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Prediction influence: first splits influence the prediction more than last splits, so the importance of a feature must be weighted by the discrimination it provides&lt;/li&gt;
&lt;li&gt;Point-to-point feature importance: following the path of a given prediction, it is possible to weigh the importance of every used feature&lt;/li&gt;
&lt;li&gt;A relevant assessment of feature importance: explanation of a given prediction, and aggregation on a set of data points&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;III - Implementation and examples&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Point-to-point feature importance illustration and implementation explanation&lt;/li&gt;
&lt;li&gt;Evolution of feature importance with respect to learning iterations&lt;/li&gt;
&lt;li&gt;Noisy variables cancellation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;IV - Limits and ways forward&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A word on correlated variables&lt;/li&gt;
&lt;li&gt;Is there a compromise performance/interpretation ?&lt;/li&gt;
&lt;/ol&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Constant Bridon</dc:creator><pubDate>Sat, 20 May 2017 12:45:00 +0200</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/feature-importance-and-ensemble-methods-a-new-perspective.html</guid><category>xgboost</category></item></channel></rss>