<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Yeounoh Chung</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 18 Sep 2024 00:00:00 +0000</lastBuildDate><item><title>Lightning Talk: Large-Scale Distributed Training with Dynamo and Triton</title><link>https://pyvideo.org/pytorch-conference-2023/lightning-talk-large-scale-distributed-training-with-dynamo-and-triton.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we cover PyTorch/XLA distributed API in relation with Torch.Dynamo. Specifically, we discuss the new PyTorch/XLA SPMD API for automatic parallelization and our latest LLaMA2 training results. PyTorch/XLA SPMD makes it simple for PyTorch developers to distribute their ML workloads (e.g., training &amp;amp; inference with Dynamo) with easy-to-use API, and uses XLA GSPMD, high-performance automatic parallelization system. Under the hood, it transforms the user single-device program into a partitioned one. We will share how we enabled advanced 2D sharding strategies for LLaMA2 using PyTorch/XLA SPMD.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yeounoh Chung</dc:creator><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-large-scale-distributed-training-with-dynamo-and-triton.html</guid><category>PyTorch Conference 2023</category><category>Lightning Talk</category></item><item><title>PyTorch/XLA Auto-Sharding</title><link>https://pyvideo.org/pytorch-conference-2024/pytorchxla-auto-sharding.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyTorch/XLA recently launched the new PyTorch/XLA SPMD feature as a first-step to automate ML workloads parallelization using GSPMD. It turns out that the performance largely depends on the quality of sharding hints provided by the user â€“ and it requires a correct and deep understanding of model architectures and much expertise to come up with optimal sharding hints. To address this problem, we propose to integrate PyTorch/XLA SPMD with XLA's auto sharding service that allows the XLA compiler to shard and optimize the whole model without any user input.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yeounoh Chung</dc:creator><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/pytorchxla-auto-sharding.html</guid><category>PyTorch Conference 2024</category><category>Lightning Talk</category></item></channel></rss>