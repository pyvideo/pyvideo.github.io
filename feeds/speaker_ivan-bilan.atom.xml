<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_ivan-bilan.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-07-07T00:00:00+00:00</updated><entry><title>Understanding and Applying Self-Attention for NLP</title><link href="https://pyvideo.org/pydata-berlin-2018/understanding-and-applying-self-attention-for-nlp.html" rel="alternate"></link><published>2018-07-07T00:00:00+00:00</published><updated>2018-07-07T00:00:00+00:00</updated><author><name>Ivan Bilan</name></author><id>tag:pyvideo.org,2018-07-07:pydata-berlin-2018/understanding-and-applying-self-attention-for-nlp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Understanding attention mechanisms and self-attention, presented in
Google's &amp;quot;Attention is all you need&amp;quot; paper, is a beneficial skill for
anyone who works on complex NLP problems. In this talk, we will go over
the main parts of the Google Transformer self-attention model and the
intuition behind it. Then we will look on how this architecture can be
used for other NLP tasks, i.e. slot filling.&lt;/p&gt;
</summary></entry></feed>