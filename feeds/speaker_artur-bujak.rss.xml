<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Fri, 13 Dec 2019 00:00:00 +0000</lastBuildDate><item><title>How we personalized onet.pl with multi-armed bandits</title><link>https://pyvideo.org/pydata-warsaw-2019/how-we-personalized-onetpl-with-multi-armed-bandits.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Imagine you need to choose ten articles out of hundreds in a way that
maximizes your profit. It's not as easy as it seems. In this talk, we
will explain how we prepare recommendations on the onet.pl home page for
millions of users with the use of a multi-armed bandit algorithm.&lt;/p&gt;
&lt;p&gt;Multi-armed bandits are a powerful solution for a diversity of
optimization problems that demand a balance between using existing
knowledge about item performance and acquiring new one. That's why we
would like to focus on the intuition behind the multi-armed bandit
approach and its application in recommender systems on the example of
onet.pl home page. Also, we will introduce E-greedy, UCB and Thompson
Sampling bandits, discuss their pros and cons and show how to tune them
in a simulated environment.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Artur Bujak</dc:creator><pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-12-13:pydata-warsaw-2019/how-we-personalized-onetpl-with-multi-armed-bandits.html</guid></item></channel></rss>