<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Howard Huang</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_howard-huang.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-09-18T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Introduction to Torch.Distributed.Pipelining</title><link href="https://pyvideo.org/pytorch-conference-2024/introduction-to-torchdistributedpipelining.html" rel="alternate"></link><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><author><name>Howard Huang</name></author><id>tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/introduction-to-torchdistributedpipelining.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pipeline parallelism is a technique employed in distributed deep learning that enhances model execution by dividing the model into distinct segments, or &amp;quot;stages.&amp;quot; As large language models and other memory-intensive models become more common, pipeline parallelism has grown increasingly important for several key areas: - Executing large-scale training jobs. - Enhancing â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pipeline parallelism is a technique employed in distributed deep learning that enhances model execution by dividing the model into distinct segments, or &amp;quot;stages.&amp;quot; As large language models and other memory-intensive models become more common, pipeline parallelism has grown increasingly important for several key areas: - Executing large-scale training jobs. - Enhancing performance in bandwidth-limited clusters. - Supporting large model inference. In this talk, we will introduce the &lt;cite&gt;torch.distributed.pipelining&lt;/cite&gt; package which provides users a seamless way of applying pipeline parallelism. We will demonstrate the following features: - Splitting of model code based on simple specification. - Support for pipeline schedules, including GPipe, 1F1B, Interleaved 1F1B and Looped BFS, and providing the infrastructure for writing customized schedules. - Composability with other PyTorch parallel techniques such as data parallel (DDP, FSDP) or tensor parallel. - Out of the box integration with Hugging Face models for efficient inference.&lt;/p&gt;
</content><category term="PyTorch Conference 2024"></category><category term="Lightning Talk"></category></entry></feed>