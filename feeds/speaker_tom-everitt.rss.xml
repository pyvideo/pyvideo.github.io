<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Tom Everitt</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 31 Jul 2023 00:00:00 +0000</lastBuildDate><item><title>Towards Causal Foundations of Safe AI</title><link>https://pyvideo.org/uai-2023/towards-causal-foundations-of-safe-ai.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Towards Causal Foundations of Safe AI&amp;quot;
James Fox, Tom Everitt&lt;/p&gt;
&lt;p&gt;With great power comes great responsibility. Artificial intelligence (AI) is rapidly gaining new capabilities, and is increasingly trusted to make decisions impacting humans in significant ways (from self-driving cars to stock-trading to hiring decisions). To ensure that AI behaves in ethical and robustly beneficial ways, we must identify potential pitfalls and develop effective mitigation strategies. In this tutorial, we will explain how (Pearlian) causality offers a useful formal framework for reasoning about AI risk and describe recent work on this topic. In particular, weâ€™ll cover: causal models of agents and how to discover them; causal definitions of fairness, intent, harm, and incentives; and risks from AI such as misgeneralization and preference manipulation, as well as how mitigation techniques including impact measures, interpretability, and path-specific objectives can help address them.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">James Fox</dc:creator><pubDate>Mon, 31 Jul 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-07-31:/uai-2023/towards-causal-foundations-of-safe-ai.html</guid><category>UAI 2023</category><category>tutorial</category></item></channel></rss>