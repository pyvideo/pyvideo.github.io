<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 06 Nov 2019 00:00:00 +0000</lastBuildDate><item><title>Advanced Software Testing for Data Scientists</title><link>https://pyvideo.org/pydata-new-york-city-2019/advanced-software-testing-for-data-scientists.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The journey to deploy a model to production starts with testing it rigorously, including its code implementation. In this tutorial, you will learn about state of the art software testing approach. You will learn how to write unit tests with enhanced diagnostics, leverage validation tools from numpy, pandas, scikit-learn, apply test doubles and generate test cases using property-based testing.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raoul-Gabriel Urma</dc:creator><pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-11-06:pydata-new-york-city-2019/advanced-software-testing-for-data-scientists.html</guid><category>tutorial</category></item><item><title>Adv. Software Testing for Data Scientists</title><link>https://pyvideo.org/pydata-london-2019/adv-software-testing-for-data-scientists.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The journey to deploy a model to production starts with testing it rigorously, including its code implementation. In this tutorial, you will learn about state of the art software testing approach. You will learn how to write unit tests with enhanced diagnostics, leverage validation tools from numpy, pandas, scikit-learn, apply test doubles and generate test cases using property-based testing.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raoul-Gabriel Urma</dc:creator><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-13:pydata-london-2019/adv-software-testing-for-data-scientists.html</guid></item><item><title>Introduction to Big Data Processing using Spark and Python</title><link>https://pyvideo.org/pydata-new-york-city-2018/introduction-to-big-data-processing-using-spark-and-python.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raoul-Gabriel Urma</dc:creator><pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-08-17:pydata-new-york-city-2018/introduction-to-big-data-processing-using-spark-and-python.html</guid></item><item><title>Making Sense of Big Data File Formats: Avro and Parquet</title><link>https://pyvideo.org/pycon-uk-2017/making-sense-of-big-data-file-formats-avro-and-parquet.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Modern applications generate and manipulate a lot of data. The growth rate of the data is staggering. Unfortunately, large datasets can be expensive to store at large scale and also slow to process. In fact, memory speed has been evolving at a much lower rate in comparison to CPUs. Thankfully, there are various file formats suited for big data systems to help. In this talk, you will learn about two popular file formats suitable for big data systems: Avro and Parquet. Through live coded examples in Python, you will learn the good, the bad, the ugly, and how you can make use of Avro and Parquet in practice.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raoul-Gabriel Urma</dc:creator><pubDate>Sun, 29 Oct 2017 12:30:00 +0100</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-29:pycon-uk-2017/making-sense-of-big-data-file-formats-avro-and-parquet.html</guid><category>avro</category><category>parquet</category></item><item><title>Interactively Analyse 100GB of Data using Spark, Amazon EMR and Zeppelin</title><link>https://pyvideo.org/pydata-london-2017/interactively-analyse-100gb-of-data-using-spark-amazon-emr-and-zeppelin.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
In this highly interactive session, you will learn how to leverage Apache Spark, Amazon Elastic Map Reduce (EMR) and Apache Zeppelin to rapidly mine a large real-world data set. You will learn how to apply common Spark patterns to extract insights as well as learn useful performance and monitoring tips.&lt;/p&gt;
&lt;p&gt;Abstract
You may have been hearing a lot of buzz around Big Data, Apache Spark, Amazon Elastic Map Reduce (EMR) and Apache Zeppelin. Whatâ€™s the fuss about, and how can you benefit from these state of the art technologies?&lt;/p&gt;
&lt;p&gt;In this highly interactive session, you will learn how to leverage Spark to rapidly mine a large real-world data set. We will conduct the analysis live entirely using an iPython Notebook to show you how easy it can be to get to grips with these technologies.&lt;/p&gt;
&lt;p&gt;In the first part of the session, we will use a sample of data from the Open Library dataset, and you will learn how to apply common Spark patterns to extract insights and aggregate data. In the second part of the session, you will see how to leverage Spark on Amazon EMR to scale your data processing queries over a cluster of machines and interactively analyse a large data set (100GB) with a Zeppelin Notebook. Along the way you will learn gotchas as well as useful performance and monitoring tips&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raoul-Gabriel Urma</dc:creator><pubDate>Sun, 07 May 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-05-07:pydata-london-2017/interactively-analyse-100gb-of-data-using-spark-amazon-emr-and-zeppelin.html</guid></item></channel></rss>