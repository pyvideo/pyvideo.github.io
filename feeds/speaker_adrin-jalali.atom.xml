<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_adrin-jalali.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-10-10T00:00:00+00:00</updated><entry><title>Current affairs, updates, and the roadmap of scikit-learn and...</title><link href="https://pyvideo.org/pydata-berlin-2019/current-affairs-updates-and-the-roadmap-of-scikit-learn-and.html" rel="alternate"></link><published>2019-10-10T00:00:00+00:00</published><updated>2019-10-10T00:00:00+00:00</updated><author><name>Adrin Jalali</name></author><id>tag:pyvideo.org,2019-10-10:pydata-berlin-2019/current-affairs-updates-and-the-roadmap-of-scikit-learn-and.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker: Adrin Jalali&lt;/p&gt;
&lt;p&gt;Track:PyData
As a scikit-learn core developer, I'd give an update on recent changes, current affairs, and the roadmap of the package and the community packages included in scikit-learn-contrib. I'd also briefly talk about how new method proposals are evaluated.&lt;/p&gt;
&lt;p&gt;Recorded at the PyConDE &amp;amp; PyData Berlin 2019 conference.
&lt;a class="reference external" href="https://pycon.de"&gt;https://pycon.de&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More details at the conference page: &lt;a class="reference external" href="https://de.pycon.org/program/X7FSX9"&gt;https://de.pycon.org/program/X7FSX9&lt;/a&gt;
Twitter:  &lt;a class="reference external" href="https://twitter.com/pydataberlin"&gt;https://twitter.com/pydataberlin&lt;/a&gt;
Twitter:  &lt;a class="reference external" href="https://twitter.com/pyconde"&gt;https://twitter.com/pyconde&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>The path between developing and serving machine learning models.</title><link href="https://pyvideo.org/pydata-berlin-2017/the-path-between-developing-and-serving-machine-learning-models.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Adrin Jalali</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/the-path-between-developing-and-serving-machine-learning-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As a data scientist, one of the challenges after you develop and train your model, is to deploy it in production where other systems would use the output of the model in real time. In this tutorial we use PipelineIO, to deploy a cluster on the cloud, which gives us a JupyterHub to develop our method, and uses PMML to persist and deploy and serve the model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whenever you have a machine learning module in your pipeline, persisting and serving the model is not yet a trivial task. This tutorial shows how an open source framework using several open source technologies could potentially solve the problem.&lt;/p&gt;
&lt;p&gt;My journey started with this[1] question on StackOverflow. I wanted to be able to do my usual data science stuff, mostly in python, and then deploy them somewhere serving like a REST API, responding to requests in real-time, using the output of the trained models. My original line of thought was this workflow:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;train the model in python or pyspark or in scala in apache spark.&lt;/li&gt;
&lt;li&gt;get the model, put it in an apache flink stream and serve.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was the point at which I had been reading and watching tutorials and attending meetups related to these technologies. I was looking for a solution which is better than:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;train models in python&lt;/li&gt;
&lt;li&gt;write a web-service using flask, put it behind a apache2 server, and put a bunch of them behind a load balancer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This just sounded wrong, or at its best, not scalable. After a bit of research, I came across PipelineIO[2,3] which seems to promise exactly what I'm looking for. In this tutorial we use PipelineIO, to deply a cluster on the cloud, which gives us a JupyterHub to develop our method, and uses PMML to persist and deploy and serve the model. My own jurney and take from PipelineIO are documented github[4]. I'll use Amazon AWS, but PipelineIO uses Kubernetes and you can easily deploy in any environment in which you can use Kubernetes.&lt;/p&gt;
&lt;p&gt;If you work in an environment in which you have different machine learning modules, which should be used in production in real time and as a part of a stream processing pipeline, this talk is for you.&lt;/p&gt;
&lt;p&gt;[1] &lt;a class="reference external" href="http://stackoverflow.com/questions/42719953/how-to-develop-a-rest-api-using-an-ml-model-trained-on-apache-spark"&gt;http://stackoverflow.com/questions/42719953/how-to-develop-a-rest-api-using-an-ml-model-trained-on-apache-spark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a class="reference external" href="http://pipeline.io"&gt;http://pipeline.io&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a class="reference external" href="https://github.com/fluxcapacitor/pipeline"&gt;https://github.com/fluxcapacitor/pipeline&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a class="reference external" href="https://github.com/adrinjalali/pipeline-docs"&gt;https://github.com/adrinjalali/pipeline-docs&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>