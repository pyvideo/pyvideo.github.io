<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Kristian Kersting</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_kristian-kersting.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-07-15T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Probabilistic Circuits That Know What They Don't Know</title><link href="https://pyvideo.org/uai-2023/probabilistic-circuits-that-know-what-they-dont-know.html" rel="alternate"></link><published>2023-07-31T00:00:00+00:00</published><updated>2023-07-31T00:00:00+00:00</updated><author><name>Fabrizio Ventola</name></author><id>tag:pyvideo.org,2023-07-31:/uai-2023/probabilistic-circuits-that-know-what-they-dont-know.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Probabilistic Circuits That Know What They Don't Know&amp;quot;
Fabrizio Ventola, Steven Braun, Zhongjie Yu, Martin Mundt, Kristian Kersting
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/ventola23a.html"&gt;https://proceedings.mlr.press/v216/ventola23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
Probabilistic circuits (PCs) are models that allow exact and tractable probabilistic inference. In contrast to neural networks, they are often assumed to be …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Probabilistic Circuits That Know What They Don't Know&amp;quot;
Fabrizio Ventola, Steven Braun, Zhongjie Yu, Martin Mundt, Kristian Kersting
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/ventola23a.html"&gt;https://proceedings.mlr.press/v216/ventola23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
Probabilistic circuits (PCs) are models that allow exact and tractable probabilistic inference. In contrast to neural networks, they are often assumed to be well-calibrated and robust to out-of-distribution (OOD) data. In this paper, we show that PCs are in fact not robust to OOD data, i.e., they don’t know what they don’t know. We then show how this challenge can be overcome by model uncertainty quantification. To this end, we propose tractable dropout inference (TDI), an inference procedure to estimate uncertainty by deriving an analytical solution to Monte Carlo dropout (MCD) through variance propagation. Unlike MCD in neural networks, which comes at the cost of multiple network evaluations, TDI provides tractable sampling-free uncertainty estimates in a single forward pass. TDI improves the robustness of PCs to distribution shift and OOD data, demonstrated through a series of experiments evaluating the classification confidence and uncertainty estimates on real-world data.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://www.auai.org/uai2023/oral_slides/HANDOUT_118.pdf"&gt;https://www.auai.org/uai2023/oral_slides/HANDOUT_118.pdf&lt;/a&gt;&lt;/p&gt;
</content><category term="UAI 2023"></category></entry><entry><title>UAI 2023 Oral Session 6: Probabilistic Flow Circuits: Towards Deep Models for Tractable Inference</title><link href="https://pyvideo.org/uai-2023/uai-2023-oral-session-6-probabilistic-flow-circuits-towards-deep-models-for-tractable-inference.html" rel="alternate"></link><published>2023-07-31T00:00:00+00:00</published><updated>2023-07-31T00:00:00+00:00</updated><author><name>Sahil Sidheekh</name></author><id>tag:pyvideo.org,2023-07-31:/uai-2023/uai-2023-oral-session-6-probabilistic-flow-circuits-towards-deep-models-for-tractable-inference.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Probabilistic Flow Circuits: Towards Unified Deep Models for Tractable Probabilistic Inference&amp;quot;
Sahil Sidheekh, Kristian Kersting, Sriraam Natarajan
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/sidheekh23a.html"&gt;https://proceedings.mlr.press/v216/sidheekh23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
We consider the problem of increasing the expressivity of probabilistic circuits by augmenting them with the successful generative models of normalizing flows. To this …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Probabilistic Flow Circuits: Towards Unified Deep Models for Tractable Probabilistic Inference&amp;quot;
Sahil Sidheekh, Kristian Kersting, Sriraam Natarajan
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/sidheekh23a.html"&gt;https://proceedings.mlr.press/v216/sidheekh23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
We consider the problem of increasing the expressivity of probabilistic circuits by augmenting them with the successful generative models of normalizing flows. To this effect, we theoretically establish the requirement of decomposability for such combinations to retain tractability of the learned models. Our model, called Probabilistic Flow Circuits, essentially extends circuits by allowing for normalizing flows at the leaves. Our empirical evaluation clearly establishes the expressivity and tractability of this new class of probabilistic circuits.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://www.auai.org/uai2023/oral_slides/526-oral-slides.pdf"&gt;https://www.auai.org/uai2023/oral_slides/526-oral-slides.pdf&lt;/a&gt;&lt;/p&gt;
</content><category term="UAI 2023"></category></entry><entry><title>UAI 2024 Oral Session 1: Deep Learning</title><link href="https://pyvideo.org/uai-2024/uai-2024-oral-session-1-deep-learning.html" rel="alternate"></link><published>2024-07-15T00:00:00+00:00</published><updated>2024-07-15T00:00:00+00:00</updated><author><name>Mengjing Wu</name></author><id>tag:pyvideo.org,2024-07-15:/uai-2024/uai-2024-oral-session-1-deep-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Functional Wasserstein Bridge Inference for Bayesian Deep Learning
Mengjing Wu, Junyu Xuan, Jie Lu
&lt;a class="reference external" href="https://openreview.net/pdf?id=Wnht2IqzlN"&gt;https://openreview.net/pdf?id=Wnht2IqzlN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Reflected Schrödinger Bridge for Constrained Generative Modeling
Wei Deng, Yu Chen, Nicole Tianjiao Yang, Hengrong Du, Qi Feng, Ricky T. Q. Chen
&lt;a class="reference external" href="https://openreview.net/pdf?id=8Wl7xRXUHK"&gt;https://openreview.net/pdf?id=8Wl7xRXUHK&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pix2Code …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Functional Wasserstein Bridge Inference for Bayesian Deep Learning
Mengjing Wu, Junyu Xuan, Jie Lu
&lt;a class="reference external" href="https://openreview.net/pdf?id=Wnht2IqzlN"&gt;https://openreview.net/pdf?id=Wnht2IqzlN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Reflected Schrödinger Bridge for Constrained Generative Modeling
Wei Deng, Yu Chen, Nicole Tianjiao Yang, Hengrong Du, Qi Feng, Ricky T. Q. Chen
&lt;a class="reference external" href="https://openreview.net/pdf?id=8Wl7xRXUHK"&gt;https://openreview.net/pdf?id=8Wl7xRXUHK&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pix2Code: Learning to Compose Neural Visual Concepts as Programs
Antonia Wüst, Wolfgang Stammer, Quentin Delfosse, Devendra Singh Dhami, Kristian Kersting
&lt;a class="reference external" href="https://openreview.net/pdf?id=EE4ikEQnOT"&gt;https://openreview.net/pdf?id=EE4ikEQnOT&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Normalizing Flows for Conformal Regression
Nicolò Colombo
&lt;a class="reference external" href="https://openreview.net/pdf?id=acgwLdoB3d"&gt;https://openreview.net/pdf?id=acgwLdoB3d&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Understanding Pathologies of Deep Heteroskedastic Regression
Eliot Wong-Toi, Alex James Boyd, Vincent Fortuin, Stephan Mandt
&lt;a class="reference external" href="https://openreview.net/pdf?id=n5faLvrsA0"&gt;https://openreview.net/pdf?id=n5faLvrsA0&lt;/a&gt;&lt;/p&gt;
</content><category term="UAI 2024"></category></entry></feed>