<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_delia-rusu.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-06-01T00:00:00+00:00</updated><entry><title>Estimating stock price correlations using Wikipedia</title><link href="https://pyvideo.org/pydata-berlin-2016/estimating-stock-price-correlations-using-wikipedia.html" rel="alternate"></link><published>2016-06-01T00:00:00+00:00</published><updated>2016-06-01T00:00:00+00:00</updated><author><name>Delia Rusu</name></author><id>tag:pyvideo.org,2016-06-01:pydata-berlin-2016/estimating-stock-price-correlations-using-wikipedia.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Building an equities portfolio is a challenging task for a finance professional as it requires, among others, future correlations between stock prices. As this data is not always available, in this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures based on Wikipedia data.&lt;/p&gt;
&lt;p&gt;According to Modern Portfolio Theory, assembling a portfolio involves forming expectations about the individual stock's future risk and return as well as future correlations between stock prices. These future correlations are typically estimated using historical stock price data. However, there are situations where this type of data is not available, such as the time preceding an IPO.&lt;/p&gt;
&lt;p&gt;In this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures in order to estimate the correlation between stock prices.&lt;/p&gt;
&lt;p&gt;The focus of the analysis will be on companies listed on the Frankfurt Stock Exchange which form the DAX. I am going to use Wikipedia articles in order to derive the textual description for each company. Additionally, I will use the Wikipedia category structure to derive a graph describing relations between companies.&lt;/p&gt;
&lt;p&gt;The analysis will be performed using the scikit-learn and networkX libraries and example code will be available to the audience.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/deliarusu/wikipedia-correlation"&gt;https://github.com/deliarusu/wikipedia-correlation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://speakerdeck.com/deliarusu/estimating-stock-price-correlations-using-wikipedia"&gt;https://speakerdeck.com/deliarusu/estimating-stock-price-correlations-using-wikipedia&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Detecting novel anomalies in Twitter</title><link href="https://pyvideo.org/pydata-london-2016/delia-rusu-mattia-boni-sforza-detecting-novel-anomalies-in-twitter.html" rel="alternate"></link><published>2016-05-16T00:00:00+00:00</published><updated>2016-05-16T00:00:00+00:00</updated><author><name>Delia Rusu</name></author><id>tag:pyvideo.org,2016-05-16:pydata-london-2016/delia-rusu-mattia-boni-sforza-detecting-novel-anomalies-in-twitter.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In recent years Twitter has gained importance as a datasource for finance professionals alongside news. However, several challenges appear when identifying relevant information and finding the latest unexpected developments. In this talk we are going to present our experience developing an anomaly detection and alerting system and analyse a particular anomalous scenario.&lt;/p&gt;
&lt;p&gt;We are overwhelmed by the amount of information available via news, blogs, social media, etc. and there is growing demand for automatic or semi-automatic systems that filter information and surface the most relevant, interesting, novel pieces to the users. Finance professionals face multiple challenges as the volume of data they are analysing increases and at the same time the data sources get more diverse. In order to address the information overload problem both established financial news providers and social media services present condensed information: in the form of news headlines (e.g. Bloomberg and Reuters) or by imposing limits on the text length (a Twitter message is formed of maximum 140 characters). On the other hand, in recent years, an increasing number of content filtering and alerting systems have been developed.&lt;/p&gt;
&lt;p&gt;Knowsis is a fintech startup focusing on social media analytics. One of the systems that we developed - Anomaly Detection and Alerting - is aimed at informing the user whenever it identifies an unexpected and novel tweet. Anomaly detection is driven by social volume, i.e. not all tweets which deviate from what is expected are anomalies, but a critical number of tweets is required. In addition, we do not want to alert the user about old information.&lt;/p&gt;
&lt;p&gt;The talk will present the approach that we took in building the system, highlighting issues that we encountered along the way and how we tackled them. In the second part of the talk we are going to show how one can build a similar anomaly detection system from scratch and use it to detect a particular anomalous scenario. We are going to use Poisson models to identify anomalies and Locality-Sensitive Hashing for novelty detection. The code will be written in Python using the scikit-learn and statsmodels libraries and made available to the audience.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://nbviewer.jupyter.org/format/slides/github/knowsis/novel-twitter-anomalies-pydatalondon2016/blob/master/pydata_presentation.ipynb#/"&gt;http://nbviewer.jupyter.org/format/slides/github/knowsis/novel-twitter-anomalies-pydatalondon2016/blob/master/pydata_presentation.ipynb#/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/knowsis/novel-twitter-anomalies-pydatalondon2016"&gt;https://github.com/knowsis/novel-twitter-anomalies-pydatalondon2016&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Estimating stock price correlations using Wikipedia</title><link href="https://pyvideo.org/pydata-london-2016/delia-rusu-estimating-stock-price-correlations-using-wikipedia.html" rel="alternate"></link><published>2016-05-08T00:00:00+00:00</published><updated>2016-05-08T00:00:00+00:00</updated><author><name>Delia Rusu</name></author><id>tag:pyvideo.org,2016-05-08:pydata-london-2016/delia-rusu-estimating-stock-price-correlations-using-wikipedia.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Building an equities portfolio is a challenging task for a finance professional as it requires, among others, future correlations between stock prices. As this data is not always available, in this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures based on Wikipedia data.&lt;/p&gt;
&lt;p&gt;According to Modern Portfolio Theory, assembling a portfolio involves forming expectations about the individual stock's future risk and return as well as future correlations between stock prices. These future correlations are typically estimated using historical stock price data. However, there are situations where this type of data is not available, such as the time preceding an IPO.&lt;/p&gt;
&lt;p&gt;In this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures in order to estimate the correlation between stock prices.&lt;/p&gt;
&lt;p&gt;The focus of the analysis will be on companies listed on the London Stock Exchange which form the FTSE 100 Index. I am going to use Wikipedia articles in order to derive the textual description for each company. Additionally, I will use the Wikipedia category structure to derive a graph describing relations between companies.&lt;/p&gt;
&lt;p&gt;The analysis will be performed using the scikit-learn and networkX libraries and example code will be available to the audience.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a class="reference external" href="https://github.com/deliarusu/wikipedia-correlation"&gt;https://github.com/deliarusu/wikipedia-correlation&lt;/a&gt;
&lt;a class="reference external" href="https://github.com/idio/wiki2vec"&gt;https://github.com/idio/wiki2vec&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>