<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - David Nicholson</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 10 Jul 2023 00:00:00 +0000</lastBuildDate><item><title>Neural Networks for Segmentation of Vocalizations</title><link>https://pyvideo.org/pydata-new-york-city-2017/neural-networks-for-segmentation-of-vocalizations.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Neural networks for speech-to-text avoid dividing speech into segments, such as syllables, but segmenting has important applications. We compare different neural networks for segmentation of vocalizations using the song of songbirds, which we study as neuroscientists. Initial results suggest a bidirectional LSTM-CNN architecture outperforms others in both segmentation and classification.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Nicholson</dc:creator><pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-11-27:/pydata-new-york-city-2017/neural-networks-for-segmentation-of-vocalizations.html</guid><category>PyData New York City 2017</category></item><item><title>Hybrid-Vocal-Classifier (HVC): a Python Package to Automate Labeling of Birdsong for Behavioral Experiments</title><link>https://pyvideo.org/scipy-2017/hybrid-vocal-classifier-hvc-a-python-package-to-automate-labeling-of-birdsong-for-behavioral-experiments.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Neuroscientists study songbirds to understand how the brain learns and produces motor skills. Like babies learning to talk, songbirds acquire their song socially during a critical period in development. To understand the neural basis of birdsong, neuroscientists carry out behavioral experiments. Often analysis requires labeling the elements of song (known as &amp;quot;syllables&amp;quot;) by hand, which consumes many person-hours. Several methods have been proposed to automate labeling syllables, but little work has been done to compare them. Hybrid-vocal-classifier (HVC) is a Python package for comparing methods and automating labeling. It is tested on a large collection of hand-labeled song, now made publicly available. Users can configure HVC with human-readable YAML files. HVC loads data collected with different programs by making use of the SciPy/Numpy stack. It includes previously proposed algorithms, implemented in scikit-learn, as well as artificial neural network models built in Keras. Initial results obtained with HVC suggest convolutional neural networks yield higher accuracy predictions with less training data than the best models previously proposed.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Nicholson</dc:creator><pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-14:/scipy-2017/hybrid-vocal-classifier-hvc-a-python-package-to-automate-labeling-of-birdsong-for-behavioral-experiments.html</guid><category>SciPy 2017</category></item><item><title>Building &amp; Replicating Models of Visual Search Behavior w/ Tensorflow, Nengo, &amp; Scientific Python Stack</title><link>https://pyvideo.org/scipy-2019/building-replicating-models-of-visual-search-behavior-w-tensorflow-nengo-scientific-python-stack.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Animals constantly use their eyes to search their environment. What can neural networks and other cognitive models tell us about this behavior? We present two related studies that leverage scientific Python libraries to address this question. The first uses Tensorflow to replicate and extend a previous study of how convolutional neural networks perform a classic visual search task. The second study compares visual search behavior of two types of models: a recurrent neural network model from Google DeepMind, and spiking cognitive models built with the Nengo neural simulator. We discuss what our results suggest about such models.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Nicholson</dc:creator><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-13:/scipy-2019/building-replicating-models-of-visual-search-behavior-w-tensorflow-nengo-scientific-python-stack.html</guid><category>SciPy 2019</category></item><item><title>vak: neural network models for acoustic behavior</title><link>https://pyvideo.org/scipy-2023/vak-neural-network-models-for-acoustic-behavior.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Research on animal acoustic communication is being revolutionized by deep learning. In this talk we present vak, a framework that allows researchers in this area to easily benchmark deep neural network models and apply them to their own data. We'll demonstrate how research groups are using vak through examples with TweetyNet, a model that automates annotation of birdsong by segmenting spectrograms. Then we'll show how adopting Lightning as a backend in version 1.0 has allowed us to incorporate more models and features, building on the foundation we put in place with help from the scientific Python stack.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Nicholson</dc:creator><pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-07-10:/scipy-2023/vak-neural-network-models-for-acoustic-behavior.html</guid><category>SciPy 2023</category></item></channel></rss>