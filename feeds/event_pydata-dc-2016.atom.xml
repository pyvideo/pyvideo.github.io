<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-dc-2016.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-10-09T00:00:00+00:00</updated><entry><title>Becoming a Data Scientist Advice From My Podcast Guests</title><link href="https://pyvideo.org/pydata-dc-2016/becoming-a-data-scientist-advice-from-my-podcast-guests.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Renee Teate</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/becoming-a-data-scientist-advice-from-my-podcast-guests.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Overwhelmed by the vast resources (of varying quality) available online for learning data science? In this talk, I compile resources from data scientists on twitter, advice from guests of my podcast, and some of my own experience to help get you started on the path to Becoming a Data Scientist.&lt;/p&gt;
&lt;p&gt;The options for learning data science online are vast and overwhelming, but it is possible to find great resources that work well for you and learn data science without going back to school if you know how to approach it.&lt;/p&gt;
&lt;p&gt;On my &amp;quot;Becoming a Data Scientist&amp;quot; podcast, I have interviewed 17 data scientists (or those on the way to becoming data scientists) about their career paths and how they learned data science. I also interact with hundreds of data scientists regularly on Twitter. In this talk, I compile the frequent advice and the best resources, and give my answers to some common questions about how to become a data scientist.&lt;/p&gt;
</summary><category term="Data"></category></entry><entry><title>Bot or Not? The Illusion of Intelligence</title><link href="https://pyvideo.org/pydata-dc-2016/bot-or-not-the-illusion-of-intelligence.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Bobby Filar</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/bot-or-not-the-illusion-of-intelligence.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Chatbots have become wildly popular interfaces for online services we use everyday. While these bots are not going to pass the Turing Test, they provide a platform to use natural language to perform a task. This talk covers the basics of chatbot development using Python NLP and Deep Learning libraries and provides a demonstration of Malbot, a simple bot that can converse about a piece of malware.&lt;/p&gt;
&lt;p&gt;Chatbots are programs that employ rules or artificial intelligence that users interact with via a dialogue interface. Chatbots are extremely useful tools for accomplishing a range of tasks. From ordering pizza to checking the weather to buying shoes, there's a chatbot for that. This bot revolution has led to the creation of several Make-A-Bot services (all with really cool .ai domains!), but these bots are not extremely powerful examples of artificial intelligence. Most are simple template-based implementations that can be developed independently in Python.&lt;/p&gt;
&lt;p&gt;This talk will provide an overview on the structure of chatbots and how tools such as SpaCy, NLTK, sklearn, and Keras can be used to develop bots ranging from a basic weather bot to a more sophisticated deep learning neural conversation model. The applicability of these tools will be demonstrated through MalBot, a chatbot capable of ingesting a piece of malware and responding to natural language inquiries into the malicious software’s capabilities. (e.g. &amp;quot;Do you record key strokes?&amp;quot;, &amp;quot;What malware family are you from?&amp;quot;)&lt;/p&gt;
</summary></entry><entry><title>Closing Session</title><link href="https://pyvideo.org/pydata-dc-2016/closing-session.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/closing-session.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016 Closing Notes&lt;/p&gt;
</summary></entry><entry><title>Dask for ad hoc distributed computing</title><link href="https://pyvideo.org/pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk discusses parallel and distributed computing in Python, particularly for ad-hoc and custom algorithms. It focuses on Dask, a Python solution for flexible distributed computing.&lt;/p&gt;
&lt;p&gt;The Python data science stack contains efficient algorithms with intuitive interfaces for sophisticated and friendly analysis. As the data science community tackles larger problems with larger hardware we naturally ask how best to parallelize this software stack both across many cores in a single computer and across computers in a cluster. This turns out to be harder than it looks, even with traditional Big Data tools like MapReduce, Storm, and Spark. Both the complexity of the algorithms and the high expectations for interactivity raise challenges for these systems. This talk lays out the benefits and challenges of parallelizing a numeric analytic stack, and then describes Dask, a parallel framework gaining traction within the Python community for interactive performant parallel computing, and finally goes through a few domains where this work is enabling novel science today.&lt;/p&gt;
</summary><category term="dask"></category><category term="distributed"></category></entry><entry><title>Data Transformation: A Framework for Exploratory Data Analysis</title><link href="https://pyvideo.org/pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Tony Ojeda</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Exploratory data analysis plays a critical role in the job of every data scientist, but very few have a structured process or framework for exploring data quickly and efficiently. This talk will introduce the exploratory framework I use in my day-to-day work and will walk attendees through a practical example of how to use the framework to unlock hidden insights with the help of Python libraries.&lt;/p&gt;
&lt;p&gt;At the heart of data analysis, there lies a need to understand the real world entities being represented in the data. Every data set we encounter is an attempt to capture a slice of our complex world and communicate some information about it in a way that has potential to be informative to humans, machines, or both. Moving from basic analyses to advanced analytics requires the ability to imagine multiple ways of conceptualizing the composition of entities and the relationships present in our data. It also requires the realization that different levels of aggregation, disaggregation, and transformation can open up new pathways to understanding our data and identifying the valuable insights it contains.&lt;/p&gt;
&lt;p&gt;In this talk, we’ll discuss several ways to think about the composition and representation of our data. We’ll also demonstrate a series of methods that leverage tools like networks, hierarchical aggregations, and unsupervised clustering to visually explore our data, transform it to discover new insights, help frame analytical problems and questions, and even improve machine learning model performance. In exploring these approaches, and with the help of Python libraries such as Pandas, Scikit-Learn, Seaborn, and NetworkX, we will provide a practical framework for thinking creatively and visually about your data and unlocking latent value and insights hidden deep beneath its surface.&lt;/p&gt;
</summary><category term="analysis"></category><category term="Data"></category><category term="Data Analysis"></category><category term="framework"></category></entry><entry><title>Design Principles</title><link href="https://pyvideo.org/pydata-dc-2016/design-principles.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/design-principles.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</summary><category term="Design"></category></entry><entry><title>Dev Ops meets Data Science Taking models from prototype to production with Docker</title><link href="https://pyvideo.org/pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Andy Terrel</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We present the evolution of a model to a production API that can scale to large e-commerce needs. On the journey we discuss metrics of success and how to use the Kubernetes cluster manager and associated tools for deploy. In addition to the use of these tools we highlight how to make use of the cluster management system for further testing and experimentation with your models.&lt;/p&gt;
&lt;p&gt;The chasm between data science and dev ops is often wide and impenetrable, but the two fields have more in common than meets the eye. Every data scientist will be able to lean in and help their career by investing in a basic understanding the basic principles of dev ops. In this talk I present the notions of service level indicators, objectives, and agreements. I cover the rigorous monitoring and testing of services. Finally we demonstrate how to build a basic data science workflow and push to production level APIs with Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes is an opinionated container cluster manager with an easy to use, robust interface. It can be use on very small and very large clusters. Docker is a container system that allows one to build code in an isolated environment. Paired with a container manager such as Kubernetes we are able to manage millions of instances as needed for a production deployment. These tools are two of many different options but are considered among the best open source solutions available.&lt;/p&gt;
</summary><category term="Data"></category><category term="data science"></category><category term="docker"></category><category term="models"></category><category term="science"></category></entry><entry><title>Dynamics in Graph Analysis Adding Time as a Structure for Visual and Statistical</title><link href="https://pyvideo.org/pydata-dc-2016/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Benjamin Bengfort</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Network analyses are powerful methods for both visual analytics and machine learning but can suffer as their complexity increases. By embedding time as a structural element rather than a property, we will explore how time series and interactive analysis can be improved on Graph structures. Primarily we will look at decomposition in NLP-extracted concept graphs using NetworkX and Graph Tool.&lt;/p&gt;
&lt;p&gt;Modeling data as networks of relationships between entities can be a powerful method for both visual analytics and machine learning; people are very good at distinguishing patterns from interconnected structures, and machine learning methods get a performance improvement when applied to graph data structures. However, as these structures become more complex or embed more information over time, both visual and algorithmic methods get messy; visual analyses suffer from the &amp;quot;hairball&amp;quot; effect, and graph algorithms require either more traversal or increased computation at each vertex. A growing area to reduce this complexity and optimize analytics is the use of interactive and subgraph techniques that model how graph structures change over time.&lt;/p&gt;
&lt;p&gt;In this talk, I demonstrate two practical techniques for embedding time into graphs, not as computational properties, but rather as structural elements. The first technique is to add time as a node to the graph, which allows the graph to remain static and complete, but minimizes traversals and allows filtering. The second is to represent a single graph as multiple subgraphs where each is a snapshot at a particular time. This allows us to use time series analytics on our graphs, but perhaps more importantly, to use animation or interactive methodologies to visually explore those changes and provide meaningful dynamics.&lt;/p&gt;
</summary><category term="analysis"></category></entry><entry><title>ElasticSearch and Redis How and When to Use Them</title><link href="https://pyvideo.org/pydata-dc-2016/elasticsearch-and-redis-how-and-when-to-use-them.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Tim Marcinowski</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/elasticsearch-and-redis-how-and-when-to-use-them.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;When working with data, you have some viable options for keeping them in short and long-term storage. I will be going over why ElasticSearch and Redis are great for data storage. This talk will explain the purpose of each datastore when visualizing on Kibana or websockets.&lt;/p&gt;
</summary><category term="elasticsearch"></category><category term="redis"></category></entry><entry><title>Exposing Algorithms</title><link href="https://pyvideo.org/pydata-dc-2016/exposing-algorithms.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Jennifer Stark</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/exposing-algorithms.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Algorithms have become an integral part of our everyday lives. While algorithms can make our lives simpler and make decisions faster, there is a growing need for algorithms to be transparent and for the users of those algorithms to be accountable for the automated decisions made by them. This talk covers where algorithms are used, how they can go wrong, and how they can be investigated.&lt;/p&gt;
&lt;p&gt;An algorithm is set of steps that perform calculations, process data, or automate tasks. Algorithms are everywhere we look (and even places we don’t look) controlling what we see, do, and where we go. They’re great for solving our problems and helping us make better and quicker decisions, or taking the decision-making out of our hands. Their guidance is perfect in their objective and unbiased calculation. Except they are not, actually. Like everything else, they are created by people, and people have biases that get encoded into the algorithms they create. Algorithms learn from data, which is also created by people, so the algorithms also learn biases from data. This can be a problem when algorithms encode these biases into their calculations and go on to perpetuate the bias.&lt;/p&gt;
&lt;p&gt;In this talk you will hear why we should care about algorithmic accountability, and details on a case study on how computational journalism can be used to investigate algorithms and advocate the need for transparency and accountability.&lt;/p&gt;
</summary></entry><entry><title>GraphGen: Conducting Graph Analytics over Relational Databases</title><link href="https://pyvideo.org/pydata-dc-2016/graphgen-conducting-graph-analytics-over-relational-databases.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Konstantinos Xirogiannopoulos</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/graphgen-conducting-graph-analytics-over-relational-databases.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/graphgen-conducting-graph-analytics-over-relational-databases-67649554"&gt;http://www.slideshare.net/PyData/graphgen-conducting-graph-analytics-over-relational-databases-67649554&lt;/a&gt;
Download and learn about GraphGen at: konstantinosx.github.io/graphgen-project/
DDL Blog Post and Tutorial at: blog.districtdatalabs.com/graph-analytics-over-relational-datasets
Note: Currently GraphGenPy is built for Python 2.0.  Python 3.0 support coming soon!&lt;/p&gt;
&lt;p&gt;Applying graph analytics on data stored in relational databases can provide tremendous value in many application domains. We discuss the importance of leveraging these analyses, and the challenges in enabling them. We present a tool, called GraphGen, that allows users to visually explore, and rapidly analyze (using NetworkX) different graph structures present in their databases.&lt;/p&gt;
</summary><category term="databases"></category></entry><entry><title>H2O Deep Water with Python early sneek</title><link href="https://pyvideo.org/pydata-dc-2016/h2o-deep-water-with-python-early-sneek.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Fabrizio Milo</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/h2o-deep-water-with-python-early-sneek.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData 2016&lt;/p&gt;
&lt;p&gt;Python as a language for DeepLearning. Python is emerging as the facto language to specify Deep Learning Networks. In this talk we will explore some of the popular libraries like Tensorflow and Keras to see the semantics used to describe such networks and look a bit more under the hood at what is the python layer actually doing for these well known deep learning libraries.&lt;/p&gt;
</summary><category term="deep learning"></category><category term="tensorflow"></category><category term="keras"></category></entry><entry><title>Improving PySpark Performance Spark performance beyond the JVM</title><link href="https://pyvideo.org/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</summary><category term="jvm"></category><category term="performance"></category><category term="pyspark"></category><category term="spark"></category></entry><entry><title>Keynote: Extending from Open to Usable: A Commerce Data Conundrum</title><link href="https://pyvideo.org/pydata-dc-2016/keynote-extending-from-open-to-usable-a-commerce-data-conundrum.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Star Ying</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/keynote-extending-from-open-to-usable-a-commerce-data-conundrum.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Keynote: Extending from Open to Usable: A Commerce Data Conundrum&lt;/p&gt;
</summary><category term="Data"></category></entry><entry><title>Keynote: The Culture of Data Transformation</title><link href="https://pyvideo.org/pydata-dc-2016/keynote-the-culture-of-data-transformation.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>SriSatish Ambati</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/keynote-the-culture-of-data-transformation.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</summary><category term="Culture"></category><category term="Data"></category></entry><entry><title>Logistic Regression Behind the Scenes</title><link href="https://pyvideo.org/pydata-dc-2016/logistic-regression-behind-the-scenes.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Christopher White</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/logistic-regression-behind-the-scenes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Logistic Regression models are powerful tools in the data science toolkit; in this talk we will explore various implementations of logistic regression in Python and SAS, with a focus on output and performance. We will also discuss both the numerical and statistical implications (including Bayesian interpretations) of the various options.&lt;/p&gt;
</summary></entry><entry><title>Machine Learning Techniques for Class Imbalances &amp; Adversaries</title><link href="https://pyvideo.org/pydata-dc-2016/machine-learning-techniques-for-class-imbalances-adversaries.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Brendan Herger</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/machine-learning-techniques-for-class-imbalances-adversaries.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;There are many areas of applied Machine Learning which require models optimized for rare occurrences (i.e. class imbalance), as well as users actively attempting to subvert the system (i.e. adversaries).&lt;/p&gt;
&lt;p&gt;This talk will guide the audience through multiple published techniques which specifically attempt to address these issues.&lt;/p&gt;
&lt;p&gt;The Data Innovation Lab at Capital One has explored more advanced modeling techniques for class imbalance &amp;amp; adversarial actors. Our use case has allowed us to survey the many related fields which deal with these issues, and attempt many of the suggested modeling techniques. Additionally, we have introduce a few novel variations of our own.&lt;/p&gt;
&lt;p&gt;This talk will provide an introduction to the problem space, a brief overview of the modeling frameworks we've chosen to work with, a brief overview of our approaches, a discussion of lessons learned, and our proposed future work.&lt;/p&gt;
&lt;p&gt;The approaches discussed will include ensemble models, deep learning, genetic algorithms, outlier detection via dimensionally reduction (PCA and neural network auto-encoders), time-decay weighting, and Synthetic Minority Over-sampling Technique (SMOTE sampling).&lt;/p&gt;
</summary><category term="class"></category><category term="learning"></category><category term="machine learning"></category></entry><entry><title>Making your code faster: Cython and parallel processing in the Jupyter Notebook</title><link href="https://pyvideo.org/pydata-dc-2016/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Gustavo Patino</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/gapatino/Making-Your-Code-Faster-Cython-and-parallel-processing-in-the-Jupyter-Notebook"&gt;https://github.com/gapatino/Making-Your-Code-Faster-Cython-and-parallel-processing-in-the-Jupyter-Notebook&lt;/a&gt;
Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook"&gt;http://www.slideshare.net/PyData/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As the complexity and scope of applications grow, it is very common to run into slow performance issues. In Python, it is possible to improve the speed of execution with the use of parallel processing and the Cython compiler. The Jupyter Notebook makes the implementation of both of them a relatively simple task, which will be the focus of this session.&lt;/p&gt;
</summary><category term="code"></category><category term="Cython"></category><category term="jupyter"></category><category term="jupyter notebook"></category><category term="notebook"></category><category term="parallel"></category><category term="processing"></category></entry><entry><title>Open Data Dashboards &amp; Python Web Scraping</title><link href="https://pyvideo.org/pydata-dc-2016/open-data-dashboards-python-web-scraping.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Marie Whittaker</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/open-data-dashboards-python-web-scraping.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Distilling a world of data down to a few key indicators can be an effective way of keeping an audience informed, and this concept is at the heart of a good dashboard. This talk will cover a few methods of scraping and reshaping open data for dashboard visualization, to automate the boring stuff so you have more time and energy to focus on the analysis and content.&lt;/p&gt;
&lt;p&gt;This talk will cover a basic scenario of curating open data into visualizations for an audience. The main goal is to automate data scraping/downloading and reshaping. I use python to automate data gathering, and Tableau and D3 as visualization tools -- but the process can be applied to numerous analytical/visualization suites.&lt;/p&gt;
&lt;p&gt;I'll discuss situations where a dashboard makes sense (and when one doesn't). I will make a case also that automation makes for a more seamless data gathering and updating process, but not always for smarter data analysis.&lt;/p&gt;
&lt;p&gt;Some python packages I'll cover for web scraping and downloading/reshaping open data include: openpyxl, pandas, xlsxwriter, and BeautifulSoup. I'll also touch on APIs.&lt;/p&gt;
</summary><category term="Data"></category><category term="scraping"></category><category term="web"></category></entry><entry><title>Sustainable scrapers</title><link href="https://pyvideo.org/pydata-dc-2016/sustainable-scrapers.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>David Eads</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/sustainable-scrapers.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scraping data from the web is an essential skill, whether you want to or not. Learn the code and systems tricks that go into testable, fast, low-maintenance scraping gleaned from years of real world practice.&lt;/p&gt;
&lt;p&gt;A recent NPR project that collects structured data about gun sale listings from Armslist.com demonstrates several of the speaker's favorite tricks for writing simple, fast scrapers with Python.&lt;/p&gt;
</summary></entry><entry><title>Triaging Feedback Form Data</title><link href="https://pyvideo.org/pydata-dc-2016/triaging-feedback-form-data.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Stephanie Kim</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/triaging-feedback-form-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk will cover how to use predictive modeling on unstructured text data including feedback form, social media or chat message data to triage issues in order to prevent future problems with a service, platform or user interface using NLP techniques in Python and R.&lt;/p&gt;
&lt;p&gt;Companies gain useful insights about their users from feedback form and other unstructured text data including live chat messages. Even though they are read and responded to, often such data is ignored when thinking about larger scale trend analysis and this can result in missed insight about how users react to a product or service. Sometimes analysis is being done by looking at changes in user sentiment or other heuristics, however it could be taken a step further by applying predictive modeling in attempt to recognize areas that need more attention and support. While you can use predictive modeling on network and log data, that is looking at how the hardware is handling your users requests, not how it's being perceived by users. By predicting areas where users are having difficulty whether it's with the UI or with the platform's response time you can triage these areas of concern to prevent future cases of negative perception. This talk will cover how to utilize common NLP tools used to gather and process the features in Python then will use R to perform trend analysis and predictive modeling then use the results to triage what areas should be focused on in the future.&lt;/p&gt;
</summary><category term="Data"></category></entry><entry><title>Visual diagnostics for more informed machine learning</title><link href="https://pyvideo.org/pydata-dc-2016/visual-diagnostics-for-more-informed-machine-learning.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Rebecca Bilbro</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/visual-diagnostics-for-more-informed-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Visualization has a critical role to play throughout the analytic process. Where static outputs and tabular data can obscure patterns, human visual analysis can open up insights that lead to more robust data products. For Python programmers who dabble in machine learning, visual diagnostics are a must-have for effective feature analysis, model selection, and parameter tuning.&lt;/p&gt;
</summary><category term="learning"></category><category term="machine learning"></category></entry><entry><title>You got your engineering in my Data Science: Addressing the reproducibility crisis</title><link href="https://pyvideo.org/pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Jon Bodner</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering"&gt;http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. Unfortunately, multiple recent studies have been found to be unreliable and non-reproducible. Adopting techniques from software engineering might help mitigate some of these problems.&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. It makes sense of everything from cancer trials to package delivery logistics. But all is not well with data science. Over the past decade, multiple studies have been found to be unreliable and non-reproducible when other scientists tried to recreate their results. This is due to a variety of factors, including fraud, pressure to publish, improper data handling practices, and bugs in analytic tools.&lt;/p&gt;
&lt;p&gt;The problems faced by data science mirror problems that software engineering has been trying to solve. While there are no silver bullets to guarantee quality software, techniques have been developed over time that have improved quality and reliability. Some of these techniques, including open source, version control, automation, and fuzzing could be adapted to the data science domain to improve reliability and help address the reproducibility crisis.&lt;/p&gt;
</summary><category term="Data"></category><category term="data science"></category><category term="engineering"></category><category term="reproducibility"></category></entry><entry><title>A Practical Guide to Dimensionality Reduction Techniques</title><link href="https://pyvideo.org/pydata-dc-2016/a-practical-guide-to-dimensionality-reduction-techniques.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Vishal Patel</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/a-practical-guide-to-dimensionality-reduction-techniques.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk provides a step-by-step overview and demonstration of several dimensionality (feature) reduction techniques. Attendees should have some basic level of understanding of data wrangling and supervised learning. The presentation will also include snippets of Python code, so familiarity with Python code will be useful.&lt;/p&gt;
</summary></entry><entry><title>Agent based modeling in Python</title><link href="https://pyvideo.org/pydata-dc-2016/agent-based-modeling-in-python.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Jackie Kazil</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/agent-based-modeling-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Agent-based modeling is a technique used to explore both complexity and emergence by simulating individual actors and their actions within a system. Think of systems such as the traffic in a city, or like those in financial markets where one actor can have an effect on the decisions of others until the system’s direction changes its course. In this talk, you will learn about ABMs in Python.&lt;/p&gt;
</summary></entry><entry><title>Bayesian Network Modeling using R and Python</title><link href="https://pyvideo.org/pydata-dc-2016/bayesian-network-modeling-using-r-and-python.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Pragyansmita Nayak</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/bayesian-network-modeling-using-r-and-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Bayesian Networks (BN) are increasingly being applied for real-world data problems. They provide the much desired complexity in representing the uncertainty of the predicted results of a model. The networks are easy to follow and better understand the relationships of the attributes of the dataset. As part of this talk, we will look into the existing R and Python packages that enables BN usage.&lt;/p&gt;
&lt;p&gt;Bayesian Networks are increasingly being applied for real-world data problems. They provide the much desired complexity in representing the uncertainty of the predicted results of a model. The networks are easy to follow and better understand the inter-relationships of the different attributes of the dataset. As part of this talk, we will look into the existing R and Python packages that enable BN learning and prediction. The pros and cons of the available packages will be discussed as well as new capabilities that will broaden the application of BN networks.&lt;/p&gt;
</summary><category term="bayesian"></category><category term="network"></category></entry><entry><title>Beyond Bag of Words A Practitioner's Guide to Advanced NLP</title><link href="https://pyvideo.org/pydata-dc-2016/beyond-bag-of-words-a-practitioners-guide-to-advanced-nlp.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Ariel M'ndange-Pfupfu</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/beyond-bag-of-words-a-practitioners-guide-to-advanced-nlp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We offer a foundation in building intelligent business applications using machine learning, walking you through all the steps to prototyping and production—data cleaning, feature engineering, model building and evaluation, and deployment—and diving into an application for anomaly detection and a personalized recommendation engine. All concepts will be presented with example code in Python.&lt;/p&gt;
</summary><category term="advanced"></category></entry><entry><title>Building a (semi) Autonomous Drone with Python</title><link href="https://pyvideo.org/pydata-dc-2016/building-a-semi-autonomous-drone-with-python.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Greg Lamp</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/building-a-semi-autonomous-drone-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;They might not be delivering our mail (or our burritos--tacocopter.com) yet but drones are now simple, small, and affordable enough that they can be considered a toy. You can even customize and program some of them! The Parrot AR Drone has an API that let's you control not only the drone's movement but also stream video and images from both of its cameras. I'll show you how you can use Python and node.js to build a drone that moves all by itself.&lt;/p&gt;
</summary></entry><entry><title>Building Continuous Learning Systems</title><link href="https://pyvideo.org/pydata-dc-2016/building-continuous-learning-systems.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Anuj Gupta</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/building-continuous-learning-systems.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;In this talk we explore how to build Machine Learning Systems that can that can learn &amp;quot;continuously&amp;quot; from their mistakes (feedback loop) and adapt to an evolving data distribution.&lt;/p&gt;
</summary><category term="learning"></category></entry><entry><title>Building Serverless Machine Learning Models in the Cloud</title><link href="https://pyvideo.org/pydata-dc-2016/building-serverless-machine-learning-models-in-the-cloud.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Alex Casalboni</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/building-serverless-machine-learning-models-in-the-cloud.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;You’ll learn how to efficiently design and train machine learning models in Python and deploy them to the cloud. This process reduces the development &amp;amp; operational efforts required to make your prototypes production-ready.&lt;/p&gt;
&lt;p&gt;We will describe the main challenges faced by data scientists involved in deploying machine learning models into real production environments with specific references, examples of Python libraries, and multi-model systems requiring advanced features such as A/B testing and high scalability &amp;amp; availability.&lt;/p&gt;
&lt;p&gt;While discussing the advantages and limitations of multiple deployment strategies in the cloud, we will focus on serverless computing (i.e. AWS Lambda) as a solution for simplifying your development &amp;amp; deployment workflows.&lt;/p&gt;
</summary><category term="Cloud"></category><category term="learning"></category><category term="machine learning"></category><category term="models"></category><category term="serverless"></category></entry><entry><title>Clustering: A Guide for the Perplexed</title><link href="https://pyvideo.org/pydata-dc-2016/clustering-a-guide-for-the-perplexed.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Leland McInnes</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/clustering-a-guide-for-the-perplexed.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Finding clusters is a powerful tool for understanding and exploring data. While the task sounds easy, it can be surprisingly difficult to do it well. Most standard clustering algorithms can, and do, provide very poor clustering results in many cases. We discuss how to do clustering correctly.&lt;/p&gt;
&lt;p&gt;Finding clusters is a powerful tool for understanding and exploring data. While the task sounds easy, it can be surprisingly difficult to it well. Most standard clustering algorithms can, and do, provide very poor clustering results in many cases. Our intuitions for what a cluster is are not as clear as we would like, and can easily be lead astray. We will attempt to find a definition of clustering that makes sense for most cases, and introduce an algorithm for finding such clusters, along with a high performance python implementation of the algorithm, building up more intuition for what clustering really means as we go.&lt;/p&gt;
</summary></entry><entry><title>Creating a Contemporary Lending Risk Management System Using Python</title><link href="https://pyvideo.org/pydata-dc-2016/creating-a-contemporary-lending-risk-management-system-using-python.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Piero Ferrante</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/creating-a-contemporary-lending-risk-management-system-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Lending involves risk and in order to be a successful lender at scale that risk needs to be mitigated. We'll be discussing how C2FO has built a suite of risk management tools for underwriting and portfolio management using the PyData ecosystem, rpy2 (for integrating R), and Spyre (for building a simple web application).&lt;/p&gt;
&lt;p&gt;Engineering a sophisticated in-house risk management solution for a commercial lending platform doesn't necessarily need to involve millions of lines of low-level code, clunky desktop applications, fancy front-end development, or messy spreadsheets. That is, so long as the problem is approached objectively and the solutions are evaluated critically.&lt;/p&gt;
</summary></entry><entry><title>Creating Python Data Pipelines in the Cloud</title><link href="https://pyvideo.org/pydata-dc-2016/creating-python-data-pipelines-in-the-cloud.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Femi Anthony</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/creating-python-data-pipelines-in-the-cloud.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;My talk will be an analysis of the various approaches to creating data pipelines the public cloud using Python.I will compare and contrast using various Python libraries such as Luigi, Airflow and native cloud frameworks such as Cloud Dataflow (Google), AWS Data Pipeline to create a real world data pipeline in Amazon AWS and Google Compute Engine.&lt;/p&gt;
</summary><category term="Cloud"></category><category term="Data"></category></entry><entry><title>Data Sciencing While Female</title><link href="https://pyvideo.org/pydata-dc-2016/data-sciencing-while-female.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Mandi Traud</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/data-sciencing-while-female.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;How can we increase the number of female data scientists in the workplace? By building a community. Dr. Amanda Traud was the only woman on her data science team when she started the group Women Data Scientists DC. In one year, the group grew to over 1,000 members. Dr. Traud will discuss the ups and downs of being a woman in data science and how to encourage and include more women in the field.&lt;/p&gt;
</summary><category term="Data"></category></entry><entry><title>Eat Your Vegetables Data Security for Data Scientists</title><link href="https://pyvideo.org/pydata-dc-2016/eat-your-vegetables-data-security-for-data-scientists.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Will Voorhees</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/eat-your-vegetables-data-security-for-data-scientists.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/WilliamVoorhees1/eat-your-vegetables-data-security-for-data-scientists"&gt;http://www.slideshare.net/WilliamVoorhees1/eat-your-vegetables-data-security-for-data-scientists&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You've got data: lots of it. People want to get their hands on that data. You don't want that, so let's go over a few things you can do to dissuade attackers from getting their grubby mitts on your hard processed datastore. We'll cover the obvious things (spoiler alert: encryption) and then some advanced techniques for keeping data secure while still keeping it usable (that is to say, analyzable).&lt;/p&gt;
</summary><category term="Data"></category><category term="Security"></category></entry><entry><title>Forecasting critical food violations at restaurants using open data</title><link href="https://pyvideo.org/pydata-dc-2016/forecasting-critical-food-violations-at-restaurants-using-open-data.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Nicole Donnelly</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/forecasting-critical-food-violations-at-restaurants-using-open-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Dc 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/NicoleDonnelly6/pydatadc-forecasting-critical-food-violations-at-restaurants-using-open-data"&gt;http://www.slideshare.net/NicoleDonnelly6/pydatadc-forecasting-critical-food-violations-at-restaurants-using-open-data&lt;/a&gt;
Github: &lt;a class="reference external" href="https://github.com/nd1/DC_RestaurantViolationForecasting"&gt;https://github.com/nd1/DC_RestaurantViolationForecasting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As many as 105 million Americans suffer from foodborne illness annually. In 2014, the City of Chicago began forecasting these outbreaks targeting limited health inspection resources toward likely sites, showing a 7 day improvement in locating critical violations at food establishments. This talk provides an end-to-end walkthrough of predicting critical violations in Washington, DC using Python.&lt;/p&gt;
</summary><category term="Data"></category></entry><entry><title>From rocks to a hammer when and how to change your company's analytical tools</title><link href="https://pyvideo.org/pydata-dc-2016/from-rocks-to-a-hammer-when-and-how-to-change-your-companys-analytical-tools.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Sebastien Genty</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/from-rocks-to-a-hammer-when-and-how-to-change-your-companys-analytical-tools.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Many organizations still rely on SPSS/SAS to do most of their analytical work. These tools are not only very costly ($10k+ per year per license), but are also limited (no scripting ability, very manual). In 2015, we began transitioning into Python to build robust tools and to reduce operational costs. Along the way, we learned a lot about propagating new tools within a company, reverse engineering, and helping others adjust to a new paradigm. This talk will outline our process of evaluating new tools, initial adoption and companywide propagation. One of the main findings was that open source does not mean free, and we had to take into account each person's experience and comfort in devising our implementation strategy. Lastly, we had to develop our own internal library in order to maintain functionality.&lt;/p&gt;
</summary><category term="tools"></category></entry><entry><title>Fuzzy Search Algorithms How and When to Use Them</title><link href="https://pyvideo.org/pydata-dc-2016/fuzzy-search-algorithms-how-and-when-to-use-them.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Jiaqi Liu</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/fuzzy-search-algorithms-how-and-when-to-use-them.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;So much of data science is about understanding the context around your data. In this talk, we hope to address how to work with messy text data by leveraging fuzzy search algorithms in python or against a database such as PostgreSQL. We will talk specifically about fuzzy algorithms such as Soundex, Trigram/n-gram search, and Levenshtein distances and demonstrate use cases in an ipython notebook.&lt;/p&gt;
</summary><category term="search"></category></entry><entry><title>How I learned to time travel, or, data pipelining and scheduling with Airflow</title><link href="https://pyvideo.org/pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Laura Lorenz</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418"&gt;http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data warehousing and analytics projects can, like ours, start out small - and fragile. With an organically growing mess of scripts glued together and triggered by cron jobs hiding on different servers, we needed better plumbing. After perusing the data pipelining landscape, we landed on Airflow, an Apache incubating batch processing pipelining and scheduler tool from Airbnb.&lt;/p&gt;
</summary><category term="airflow"></category><category term="Data"></category></entry><entry><title>JupyterLab: Building Blocks for Interactive Computing</title><link href="https://pyvideo.org/pydata-dc-2016/jupyterlab-building-blocks-for-interactive-computing.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Jason Grout</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/jupyterlab-building-blocks-for-interactive-computing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData D 2016&lt;/p&gt;
&lt;p&gt;JupyterLab is an extensible web application (still in alpha) for scientific computation. Users arrange multiple notebooks, editors, terminals, and custom components with tabs, splitters, and sidebars. JupyterLab also has a file browser, command palette and quick help system. We demonstrate JupyterLab and recent developments and show how JupyterLab fits in the vision of the Jupyter project.&lt;/p&gt;
</summary></entry><entry><title>Keynote: Become a Data Superhero How Data Can Change the World</title><link href="https://pyvideo.org/pydata-dc-2016/keynote-become-a-data-superhero-how-data-can-change-the-world.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Elizabeth Lindsey</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-become-a-data-superhero-how-data-can-change-the-world.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;The capacity to gather and interpret data can be low for many nonprofits. Working together, data scientists and organizations can be a world-changing combination. Byte Back has found a way to use data analysis for good and will help you learn how to tap into your own superpowers.&lt;/p&gt;
</summary><category term="Data"></category></entry><entry><title>Keynote: Building a Data Driven Dialogue From Filling Potholes to Disrupting the Cycle</title><link href="https://pyvideo.org/pydata-dc-2016/keynote-building-a-data-driven-dialogue-from-filling-potholes-to-disrupting-the-cycle.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Kelly Jin</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-building-a-data-driven-dialogue-from-filling-potholes-to-disrupting-the-cycle.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</summary><category term="Data"></category></entry><entry><title>Keynote: How Open Data Science Opens the World of Innovation</title><link href="https://pyvideo.org/pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Robert Cohn</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Innovation today appears to be instantaneous in large part due to open source technology. Open Data Science is no exception. Python, a pillar in the Open Data Science bedrock, is well positioned to harvest innovation in software and with Anaconda, it’s also well positioned to capitalize on the latest hardware innovations. Anaconda and Intel are blazing a path for the Python community to take advantage of cognitive computing, including machine learning and deep learning.&lt;/p&gt;
&lt;p&gt;In this keynote, Peter and Robert will talk about how Open Data Science––a connected ecosystem of data, analytics and compute––streamlines the path to high performance and innovation to achieve breakthrough results.&lt;/p&gt;
</summary><category term="Data"></category><category term="data science"></category><category term="science"></category></entry><entry><title>NoSQL doesn't mean No Schema</title><link href="https://pyvideo.org/pydata-dc-2016/nosql-doesnt-mean-no-schema.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Steven Lott</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/nosql-doesnt-mean-no-schema.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;While databases like MongoDB don't require a formal schema, there's still a schema somewhere. It might be merely implied by validation rules in the code. Or, there might be a more formal representation. In some cases, the lack of strict schema creates a dynamic flexibility that creates value rapidly. Other times, the lack of formal structures leads to chaos. How can we find a balance?&lt;/p&gt;
</summary><category term="nosql"></category></entry><entry><title>Predicting Usage for Capital Bikeshare stations based upon Spatial Characteristics</title><link href="https://pyvideo.org/pydata-dc-2016/predicting-usage-for-capital-bikeshare-stations-based-upon-spatial-characteristics.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Darshan Pandit</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/predicting-usage-for-capital-bikeshare-stations-based-upon-spatial-characteristics.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;A step after trying to model Continuous variables from Regression models, lies a very rewarding problem of estimating decisions which more lie in the discrete domain. In this talk, we work towards developing Logistic models to predict traffic for Capital Bikeshare, and work towards finding optimal station locations for Network expansion.&lt;/p&gt;
</summary><category term="spatial"></category></entry><entry><title>Promoting a data driven culture in a world of microservices</title><link href="https://pyvideo.org/pydata-dc-2016/promoting-a-data-driven-culture-in-a-world-of-microservices.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Alex DeBrie</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/promoting-a-data-driven-culture-in-a-world-of-microservices.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/promoting-a-data-driven-culture-in-a-microservices-environment"&gt;http://www.slideshare.net/PyData/promoting-a-data-driven-culture-in-a-microservices-environment&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At Hudl, we give every employee full access to our data warehouse, and over 50% of our employees have personally written a query against it. In this talk, I discuss our journey to democratize our data. I touch on technical and non-technical challenges, including the tools we use and the structure of our teams.&lt;/p&gt;
</summary><category term="Culture"></category><category term="Data"></category></entry><entry><title>Python useRs</title><link href="https://pyvideo.org/pydata-dc-2016/python-users.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Daniel Chen</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/python-users.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;According to the most recent IEEE language rankings, R is now the 5th most popular language. It is the only domain specific language in the top 5, behind the general-purpose languages C, Java, Python, and C++, respectively. It is common for data science teams to work in multiple languages and for the Pythonista, a working knowledge in R is useful for collaboration, analysis, and performance.&lt;/p&gt;
&lt;p&gt;It may be better to extend a simple analysis in R than passing around CSV files. Even worse, there might be a new analysis package that only exists in R, or certain bits infrastructure are already implemented in R and it needs to incorporate a bit of Python analysis.&lt;/p&gt;
</summary></entry><entry><title>Scaling up to Big Data Devops for Data Science</title><link href="https://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Marck Vaisman</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</summary><category term="big data"></category><category term="Data"></category><category term="data science"></category><category term="devops"></category><category term="scaling"></category><category term="science"></category></entry><entry><title>Variational Inference in Python</title><link href="https://pyvideo.org/pydata-dc-2016/variational-inference-in-python.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Austin Rochford</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/variational-inference-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Jupyter notebook: &lt;a class="reference external" href="https://nbviewer.jupyter.org/gist/AustinRochford/91cabfd2e1eecf9049774ce529ba4c16"&gt;https://nbviewer.jupyter.org/gist/AustinRochford/91cabfd2e1eecf9049774ce529ba4c16&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Inference in Bayesian models often requires simulation to approximate posterior distributions. Variational inference (VI) instead approximates posteriors through optimization. Recent theoretical and computational advances in automatic variational inference have made VI more accessible. This talk will give an introduction to VI and show software packages for performing VI in Python.&lt;/p&gt;
</summary></entry><entry><title>Beyond Sentiment Emotion Mining with Python and machine learning</title><link href="https://pyvideo.org/pydata-dc-2016/beyond-sentiment-emotion-mining-with-python-and-machine-learning.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Max Tsvetovat</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/beyond-sentiment-emotion-mining-with-python-and-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Learn how to extract emotional content from textual data - and how to build a sentiment analysis tool that does not suck.&lt;/p&gt;
&lt;p&gt;Typical sentiment analysis tries to map the entire rich and varied world of human emotions into &amp;quot;good&amp;quot; vs &amp;quot;bad&amp;quot;. In this tutorial, we use the characters of &amp;quot;Inside Out&amp;quot; and machine learning to build a nuanced model of human emotions -- and put it in production!&lt;/p&gt;
</summary><category term="learning"></category><category term="machine learning"></category></entry><entry><title>Building Your First Data Pipelines</title><link href="https://pyvideo.org/pydata-dc-2016/building-your-first-data-pipelines.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Hunter Owens</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/building-your-first-data-pipelines.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;You need a data pipeline. This talk will discuss the lifecycle of projects using Jupyter notebooks &amp;amp; Luigi as a data pipeline management tool for a variety of projects, from greenfield to retrofitting complex systems. It will included a hands on demo.&lt;/p&gt;
&lt;p&gt;Data pipelines are hard. Too often we resort to retrofitting janky scripts, relying on keeping a readme up to data, etc.&lt;/p&gt;
&lt;p&gt;First, this proposal lays out the variety of tools that are available to build data pipelines. This talk will discuss why you should be using Luigi and how to use it in a variety of common use cases.&lt;/p&gt;
&lt;p&gt;Next, we will build a basic exploratory analysis using DC open data and Luigi to demonstrate the power of this concept and how it works with Jupyter.&lt;/p&gt;
&lt;p&gt;Finally, we'll retrofit a larger, more complex project to use Luigi to show how you can use it in bigger organizations.&lt;/p&gt;
</summary><category term="Data"></category></entry><entry><title>Doing frequentist statistics with Scipy</title><link href="https://pyvideo.org/pydata-dc-2016/doing-frequentist-statistics-with-scipy.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Gustavo Patino</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/doing-frequentist-statistics-with-scipy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/gapatino/Doing-frequentist-statistics-with-Scipy"&gt;https://github.com/gapatino/Doing-frequentist-statistics-with-Scipy&lt;/a&gt;
Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/doing-frequentist-statistics-with-scipy"&gt;http://www.slideshare.net/PyData/doing-frequentist-statistics-with-scipy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Frequentist statistical tests are still very common, and in some fields they continue to represent the technical standard. In this session we will cover the execution and interpretation of the most common tests using the SciPy.stats package, and plotting the results with Matplotlib and Seaborn. The focus will be on traditional approaches to the tests, not on Bayesian and bootstrapping approaches&lt;/p&gt;
&lt;p&gt;The session will cover: - Normality testing - Student's t-test and ANOVA - Wilcoxon rank sum and Kruskal-Wallis - Correlation - Univariate linear and logistic regression - Chi-square - p-value interpretation - Effect size calculation&lt;/p&gt;
</summary><category term="scipy"></category><category term="statistics"></category></entry><entry><title>Educational framework for Black Box optimization methods design</title><link href="https://pyvideo.org/pydata-dc-2016/educational-framework-for-black-box-optimization-methods-design.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Nadia Udler</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/educational-framework-for-black-box-optimization-methods-design.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Many pressing real world problems can be stated as problems of global optimization, where target function is a black box. Such problems are best approached with a library of optimization methods to help study the nature of the problem. We show how to use Scipy.optimize and Scikit-learn modules to create global optimization methods with desired properties.&lt;/p&gt;
</summary><category term="Design"></category><category term="framework"></category><category term="optimization"></category></entry><entry><title>Getting started with H2O on Python</title><link href="https://pyvideo.org/pydata-dc-2016/getting-started-with-h2o-on-python.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Ashrith Barthur</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/getting-started-with-h2o-on-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/h2oai/pydata2016-h2o-loganalysis"&gt;https://github.com/h2oai/pydata2016-h2o-loganalysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;H2O helps Python users make the leap from single machine based processing to large-scale distributed environments. Hadoop lets H2O users scale their data processing capabilities based on their current needs. Using H2O, Python, and Hadoop, you can create a complete end-to-end data analysis solution. In this presentation the speaker will show - and highlight a start to end process of how to design an algorithm, optimize, and implement it using H2O. The use case will focus much on the security work done between H2O and Capital One.&lt;/p&gt;
</summary></entry><entry><title>How to Build Your Own Self Driving Toy Car</title><link href="https://pyvideo.org/pydata-dc-2016/how-to-build-your-own-self-driving-toy-car.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Ryan Zotti</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/how-to-build-your-own-self-driving-toy-car.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;I’ve spent the past 6 months building a self-driving toy car using a Raspberry Pi, OpenCV, and TensorFlow. If you’ve ever thought about building your own self-driving toy car, this presentation will help you avoid common pitfalls and shed light on important tradeoffs that you’ll have to weigh along the way. I’ll cover things like how to parse images, how to effectively tune machine learning neural&lt;/p&gt;
</summary><category term="opencv"></category><category term="tensorflow"></category></entry><entry><title>Interactive multi scale time series exploration with matplotlib</title><link href="https://pyvideo.org/pydata-dc-2016/interactive-multi-scale-time-series-exploration-with-matplotlib.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Thomas Caswell</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/interactive-multi-scale-time-series-exploration-with-matplotlib.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This tutorial is an introduction to how to matplotlib's event handling system to build an tool for interactively exploring multi-scale time series data.&lt;/p&gt;
&lt;p&gt;The primary example will be how to 'drill down' through summary data sets to view the underlying data using hourly weather data from NOAA.&lt;/p&gt;
</summary><category term="matplotlib"></category></entry><entry><title>Julia Tutorial</title><link href="https://pyvideo.org/pydata-dc-2016/julia-tutorial.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Chase Coleman</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/julia-tutorial.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/cc7768/PyDataDC_julia"&gt;https://github.com/cc7768/PyDataDC_julia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Julia is a programming language oriented around the just in time (JIT) compiler technology. This tutorial will be an introduction to Julia and the core concepts that make it a good choice for certain types of problems.&lt;/p&gt;
</summary><category term="julia"></category><category term="tutorial"></category></entry><entry><title>Learn how to Make Life Easier with Anaconda</title><link href="https://pyvideo.org/pydata-dc-2016/learn-how-to-make-life-easier-with-anaconda.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Dhavide Aruliah</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/learn-how-to-make-life-easier-with-anaconda.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Bring your laptop &amp;amp; get started with Anaconda, the leading platform for Open Data Science powered by Python. You'll get hands-on experience using conda &amp;amp; Anaconda Cloud to share notebooks, packages, codes, &amp;amp; environments and to get stuff done. No software pre-installation required (although installing Anaconda beforehand may be helpful).&lt;/p&gt;
</summary><category term="anaconda"></category></entry><entry><title>Machine Learning with Text in scikit learn</title><link href="https://pyvideo.org/pydata-dc-2016/machine-learning-with-text-in-scikit-learn.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Kevin Markham</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/machine-learning-with-text-in-scikit-learn.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/justmarkham/pydata-dc-2016-tutorial"&gt;https://github.com/justmarkham/pydata-dc-2016-tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Although numeric data is easy to work with in Python, most knowledge created by humans is actually raw, unstructured text. By learning how to transform text into data that is usable by machine learning models, you drastically increase the amount of data that your models can learn from. In this tutorial, we'll build and evaluate predictive models from real-world text using scikit-learn.&lt;/p&gt;
</summary><category term="learning"></category><category term="machine learning"></category><category term="scikit"></category></entry><entry><title>Modern NLP in Python</title><link href="https://pyvideo.org/pydata-dc-2016/modern-nlp-in-python.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Patrick Harrison</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/modern-nlp-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Academic and industry research in Natural Language Processing (NLP) has progressed at an accelerating pace over the last several years. Members of the Python community have been hard at work moving cutting-edge research out of papers and into open source, &amp;quot;batteries included&amp;quot; software libraries that can be applied to practical problems. We'll explore some of these tools for modern NLP in Python.&lt;/p&gt;
</summary></entry><entry><title>Pandas from the Inside</title><link href="https://pyvideo.org/pydata-dc-2016/pandas-from-the-inside.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Stephen Simmons</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/pandas-from-the-inside.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Pandas is great for data analysis in Python: intuitive DataFrames from R; fast numpy arrays under the hood; groupby like in SQL. But this familiarity is deceptive: pandas users often get stuck on things they feel should be simple. This talk look inside pandas to see how DataFrames actually work when building, indexing and grouping tables. You will learn how to write fast, efficient pandas code.&lt;/p&gt;
</summary><category term="pandas"></category></entry><entry><title>Parallel Python Analyzing Large Data Sets</title><link href="https://pyvideo.org/pydata-dc-2016/parallel-python-analyzing-large-data-sets.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Aron Ahmadia</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/parallel-python-analyzing-large-data-sets.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Students will walk away with a high-level understanding of both parallel problems and how to reason about parallel computing frameworks. They will also walk away with hands-on experience using a variety of frameworks easily accessible from Python.&lt;/p&gt;
</summary><category term="Data"></category><category term="parallel"></category><category term="sets"></category></entry><entry><title>The Five Kinds of Python Functions</title><link href="https://pyvideo.org/pydata-dc-2016/the-five-kinds-of-python-functions.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Steven Lott</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/the-five-kinds-of-python-functions.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We'll look at the wide variety of ways that we can leverage Python functions. This will show provide helpful background in ordinary functions, as well as callable objects and lambdas. We'll look closely at how to use generator functions, also. The fifth type of function is a function wraps a special method, like len().&lt;/p&gt;
</summary><category term="functions"></category></entry><entry><title>Using Dask for Parallel Computing in Python</title><link href="https://pyvideo.org/pydata-dc-2016/using-dask-for-parallel-computing-in-python.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/using-dask-for-parallel-computing-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Dask is a relatively new library for parallel computing in Python. It builds around familiar data structures to users of the PyData stack and enables them to scale up their work on one or many machines. This tutorial will introduce users to the core concepts of dask by working through some example problems. The tutorial will be distributed via Jupyter Notebooks.&lt;/p&gt;
</summary><category term="dask"></category><category term="parallel"></category><category term="parallel computing"></category></entry></feed>