<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_nlp.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-12-07T00:00:00+00:00</updated><entry><title>Born to adapt: How Dathena solves the industry diversity problem</title><link href="https://pyvideo.org/pycon-se-2019/born-to-adapt-how-dathena-solves-the-industry-diversity-problem.html" rel="alternate"></link><published>2019-10-31T00:00:00+00:00</published><updated>2019-10-31T00:00:00+00:00</updated><author><name>Tetiana Kodliuk</name></author><id>tag:pyvideo.org,2019-10-31:pycon-se-2019/born-to-adapt-how-dathena-solves-the-industry-diversity-problem.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How adjust your AI solution to the new domain? How retrain your model on new industry-specific data? How to increase labeled dataset for minimum cost? Do oracles exist? We will look at these problems from the data protection point of view and bring possible solutions. We will talk about methods, which discuss how to adapt NLP and CV solutions to any type of industry.&lt;/p&gt;
</summary><category term="ai"></category><category term="nlp"></category></entry><entry><title>An Introduction to Sentiment Analysis of Textual Data</title><link href="https://pyvideo.org/pydata-austin-2019/an-introduction-to-sentiment-analysis-of-textual-data.html" rel="alternate"></link><published>2019-12-07T00:00:00+00:00</published><updated>2019-12-07T00:00:00+00:00</updated><author><name>Fatma Tarlaci</name></author><id>tag:pyvideo.org,2019-12-07:pydata-austin-2019/an-introduction-to-sentiment-analysis-of-textual-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this tutorial, you’ll be introduced to Sentiment Analysis (SA), the extraction of subjective, affective information from text through Natural Language Processing (NLP) to enable data-driven decisions. Participants will work through a step-by-step application of SA to build a sound knowledge of its different components and an understanding of this powerful technique in various business settings&lt;/p&gt;
</summary><category term="sentiment analysis"></category><category term="natural language processing"></category><category term="nlp"></category></entry><entry><title>Enhancing Common Natural Language Processing with Cognitive Linguistics</title><link href="https://pyvideo.org/pydata-austin-2019/enhancing-common-natural-language-processing-with-cognitive-linguistics.html" rel="alternate"></link><published>2019-12-07T00:00:00+00:00</published><updated>2019-12-07T00:00:00+00:00</updated><author><name>Chris Leonard</name></author><id>tag:pyvideo.org,2019-12-07:pydata-austin-2019/enhancing-common-natural-language-processing-with-cognitive-linguistics.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;I will present an overview of what we have learned in my company from combining our own proprietary cognitive linguistics technology with simple NLP methods such as sentiment analysis and named entity recognition. We will explore several use cases and discuss the impact of using both approaches together.&lt;/p&gt;
</summary><category term="nlp"></category><category term="natural language processing"></category><category term="cognitive linguistics"></category></entry><entry><title>Multilingual embeddings to scale NLP models to multiple languages</title><link href="https://pyvideo.org/pydata-austin-2019/multilingual-embeddings-to-scale-nlp-models-to-multiple-languages.html" rel="alternate"></link><published>2019-12-07T00:00:00+00:00</published><updated>2019-12-07T00:00:00+00:00</updated><author><name>Deeksha Yennam</name></author><id>tag:pyvideo.org,2019-12-07:pydata-austin-2019/multilingual-embeddings-to-scale-nlp-models-to-multiple-languages.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In NLP, we often come across situations where the text documents are not limited to one language. In this talk, we'll explore multilingual embeddings as an alternative to traditional word embeddings for creating NLP models that can scale effectively to data in multiple languages.&lt;/p&gt;
</summary><category term="nlp"></category><category term="natural language processing"></category></entry><entry><title>Let the AI Do the Talk: Adventures with Natural Language Generation</title><link href="https://pyvideo.org/pycon-italia-2019/let-the-ai-do-the-talk-adventures-with-natural-language-generation.html" rel="alternate"></link><published>2019-05-05T00:00:00+00:00</published><updated>2019-05-05T00:00:00+00:00</updated><author><name>Marco Bonzanini</name></author><id>tag:pyvideo.org,2019-05-05:pycon-italia-2019/let-the-ai-do-the-talk-adventures-with-natural-language-generation.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recent advances in Artificial Intelligence have shown how computers can
compete with humans in a variety of mundane tasks, but what happens when
creativity is required?&lt;/p&gt;
&lt;p&gt;This talk introduces the concept of Natural Language Generation, the
task of automatically generating text, for examples articles on a
particular topic, poems that follow a particular style, or speech
transcripts that express some attitude. Specifically, we’ll discuss the
case for Recurrent Neural Networks, a family of algorithms that can be
trained on sequential data, and how they improve on traditional language
models.&lt;/p&gt;
&lt;p&gt;The talk is for beginners, we’ll focus more on the intuitions behind the
algorithms and their practical implications, and less on the
mathematical details. Practical examples with Python will showcase
Keras, a library to quickly prototype deep learning architectures.&lt;/p&gt;
&lt;p&gt;Brief outline:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to Natural Language Generation&lt;/li&gt;
&lt;li&gt;Language Modelling&lt;/li&gt;
&lt;li&gt;Recurrent Neural Networks and Long Short Term Memory for NLG&lt;/li&gt;
&lt;li&gt;RNN examples with Keras&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feedback form: &lt;a class="reference external" href="https://python.it/feedback-1574"&gt;https://python.it/feedback-1574&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Sunday 5 May&lt;/strong&gt; at 10:15 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="nlp"></category><category term="Keras"></category><category term="natural-language-processing"></category><category term="pydata"></category></entry><entry><title>Multi-modal classification with PyTorch</title><link href="https://pyvideo.org/pycon-italia-2019/multi-modal-classification-with-pytorch.html" rel="alternate"></link><published>2019-05-04T00:00:00+00:00</published><updated>2019-05-04T00:00:00+00:00</updated><author><name>Jennifer Seale</name></author><id>tag:pyvideo.org,2019-05-04:pycon-italia-2019/multi-modal-classification-with-pytorch.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recent work by Kiela et al. (2018) reveals that image and text
multi-modal classification models far outperform both text- and
image-only models. This talk will review work that extends Kiela et
al.’s (2018) research by determining if accuracy in classification may
be increased by the implementation of transfer learning in language
processing. The performance of the model over a MM-IMDb (Arevalo et al.
2017) dataset is analyzed and compared to the baseline provided by Kiela
et al. (2018).&lt;/p&gt;
&lt;p&gt;The work is implemented with PyTorch and the goal of the talk will be to
review details of the implementation, and performance of the model as
compared to that recorded in Kiela et al. (2018). Attendees of this talk
should have a basic familiarity with neural nets developed in PyTorch
for the purposes of NLP and computer vision.&lt;/p&gt;
&lt;p&gt;References: Arevalo, J., Solorio, T., Montes-y-Gómez, M., &amp;amp; González, F.
A. 2017. Gated multimodal units for information fusion. arXiv preprint
arXiv:1702.01992.&lt;/p&gt;
&lt;p&gt;Kiela, Douwe, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2018.
Efficient large-scale multi-modal classification. arXiv preprint
arXiv:1802.02892.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1754"&gt;https://python.it/feedback-1754&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Saturday 4 May&lt;/strong&gt; at 10:45 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="nlp"></category><category term="mathematical-modelling"></category><category term="computer-vision"></category></entry><entry><title>Deep Learning with PyTorch for Fun and Profit (Part III / Italian Edition: Divina Commedia)</title><link href="https://pyvideo.org/pycon-italia-2019/deep-learning-with-pytorch-for-fun-and-profit-part-iii-italian-edition-divina-commedia.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Alexander Hendorf</name></author><id>tag:pyvideo.org,2019-05-03:pycon-italia-2019/deep-learning-with-pytorch-for-fun-and-profit-part-iii-italian-edition-divina-commedia.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There are all these great articles and blog posts about Deep Learning
describing all that awesome stuff. - Is it all that easy? Let’s check!&lt;/p&gt;
&lt;p&gt;We’ll look into: style transfer (making a picture look like painting),
speech generation (like Siri or Alexa) and text generation (writing a
story). In this talk I’ll describe the whole journey: A fun ride from
the idea to the very end including all the struggles, failures and
successes.&lt;/p&gt;
&lt;p&gt;This is an ongoing talk on how far we can get creating a full &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Radio_drama"&gt;radio
drama (Hörspiel)&lt;/a&gt; with
deep learning and the resources required.&lt;/p&gt;
&lt;p&gt;Steps, we’ll cover:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The data challenge: get the data ready&lt;/li&gt;
&lt;li&gt;Creating a character-level language models with an Recurrent Neural
Network&lt;/li&gt;
&lt;li&gt;Creating a text generator&lt;/li&gt;
&lt;li&gt;Creating artwork&lt;/li&gt;
&lt;li&gt;Synthesising speech&lt;/li&gt;
&lt;li&gt;Making it sound like a real person&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this special PyCon X edition we will also try to recreate text in the
style of Dante’ Divina Commedia.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1687"&gt;https://python.it/feedback-1687&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 12:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="deep learning"></category><category term="Pytorch"></category><category term="art"></category><category term="Artificial Intelligence"></category><category term="nlp"></category></entry><entry><title>Build text classification models ( CBOW and Skip-gram) with FastText in python</title><link href="https://pyvideo.org/pycon-de-2018/build-text-classification-models-cbow-and-skip-gram-with-fasttext-in-python.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Kajal Puri</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/build-text-classification-models-cbow-and-skip-gram-with-fasttext-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;FastText has been open-sourced by Facebook in 2016 and with its release,
it became the fastest and most accurate library in Python for text
classification and word representation. It is to be seen as a substitute
for gensim package's word2vec. It includes the implementation of two
extremely important methodologies in NLP i.e Continuous Bag of Words and
Skip-gram model. Fasttext performs exceptionally well with supervised as
well as unsupervised learning.&lt;/p&gt;
&lt;p&gt;The tutorial will be divided in following four segments :&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="NLP"></category><category term="Machine Learning"></category></entry><entry><title>Building your own conversational AI with open source tools</title><link href="https://pyvideo.org/pycon-de-2018/building-your-own-conversational-ai-with-open-source-tools.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Justina Petraitytė</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/building-your-own-conversational-ai-with-open-source-tools.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Conversational AI is far from being a solved problem, but you don’t need
to rely on third-party APIs to build great chat and voice apps.&lt;/p&gt;
&lt;p&gt;In this talk we will live-code a useful, engaging conversational AI bot
based entirely on machine learning. We’ll be using Rasa NLU &amp;amp; Rasa Core,
which are open source libraries for building machine learning-based
chatbots and voice assistants. We will teach our system how to hold
multi-turn conversations by creating some initial training data, and
then refine its behaviour by interacting with the system and providing
feedback. We will cover the fundamentals of conversational AI, including
the most important algorithms for intent classification, entity
extraction, and dialogue management.&lt;/p&gt;
&lt;p&gt;What will attendees learn:&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="NLP"></category><category term="Machine Learning"></category><category term="Python"></category></entry><entry><title>Germany's next topic model</title><link href="https://pyvideo.org/pycon-de-2018/germanys-next-topic-model.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Thomas Mayer</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/germanys-next-topic-model.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Identifying topic models for user generated content like hotel reviews
turns out to be difficult with the standard approach of LDA (Latent
Dirichlet Allocation; Blei et al., 2003). Hotel review texts usually
don't differ as much in the topics that are covered as is typical with
other genres such as Wikipedia or newsgroup articles where there is
commonly only a very small set of topics present in each document.&lt;/p&gt;
&lt;p&gt;To this end, we developed our own approach to topic modeling that is
especially tailored to non-edited texts like hotel reviews. The approach
can be divided into three major steps. First, using the concept of
second-order cooccurrences we define a contextual similarity score that
enables us to identify words that are similar with respect to certain
topics. This score allows us to build up a topic network where nodes are
words and edges the contextual similarity between the words. With the
help of algorithms from graph theory, like the Infomap algorithm
(Rosvall and Bergstrom, 2008), we are able to detect clusters of highly
connected words that can be identified as topics in our review texts. In
a further step, we use these clusters and the respective words to get a
topic similarity score for each word in the network. In other words, we
transform a hard clustering of words into topics into a probability
score of how likely a certain word belongs to a given topic/cluster.&lt;/p&gt;
&lt;p&gt;The presentation is structured as follows:&lt;/p&gt;
&lt;p&gt;References: David M. Blei, Andrew Y. Ng, Michael I. Jordan: Latent
dirichlet allocation. In: Journal of Machine Learning Research, Jg. 3
(2003), S. 993–1022, ISSN 1532-4435 M. Rosvall and C. T. Bergstrom, Maps
of information flow reveal community structure in complex networks, PNAS
105, 1118 (2008) &lt;a class="reference external" href="http://dx.doi.org/10.1073/pnas.0706851105"&gt;http://dx.doi.org/10.1073/pnas.0706851105&lt;/a&gt;,
&lt;a class="reference external" href="http://arxiv.org/abs/0707.0609"&gt;http://arxiv.org/abs/0707.0609&lt;/a&gt;&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="Networks"></category><category term="NLP"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>reticulate: R interface to Python</title><link href="https://pyvideo.org/pycon-de-2018/reticulate-r-interface-to-python.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Jens Bruno Wittek</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/reticulate-r-interface-to-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python and R are the preferred languages for data science. In 2018,
RStudio introduced its package reticulate and clearly demonstrates that
it favours to join forces. Both languages have strengths and weaknesses.
Tools to combine the strengths will enable easier collaboration in
projects and more possibilities to succeed. Using Python from R gives R
users wider access to functions and makes it easier for Python beginners
to just run scripts and being able to collaborate in Python projects.
The talk will show the possibilities of reticulate: The main part starts
with demonstrating the Python interpreter within R. It will show how to
source Python scripts as well as install and import modules. Then it
will deal with the most important types of Python objects, how they are
represented in R and how to further manipulate them. Thereby, a special
focus is on using Python for data science. In addition, it will be
presented how Conda environments can be created and used from R. A
further application will be the creation of reports with Markdown and
LaTeX where R and Python can be used within one document and share
objects. A last topic is about showing the possibilities for easier
development in RStudio (help regarding Python functions, auto
completion).&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="NLP"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>Your first NLP project: peaks and pitfalls of unstructured data</title><link href="https://pyvideo.org/pycon-de-2018/your-first-nlp-project-peaks-and-pitfalls-of-unstructured-data.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Anna Widiger</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/your-first-nlp-project-peaks-and-pitfalls-of-unstructured-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Natural Language Processing improves the quality of your text data for
future analysis and increases the accuracy of your machine learning
model. It’s important to know what goes into the bag of words and what
are some potential do's and don'ts of text pre-processing. Which text
normalization steps are necessary and which ones are “nice-to-have”? Why
is classic NLP still relevant in the age of Deep Learning? What metrics
can be used to compare word frequencies and what can machine learning
algorithms do with those numbers? This NLP talk provides answers to
these questions and more! You'll see three examples of NLP pipelines
using spaCy: sentiment analysis and emoji in tweets, named entity
recognition in Yelp reviews, and multilingual topic modeling for news
articles.&lt;/p&gt;
</summary><category term="NLP"></category></entry><entry><title>Python e Elasticsearch: dal Text Search a NLP e oltre</title><link href="https://pyvideo.org/pycon-italia-2018/python-e-elasticsearch-dal-text-search-a-nlp-e-oltre.html" rel="alternate"></link><published>2018-04-22T00:00:00+00:00</published><updated>2018-04-22T00:00:00+00:00</updated><author><name>Dario Balinzo</name></author><id>tag:pyvideo.org,2018-04-22:pycon-italia-2018/python-e-elasticsearch-dal-text-search-a-nlp-e-oltre.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Il Talk è rivolto a sviluppatori Python intermedi. Non è richiesta
nessuna conoscenza su Elasticsearch.&lt;/p&gt;
&lt;p&gt;Nell’introduzione presenteremo le librerie elasticsearch-py e
elasticsearch- dsl introducendo i concetti base di ElasticSearch.
Saranno prima presentate le metodologie di indicizzazione per
ottimizzare la ricerca su grandi quantità di dati, mostrando come
inserire i propri dati nel motore di ricerca.&lt;/p&gt;
&lt;p&gt;Dopo passeremo alle query (dal text search alle geo queries ) e relative
aggregazioni, facendo vedere come estrarre informazioni dai dati in
maniera veloce e migliorare così la user experience.&lt;/p&gt;
&lt;p&gt;In seguito saranno presentate funzionalità di ricerca avanzate,
spiegando come arricchire le proprie webapp con le funzionalità
dinamiche di “search as you type”, autocompletamento e suggerimento.&lt;/p&gt;
&lt;p&gt;Infine mostreremo come utilizzare tecniche di Data Analytics avanzate
come il NLP: analizzando i testi sarà possibile fare “language
detection”, “text classification” e “keyword extraction”. In tal modo
non solo si può trovare velocemente cosa si sta cercando, ma analizzare
commenti e recensioni per capire se i clienti sono soddisfatti.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;domenica 22 aprile&lt;/strong&gt; at 14:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="nlp"></category><category term="nosql"></category><category term="Python"></category><category term="bigdata"></category><category term="elasticsearch"></category><category term="Full Text Search"></category></entry><entry><title>Recent advancements in NLP and Deep Learning: A Quant's Perspective</title><link href="https://pyvideo.org/pycon-italia-2018/recent-advancements-in-nlp-and-deep-learning-a-quants-perspective.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Umit Mert Cakmak</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/recent-advancements-in-nlp-and-deep-learning-a-quants-perspective.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a gold-rush among hedge-funds for text mining algorithms to
quantify textual data and generate trading signals. Harnessing the power
of alternative data sources became crucial to find novel ways of
enhancing trading strategies.&lt;/p&gt;
&lt;p&gt;With the proliferation of new data sources, natural language data became
one of the most important data sources which could represent the public
sentiment and opinion about market events, which then can be used to
predict financial markets.&lt;/p&gt;
&lt;p&gt;Talk is split into 5 parts;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Who is a quant and how do they use NLP?&lt;/li&gt;
&lt;li&gt;How deep learning has changed NLP?&lt;/li&gt;
&lt;li&gt;Let’s get dirty with word embeddings&lt;/li&gt;
&lt;li&gt;Performant deep learning layer for NLP: The Recurrent Layer&lt;/li&gt;
&lt;li&gt;Using all that to make money&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="who-is-a-quant-and-how-do-they-use-nlp"&gt;
&lt;h4&gt;1. Who is a quant and how do they use NLP?&lt;/h4&gt;
&lt;p&gt;Quants use mathematical and statistical methods to create algorithmic
trading strategies.&lt;/p&gt;
&lt;p&gt;Due to recent advances in available deep learning frameworks and
datasets (time series, text, video etc) together with decreasing cost of
parallelisable hardware, quants are experimenting with various NLP
methods which are applicable to quantitative trading.&lt;/p&gt;
&lt;p&gt;In this section, we will get familiar with the brief history of text
mining work that quants have done so far and recent advancements.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-deep-learning-has-changed-nlp"&gt;
&lt;h4&gt;2. How deep learning has changed NLP?&lt;/h4&gt;
&lt;p&gt;In recent years, data representation and modeling methods are vastly
improved. For example when it comes to textual data, rather than using
high dimensional sparse matrices and suffering from curse of
dimensionality, distributional vectors are more efficient to work with.&lt;/p&gt;
&lt;p&gt;In this section, I will talk about distributional vectors a.k.a. word
embeddings and recent neural network architectures used when building
NLP models.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lets-get-dirty-with-word-embeddings"&gt;
&lt;h4&gt;3. Let’s get dirty with word embeddings&lt;/h4&gt;
&lt;p&gt;Models such as Word2vec or GloVe helps us create word embeddings from
large unlabeled corpus which represent the relation between words, their
contextual relationships in numerical vector spaces and these
representations not only work for words but also could be used for
phrases and sentences.&lt;/p&gt;
&lt;p&gt;In this section, I will talk about inner workings of these models and
important points when creating domain-specific embeddings (e.g. for
sentiment analysis in financial domain).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="performant-deep-learning-layer-for-nlp-the-recurrent-layer"&gt;
&lt;h4&gt;4. Performant deep learning layer for NLP: The Recurrent Layer&lt;/h4&gt;
&lt;p&gt;Recurrent Neural Networks (RNNs) can capture and hold the information
which was seen before (context), which is important for dealing with
unbounded context in NLP tasks.&lt;/p&gt;
&lt;p&gt;Long Short Term Memory (LSTM) networks, which is a special type of RNN,
can understand the context even if words have long term dependencies,
words which are far back in their sequence.&lt;/p&gt;
&lt;p&gt;In this talk, I will compare LSTMs with other deep learning
architectures and will look at LSTM unit from a technical point of view.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-all-that-to-make-money"&gt;
&lt;h4&gt;5. Using all that to make money&lt;/h4&gt;
&lt;p&gt;Financial news, especially if it’s major, can change the sentiment among
investors and affect the related asset price with immediate price
corrections.&lt;/p&gt;
&lt;p&gt;For example, what’s been communicated in quarterly earnings calls might
indicate whether the price of share will drop or increase based on the
language used. If the message of the company is not direct and featuring
complex sounding language, it usually indicates that there’s some shady
stuff going on and if this information extracted right, it’s a valuable
trading signal. For similar reasons, scanning announcements and
financial disclosures for trading signals became a common NLP practice
in investment industry.&lt;/p&gt;
&lt;p&gt;In this section, I will talk about the various data sources that
researchers can use and also explain common NLP workflows and deep
learning practices for quantifying textual data for generating trading
signals.&lt;/p&gt;
&lt;p&gt;I will end with summary with application architecture in case anyone
would like to implement similar systems for their own use.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 14:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</summary><category term="nlp"></category><category term="data-science"></category><category term="Keras"></category><category term="Python"></category><category term="Deep-Learning"></category><category term="machine-learning"></category><category term="spaCy"></category><category term="nltk"></category></entry><entry><title>Brief Intro to Natural Language Processing (NLP)</title><link href="https://pyvideo.org/pydata-indy-2018/brief-intro-to-natural-language-processing-nlp.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>Andrew (AJ) Rader</name></author><id>tag:pyvideo.org,2018-10-12:pydata-indy-2018/brief-intro-to-natural-language-processing-nlp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Natural Language Processing (NLP) is a broad domain that deals with analyzing and understanding human text and words. Some typical areas of application for NLP involve text classification, speech recognition, machine translation, chatbots, and caption generation. Fundamentally, NLP involves converting words into numbers and doing math on these numbers in order to identify relationships between the words and documents they live in. The goal of this talk is to present the basic theory of what NLP is and demonstrate how to utilize machine learning approaches in Python to extract insights from text. An example text classification problem is presented; illustrating the steps required to ingest, preprocess, build and test a model for an example text corpus.&lt;/p&gt;
</summary><category term="nlp"></category></entry><entry><title>Real-time personalized recommendations using embeddings</title><link href="https://pyvideo.org/pycon-sk-2018/real-time-personalized-recommendations-using-embeddings.html" rel="alternate"></link><published>2018-03-09T00:00:00+00:00</published><updated>2018-03-09T00:00:00+00:00</updated><author><name>Jakub Mačina</name></author><id>tag:pyvideo.org,2018-03-09:pycon-sk-2018/real-time-personalized-recommendations-using-embeddings.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recommender systems are successfully used in several domains, e.g.
product or movie recommendation. In e-commerce, the aim is to provide
personalized suggestions to users for relevant items out of all products
available.&lt;/p&gt;
&lt;p&gt;In this talk, I will focus on item recommendation for anonymous users
when no historical data about user is available (also referred as a
cold-start problem) and challenges we have encountered. Firstly, I will
dig deeper into similar item recommendation by NLP model comparing
textual descriptions of items. This approach is based on word embeddings
extracted from neural network models, such as word2vec or fasttext.&lt;/p&gt;
&lt;p&gt;Finally, I will talk about how to apply the same idea of word embeddings
to learn a representation of each product. With the product embedding
representation, it is easy to calculate similarities between products in
real- time. Moreover, we found out that product embeddings are able to
capture style of a product, color, category or a price level.&lt;/p&gt;
&lt;p&gt;All of the examples will be practical using data about restaurants
reviews and fashion products. Open-source NLP library Gensim is used in
code samples. Presentation will be supported by visualization of
embeddings to get the idea behind. Everybody with any interest in
machine learning is welcome. After the presentation, you will know how
to compute relationship between pizza and pasta or how to capture a
fashion style of a user.&lt;/p&gt;
</summary><category term="NLP"></category><category term="PyCon SK"></category><category term="Python"></category></entry><entry><title>How to turn Wikipedia into a Quiz Game</title><link href="https://pyvideo.org/pycon-italia-2017/how-to-turn-wikipedia-into-a-quiz-game.html" rel="alternate"></link><published>2017-04-08T00:00:00+00:00</published><updated>2017-04-08T00:00:00+00:00</updated><author><name>Andrea Cappelli</name></author><id>tag:pyvideo.org,2017-04-08:pycon-italia-2017/how-to-turn-wikipedia-into-a-quiz-game.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Quiz games let people test their knowledge through multi-choice
questions. Unfortunately, generating such questions can be very
time-consuming and it is typically done manually. In this talk we
will present a pipeline to automatically generate quiz games starting
from generic knowledge (e.g. Wikipedia). The pipeline consists of the
following components: (i) a parser to retrieve text from Wikipedia
pages, (ii) a Natural Language Processing module (based on the Google
Natural Language API) to extract information about syntax, entities
and relations, (iii) a Natural Language Generation module to generate
test questions and correct answers, and finally (iv) a domain-aware
module that uses domain-specific knowledge to generate wrong answers
(i.e. distractors). Every module is written in Python and it is based
on either available libraries or Cloud services (e.g., Google Natural
Language).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: basic knowledge of python, minimal understanding
of text syntactic analysis.&lt;/p&gt;
</summary><category term="nlp"></category><category term="AI"></category><category term="games"></category><category term="computational-linguistics"></category><category term="data-analysis"></category><category term="text-analysis"></category><category term="google-cloud"></category><category term="Artificial Intelligence"></category><category term="linked-data"></category></entry><entry><title>Word Embeddings for Natural Language Processing in Python</title><link href="https://pyvideo.org/pycon-italia-2017/word-embeddings-for-natural-language-processing-in-python.html" rel="alternate"></link><published>2017-04-08T00:00:00+00:00</published><updated>2017-04-08T00:00:00+00:00</updated><author><name>Marco Bonzanini</name></author><id>tag:pyvideo.org,2017-04-08:pycon-italia-2017/word-embeddings-for-natural-language-processing-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Word embeddings are a family of Natural Language Processing (NLP)
algorithms where words are mapped to vectors in low-dimensional space.
The interest around word embeddings has been on the rise in the past few
years, because these techniques have been driving important improvements
in many NLP applications like text classification, sentiment analysis or
machine translation.&lt;/p&gt;
&lt;p&gt;In this talk we’ll describe the intuitions behind this family of
algorithms, we’ll explore some of the Python tools that allow us to
implement modern NLP applications and we’ll conclude with some practical
considerations.&lt;/p&gt;
</summary><category term="Python"></category><category term="nlp"></category><category term="text-analysis"></category><category term="pydata"></category></entry><entry><title>DeepCare Chatbot - Generating answers to customers using Deep Learning and NLP</title><link href="https://pyvideo.org/pydata-barcelona-2017/deepcare-chatbot-generating-answers-to-customers-using-deep-learning-and-nlp.html" rel="alternate"></link><published>2017-05-20T15:45:00+02:00</published><updated>2017-05-20T15:45:00+02:00</updated><author><name>Pascal van Kooten</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/deepcare-chatbot-generating-answers-to-customers-using-deep-learning-and-nlp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The DeepCare chatbot is capable of learning to answer customer questions. Using a hybrid approach of NLP and Deep Learning, it tries to combat logical fallacies that occur in pure deep learning bots, while still coming up with unique answers.&lt;/p&gt;
&lt;p&gt;In particular, it uses a sequence-to-sequence (seq2seq) long-short-term-memory LSTM deep learning model to capture intricacies in questions. As organisations cannot afford a bot making logical mistakes, verification through NLP is used. This two-step model prevents the downside of &amp;quot;no control&amp;quot; on deep learning, as well as the too static nature of classical rule based NLP models, and thus enables potentially higher quality answers.&lt;/p&gt;
&lt;p&gt;A live demo will be available at &lt;a class="reference external" href="http://deepcare.online"&gt;http://deepcare.online&lt;/a&gt; on the day of the talk.&lt;/p&gt;
</summary><category term="chatbot"></category><category term="nlp"></category><category term="deep learning"></category></entry></feed>