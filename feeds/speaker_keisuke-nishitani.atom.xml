<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Keisuke Nishitani</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_keisuke-nishitani.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2021-10-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Let's make your data pipeline robust with Great Expectations! – Keisuke Nishitani(PyCon Taiwan 2021)</title><link href="https://pyvideo.org/pycon-taiwan-2021/lets-make-your-data-pipeline-robust-with-great-expectations-keisuke-nishitanipycon-taiwan-2021.html" rel="alternate"></link><published>2021-10-03T00:00:00+00:00</published><updated>2021-10-03T00:00:00+00:00</updated><author><name>Keisuke Nishitani</name></author><id>tag:pyvideo.org,2021-10-03:/pycon-taiwan-2021/lets-make-your-data-pipeline-robust-with-great-expectations-keisuke-nishitanipycon-taiwan-2021.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Day 2, 11:30-12:00&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Imagine you are a data engineer and in charge of a data pipeline. What do you think is the most important thing for the data pipeline? I think it is definitely the quality of data! However, the more complex your data pipeline becomes …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Day 2, 11:30-12:00&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Imagine you are a data engineer and in charge of a data pipeline. What do you think is the most important thing for the data pipeline? I think it is definitely the quality of data! However, the more complex your data pipeline becomes, the harder it is to maintain the data quality. For example, what if the format of some source data is changed without being noticed? What if some program update includes a bug? Such things cause data issues. It can take a long time to find the issues. Or even worse, your stakeholders may find the issues before you do! Great Expectations helps you solve such problems. It is a Python-based open-source library for validating, documenting, and profiling your data. It allows you to define the shape of data, test data, and document the results. In this talk, I will introduce you Great Expectations and share my experience with it. Let's make your data pipeline robust with Great Expectations!&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;Great Expectations A Python-based open-source library for validating, documenting, and profiling your data.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1ttwYJdFYLT9x87fusVAPBgcxZpEAE3g7AZ5eic34I7s/edit#slide=id.p"&gt;https://docs.google.com/presentation/d/1ttwYJdFYLT9x87fusVAPBgcxZpEAE3g7AZ5eic34I7s/edit#slide=id.p&lt;/a&gt;
HackMD: &lt;a class="reference external" href="https://hackmd.io/&amp;#64;pycontw/2021/%2F%40pycontw%2FHJRVK7qMt"&gt;https://hackmd.io/&amp;#64;pycontw/2021/%2F%40pycontw%2FHJRVK7qMt&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speaker: Keisuke Nishitani&lt;/p&gt;
&lt;p&gt;A data engineer and python programmer in Osaka, Japan. Working on a data pipeline built on Amazon Redshift, Amazon S3 and AWS Lambda. Interested in data workflow frameworks, data analysis and data visualization.&lt;/p&gt;
</content><category term="PyCon Taiwan 2021"></category></entry></feed>