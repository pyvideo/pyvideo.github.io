<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sun, 09 Oct 2016 00:00:00 +0000</lastBuildDate><item><title>Becoming a Data Scientist Advice From My Podcast Guests</title><link>https://pyvideo.org/pydata-dc-2016/becoming-a-data-scientist-advice-from-my-podcast-guests.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Overwhelmed by the vast resources (of varying quality) available online for learning data science? In this talk, I compile resources from data scientists on twitter, advice from guests of my podcast, and some of my own experience to help get you started on the path to Becoming a Data Scientist.&lt;/p&gt;
&lt;p&gt;The options for learning data science online are vast and overwhelming, but it is possible to find great resources that work well for you and learn data science without going back to school if you know how to approach it.&lt;/p&gt;
&lt;p&gt;On my &amp;quot;Becoming a Data Scientist&amp;quot; podcast, I have interviewed 17 data scientists (or those on the way to becoming data scientists) about their career paths and how they learned data science. I also interact with hundreds of data scientists regularly on Twitter. In this talk, I compile the frequent advice and the best resources, and give my answers to some common questions about how to become a data scientist.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Renee Teate</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/becoming-a-data-scientist-advice-from-my-podcast-guests.html</guid><category>Data</category></item><item><title>Bot or Not? The Illusion of Intelligence</title><link>https://pyvideo.org/pydata-dc-2016/bot-or-not-the-illusion-of-intelligence.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Chatbots have become wildly popular interfaces for online services we use everyday. While these bots are not going to pass the Turing Test, they provide a platform to use natural language to perform a task. This talk covers the basics of chatbot development using Python NLP and Deep Learning libraries and provides a demonstration of Malbot, a simple bot that can converse about a piece of malware.&lt;/p&gt;
&lt;p&gt;Chatbots are programs that employ rules or artificial intelligence that users interact with via a dialogue interface. Chatbots are extremely useful tools for accomplishing a range of tasks. From ordering pizza to checking the weather to buying shoes, there's a chatbot for that. This bot revolution has led to the creation of several Make-A-Bot services (all with really cool .ai domains!), but these bots are not extremely powerful examples of artificial intelligence. Most are simple template-based implementations that can be developed independently in Python.&lt;/p&gt;
&lt;p&gt;This talk will provide an overview on the structure of chatbots and how tools such as SpaCy, NLTK, sklearn, and Keras can be used to develop bots ranging from a basic weather bot to a more sophisticated deep learning neural conversation model. The applicability of these tools will be demonstrated through MalBot, a chatbot capable of ingesting a piece of malware and responding to natural language inquiries into the malicious software’s capabilities. (e.g. &amp;quot;Do you record key strokes?&amp;quot;, &amp;quot;What malware family are you from?&amp;quot;)&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bobby Filar</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/bot-or-not-the-illusion-of-intelligence.html</guid></item><item><title>Closing Session</title><link>https://pyvideo.org/pydata-dc-2016/closing-session.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016 Closing Notes&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">James Powell</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/closing-session.html</guid></item><item><title>Dask for ad hoc distributed computing</title><link>https://pyvideo.org/pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk discusses parallel and distributed computing in Python, particularly for ad-hoc and custom algorithms. It focuses on Dask, a Python solution for flexible distributed computing.&lt;/p&gt;
&lt;p&gt;The Python data science stack contains efficient algorithms with intuitive interfaces for sophisticated and friendly analysis. As the data science community tackles larger problems with larger hardware we naturally ask how best to parallelize this software stack both across many cores in a single computer and across computers in a cluster. This turns out to be harder than it looks, even with traditional Big Data tools like MapReduce, Storm, and Spark. Both the complexity of the algorithms and the high expectations for interactivity raise challenges for these systems. This talk lays out the benefits and challenges of parallelizing a numeric analytic stack, and then describes Dask, a parallel framework gaining traction within the Python community for interactive performant parallel computing, and finally goes through a few domains where this work is enabling novel science today.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matthew Rocklin</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html</guid><category>dask</category><category>distributed</category></item><item><title>Data Transformation: A Framework for Exploratory Data Analysis</title><link>https://pyvideo.org/pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Exploratory data analysis plays a critical role in the job of every data scientist, but very few have a structured process or framework for exploring data quickly and efficiently. This talk will introduce the exploratory framework I use in my day-to-day work and will walk attendees through a practical example of how to use the framework to unlock hidden insights with the help of Python libraries.&lt;/p&gt;
&lt;p&gt;At the heart of data analysis, there lies a need to understand the real world entities being represented in the data. Every data set we encounter is an attempt to capture a slice of our complex world and communicate some information about it in a way that has potential to be informative to humans, machines, or both. Moving from basic analyses to advanced analytics requires the ability to imagine multiple ways of conceptualizing the composition of entities and the relationships present in our data. It also requires the realization that different levels of aggregation, disaggregation, and transformation can open up new pathways to understanding our data and identifying the valuable insights it contains.&lt;/p&gt;
&lt;p&gt;In this talk, we’ll discuss several ways to think about the composition and representation of our data. We’ll also demonstrate a series of methods that leverage tools like networks, hierarchical aggregations, and unsupervised clustering to visually explore our data, transform it to discover new insights, help frame analytical problems and questions, and even improve machine learning model performance. In exploring these approaches, and with the help of Python libraries such as Pandas, Scikit-Learn, Seaborn, and NetworkX, we will provide a practical framework for thinking creatively and visually about your data and unlocking latent value and insights hidden deep beneath its surface.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tony Ojeda</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html</guid><category>analysis</category><category>Data</category><category>Data Analysis</category><category>framework</category></item><item><title>Design Principles</title><link>https://pyvideo.org/pydata-dc-2016/design-principles.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">James Powell</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/design-principles.html</guid><category>Design</category></item><item><title>Dev Ops meets Data Science Taking models from prototype to production with Docker</title><link>https://pyvideo.org/pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We present the evolution of a model to a production API that can scale to large e-commerce needs. On the journey we discuss metrics of success and how to use the Kubernetes cluster manager and associated tools for deploy. In addition to the use of these tools we highlight how to make use of the cluster management system for further testing and experimentation with your models.&lt;/p&gt;
&lt;p&gt;The chasm between data science and dev ops is often wide and impenetrable, but the two fields have more in common than meets the eye. Every data scientist will be able to lean in and help their career by investing in a basic understanding the basic principles of dev ops. In this talk I present the notions of service level indicators, objectives, and agreements. I cover the rigorous monitoring and testing of services. Finally we demonstrate how to build a basic data science workflow and push to production level APIs with Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes is an opinionated container cluster manager with an easy to use, robust interface. It can be use on very small and very large clusters. Docker is a container system that allows one to build code in an isolated environment. Paired with a container manager such as Kubernetes we are able to manage millions of instances as needed for a production deployment. These tools are two of many different options but are considered among the best open source solutions available.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andy Terrel</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</guid><category>Data</category><category>data science</category><category>docker</category><category>models</category><category>science</category></item><item><title>Dynamics in Graph Analysis Adding Time as a Structure for Visual and Statistical</title><link>https://pyvideo.org/pydata-dc-2016/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Network analyses are powerful methods for both visual analytics and machine learning but can suffer as their complexity increases. By embedding time as a structural element rather than a property, we will explore how time series and interactive analysis can be improved on Graph structures. Primarily we will look at decomposition in NLP-extracted concept graphs using NetworkX and Graph Tool.&lt;/p&gt;
&lt;p&gt;Modeling data as networks of relationships between entities can be a powerful method for both visual analytics and machine learning; people are very good at distinguishing patterns from interconnected structures, and machine learning methods get a performance improvement when applied to graph data structures. However, as these structures become more complex or embed more information over time, both visual and algorithmic methods get messy; visual analyses suffer from the &amp;quot;hairball&amp;quot; effect, and graph algorithms require either more traversal or increased computation at each vertex. A growing area to reduce this complexity and optimize analytics is the use of interactive and subgraph techniques that model how graph structures change over time.&lt;/p&gt;
&lt;p&gt;In this talk, I demonstrate two practical techniques for embedding time into graphs, not as computational properties, but rather as structural elements. The first technique is to add time as a node to the graph, which allows the graph to remain static and complete, but minimizes traversals and allows filtering. The second is to represent a single graph as multiple subgraphs where each is a snapshot at a particular time. This allows us to use time series analytics on our graphs, but perhaps more importantly, to use animation or interactive methodologies to visually explore those changes and provide meaningful dynamics.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Benjamin Bengfort</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical.html</guid><category>analysis</category></item><item><title>ElasticSearch and Redis How and When to Use Them</title><link>https://pyvideo.org/pydata-dc-2016/elasticsearch-and-redis-how-and-when-to-use-them.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;When working with data, you have some viable options for keeping them in short and long-term storage. I will be going over why ElasticSearch and Redis are great for data storage. This talk will explain the purpose of each datastore when visualizing on Kibana or websockets.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Marcinowski</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/elasticsearch-and-redis-how-and-when-to-use-them.html</guid><category>elasticsearch</category><category>redis</category></item><item><title>Exposing Algorithms</title><link>https://pyvideo.org/pydata-dc-2016/exposing-algorithms.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Algorithms have become an integral part of our everyday lives. While algorithms can make our lives simpler and make decisions faster, there is a growing need for algorithms to be transparent and for the users of those algorithms to be accountable for the automated decisions made by them. This talk covers where algorithms are used, how they can go wrong, and how they can be investigated.&lt;/p&gt;
&lt;p&gt;An algorithm is set of steps that perform calculations, process data, or automate tasks. Algorithms are everywhere we look (and even places we don’t look) controlling what we see, do, and where we go. They’re great for solving our problems and helping us make better and quicker decisions, or taking the decision-making out of our hands. Their guidance is perfect in their objective and unbiased calculation. Except they are not, actually. Like everything else, they are created by people, and people have biases that get encoded into the algorithms they create. Algorithms learn from data, which is also created by people, so the algorithms also learn biases from data. This can be a problem when algorithms encode these biases into their calculations and go on to perpetuate the bias.&lt;/p&gt;
&lt;p&gt;In this talk you will hear why we should care about algorithmic accountability, and details on a case study on how computational journalism can be used to investigate algorithms and advocate the need for transparency and accountability.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jennifer Stark</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/exposing-algorithms.html</guid></item><item><title>GraphGen: Conducting Graph Analytics over Relational Databases</title><link>https://pyvideo.org/pydata-dc-2016/graphgen-conducting-graph-analytics-over-relational-databases.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/graphgen-conducting-graph-analytics-over-relational-databases-67649554"&gt;http://www.slideshare.net/PyData/graphgen-conducting-graph-analytics-over-relational-databases-67649554&lt;/a&gt;
Download and learn about GraphGen at: konstantinosx.github.io/graphgen-project/
DDL Blog Post and Tutorial at: blog.districtdatalabs.com/graph-analytics-over-relational-datasets
Note: Currently GraphGenPy is built for Python 2.0.  Python 3.0 support coming soon!&lt;/p&gt;
&lt;p&gt;Applying graph analytics on data stored in relational databases can provide tremendous value in many application domains. We discuss the importance of leveraging these analyses, and the challenges in enabling them. We present a tool, called GraphGen, that allows users to visually explore, and rapidly analyze (using NetworkX) different graph structures present in their databases.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Konstantinos Xirogiannopoulos</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/graphgen-conducting-graph-analytics-over-relational-databases.html</guid><category>databases</category></item><item><title>H2O Deep Water with Python early sneek</title><link>https://pyvideo.org/pydata-dc-2016/h2o-deep-water-with-python-early-sneek.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData 2016&lt;/p&gt;
&lt;p&gt;Python as a language for DeepLearning. Python is emerging as the facto language to specify Deep Learning Networks. In this talk we will explore some of the popular libraries like Tensorflow and Keras to see the semantics used to describe such networks and look a bit more under the hood at what is the python layer actually doing for these well known deep learning libraries.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fabrizio Milo</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/h2o-deep-water-with-python-early-sneek.html</guid><category>deep learning</category><category>tensorflow</category><category>keras</category></item><item><title>Improving PySpark Performance Spark performance beyond the JVM</title><link>https://pyvideo.org/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Holden Karau</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</guid><category>jvm</category><category>performance</category><category>pyspark</category><category>spark</category></item><item><title>Keynote: Extending from Open to Usable: A Commerce Data Conundrum</title><link>https://pyvideo.org/pydata-dc-2016/keynote-extending-from-open-to-usable-a-commerce-data-conundrum.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Keynote: Extending from Open to Usable: A Commerce Data Conundrum&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Star Ying</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/keynote-extending-from-open-to-usable-a-commerce-data-conundrum.html</guid><category>Data</category></item><item><title>Keynote: The Culture of Data Transformation</title><link>https://pyvideo.org/pydata-dc-2016/keynote-the-culture-of-data-transformation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">SriSatish Ambati</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/keynote-the-culture-of-data-transformation.html</guid><category>Culture</category><category>Data</category></item><item><title>Logistic Regression Behind the Scenes</title><link>https://pyvideo.org/pydata-dc-2016/logistic-regression-behind-the-scenes.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Logistic Regression models are powerful tools in the data science toolkit; in this talk we will explore various implementations of logistic regression in Python and SAS, with a focus on output and performance. We will also discuss both the numerical and statistical implications (including Bayesian interpretations) of the various options.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Christopher White</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/logistic-regression-behind-the-scenes.html</guid></item><item><title>Machine Learning Techniques for Class Imbalances &amp; Adversaries</title><link>https://pyvideo.org/pydata-dc-2016/machine-learning-techniques-for-class-imbalances-adversaries.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;There are many areas of applied Machine Learning which require models optimized for rare occurrences (i.e. class imbalance), as well as users actively attempting to subvert the system (i.e. adversaries).&lt;/p&gt;
&lt;p&gt;This talk will guide the audience through multiple published techniques which specifically attempt to address these issues.&lt;/p&gt;
&lt;p&gt;The Data Innovation Lab at Capital One has explored more advanced modeling techniques for class imbalance &amp;amp; adversarial actors. Our use case has allowed us to survey the many related fields which deal with these issues, and attempt many of the suggested modeling techniques. Additionally, we have introduce a few novel variations of our own.&lt;/p&gt;
&lt;p&gt;This talk will provide an introduction to the problem space, a brief overview of the modeling frameworks we've chosen to work with, a brief overview of our approaches, a discussion of lessons learned, and our proposed future work.&lt;/p&gt;
&lt;p&gt;The approaches discussed will include ensemble models, deep learning, genetic algorithms, outlier detection via dimensionally reduction (PCA and neural network auto-encoders), time-decay weighting, and Synthetic Minority Over-sampling Technique (SMOTE sampling).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Brendan Herger</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/machine-learning-techniques-for-class-imbalances-adversaries.html</guid><category>class</category><category>learning</category><category>machine learning</category></item><item><title>Making your code faster: Cython and parallel processing in the Jupyter Notebook</title><link>https://pyvideo.org/pydata-dc-2016/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/gapatino/Making-Your-Code-Faster-Cython-and-parallel-processing-in-the-Jupyter-Notebook"&gt;https://github.com/gapatino/Making-Your-Code-Faster-Cython-and-parallel-processing-in-the-Jupyter-Notebook&lt;/a&gt;
Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook"&gt;http://www.slideshare.net/PyData/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As the complexity and scope of applications grow, it is very common to run into slow performance issues. In Python, it is possible to improve the speed of execution with the use of parallel processing and the Cython compiler. The Jupyter Notebook makes the implementation of both of them a relatively simple task, which will be the focus of this session.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gustavo Patino</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook.html</guid><category>code</category><category>Cython</category><category>jupyter</category><category>jupyter notebook</category><category>notebook</category><category>parallel</category><category>processing</category></item><item><title>Open Data Dashboards &amp; Python Web Scraping</title><link>https://pyvideo.org/pydata-dc-2016/open-data-dashboards-python-web-scraping.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Distilling a world of data down to a few key indicators can be an effective way of keeping an audience informed, and this concept is at the heart of a good dashboard. This talk will cover a few methods of scraping and reshaping open data for dashboard visualization, to automate the boring stuff so you have more time and energy to focus on the analysis and content.&lt;/p&gt;
&lt;p&gt;This talk will cover a basic scenario of curating open data into visualizations for an audience. The main goal is to automate data scraping/downloading and reshaping. I use python to automate data gathering, and Tableau and D3 as visualization tools -- but the process can be applied to numerous analytical/visualization suites.&lt;/p&gt;
&lt;p&gt;I'll discuss situations where a dashboard makes sense (and when one doesn't). I will make a case also that automation makes for a more seamless data gathering and updating process, but not always for smarter data analysis.&lt;/p&gt;
&lt;p&gt;Some python packages I'll cover for web scraping and downloading/reshaping open data include: openpyxl, pandas, xlsxwriter, and BeautifulSoup. I'll also touch on APIs.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marie Whittaker</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/open-data-dashboards-python-web-scraping.html</guid><category>Data</category><category>scraping</category><category>web</category></item><item><title>Sustainable scrapers</title><link>https://pyvideo.org/pydata-dc-2016/sustainable-scrapers.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scraping data from the web is an essential skill, whether you want to or not. Learn the code and systems tricks that go into testable, fast, low-maintenance scraping gleaned from years of real world practice.&lt;/p&gt;
&lt;p&gt;A recent NPR project that collects structured data about gun sale listings from Armslist.com demonstrates several of the speaker's favorite tricks for writing simple, fast scrapers with Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Eads</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/sustainable-scrapers.html</guid></item><item><title>Triaging Feedback Form Data</title><link>https://pyvideo.org/pydata-dc-2016/triaging-feedback-form-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk will cover how to use predictive modeling on unstructured text data including feedback form, social media or chat message data to triage issues in order to prevent future problems with a service, platform or user interface using NLP techniques in Python and R.&lt;/p&gt;
&lt;p&gt;Companies gain useful insights about their users from feedback form and other unstructured text data including live chat messages. Even though they are read and responded to, often such data is ignored when thinking about larger scale trend analysis and this can result in missed insight about how users react to a product or service. Sometimes analysis is being done by looking at changes in user sentiment or other heuristics, however it could be taken a step further by applying predictive modeling in attempt to recognize areas that need more attention and support. While you can use predictive modeling on network and log data, that is looking at how the hardware is handling your users requests, not how it's being perceived by users. By predicting areas where users are having difficulty whether it's with the UI or with the platform's response time you can triage these areas of concern to prevent future cases of negative perception. This talk will cover how to utilize common NLP tools used to gather and process the features in Python then will use R to perform trend analysis and predictive modeling then use the results to triage what areas should be focused on in the future.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Stephanie Kim</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/triaging-feedback-form-data.html</guid><category>Data</category></item><item><title>Visual diagnostics for more informed machine learning</title><link>https://pyvideo.org/pydata-dc-2016/visual-diagnostics-for-more-informed-machine-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Visualization has a critical role to play throughout the analytic process. Where static outputs and tabular data can obscure patterns, human visual analysis can open up insights that lead to more robust data products. For Python programmers who dabble in machine learning, visual diagnostics are a must-have for effective feature analysis, model selection, and parameter tuning.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rebecca Bilbro</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/visual-diagnostics-for-more-informed-machine-learning.html</guid><category>learning</category><category>machine learning</category></item><item><title>You got your engineering in my Data Science: Addressing the reproducibility crisis</title><link>https://pyvideo.org/pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering"&gt;http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. Unfortunately, multiple recent studies have been found to be unreliable and non-reproducible. Adopting techniques from software engineering might help mitigate some of these problems.&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. It makes sense of everything from cancer trials to package delivery logistics. But all is not well with data science. Over the past decade, multiple studies have been found to be unreliable and non-reproducible when other scientists tried to recreate their results. This is due to a variety of factors, including fraud, pressure to publish, improper data handling practices, and bugs in analytic tools.&lt;/p&gt;
&lt;p&gt;The problems faced by data science mirror problems that software engineering has been trying to solve. While there are no silver bullets to guarantee quality software, techniques have been developed over time that have improved quality and reliability. Some of these techniques, including open source, version control, automation, and fuzzing could be adapted to the data science domain to improve reliability and help address the reproducibility crisis.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jon Bodner</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</guid><category>Data</category><category>data science</category><category>engineering</category><category>reproducibility</category></item><item><title>A Practical Guide to Dimensionality Reduction Techniques</title><link>https://pyvideo.org/pydata-dc-2016/a-practical-guide-to-dimensionality-reduction-techniques.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk provides a step-by-step overview and demonstration of several dimensionality (feature) reduction techniques. Attendees should have some basic level of understanding of data wrangling and supervised learning. The presentation will also include snippets of Python code, so familiarity with Python code will be useful.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Vishal Patel</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/a-practical-guide-to-dimensionality-reduction-techniques.html</guid></item><item><title>Agent based modeling in Python</title><link>https://pyvideo.org/pydata-dc-2016/agent-based-modeling-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Agent-based modeling is a technique used to explore both complexity and emergence by simulating individual actors and their actions within a system. Think of systems such as the traffic in a city, or like those in financial markets where one actor can have an effect on the decisions of others until the system’s direction changes its course. In this talk, you will learn about ABMs in Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jackie Kazil</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/agent-based-modeling-in-python.html</guid></item><item><title>Bayesian Network Modeling using R and Python</title><link>https://pyvideo.org/pydata-dc-2016/bayesian-network-modeling-using-r-and-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Bayesian Networks (BN) are increasingly being applied for real-world data problems. They provide the much desired complexity in representing the uncertainty of the predicted results of a model. The networks are easy to follow and better understand the relationships of the attributes of the dataset. As part of this talk, we will look into the existing R and Python packages that enables BN usage.&lt;/p&gt;
&lt;p&gt;Bayesian Networks are increasingly being applied for real-world data problems. They provide the much desired complexity in representing the uncertainty of the predicted results of a model. The networks are easy to follow and better understand the inter-relationships of the different attributes of the dataset. As part of this talk, we will look into the existing R and Python packages that enable BN learning and prediction. The pros and cons of the available packages will be discussed as well as new capabilities that will broaden the application of BN networks.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Pragyansmita Nayak</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/bayesian-network-modeling-using-r-and-python.html</guid><category>bayesian</category><category>network</category></item><item><title>Beyond Bag of Words A Practitioner's Guide to Advanced NLP</title><link>https://pyvideo.org/pydata-dc-2016/beyond-bag-of-words-a-practitioners-guide-to-advanced-nlp.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We offer a foundation in building intelligent business applications using machine learning, walking you through all the steps to prototyping and production—data cleaning, feature engineering, model building and evaluation, and deployment—and diving into an application for anomaly detection and a personalized recommendation engine. All concepts will be presented with example code in Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ariel M'ndange-Pfupfu</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/beyond-bag-of-words-a-practitioners-guide-to-advanced-nlp.html</guid><category>advanced</category></item><item><title>Building a (semi) Autonomous Drone with Python</title><link>https://pyvideo.org/pydata-dc-2016/building-a-semi-autonomous-drone-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;They might not be delivering our mail (or our burritos--tacocopter.com) yet but drones are now simple, small, and affordable enough that they can be considered a toy. You can even customize and program some of them! The Parrot AR Drone has an API that let's you control not only the drone's movement but also stream video and images from both of its cameras. I'll show you how you can use Python and node.js to build a drone that moves all by itself.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Greg Lamp</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/building-a-semi-autonomous-drone-with-python.html</guid></item><item><title>Building Continuous Learning Systems</title><link>https://pyvideo.org/pydata-dc-2016/building-continuous-learning-systems.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;In this talk we explore how to build Machine Learning Systems that can that can learn &amp;quot;continuously&amp;quot; from their mistakes (feedback loop) and adapt to an evolving data distribution.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anuj Gupta</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/building-continuous-learning-systems.html</guid><category>learning</category></item><item><title>Building Serverless Machine Learning Models in the Cloud</title><link>https://pyvideo.org/pydata-dc-2016/building-serverless-machine-learning-models-in-the-cloud.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;You’ll learn how to efficiently design and train machine learning models in Python and deploy them to the cloud. This process reduces the development &amp;amp; operational efforts required to make your prototypes production-ready.&lt;/p&gt;
&lt;p&gt;We will describe the main challenges faced by data scientists involved in deploying machine learning models into real production environments with specific references, examples of Python libraries, and multi-model systems requiring advanced features such as A/B testing and high scalability &amp;amp; availability.&lt;/p&gt;
&lt;p&gt;While discussing the advantages and limitations of multiple deployment strategies in the cloud, we will focus on serverless computing (i.e. AWS Lambda) as a solution for simplifying your development &amp;amp; deployment workflows.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Casalboni</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/building-serverless-machine-learning-models-in-the-cloud.html</guid><category>Cloud</category><category>learning</category><category>machine learning</category><category>models</category><category>serverless</category></item><item><title>Clustering: A Guide for the Perplexed</title><link>https://pyvideo.org/pydata-dc-2016/clustering-a-guide-for-the-perplexed.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Finding clusters is a powerful tool for understanding and exploring data. While the task sounds easy, it can be surprisingly difficult to do it well. Most standard clustering algorithms can, and do, provide very poor clustering results in many cases. We discuss how to do clustering correctly.&lt;/p&gt;
&lt;p&gt;Finding clusters is a powerful tool for understanding and exploring data. While the task sounds easy, it can be surprisingly difficult to it well. Most standard clustering algorithms can, and do, provide very poor clustering results in many cases. Our intuitions for what a cluster is are not as clear as we would like, and can easily be lead astray. We will attempt to find a definition of clustering that makes sense for most cases, and introduce an algorithm for finding such clusters, along with a high performance python implementation of the algorithm, building up more intuition for what clustering really means as we go.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Leland McInnes</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/clustering-a-guide-for-the-perplexed.html</guid></item><item><title>Creating a Contemporary Lending Risk Management System Using Python</title><link>https://pyvideo.org/pydata-dc-2016/creating-a-contemporary-lending-risk-management-system-using-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Lending involves risk and in order to be a successful lender at scale that risk needs to be mitigated. We'll be discussing how C2FO has built a suite of risk management tools for underwriting and portfolio management using the PyData ecosystem, rpy2 (for integrating R), and Spyre (for building a simple web application).&lt;/p&gt;
&lt;p&gt;Engineering a sophisticated in-house risk management solution for a commercial lending platform doesn't necessarily need to involve millions of lines of low-level code, clunky desktop applications, fancy front-end development, or messy spreadsheets. That is, so long as the problem is approached objectively and the solutions are evaluated critically.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Piero Ferrante</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/creating-a-contemporary-lending-risk-management-system-using-python.html</guid></item><item><title>Creating Python Data Pipelines in the Cloud</title><link>https://pyvideo.org/pydata-dc-2016/creating-python-data-pipelines-in-the-cloud.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;My talk will be an analysis of the various approaches to creating data pipelines the public cloud using Python.I will compare and contrast using various Python libraries such as Luigi, Airflow and native cloud frameworks such as Cloud Dataflow (Google), AWS Data Pipeline to create a real world data pipeline in Amazon AWS and Google Compute Engine.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Femi Anthony</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/creating-python-data-pipelines-in-the-cloud.html</guid><category>Cloud</category><category>Data</category></item><item><title>Data Sciencing While Female</title><link>https://pyvideo.org/pydata-dc-2016/data-sciencing-while-female.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;How can we increase the number of female data scientists in the workplace? By building a community. Dr. Amanda Traud was the only woman on her data science team when she started the group Women Data Scientists DC. In one year, the group grew to over 1,000 members. Dr. Traud will discuss the ups and downs of being a woman in data science and how to encourage and include more women in the field.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mandi Traud</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/data-sciencing-while-female.html</guid><category>Data</category></item><item><title>Eat Your Vegetables Data Security for Data Scientists</title><link>https://pyvideo.org/pydata-dc-2016/eat-your-vegetables-data-security-for-data-scientists.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/WilliamVoorhees1/eat-your-vegetables-data-security-for-data-scientists"&gt;http://www.slideshare.net/WilliamVoorhees1/eat-your-vegetables-data-security-for-data-scientists&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You've got data: lots of it. People want to get their hands on that data. You don't want that, so let's go over a few things you can do to dissuade attackers from getting their grubby mitts on your hard processed datastore. We'll cover the obvious things (spoiler alert: encryption) and then some advanced techniques for keeping data secure while still keeping it usable (that is to say, analyzable).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Will Voorhees</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/eat-your-vegetables-data-security-for-data-scientists.html</guid><category>Data</category><category>Security</category></item><item><title>Forecasting critical food violations at restaurants using open data</title><link>https://pyvideo.org/pydata-dc-2016/forecasting-critical-food-violations-at-restaurants-using-open-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Dc 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/NicoleDonnelly6/pydatadc-forecasting-critical-food-violations-at-restaurants-using-open-data"&gt;http://www.slideshare.net/NicoleDonnelly6/pydatadc-forecasting-critical-food-violations-at-restaurants-using-open-data&lt;/a&gt;
Github: &lt;a class="reference external" href="https://github.com/nd1/DC_RestaurantViolationForecasting"&gt;https://github.com/nd1/DC_RestaurantViolationForecasting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As many as 105 million Americans suffer from foodborne illness annually. In 2014, the City of Chicago began forecasting these outbreaks targeting limited health inspection resources toward likely sites, showing a 7 day improvement in locating critical violations at food establishments. This talk provides an end-to-end walkthrough of predicting critical violations in Washington, DC using Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicole Donnelly</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/forecasting-critical-food-violations-at-restaurants-using-open-data.html</guid><category>Data</category></item><item><title>From rocks to a hammer when and how to change your company's analytical tools</title><link>https://pyvideo.org/pydata-dc-2016/from-rocks-to-a-hammer-when-and-how-to-change-your-companys-analytical-tools.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Many organizations still rely on SPSS/SAS to do most of their analytical work. These tools are not only very costly ($10k+ per year per license), but are also limited (no scripting ability, very manual). In 2015, we began transitioning into Python to build robust tools and to reduce operational costs. Along the way, we learned a lot about propagating new tools within a company, reverse engineering, and helping others adjust to a new paradigm. This talk will outline our process of evaluating new tools, initial adoption and companywide propagation. One of the main findings was that open source does not mean free, and we had to take into account each person's experience and comfort in devising our implementation strategy. Lastly, we had to develop our own internal library in order to maintain functionality.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sebastien Genty</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/from-rocks-to-a-hammer-when-and-how-to-change-your-companys-analytical-tools.html</guid><category>tools</category></item><item><title>Fuzzy Search Algorithms How and When to Use Them</title><link>https://pyvideo.org/pydata-dc-2016/fuzzy-search-algorithms-how-and-when-to-use-them.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;So much of data science is about understanding the context around your data. In this talk, we hope to address how to work with messy text data by leveraging fuzzy search algorithms in python or against a database such as PostgreSQL. We will talk specifically about fuzzy algorithms such as Soundex, Trigram/n-gram search, and Levenshtein distances and demonstrate use cases in an ipython notebook.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jiaqi Liu</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/fuzzy-search-algorithms-how-and-when-to-use-them.html</guid><category>search</category></item><item><title>How I learned to time travel, or, data pipelining and scheduling with Airflow</title><link>https://pyvideo.org/pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418"&gt;http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data warehousing and analytics projects can, like ours, start out small - and fragile. With an organically growing mess of scripts glued together and triggered by cron jobs hiding on different servers, we needed better plumbing. After perusing the data pipelining landscape, we landed on Airflow, an Apache incubating batch processing pipelining and scheduler tool from Airbnb.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Laura Lorenz</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html</guid><category>airflow</category><category>Data</category></item><item><title>JupyterLab: Building Blocks for Interactive Computing</title><link>https://pyvideo.org/pydata-dc-2016/jupyterlab-building-blocks-for-interactive-computing.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData D 2016&lt;/p&gt;
&lt;p&gt;JupyterLab is an extensible web application (still in alpha) for scientific computation. Users arrange multiple notebooks, editors, terminals, and custom components with tabs, splitters, and sidebars. JupyterLab also has a file browser, command palette and quick help system. We demonstrate JupyterLab and recent developments and show how JupyterLab fits in the vision of the Jupyter project.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Grout</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/jupyterlab-building-blocks-for-interactive-computing.html</guid></item><item><title>Keynote: Become a Data Superhero How Data Can Change the World</title><link>https://pyvideo.org/pydata-dc-2016/keynote-become-a-data-superhero-how-data-can-change-the-world.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;The capacity to gather and interpret data can be low for many nonprofits. Working together, data scientists and organizations can be a world-changing combination. Byte Back has found a way to use data analysis for good and will help you learn how to tap into your own superpowers.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Elizabeth Lindsey</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-become-a-data-superhero-how-data-can-change-the-world.html</guid><category>Data</category></item><item><title>Keynote: Building a Data Driven Dialogue From Filling Potholes to Disrupting the Cycle</title><link>https://pyvideo.org/pydata-dc-2016/keynote-building-a-data-driven-dialogue-from-filling-potholes-to-disrupting-the-cycle.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kelly Jin</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-building-a-data-driven-dialogue-from-filling-potholes-to-disrupting-the-cycle.html</guid><category>Data</category></item><item><title>Keynote: How Open Data Science Opens the World of Innovation</title><link>https://pyvideo.org/pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Innovation today appears to be instantaneous in large part due to open source technology. Open Data Science is no exception. Python, a pillar in the Open Data Science bedrock, is well positioned to harvest innovation in software and with Anaconda, it’s also well positioned to capitalize on the latest hardware innovations. Anaconda and Intel are blazing a path for the Python community to take advantage of cognitive computing, including machine learning and deep learning.&lt;/p&gt;
&lt;p&gt;In this keynote, Peter and Robert will talk about how Open Data Science––a connected ecosystem of data, analytics and compute––streamlines the path to high performance and innovation to achieve breakthrough results.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Robert Cohn</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</guid><category>Data</category><category>data science</category><category>science</category></item><item><title>NoSQL doesn't mean No Schema</title><link>https://pyvideo.org/pydata-dc-2016/nosql-doesnt-mean-no-schema.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;While databases like MongoDB don't require a formal schema, there's still a schema somewhere. It might be merely implied by validation rules in the code. Or, there might be a more formal representation. In some cases, the lack of strict schema creates a dynamic flexibility that creates value rapidly. Other times, the lack of formal structures leads to chaos. How can we find a balance?&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Steven Lott</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/nosql-doesnt-mean-no-schema.html</guid><category>nosql</category></item><item><title>Predicting Usage for Capital Bikeshare stations based upon Spatial Characteristics</title><link>https://pyvideo.org/pydata-dc-2016/predicting-usage-for-capital-bikeshare-stations-based-upon-spatial-characteristics.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;A step after trying to model Continuous variables from Regression models, lies a very rewarding problem of estimating decisions which more lie in the discrete domain. In this talk, we work towards developing Logistic models to predict traffic for Capital Bikeshare, and work towards finding optimal station locations for Network expansion.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Darshan Pandit</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/predicting-usage-for-capital-bikeshare-stations-based-upon-spatial-characteristics.html</guid><category>spatial</category></item><item><title>Promoting a data driven culture in a world of microservices</title><link>https://pyvideo.org/pydata-dc-2016/promoting-a-data-driven-culture-in-a-world-of-microservices.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/promoting-a-data-driven-culture-in-a-microservices-environment"&gt;http://www.slideshare.net/PyData/promoting-a-data-driven-culture-in-a-microservices-environment&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At Hudl, we give every employee full access to our data warehouse, and over 50% of our employees have personally written a query against it. In this talk, I discuss our journey to democratize our data. I touch on technical and non-technical challenges, including the tools we use and the structure of our teams.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex DeBrie</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/promoting-a-data-driven-culture-in-a-world-of-microservices.html</guid><category>Culture</category><category>Data</category></item><item><title>Python useRs</title><link>https://pyvideo.org/pydata-dc-2016/python-users.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;According to the most recent IEEE language rankings, R is now the 5th most popular language. It is the only domain specific language in the top 5, behind the general-purpose languages C, Java, Python, and C++, respectively. It is common for data science teams to work in multiple languages and for the Pythonista, a working knowledge in R is useful for collaboration, analysis, and performance.&lt;/p&gt;
&lt;p&gt;It may be better to extend a simple analysis in R than passing around CSV files. Even worse, there might be a new analysis package that only exists in R, or certain bits infrastructure are already implemented in R and it needs to incorporate a bit of Python analysis.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Chen</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/python-users.html</guid></item><item><title>Scaling up to Big Data Devops for Data Science</title><link>https://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marck Vaisman</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</guid><category>big data</category><category>Data</category><category>data science</category><category>devops</category><category>scaling</category><category>science</category></item><item><title>Variational Inference in Python</title><link>https://pyvideo.org/pydata-dc-2016/variational-inference-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Jupyter notebook: &lt;a class="reference external" href="https://nbviewer.jupyter.org/gist/AustinRochford/91cabfd2e1eecf9049774ce529ba4c16"&gt;https://nbviewer.jupyter.org/gist/AustinRochford/91cabfd2e1eecf9049774ce529ba4c16&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Inference in Bayesian models often requires simulation to approximate posterior distributions. Variational inference (VI) instead approximates posteriors through optimization. Recent theoretical and computational advances in automatic variational inference have made VI more accessible. This talk will give an introduction to VI and show software packages for performing VI in Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Austin Rochford</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/variational-inference-in-python.html</guid></item><item><title>Beyond Sentiment Emotion Mining with Python and machine learning</title><link>https://pyvideo.org/pydata-dc-2016/beyond-sentiment-emotion-mining-with-python-and-machine-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Learn how to extract emotional content from textual data - and how to build a sentiment analysis tool that does not suck.&lt;/p&gt;
&lt;p&gt;Typical sentiment analysis tries to map the entire rich and varied world of human emotions into &amp;quot;good&amp;quot; vs &amp;quot;bad&amp;quot;. In this tutorial, we use the characters of &amp;quot;Inside Out&amp;quot; and machine learning to build a nuanced model of human emotions -- and put it in production!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Max Tsvetovat</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/beyond-sentiment-emotion-mining-with-python-and-machine-learning.html</guid><category>learning</category><category>machine learning</category></item><item><title>Building Your First Data Pipelines</title><link>https://pyvideo.org/pydata-dc-2016/building-your-first-data-pipelines.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;You need a data pipeline. This talk will discuss the lifecycle of projects using Jupyter notebooks &amp;amp; Luigi as a data pipeline management tool for a variety of projects, from greenfield to retrofitting complex systems. It will included a hands on demo.&lt;/p&gt;
&lt;p&gt;Data pipelines are hard. Too often we resort to retrofitting janky scripts, relying on keeping a readme up to data, etc.&lt;/p&gt;
&lt;p&gt;First, this proposal lays out the variety of tools that are available to build data pipelines. This talk will discuss why you should be using Luigi and how to use it in a variety of common use cases.&lt;/p&gt;
&lt;p&gt;Next, we will build a basic exploratory analysis using DC open data and Luigi to demonstrate the power of this concept and how it works with Jupyter.&lt;/p&gt;
&lt;p&gt;Finally, we'll retrofit a larger, more complex project to use Luigi to show how you can use it in bigger organizations.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hunter Owens</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/building-your-first-data-pipelines.html</guid><category>Data</category></item><item><title>Doing frequentist statistics with Scipy</title><link>https://pyvideo.org/pydata-dc-2016/doing-frequentist-statistics-with-scipy.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/gapatino/Doing-frequentist-statistics-with-Scipy"&gt;https://github.com/gapatino/Doing-frequentist-statistics-with-Scipy&lt;/a&gt;
Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/doing-frequentist-statistics-with-scipy"&gt;http://www.slideshare.net/PyData/doing-frequentist-statistics-with-scipy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Frequentist statistical tests are still very common, and in some fields they continue to represent the technical standard. In this session we will cover the execution and interpretation of the most common tests using the SciPy.stats package, and plotting the results with Matplotlib and Seaborn. The focus will be on traditional approaches to the tests, not on Bayesian and bootstrapping approaches&lt;/p&gt;
&lt;p&gt;The session will cover: - Normality testing - Student's t-test and ANOVA - Wilcoxon rank sum and Kruskal-Wallis - Correlation - Univariate linear and logistic regression - Chi-square - p-value interpretation - Effect size calculation&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gustavo Patino</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/doing-frequentist-statistics-with-scipy.html</guid><category>scipy</category><category>statistics</category></item><item><title>Educational framework for Black Box optimization methods design</title><link>https://pyvideo.org/pydata-dc-2016/educational-framework-for-black-box-optimization-methods-design.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Many pressing real world problems can be stated as problems of global optimization, where target function is a black box. Such problems are best approached with a library of optimization methods to help study the nature of the problem. We show how to use Scipy.optimize and Scikit-learn modules to create global optimization methods with desired properties.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nadia Udler</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/educational-framework-for-black-box-optimization-methods-design.html</guid><category>Design</category><category>framework</category><category>optimization</category></item><item><title>Getting started with H2O on Python</title><link>https://pyvideo.org/pydata-dc-2016/getting-started-with-h2o-on-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/h2oai/pydata2016-h2o-loganalysis"&gt;https://github.com/h2oai/pydata2016-h2o-loganalysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;H2O helps Python users make the leap from single machine based processing to large-scale distributed environments. Hadoop lets H2O users scale their data processing capabilities based on their current needs. Using H2O, Python, and Hadoop, you can create a complete end-to-end data analysis solution. In this presentation the speaker will show - and highlight a start to end process of how to design an algorithm, optimize, and implement it using H2O. The use case will focus much on the security work done between H2O and Capital One.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ashrith Barthur</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/getting-started-with-h2o-on-python.html</guid></item><item><title>How to Build Your Own Self Driving Toy Car</title><link>https://pyvideo.org/pydata-dc-2016/how-to-build-your-own-self-driving-toy-car.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;I’ve spent the past 6 months building a self-driving toy car using a Raspberry Pi, OpenCV, and TensorFlow. If you’ve ever thought about building your own self-driving toy car, this presentation will help you avoid common pitfalls and shed light on important tradeoffs that you’ll have to weigh along the way. I’ll cover things like how to parse images, how to effectively tune machine learning neural&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ryan Zotti</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/how-to-build-your-own-self-driving-toy-car.html</guid><category>opencv</category><category>tensorflow</category></item><item><title>Interactive multi scale time series exploration with matplotlib</title><link>https://pyvideo.org/pydata-dc-2016/interactive-multi-scale-time-series-exploration-with-matplotlib.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This tutorial is an introduction to how to matplotlib's event handling system to build an tool for interactively exploring multi-scale time series data.&lt;/p&gt;
&lt;p&gt;The primary example will be how to 'drill down' through summary data sets to view the underlying data using hourly weather data from NOAA.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Caswell</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/interactive-multi-scale-time-series-exploration-with-matplotlib.html</guid><category>matplotlib</category></item><item><title>Julia Tutorial</title><link>https://pyvideo.org/pydata-dc-2016/julia-tutorial.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/cc7768/PyDataDC_julia"&gt;https://github.com/cc7768/PyDataDC_julia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Julia is a programming language oriented around the just in time (JIT) compiler technology. This tutorial will be an introduction to Julia and the core concepts that make it a good choice for certain types of problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chase Coleman</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/julia-tutorial.html</guid><category>julia</category><category>tutorial</category></item><item><title>Learn how to Make Life Easier with Anaconda</title><link>https://pyvideo.org/pydata-dc-2016/learn-how-to-make-life-easier-with-anaconda.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Bring your laptop &amp;amp; get started with Anaconda, the leading platform for Open Data Science powered by Python. You'll get hands-on experience using conda &amp;amp; Anaconda Cloud to share notebooks, packages, codes, &amp;amp; environments and to get stuff done. No software pre-installation required (although installing Anaconda beforehand may be helpful).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dhavide Aruliah</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/learn-how-to-make-life-easier-with-anaconda.html</guid><category>anaconda</category></item><item><title>Machine Learning with Text in scikit learn</title><link>https://pyvideo.org/pydata-dc-2016/machine-learning-with-text-in-scikit-learn.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/justmarkham/pydata-dc-2016-tutorial"&gt;https://github.com/justmarkham/pydata-dc-2016-tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Although numeric data is easy to work with in Python, most knowledge created by humans is actually raw, unstructured text. By learning how to transform text into data that is usable by machine learning models, you drastically increase the amount of data that your models can learn from. In this tutorial, we'll build and evaluate predictive models from real-world text using scikit-learn.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/machine-learning-with-text-in-scikit-learn.html</guid><category>learning</category><category>machine learning</category><category>scikit</category></item><item><title>Modern NLP in Python</title><link>https://pyvideo.org/pydata-dc-2016/modern-nlp-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Academic and industry research in Natural Language Processing (NLP) has progressed at an accelerating pace over the last several years. Members of the Python community have been hard at work moving cutting-edge research out of papers and into open source, &amp;quot;batteries included&amp;quot; software libraries that can be applied to practical problems. We'll explore some of these tools for modern NLP in Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Patrick Harrison</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/modern-nlp-in-python.html</guid></item><item><title>Pandas from the Inside</title><link>https://pyvideo.org/pydata-dc-2016/pandas-from-the-inside.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Pandas is great for data analysis in Python: intuitive DataFrames from R; fast numpy arrays under the hood; groupby like in SQL. But this familiarity is deceptive: pandas users often get stuck on things they feel should be simple. This talk look inside pandas to see how DataFrames actually work when building, indexing and grouping tables. You will learn how to write fast, efficient pandas code.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Stephen Simmons</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/pandas-from-the-inside.html</guid><category>pandas</category></item><item><title>Parallel Python Analyzing Large Data Sets</title><link>https://pyvideo.org/pydata-dc-2016/parallel-python-analyzing-large-data-sets.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Students will walk away with a high-level understanding of both parallel problems and how to reason about parallel computing frameworks. They will also walk away with hands-on experience using a variety of frameworks easily accessible from Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aron Ahmadia</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/parallel-python-analyzing-large-data-sets.html</guid><category>Data</category><category>parallel</category><category>sets</category></item><item><title>The Five Kinds of Python Functions</title><link>https://pyvideo.org/pydata-dc-2016/the-five-kinds-of-python-functions.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We'll look at the wide variety of ways that we can leverage Python functions. This will show provide helpful background in ordinary functions, as well as callable objects and lambdas. We'll look closely at how to use generator functions, also. The fifth type of function is a function wraps a special method, like len().&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Steven Lott</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/the-five-kinds-of-python-functions.html</guid><category>functions</category></item><item><title>Using Dask for Parallel Computing in Python</title><link>https://pyvideo.org/pydata-dc-2016/using-dask-for-parallel-computing-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Dask is a relatively new library for parallel computing in Python. It builds around familiar data structures to users of the PyData stack and enables them to scale up their work on one or many machines. This tutorial will introduce users to the core concepts of dask by working through some example problems. The tutorial will be distributed via Jupyter Notebooks.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matthew Rocklin</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/using-dask-for-parallel-computing-in-python.html</guid><category>dask</category><category>parallel</category><category>parallel computing</category></item></channel></rss>