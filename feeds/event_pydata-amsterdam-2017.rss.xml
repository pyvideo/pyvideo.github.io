<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sun, 09 Apr 2017 00:00:00 +0000</lastBuildDate><item><title>A billion stars in the Jupyter Notebook</title><link>https://pyvideo.org/pydata-amsterdam-2017/a-billion-stars-in-the-jupyter-notebook.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/maartenbreddels/ipyvolume"&gt;https://github.com/maartenbreddels/ipyvolume&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I will present two Python packages: Vaex enables calculating statistics for a billion samples per second on a regular N-dimensional grid. Using a new Python package, ipyvolume, that enabled volume rendering and glyph rendering, this allows one to interactively visualise and explore these billion sample tables for high dimensional spaces.&lt;/p&gt;
&lt;p&gt;With large astronomical catalogues (＞1 billion) already available, we are preparing for methods to visualize and explore these large datasets. Instead of using cluttered scatter plots, these data volumes require different visualization techniques, in the form of binned statistics, e.g. histograms, density maps, and volume rendering in 3d. The calculation of statistics on N-dimensional grids is handled by Python library called vaex, which I will introduce. It can process at least a billion stars/samples per second, to produce for instance the mean of a quantity on a regular grid. This statistics can be calculated for any mathematical expression on the data (numpy style) and can be on the full dataset or subsets, specified by queries/selections, .&lt;/p&gt;
&lt;p&gt;However, to visualize higher dimensional data in the notebook interactively, no proper solution existed. This led to the development of ipyvolume, which can render 3d volumes and up to a million glyphs (scatter plots and quiver) in the (Jupyter) notebook as a widget. With the browser as a platform, and the release of ipywidgets 6.0, these 3d plots can also be embedded in static html files and renders on nbviewer. This allows for sharing with colleagues, paperless office (render on your tablet), outreach, press release material, etc. Full screen stereo rendering allows for a virtual reality experience using your phone and Google Cardboard, a minor investment compared to other VR head mountables. Overlaying 3d quiver plots on a 3d volume rendering allows visualizing a 6d space.&lt;/p&gt;
&lt;p&gt;Vaex and ipyvolume can be used together to explore and visualize any large tabular data set, or separately to calculate statistics, and render 3d plots in the notebook and outside.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maarten Breddels</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/a-billion-stars-in-the-jupyter-notebook.html</guid></item><item><title>Applied Data Science</title><link>https://pyvideo.org/pydata-amsterdam-2017/applied-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;This is a combination of a talks that I gave at the PyData Amsterdam meetup this year and at various institutions that payed me a good amount of money to tell them what they were doing wrong.&lt;/p&gt;
&lt;p&gt;I talk about the data science workflow and where things mostly go wrong. The talk also touches on the type of people that can help companies do applied data science, and what most MOOCs don't teach you.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Giovanni Lanzani</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/applied-data-science.html</guid></item><item><title>Data Science in the Internet of Things with Python and Spark</title><link>https://pyvideo.org/pydata-amsterdam-2017/data-science-in-the-internet-of-things-with-python-and-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;In this talk I will present how we use Python, Spark and AWS as our preferred data science stack for the Internet of Things, which allows us to efficiently develop and deploy smart data applications on top of IoT sensor data. We use these technologies to analyse and model IoT time-series data, as well as to build automated and scalable data pipelines for smart IoT data products.&lt;/p&gt;
&lt;p&gt;The Internet of Things and Industry 4.0 are here, bringing along a vast amount of connected devices and sensors producing even more data. In order to build smart applications on top of IoT sensor data we need to deal with the challenges that come along time-series data from a large amount of devices.&lt;/p&gt;
&lt;p&gt;At WATTx we build data application prototypes in the field of smart homes, smart buildings, and smart climate, which involves making use of data coming from a great deal of IoT sensors measuring -- amongst others -- temperature, humidity, motion, and luminance.&lt;/p&gt;
&lt;p&gt;The purpose of this talk is to present how we use Python and Spark to effectively analyse and model IoT data. In particular I will introduce how we use Python to process and model data from multiple IoT sensors, build machine learning models on top of it, and use Spark to scale and deploy our models in automated data pipelines in the cloud as smart IoT applications. I will use the development of predictive models for smart building applications as a real-world example to demonstrate this setup.&lt;/p&gt;
&lt;p&gt;We hope that this talk will give valuable insights on how Python and PySpark in conjunction with AWS are powerful tools to work with time-series sensor data from the Internet of Things and build data applications on top of it.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rafael Schultze-Kraft</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/data-science-in-the-internet-of-things-with-python-and-spark.html</guid></item><item><title>Debugging PySpark - Pretending to make sense of JVM stack traces</title><link>https://pyvideo.org/pydata-amsterdam-2017/debugging-pyspark-pretending-to-make-sense-of-jvm-stack-traces.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common er&lt;/p&gt;
&lt;p&gt;Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common errors and how to detect them.&lt;/p&gt;
&lt;p&gt;Spark’s own internal logging can often be quite verbose, and this talk will examine how to effectively search logs from Apache Spark to spot common problems. In addition to the internal logging, this talk will look at options for logging from within our program itself.&lt;/p&gt;
&lt;p&gt;Spark’s accumulators have gotten a bad rap because of how they interact in the event of cache misses or partial recomputes, but this talk will look at how to effectively use Spark’s current accumulators for debugging as well as a look to future for data property type accumulators which may be coming to Spark in future version.&lt;/p&gt;
&lt;p&gt;In addition to reading logs, and instrumenting our program with accumulators, Spark’s UI can be of great help for quickly detecting certain types of problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Holden Karau</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/debugging-pyspark-pretending-to-make-sense-of-jvm-stack-traces.html</guid></item><item><title>Deep Learning at Booking.com</title><link>https://pyvideo.org/pydata-amsterdam-2017/deep-learning-at-bookingcom.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Deep learning has been widely influential in academic research and also recently finding its way into the industry. This talk explains how we utilize deep learning techniques at the Booking.com scale and verify the value from the customers through A/B testing. Use cases go beyond image analysis towards solving a variety of regression and classification problems.&lt;/p&gt;
&lt;p&gt;Deep learning has been widely influential in academic research and also recently finding its way into the industry. This talk explains how we utilize deep learning techniques at the Booking.com scale and verify the value from the customers through A/B testing. Use cases go beyond image analysis towards solving a variety of regression and classification problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Emrah Tasli</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/deep-learning-at-bookingcom.html</guid></item><item><title>Deploying Python models to production</title><link>https://pyvideo.org/pydata-amsterdam-2017/deploying-python-models-to-production.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Deploying models to production can sometimes be more difficult than developing the model itself. In this talk we'll explain how we deploy our Pandas/Scikit machine learning models to production using Flask, Docker, and Kubernetes. Moreover, we'll describe the CI process which automated away all the manual steps which were required.&lt;/p&gt;
&lt;p&gt;By using an internally developed framework, we allowed Data Scientists to develop models which can easily be deployed to production. The framework exposes models either over HTTP (REST) or binds them to a Kafka topic. Additionally, the framework packages the model and its dependencies in a Docker container, and generates all deployment templates required for deploying to Kubernetes. Finally, we'll describe our Jenkins jobs which automated away all the manual steps.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Niels Zeilemaker</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/deploying-python-models-to-production.html</guid></item><item><title>Lightning Talks &amp; Closing Remarks</title><link>https://pyvideo.org/pydata-amsterdam-2017/lightning-talks-closing-remarks.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Lighting Talks and Closing Remarks&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Various speakers</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/lightning-talks-closing-remarks.html</guid><category>lightning talks</category></item><item><title>Neural Networks for Recommender Systems</title><link>https://pyvideo.org/pydata-amsterdam-2017/neural-networks-for-recommender-systems.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Neural networks are quickly becoming the tool of choice for recommender systems. In this talk, I'm going to present a number of neural network recommender models: from simple matrix factorization, through learning-to-rank, to recurrent architectures for sequential prediction. All my examples are accompanied by links to implementations to give a starting point for further experimentation.&lt;/p&gt;
&lt;p&gt;The versatility and representational power of artificial neural networks is quickly making them the preferred tool for many machine learning tasks. The same is true of recommender systems: neural networks allow us to quickly iterate over new models and to easily incorporate new user, item, and contextual features. In this talk, I'm going to present a number of useful architectures: from simple matrix factorization in neural network form, through learning-to-rank models, to more complex recurrent architectures for sequential prediction. All my examples are accompanied by links to implementations to provide a starting point for further experimentation.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maciej Kula</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/neural-networks-for-recommender-systems.html</guid></item><item><title>Python vs Orangutan</title><link>https://pyvideo.org/pydata-amsterdam-2017/python-vs-orangutan.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Somewhere in 2000km2 worth of Bornean Jungle there are 6 orangutans that need to be found and tracked. All you have is a short radio ping against a noisy background resulting in a unique and interesting anomaly detection problem.&lt;/p&gt;
&lt;p&gt;Borneo and Sumatra in South East Asia are home to a number of orangutan rescue and rehabilitation centres. A common problem each of these have is keeping track of the animals they release back in the wild. Traditionally this is done with ground teams using a radio antenna to search for the weak radio pings each animal sends out. A true needle in the haystack problem.&lt;/p&gt;
&lt;p&gt;This talk will discuss a drone-based tracking system that is being developed to alleviate this problem. More specifically the data analysis problem of searching and locating the signal of each animal against a noisy background.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dirk Gorissen</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/python-vs-orangutan.html</guid></item><item><title>Pythonic Metal</title><link>https://pyvideo.org/pydata-amsterdam-2017/pythonic-metal.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Computers are excellent counting devices.&lt;/p&gt;
&lt;p&gt;Heavy Metal is objectively the greatest genre of music ever created my humans.&lt;/p&gt;
&lt;p&gt;Can we use one to understand the other?&lt;/p&gt;
&lt;p&gt;In this talk I'll discuss what happens when we try and use computers to analyses heavy metal lyrics. Along the way we will learn how far counting can get us in understanding natural language, how we can get computers to generate lyrics for us and why the most metal word in the English language is &amp;quot;Burn&amp;quot;.&lt;/p&gt;
&lt;p&gt;The talk is based on the following blog posts: - &lt;a class="reference external" href="http://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/"&gt;http://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.degeneratestate.org/posts/2016/Sep/12/heavy-metal-and-natural-language-processing-part-2/"&gt;http://www.degeneratestate.org/posts/2016/Sep/12/heavy-metal-and-natural-language-processing-part-2/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Iain Barr</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/pythonic-metal.html</guid></item><item><title>Risk Analysis</title><link>https://pyvideo.org/pydata-amsterdam-2017/risk-analysis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Training a genetic algorithm to play the game of Risk.&lt;/p&gt;
&lt;p&gt;The popular board game of Risk has many fans around the world. Using a Python-based simulation of the game, we use a genetic algorithm to train a risk-playing algorithm.&lt;/p&gt;
&lt;p&gt;During this talk we’ll explain what genetic algorithms are and we’ll explain an entertaining use-case: how to win at popular board games. During the talk we’ll demo how object oriented patterns help with the design and implementation of these algorithms. We’ll also demonstrate a library that allows users to push their own risk bots into a game and battle it out on.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rogier van der Geer</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/risk-analysis.html</guid></item><item><title>Siamese LSTM in Keras: Learning Character-Based Phrase Representations</title><link>https://pyvideo.org/pydata-amsterdam-2017/siamese-lstm-in-keras-learning-character-based-phrase-representations.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Siamese LSTM in Keras: Learning Character-Based Phrase Representations&lt;/p&gt;
&lt;p&gt;In this talk we will explain how we solved the problem of classifying job titles into a job ontology with more than 5000 different classes. We do this by learning a character-based representation of job titles with a B-LSTM encoder trained as a Siamese network. You will learn about the methods in theory and how these can be implemented with the Keras deep learning library.&lt;/p&gt;
&lt;p&gt;Learning representations of textual data is a crucial component in NLP systems. An important application is linking entities extracted from unstructured text to a knowledge base. In our use case, the entities are job titles extracted from resumes or vacancies, and the knowledge base is a hierarchical job title taxonomy. Successfully linking job titles is particularly important in our application, as it directly influences the performance of information retrieval- and data analytics solutions.&lt;/p&gt;
&lt;p&gt;In this talk we will explain how we solved the problem of classifying job titles into a job ontology with more than 5000 different classes. We do this by learning a character-based representation of job titles with a B-LSTM encoder trained as a Siamese network. You will learn about the methods in theory and how these can be implemented with the Keras deep learning library.&lt;/p&gt;
&lt;p&gt;We will walk you through how we constructed training examples in a domain where large-scale manual annotation is nearly impossible. We will show you how we built a framework to test invariances we would like to model in our data, such as extra words in automatically extracted phrases (e.g. &amp;quot;class 1 driver using own vehicle, london&amp;quot;) and spelling variation (e.g. “C Sharp” vs “C#”). Lastly we introduce a negative sampling strategy such that the network learns to recognize subtle differences between phrases (e.g. “pipe fitter” versus “ship fitter”).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carsten van Weelden</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/siamese-lstm-in-keras-learning-character-based-phrase-representations.html</guid></item><item><title>Survival analysis for conversion rates</title><link>https://pyvideo.org/pydata-amsterdam-2017/survival-analysis-for-conversion-rates.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;What percentage of your users will spend? Typically, analysts use the conversion rate to assess how successful a website is at converting trial users into paying ones. But is this calculation giving us results that are lower than reality? With a talk rich in examples, Tristan will show how Shopify reframes the traditional conversion questions in survival analysis terms.&lt;/p&gt;
&lt;p&gt;Abstract
What percentage of your users will spend? Typically, analysts use the conversion rate to assess how successful a website is at converting trial users into paying ones. But is this calculation giving us results that are lower than reality? With a talk rich in examples, Tristan will show how Shopify reframes the traditional conversion questions in survival analysis terms.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;To get started we will see examples of how skewed conversion rates can be when analyzing conversions that are delayed.&lt;/li&gt;
&lt;li&gt;Then, we will cover the basics of survival analysis (survival function, hazard function) and the kind of data it requires.&lt;/li&gt;
&lt;li&gt;Using the lifelines python library, we will apply survival analysis to answer the limitations of conversion rates and gain insight about new merchants on Shopify.&lt;/li&gt;
&lt;li&gt;In a second case study, we will code to extend our toolbox outside of lifelines with the analysis of multiple outcomes (competing risks).&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tristan Boudreault</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/survival-analysis-for-conversion-rates.html</guid></item><item><title>Using deep learning in natural language processing: explaining Google's Neural Machine Translation</title><link>https://pyvideo.org/pydata-amsterdam-2017/using-deep-learning-in-natural-language-processing-explaining-googles-neural-machine-translation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Using deep learning in natural language processing: explaining Google's Neural Machine Translation&lt;/p&gt;
&lt;p&gt;Recent advancements in Natural Language Processing (NLP) use deep learning to improve performance. In September 2016, Google announced that Google Translate will shift from phrase-based to neural machine translation. Other fields of NLP are making a similar shift. This talk motivates and explains these algorithms and discusses implementations.&lt;/p&gt;
&lt;p&gt;Recent advancements in Natural Language Processing (NLP) use deep learning algorithms to improve performance. Google Translate shifts to neural machine translation, Baidu speech genetarion uses neural nets and question answering too. These neural networks share common architectures. They exploit recurrent computation to traverse the input and output. This talk will motivate the recurrent neural networks and discuss architectures. In the second half we discuss extensions such as attention mechanisms. Key words: RNN, seq2seq, attention, word vectors, data/model parallelism, low precision inference, TPU (explained in this order)&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rob Romijnders</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/using-deep-learning-in-natural-language-processing-explaining-googles-neural-machine-translation.html</guid></item><item><title>xtensor: the lazy tensor algebra library</title><link>https://pyvideo.org/pydata-amsterdam-2017/xtensor-the-lazy-tensor-algebra-library.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;xtensor is a C++ template tensor algebra library supporting numpy-style broadcasting and universal functions, aiming at feature parity with numpy with a native feel. In this talk we will present the highlights of the expression system. Then we will show how xtensor can be used to create numpy-aware Python extension modules with the xtensor-python project and the xtensor-cookiecutter template.&lt;/p&gt;
&lt;p&gt;Abstract
In this presentation, we will focus on the authoring numpy-aware Python extensions with xtensor.&lt;/p&gt;
&lt;p&gt;First, we present a general overview of the xtensor expression system and its main features:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;lazy broadcasting and universal functions.&lt;/li&gt;
&lt;li&gt;an API following the idioms of the C++ standard library.&lt;/li&gt;
&lt;li&gt;tools to manipulate array expressions and build upon xtensor.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then we show how the expression-based system can be used to symbolically manipulate array expressions based on other data structures, such as a numpy arrays thanks to Python's buffer protocol, but also a database or the file system.&lt;/p&gt;
&lt;p&gt;The special case of the bindings with numpy is handled by the xtensor-python project which makes it extremely easy to author Python extension operating inplace on numpy arrays, with a modern STL-compliant API. We conclude the talk with a demonstration of the xtensor-cookiecutter template project for a Python extension defining a NumPy ufunc, and exposing other C++ functionalities.&lt;/p&gt;
&lt;p&gt;The cookiecutter produces a complete project including a documentation skeleton, unit testing and all the python packaging boilerplate.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Corlay</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/xtensor-the-lazy-tensor-algebra-library.html</guid></item><item><title>A practical guide to speed up your application with Asyncio</title><link>https://pyvideo.org/pydata-amsterdam-2017/a-practical-guide-to-speed-up-your-application-with-asyncio.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Asynchronous programming in Python can be a very useful skill to have, especially when working with programs that require lots of data processing or other I/O operations. During this talk I'd like to introduce you to a package called asyncio. I'll cover the basics of why you would want to use it, how and some learning points I got out of working with it myself.&lt;/p&gt;
&lt;p&gt;Single threaded synchronous code allows for only one task to be executed at a time. Apply this to a UI of a website for example and that means every button you click makes the website unresponsive until this action is completely finished. If you accidentally typed the wrong password upon login, you'll have to wait for the system to figure that out itself before you can retry.&lt;/p&gt;
&lt;p&gt;Now this problem can be solved in a couple different ways, among which is asynchrony. In short, this can allow a program to execute other functions while waiting for I/O operations.&lt;/p&gt;
&lt;p&gt;Unfortunately not all syntax is straightforward when writing asynchronous code in Python. The way functions are called and exceptions are handled can be surprising. During this talk I'll elaborate on how to write asynchronous code in Python and share some experiences I've had with asyncio.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Niels Denissen</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/a-practical-guide-to-speed-up-your-application-with-asyncio.html</guid></item><item><title>A Pythonic Tour of Neo4j and the Cypher Query Language</title><link>https://pyvideo.org/pydata-amsterdam-2017/a-pythonic-tour-of-neo4j-and-the-cypher-query-language.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;This talk gives an overview of the Neo4j graph database and the Cypher query language from the point of view of a Python user. We'll look at how to run queries and visualise or extract those results into software such as Pandas. We'll also explore the property graph data model and look at how it differs from other data models.&lt;/p&gt;
&lt;p&gt;Graph databases offer a fresh perspective on data modelling and one that is often closer to the real world than a traditional RDBMS. In this talk, we'll look at how to work with Neo4j's property graph data model from the point of view of a Python user, how this model differs from other database models and we'll also show how to integrate the Cypher query language into a Python application.&lt;/p&gt;
&lt;p&gt;This talk will (hopefully!) contain a couple of live demonstrations. We'll explore how to integrate Cypher query results with data analysis tools such as Pandas as well as how to visualise graph data through the Neo4j browser.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nigel Small</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/a-pythonic-tour-of-neo4j-and-the-cypher-query-language.html</guid></item><item><title>Bayesian optimization with Scikit-Optimize</title><link>https://pyvideo.org/pydata-amsterdam-2017/bayesian-optimization-with-scikit-optimize.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;You are given access to an espresso machine with many buttons and knobs to tweak. Your task is to brew the best cup of espresso possible. How do you find the best settings before dying of a caffeine overdose? The answer is bayesian optimisation: the art of minimising an extremely expensive to evaluate function in as few calls as possible.&lt;/p&gt;
&lt;p&gt;Bayesian optimisation can be applied to finding the optimal hyper-parameters for a deep neural network, optimizing the click-through-rate in online advertising or simulator settings of an optimal physics experiment. This talk will teach you about the basics of bayesian optimisation and how to use Scikit-Optimize to apply it to your own real world problems.&lt;/p&gt;
&lt;p&gt;In addition to learning how to use this toolkit, you will also learn the answers to questions like:&lt;/p&gt;
&lt;p&gt;When is bayesian optimisation useful? What is bayesian about bayesian optimisation? When can a statistical model be used for bayesian optimisation? What is an acquisition function and how does it help to exploit / explore the search space?&lt;/p&gt;
&lt;p&gt;We will end the talk with an example to optimize hyperparameters of a neural-network using bayesian optimisation.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gilles Louppe</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/bayesian-optimization-with-scikit-optimize.html</guid></item><item><title>Creativity and AI: Deep Neural Nets "Going Wild"</title><link>https://pyvideo.org/pydata-amsterdam-2017/creativity-and-ai-deep-neural-nets-going-wild.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;We live in times, where science fiction authors are struggling to keep up with reality. This talk will make sense of the explosion of research and experiments that deal with creativity and A.I. and take you through the wonderful trippy world of neural nets &amp;quot;going wild&amp;quot; and show some of the exciting possibilities new technologies offer for making us all more creative, more of the time.&lt;/p&gt;
&lt;p&gt;We live in times, where science fiction authors are struggling to keep up with reality. This talk will make sense of the explosion of research and experiments that deal with creativity and A.I. and take you through the wonderful trippy world of neural nets &amp;quot;going wild&amp;quot; with their dancing moves, freestyle raps, impressionist paintings. and show some of the exciting possibilities new technologies offer for creative use and explorations of human-machine interaction, where the main theorem is &amp;quot;augmentation, not automation&amp;quot;. The talk will particularly focus on &amp;quot;generative&amp;quot; models, and show the python fanatic how to make your move with these particular forms of Deep Neural Nets, to then finish with an &amp;quot;experiment&amp;quot;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Roelof Pieters</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/creativity-and-ai-deep-neural-nets-going-wild.html</guid></item><item><title>Deep learning for time series made easy</title><link>https://pyvideo.org/pydata-amsterdam-2017/deep-learning-for-time-series-made-easy.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Deep learning is a state of the art method for many tasks, such as image classification and object detection. For researchers that have time series data, but are not an expert on deep learning, the barrier can be high to start using deep learning. We developed mcfly, an open source python library, to help machine learning novices explore the value of deep learning for time series data.&lt;/p&gt;
&lt;p&gt;In this talk, we will explore how machine learning novices can be aided in the use of deep learning for time series classification.&lt;/p&gt;
&lt;p&gt;In a variety of scientific fields researchers face the challenge of time series classification. For example, to classify activity types from wrist-worn accelerometer data or to classify epilepsy from electroencephalogram (EEG) data. For researchers who are new to the field of deep learning, the barrier can be high to start using deep learning. In contrast to computer vision use cases, where there are tools such as caffe that provide pre-defined models to apply on new data, it takes some knowledge to choose an architecture and hyperparameters for the model when working with time series data.&lt;/p&gt;
&lt;p&gt;We developed mcfly, an open source python library to make time series classification with deep learning easy. It is a wrapper around Keras, a popular python library for deep learning. Mcfly provides a set of suitable architectures to start with, and performs a search over possible hyper-parameters to propose a most suitable model for the classification task provided. We will demonstrate mcfly with excerpts from (multi-channel) time series data from movement sensors that are associated with a class label, namely activity type (sleeping, walking, climbing stairs). In our example, mcfly will be used to train a deep learning model to label new data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dafne van Kuppevelt</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/deep-learning-for-time-series-made-easy.html</guid></item><item><title>Deep Reinforcement Learning: theory, intuition, code</title><link>https://pyvideo.org/pydata-amsterdam-2017/deep-reinforcement-learning-theory-intuition-code.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;In this talk I'd like to give practical introduction into deep reinforcement learning methods, used to solve complex control problems in robotics, play Atari games, self-driving car control and lots more.&lt;/p&gt;
&lt;p&gt;Deep Reinforcement Learning is a very hot topic, successfully applied in lots of areas which require planning of actions in complex, noisy and partially-observed environments. Concrete examples vary from playing arcade games, navigating websites, helicopter, quadrocopter and car control, protein folding and lots of others.&lt;/p&gt;
&lt;p&gt;Surprisingly, during my own delving into this wide topic, I've discovered that (with rare exceptions) there is a lack of concrete, understandable explanation of most successful and useful algorithms and methods, such as Deep Q-Networks (DQN), Policy Gradients (PG) and Asynchronous Advantage Actor-Critic (A3C). The situation is even worse with simple code examples of the above methods.&lt;/p&gt;
&lt;p&gt;On the one side, there are lots of scientific papers on arxiv.org where researchers tune ideas and methods. On the other side there is a couple full-sized open-source projects implementing those methods plus dozens of &amp;quot;tricks&amp;quot; to improve stability and performance of those methods.&lt;/p&gt;
&lt;p&gt;In this talk, I'll try to fill the gap between them by showing the intuition behind the math and demonstrating how those three approaches (DQN, PG and A3C) can be implemented in less than 200 lines of python code using keras.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maxim Lapan</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/deep-reinforcement-learning-theory-intuition-code.html</guid></item><item><title>Diagnosing Machine Learning Models</title><link>https://pyvideo.org/pydata-amsterdam-2017/diagnosing-machine-learning-models.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;A Machine Learning Models is never perfect. If it completely fails, it must be fixed, if it performs well, we want to improve it.
In this talk, several techniques to diagnose the source of errors a model makes will be presented.&lt;/p&gt;
&lt;p&gt;Your model performs worse than a random model. What do you do? Your model has 99.9% ROC AUC, should you just celebrate? Every time you add a new feature the interpretation of the model's parameters changes completely. What 's wrong? Your model has 75% ROC AUC. Should you add more data? More features? Use a more complex model? You are out of new features, no matter what you do, the model performance is the same? What is happening?&lt;/p&gt;
&lt;p&gt;Many of these questions appear once and again when working with Machine Learning,, answering them takes time and has a huge impact on the final outcome of a Machine Learning project. Understanding the current condition of a model is the key to decide what to do next.&lt;/p&gt;
&lt;p&gt;In this talk I will describe several techniques to diagnose algorithms and models, some of them are: Bias and Variance Decomposition Calibration Curves Response Distribution Chart Residual Plots etc.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lucas Javier Bernardi</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/diagnosing-machine-learning-models.html</guid></item><item><title>Different Strategies of Scaling H2O Machine Learning on Apache Spark</title><link>https://pyvideo.org/pydata-amsterdam-2017/different-strategies-of-scaling-h2o-machine-learning-on-apache-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;This talk gives a basic overview of machine learning on top of H2O and Spark and explains different ways how to scale your tasks on top of these technologies to fit your use case(s).&lt;/p&gt;
&lt;p&gt;Sparkling Water integrates H2O, open source distributed machine learning platform, with the capabilities of Apache Spark. It allows users to leverage H2O’s machine learning algorithms with Apache Spark applications via Scala, Python, R or H2O’s Flow GUI which makes Sparkling Water a great enterprise solution. Sparkling Water 2.0 was built to coincide with the release of Apache Spark 2.0 and introduces several new features. One of the latest and largest features is the ability to configure Sparkling Water for different workloads, scale and optimize the platform according to your data and needs. In this talk we will introduce the basic architecture of Sparkling Water, go over different scaling strategies and explain the pros and cons of each solution. We will also compare the approaches with regards to the specific use cases and provide the rationale why or why not each solution may be a good fit for the desired use case. This talk will finish with a live demo demonstrating the mentioned approaches and should give you a real time experience of configuring and running Sparkling Water for your use case(s)!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jakub Hava</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/different-strategies-of-scaling-h2o-machine-learning-on-apache-spark.html</guid></item><item><title>Ethical Machine Learning: Creating Fair Models in an Unjust World</title><link>https://pyvideo.org/pydata-amsterdam-2017/ethical-machine-learning-creating-fair-models-in-an-unjust-world.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;The increased use of machine learning (both simple and advanced) to make decisions that impact business and at times, our culture and our lives, is simply a fact of our current world. Determining ways to make accountable, fair and ethical decisions when using AI or machine learning is a subject of much debate and current research. Join the conversation and explore how we can help build a more just&lt;/p&gt;
&lt;p&gt;This talk aims to cover the current research in training and producing ethical and fair machine learning models. Based on many papers released in the past year, we can see we have a massive problem when it comes to AI and machine learning algorithms and models which make (often accurate) biased decisions based on race, location and gender. As machine learning researchers, data engineers and scientists, our decisions in choosing accuracy over ethics (or the opposite) can impact our lives, our research but also the world. This talk will provide questions and (some) answers about how to fight prejudicial training and work to create less prejudiced models in a biased world.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Katharine Jarmul</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/ethical-machine-learning-creating-fair-models-in-an-unjust-world.html</guid></item><item><title>Finding Needles in a Growing Haystack: An Architecture for Intelligent Reporting at Scale</title><link>https://pyvideo.org/pydata-amsterdam-2017/finding-needles-in-a-growing-haystack-an-architecture-for-intelligent-reporting-at-scale.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Finding Needles in a Growing Haystack: An Architecture for Intelligent Reporting at Scale&lt;/p&gt;
&lt;p&gt;As we collect more and more data in the world, one of the major challenges is how to identify, summarize, and present relevant data to users. In this talk, I will present an architecture we have developed at Optiver to automate the identification of important changes in streaming time series data in a scalable way.&lt;/p&gt;
&lt;p&gt;At Optiver, we provide liquidity on markets around the world, performing millions of interactions with financial exchanges each day and generating terabytes of raw data spread across multiple datacenters. Performance is critical in our industry, and every aspect of our business is continually changing in a pursuit of perfection. On the Data team at Optiver, we face the challenge of converting this raw data into a highly aggregated and refined stream of information that people throughout the company can use to make decisions and track our performance.&lt;/p&gt;
&lt;p&gt;Even after refining this raw data, it is still impossible for people to look through every possible subset of the data to identify important changes. The problem also gets worse over time as trading execution times decrease and the number of products we trade expands. On the Data team, we have tried to solve this by developing automated, smart reporting tools that can scale horizontally.&lt;/p&gt;
&lt;p&gt;In this talk, I will discuss the architectural decisions and implementations we have used to build our automated reporting architecture. This includes the use of online (recursive) implementations of Bayesian statistics for estimating metrics and detecting trends and outliers, NoSQL databases for scalable storage and evolving data schemas, parallel execution across a Mesos cluster, and publication of the reports through REST APIs. All implemented in Python of course!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Stephen Helms</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/finding-needles-in-a-growing-haystack-an-architecture-for-intelligent-reporting-at-scale.html</guid></item><item><title>Simulate your language. ish.</title><link>https://pyvideo.org/pydata-amsterdam-2017/simulate-your-language-ish.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;John will present a simple character-level Markov model for simulating language in Python. The goal is to generate text that demonstrates how English looks to non-English readers. The model generates text that is simultaneously totally foreign and yet weirdly familiar, using logic simple enough that anyone could replicate it.&lt;/p&gt;
&lt;p&gt;engl_ish is a Python model for text generation. It is based on a character level Markov model augmented with some additional logic, with the aim of capturing the &amp;quot;feel&amp;quot; of a language. The goal is to generate text in e.g. English, such that it contains no actual English meaning, but nonetheless looks like English to someone who doesn't speak the language.&lt;/p&gt;
&lt;p&gt;In this talk, John will share his inspiration for creating the model, go into detail about its logic and how it was implemented in Python, share results from a variety of training sets and settings, and talk about issues and opportunities for improvement.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">John Paton</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/simulate-your-language-ish.html</guid></item><item><title>Smoothing your data with polynomial fitting: a signal processing perspective</title><link>https://pyvideo.org/pydata-amsterdam-2017/smoothing-your-data-with-polynomial-fitting-a-signal-processing-perspective.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/chtaal/pydata2017"&gt;https://github.com/chtaal/pydata2017&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://github.com/chtaal/pydata2017/raw/master/ppt/savitzky.pptx"&gt;https://github.com/chtaal/pydata2017/raw/master/ppt/savitzky.pptx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The main goal of this talk is to get people acquainted with frequency domain analysis of existing data processing methods, such as polynomial fitting also known as a Savitzky-Golay filter. I will give examples on how to implement these signal processing techniques by using the functionality of the Numpy and Scipy packages.&lt;/p&gt;
&lt;p&gt;In the field of data processing and analysis we typically have to deal with noisy signals. One possible approach to attenuate the noise is by fitting a polynomial to a subset of samples where the smoothed value is obtained by evaluating the polynomial at the desired time location. In 1964, Abraham Savitzky and Marcel Golay found out that this approach can be interpreted as a convolution between the noisy input signal and a second signal which depends on the settings of the polynomial. Since convolution is a well-known process from the field of signal processing this facilitates frequency domain analysis of such a polynomial smoother. This gives better insights on how to choose free parameters such as the degree of the polynomial and the number of samples used in the fit. The main goal of this talk is to get people acquainted with frequency domain analysis of existing data processing methods, such as polynomial fitting. I will give examples on how to implement these techniques by using the functionality of the Numpy and Scipy packages.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Cees Taal</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/smoothing-your-data-with-polynomial-fitting-a-signal-processing-perspective.html</guid></item><item><title>Successfully applying Bayesian statistics to A/B testing in your business</title><link>https://pyvideo.org/pydata-amsterdam-2017/successfully-applying-bayesian-statistics-to-ab-testing-in-your-business.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;A lot of theory is available on how the statistics of A/B testing could be improved using Bayesian statistics. In this talk I will discuss several theoretical problems and I will share my experiences on whether they actually impact A/B testing in practice. This will be demonstrated using hierarchical models build with pymc. Finally, I will share how I successfully implemented this into business.&lt;/p&gt;
&lt;p&gt;I will first shortly discuss frequentist calculation of an A/B test, and three problems: the normal distribution instead of the beta distribution, multiple comparison problem and biased stopping times. Using these topics, I will shortly introduce Bayesian statistics and more specifically hierarchical Bayes, by using examples in pymc. I will then share whether these topics actually have direct implications for testing in practice and illustrate why several aspects hardly change the decisions made. I will then focus on one of the most important aspects from a business perspective: when to stop an insignificant test. I will present the stopping rule I currently use, explain how this works in practice and how this relates to solving the theoretical problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ruben Mak</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/successfully-applying-bayesian-statistics-to-ab-testing-in-your-business.html</guid></item><item><title>Training a TensorFlow model to detect lung nodules on CT scans</title><link>https://pyvideo.org/pydata-amsterdam-2017/training-a-tensorflow-model-to-detect-lung-nodules-on-ct-scans.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;To detect early-stage lung cancer, radiologists make CT scans which they interpret to find potential abnormalities. Deep learning can be applied to automate this process. In this talk, we will describe the Python-based training pipeline for a TensorFlow model to detect these abnormalities.&lt;/p&gt;
&lt;p&gt;Finding a nodule of a few pixels in size is a challenge in a 3D volume of 200512512 voxels. To solve this, a two-stage deep learning model is defined to generate potential nodule locations, and to classify those candidates. The pipeline is written in Python and addresses a number of challenges regarding the size of the 3D dataset, normalization, annotation and labeling, model design and validation.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mark-Jan Harte</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/training-a-tensorflow-model-to-detect-lung-nodules-on-ct-scans.html</guid></item></channel></rss>