<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Marin Aglić Čuvić</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_marin-aglic-cuvic.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-07-08T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Data pipelines with Celery: modular, signal-driven and manageable</title><link href="https://pyvideo.org/europython-2024/data-pipelines-with-celery-modular-signal-driven-and-manageable.html" rel="alternate"></link><published>2024-07-08T00:00:00+00:00</published><updated>2024-07-08T00:00:00+00:00</updated><author><name>Marin Aglić Čuvić</name></author><id>tag:pyvideo.org,2024-07-08:/europython-2024/data-pipelines-with-celery-modular-signal-driven-and-manageable.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;[EuroPython 2024 — Terrace 2A on 2024-07-10]&lt;/p&gt;
&lt;p&gt;Data pipelines with Celery: modular, signal-driven and manageable by Marin Aglić Čuvić&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://ep2024.europython.eu/session/data-pipelines-with-celery-modular-signal-driven-and-manageable"&gt;https://ep2024.europython.eu/session/data-pipelines-with-celery-modular-signal-driven-and-manageable&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Writing pipelines for processing large datasets has its challenges – processing data within an acceptable time frame, dealing with unreliable and rate-limited APIs, and unexpected failures …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;[EuroPython 2024 — Terrace 2A on 2024-07-10]&lt;/p&gt;
&lt;p&gt;Data pipelines with Celery: modular, signal-driven and manageable by Marin Aglić Čuvić&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://ep2024.europython.eu/session/data-pipelines-with-celery-modular-signal-driven-and-manageable"&gt;https://ep2024.europython.eu/session/data-pipelines-with-celery-modular-signal-driven-and-manageable&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Writing pipelines for processing large datasets has its challenges – processing data within an acceptable time frame, dealing with unreliable and rate-limited APIs, and unexpected failures that can cause data incompleteness. In this talk we’ll discuss how to design &amp;amp; implement modular, efficient, and manageable workflows with Celery, Redis, and signal-based triggering.&lt;/p&gt;
&lt;p&gt;We’ll begin by exploring the motivation behind segmenting pipelines into smaller, more manageable ones. The segmentation simplifies development, enhances fault tolerance, and improves modularity, making it easier to test and debug each component. By leveraging Redis as a data store and Celery’s signals, we introduce self-triggering (or looped) pipelines that efficiently manage data batches within API rate limits and system resource constraints. We will look at an example of how we did things in the past using periodic tasks and how this new approach, instead, simplifies and increases our data throughput and completeness. Additionally, this facilitates triggering pipelines with secondary benefits, such as persisting and reporting results, which allows analysis and insight into the processed data. This can help us tackle inaccuracies and optimise data handling in budget-sensitive environments.&lt;/p&gt;
&lt;p&gt;The talk offers the attendees a perspective on designing data pipelines in Celery that they may have not seen before. We will share the techniques for implementing more effective and maintainable data pipelines in their own projects.&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;p&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License: &lt;a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;https://creativecommons.org/licenses/by-nc-sa/4.0/&lt;/a&gt;&lt;/p&gt;
</content><category term="EuroPython 2024"></category></entry></feed>