<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Iris Zhang</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 16 Oct 2023 00:00:00 +0000</lastBuildDate><item><title>Distributed Checkpoint</title><link>https://pyvideo.org/pytorch-conference-2023/distributed-checkpoint.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will present checkpoint features for distributed training. Distributed checkpoint support saving and loading from multiple ranks in parallel. It handles load-time resharding which enables saving in one cluster topolgy and loading to another. It also supports saving in one parallelism and loading into another. It is currently adopted by IBM, Mosaic, and XLA for FSDP checkpoint, and it is also being used for Shampoo OSS release checkpointing support. We will talk about distributed checkpoint support today and what is coming up next.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Iris Zhang</dc:creator><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/distributed-checkpoint.html</guid><category>PyTorch Conference 2023</category></item></channel></rss>