<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Philip Torr</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_philip-torr.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Lagrangian Decomposition for Neural Network Verification</title><link href="https://pyvideo.org/uai-2020/lagrangian-decomposition-for-neural-network-verification.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Rudy Bunel</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/lagrangian-decomposition-for-neural-network-verification.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Lagrangian Decomposition for Neural Network Verification&lt;/p&gt;
&lt;p&gt;Rudy Bunel (); Alessandro De Palma (University of Oxford)*; Alban Desmaison (University of Oxford); Krishnamurthy Dvijotham (DeepMind); Pushmeet Kohli (DeepMind); Philip Torr (University of Oxford); M. Pawan Kumar (University of Oxford)&lt;/p&gt;
&lt;p&gt;A fundamental component of neural network verification is the computation of bounds on â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Lagrangian Decomposition for Neural Network Verification&lt;/p&gt;
&lt;p&gt;Rudy Bunel (); Alessandro De Palma (University of Oxford)*; Alban Desmaison (University of Oxford); Krishnamurthy Dvijotham (DeepMind); Pushmeet Kohli (DeepMind); Philip Torr (University of Oxford); M. Pawan Kumar (University of Oxford)&lt;/p&gt;
&lt;p&gt;A fundamental component of neural network verification is the computation of bounds on the values their outputs can take. Previous methods have either used off-the-shelf solvers, discarding the problem structure, or relaxed the problem even further, making the bounds unnecessarily loose. We propose a novel approach based on Lagrangian Decomposition. Our formulation admits an efficient supergradient ascent algorithm, as well as an improved proximal algorithm. Both the algorithms offer three advantages: (i) they yield bounds that are provably at least as tight as previous dual algorithms relying on Lagrangian relaxations; (ii) they are based on operations analogous to forward/backward pass of neural networks layers and are therefore easily parallelizable, amenable to GPU implementation and able to take advantage of the convolutional structure of problems; and (iii) they allow for anytime stopping while still providing valid bounds. Empirically, we show that we obtain bounds comparable with off-the-shelf solvers in a fraction of their running time, and obtain tighter bounds in the same time as previous dual algorithms. This results in an overall speed-up when employing the bounds for formal verification. Code for our algorithms is available at &lt;a class="reference external" href="https://github.com/oval-group/decomposition-plnn-bounds"&gt;https://github.com/oval-group/decomposition-plnn-bounds&lt;/a&gt;.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>