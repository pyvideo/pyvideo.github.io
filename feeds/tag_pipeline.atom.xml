<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_pipeline.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-04-21T00:00:00+00:00</updated><entry><title>Heroku: come deployare un'app Django in 10 minuti!</title><link href="https://pyvideo.org/pycon-italia-2018/heroku-come-deployare-unapp-django-in-10-minuti.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Sabatino Severino</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/heroku-come-deployare-unapp-django-in-10-minuti.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Se sei stanco di occuparti di &lt;em&gt;load balancing&lt;/em&gt; , &lt;em&gt;routing&lt;/em&gt; e &lt;em&gt;cloud
monitoring&lt;/em&gt; e vorresti dedicarti solo a &lt;em&gt;lambda function&lt;/em&gt; , &lt;em&gt;list
comprehension&lt;/em&gt; o &lt;em&gt;class-based view&lt;/em&gt; allora questo è il talk che fa per
te.&lt;/p&gt;
&lt;p&gt;Heroku è la piattaforma PaaS ( &lt;em&gt;Platform as a Service&lt;/em&gt; ) che consente
agli sviluppatori di creare, eseguire e gestire applicazioni interamente
nel cloud. Ad essere fornito come servizio non c’è solo l’hardware, ma
anche la piattaforma che astrae l’hardware stesso e permette di
usufruire di funzionalità che consentono di ottenere bilanciamenti
automatici, gestione del deployment e altro ancora.&lt;/p&gt;
&lt;p&gt;Il vantaggio per l’utente è quello di concentrarsi solo ed
esclusivamente sullo sviluppo dell’applicazione senza perdersi
nell’analisi di problematiche legate all’ambiente in cui essa deve
essere distribuita ottenendo così la scalabilità e l’affidabilità
necessaria.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 15:30 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="paas"></category><category term="continuous-integration"></category><category term="github"></category><category term="continuous-delivery"></category><category term="Python"></category><category term="pipeline"></category><category term="django"></category><category term="heroku"></category><category term="git"></category><category term="deployment"></category><category term="web development"></category></entry><entry><title>Modern ETL-ing with Python and Airflow (and Spark)</title><link href="https://pyvideo.org/pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Tamara Mendt</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Tamara Mendt&lt;/strong&gt; (&amp;#64;TamaraMendt)&lt;/p&gt;
&lt;p&gt;Tamara Mendt is a Data Engineer at HelloFresh, a meal kit delivery service headquartered in Berlin, and one of the top 3 tech startups to come out of Europe over the past 4 years. She devotes her time to building data pipelines and designing and maintaining the company's data infrastructure. Tamara has a computer engineering degree from her native country Venezuela, and an Erasmus Mundus Masters degree in IT for Business Intelligence. She wrote her Master thesis at the TU Berlin with the research group where Apache Flink was born. At HelloFresh she is continuing to work with distributed technologies such has Apache Hadoop, Apache Kafka and Apache Spark to cope with the scalability that the fast growing company requires for dealing with their data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The challenge of data integration is real. The sheer amount of tools that exist to address this problem is proof that organizations struggle with it. This talk will discuss the inherent challenges of data integration, and show how it can be tackled using Python and Apache Airflow and Apache Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The way organizations analyze their data has evolved very quickly since the beginning of the millennium. The development of Hadoop, and the explosion in the variety of data that companies are dealing with nowadays, has fostered the appearance of the concept of data lake, and the shift of traditional ETL (extract, transform, load), to ELT (extract, load, transform). Yet, the challenge of integrating data to obtain valuable insights still remains, and despite the hype and attention being focused on data, very few organizations have actually managed to become data driven. In this talk I will present insights into how we are currently building data pipelines using Python (as a replacement to high level ETL software), Apache Airflow as a scheduler to our coded transformations, and Apache Spark for achieve scalability. Though building data pipelines is not the only element required to become data driven, it is a crucial one, and I hope to encourage the audience to use these open source technologies in their own ETL-ing (or ELT-ing) efforts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data"></category><category term="data-science"></category><category term="pipeline"></category></entry><entry><title>Catching Neutrinos with Python and KM3NeT</title><link href="https://pyvideo.org/pycon-italia-2017/catching-neutrinos-with-python-and-km3net.html" rel="alternate"></link><published>2017-04-07T00:00:00+00:00</published><updated>2017-04-07T00:00:00+00:00</updated><author><name>Moritz Lotze</name></author><id>tag:pyvideo.org,2017-04-07:pycon-italia-2017/catching-neutrinos-with-python-and-km3net.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;KM3NeT is the next generation underwater neutrino telescope located in
the deepest seas of the Mediterranean. Once completed, it will have an
instrumented volume of several cubic kilometres. One of the technical
challenges in such a huge project is providing software tools, which can
be rapidly developed and maintained while keeping a focus on
portability, compatibility and usability. The area of application ranges
from live monitoring over offline processing and analysis to data
visualisation and 3D event displays. Python with it’s wonderful standard
library combined with extraordinary open source frameworks proves to be
able to handle all these different tasks with ease. This talk covers
some of the projects used in our collaboration to make our lives easier,
featuring technologies such as Cython, Numpy, Scipy, Pandas, Matplotlib,
Jupyter, Tornado, PyOpenGL, urwid and more, and focusses on one of our
key ingredients: a framework called KM3Pipe, which provides a
pipeline-based workflow to allow us building complex analysis chains by
stitching together modules.&lt;/p&gt;
</summary><category term="physics"></category><category term="pipeline"></category><category term="neutrinos"></category><category term="hephysiscs"></category></entry></feed>