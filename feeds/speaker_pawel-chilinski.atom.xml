<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Pawel Chilinski</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_pawel-chilinski.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Neural Likelihoods via Cumulative Distribution Functions</title><link href="https://pyvideo.org/uai-2020/neural-likelihoods-via-cumulative-distribution-functions.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Pawel Chilinski</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/neural-likelihoods-via-cumulative-distribution-functions.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Neural Likelihoods via Cumulative Distribution Functions&lt;/p&gt;
&lt;p&gt;Pawel Chilinski (UCL)*; Ricardo Silva (University College London)&lt;/p&gt;
&lt;p&gt;We leverage neural networks as universal approximators of monotonic functions to build a parameterization of conditional cumulative distribution functions (CDFs). By the application of automatic differentiation with respect to response variables and then to parameters â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Neural Likelihoods via Cumulative Distribution Functions&lt;/p&gt;
&lt;p&gt;Pawel Chilinski (UCL)*; Ricardo Silva (University College London)&lt;/p&gt;
&lt;p&gt;We leverage neural networks as universal approximators of monotonic functions to build a parameterization of conditional cumulative distribution functions (CDFs). By the application of automatic differentiation with respect to response variables and then to parameters of this CDF representation, we are able to build black box CDF and density estimators. A suite of families is introduced as alternative constructions for the multivariate case. At one extreme, the simplest construction is a competitive density estimator against state-of-the-art deep learning methods, although it does not provide an easily computable representation of multivariate CDFs. At the other extreme, we have a flexible construction from which multivariate CDF evaluations and marginalizations can be obtained by a simple forward pass in a deep neural net, but where the computation of the likelihood scales exponentially with dimensionality. Alternatives in between the extremes are discussed. We evaluate the different representations empirically on a variety of tasks involving tail area probabilities, tail dependence and (partial) density estimation.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>