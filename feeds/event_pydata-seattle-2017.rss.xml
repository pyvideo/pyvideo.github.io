<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Thu, 06 Jul 2017 00:00:00 +0000</lastBuildDate><item><title>A Quick Primer on TensorFrames: Apache Spark and TensorFlow Together</title><link>https://pyvideo.org/pydata-seattle-2017/a-quick-primer-on-tensorframes-apache-spark-and-tensorflow-together.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This session will provide a high-level primer on the burgeoning field of Deep Learning and the reasons why it is important. It will provide the fundamentals surrounding feature learning and neural networks required for deep learning. As well, this session will provide a quick start for TensorFrames for Apache Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This session will provide a high-level primer on the burgeoning field of Deep Learning and the reasons why it is important. It will provide the fundamentals surrounding feature learning and neural networks required for deep learning; more specifically:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What is Deep Learning?&lt;/li&gt;
&lt;li&gt;A primer on feature learning&lt;/li&gt;
&lt;li&gt;What is feature engineering?&lt;/li&gt;
&lt;li&gt;What is TensorFlow?&lt;/li&gt;
&lt;li&gt;Introducing TensorFrames&lt;/li&gt;
&lt;li&gt;TensorFrames – quick start&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to reviewing the concepts, we will have demos and provide links to notebooks that attendees will be able to use.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Denny Lee</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/a-quick-primer-on-tensorframes-apache-spark-and-tensorflow-together.html</guid></item><item><title>Accelerating AI development</title><link>https://pyvideo.org/pydata-seattle-2017/accelerating-ai-development.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R.&lt;/p&gt;
&lt;p&gt;PyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joseph Sirosh</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/accelerating-ai-development.html</guid><category>keynote</category></item><item><title>Applying the four step "Embed, Encode, Attend, Predict" framework to predict document similarity</title><link>https://pyvideo.org/pydata-seattle-2017/applying-the-four-step-embed-encode-attend-predict-framework-to-predict-document-similarity.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This presentation will demonstrate Matthew Honnibal's four-step &amp;quot;Embed, Encode, Attend, Predict&amp;quot; framework to build Deep Neural Networks to do document classification and predict similarity between document and sentence pairs using the Keras Deep Learning Library.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A new framework for building Natural Language Processing (NLP) models in the Deep Learning era has been proposed by Matthew Honnibal (creator of the SpaCy NLP toolkit). It is composed of the following four steps - Embed, Encode, Attend and Predict. Embed converts incoming text into dense word vectors that encode its meaning as well as its context; Encode adapts the vector to the target task; Attend forces the network to focus on the most important parts of the data; and Predict produces the network's output representation. Word Embeddings have revolutionized many NLP tasks, and today it is the most effective way of representing text as vectors. Combined with the other three steps, this framework provides a principled way to make predictions starting from unstructured text data. This presentation will demonstrate the use of this four step framework to build Deep Neural Networks that do document classification and predict similarity between sentence and document pairs, using the Keras Deep Learning Library for Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sujit Pal</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/applying-the-four-step-embed-encode-attend-predict-framework-to-predict-document-similarity.html</guid></item><item><title>Automatic Citation generation with Natural Language Processing</title><link>https://pyvideo.org/pydata-seattle-2017/automatic-citation-generation-with-natural-language-processing.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Can citations write themselves? Using a topic modeling approach we model similarity between patents from the US patent database via cosine similarity. Mining the text of millions of patents for similarities allows us to algorithmically recommend possible citations for new patents. We use python, pyspark and big query to parallelize complex operations over millions of rows of data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this project we use natural language processing to investigate ways to generate recommendations for citations for new patents. Using a topic modeling approach and cosign similarity we attempt to recommend patents similar to the text of a potential new patent. This would allow inventors and engineers to quickly reference patents similar to their own and easily cite them.&lt;/p&gt;
&lt;p&gt;From an architectural stand point we use big query to handle data ingestion and pre-processing. The data management functionality of big query is key because there are millions of patents some of which may contain hundreds of pages of text. After pre processing in big query the bulk of our analytic work is done in spark through the python wrapper which allows for easy parallelization of the analytical portion.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Claire Kelley</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/automatic-citation-generation-with-natural-language-processing.html</guid></item><item><title>Batch and Streaming Processing in the World of Data Engineering and Data Science</title><link>https://pyvideo.org/pydata-seattle-2017/batch-and-streaming-processing-in-the-world-of-data-engineering-and-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Streaming or batch is an ongoing debate with the large-scale adoption of “Big Data”. In this talk, we discuss the pros &amp;amp; cons of batch vs. streaming processing, especially with respect to the workflow of data engineers and data scientists. We also present a a demo using PySpark Logistic Regression for offline and online model training to illustrate the difference between the two.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The large-scale adoption of “Big Data” has created a multitude of exciting new job roles and technologies. In line with this, data scientists and data engineers have both become key members of many technology teams, a coexistence which has often motivated the debate: streaming or batch? In this talk, we discuss the pros &amp;amp; cons of batch vs. streaming processing, especially with respect to finding common ground between the data engineer and data scientist’s workflows. We address the types of cases that are applicable to batch processing, streaming processing, or both. Finally, we present a demo using PySpark Logistic Regression for offline or online decisioning and model updating, to illustrate different ways to utilize batch and/or streaming processing to apply machine learning to real-time data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Keira Zhou</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/batch-and-streaming-processing-in-the-world-of-data-engineering-and-data-science.html</guid></item><item><title>Beginning Julia: Language and Landscape</title><link>https://pyvideo.org/pydata-seattle-2017/beginning-julia-language-and-landscape.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Math operations are discounted in the small where I/O dominates, yet gate compute costs in the large. Be it square or root, exponential or matrix ops, time/space efficacy matters for science. engineering, and machine learning. Julia provides right sizing for precision, accuracy, and performance. Julia's Type system, Just-in-Time, and Dispatch are introduced via REPL, IJulia, Juno, JuliaBox&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Why is an alternative to Python needed for numerical work? Data Science (clustering) and Machine Learning (neural network activation functions) provide two good reasons to look forward to Julia 1.0. This talk is built on the just released version 0.6.&lt;/p&gt;
&lt;p&gt;Calculation, precision, storage, representation and graphing all need to be considered, especially at scale. Exponential and Matrix operations are prone to underflow, overflow and and accumulated rounding errors could lead to dire consequences. For abritrary precision values, it may take longer to display or transmit the result than to calculate it. Speed does matter when billions of flops are being done. Julia's clever common sense is indeed a &amp;quot;A fresh approach to technical computing.&amp;quot;&lt;/p&gt;
&lt;p&gt;A very brief recap language and version differences. While the focus is Julia, some compare and contrast with Python is appropriate.&lt;/p&gt;
&lt;p&gt;Julia's Type system, Just-in-Time, and Dispatch style are worth a good look. Graphing as well. GrElegance lurks above and under the hood.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">en zyme</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/beginning-julia-language-and-landscape.html</guid></item><item><title>Bokeh and Friends</title><link>https://pyvideo.org/pydata-seattle-2017/bokeh-and-friends.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As Bokeh rapidly approaches a 1.0 release, the project focus is shifting towards making Bokeh a solid, stable, minimal (but extensible) platform for interactive web-based visualization. This talk will briefly discuss some of the history of the project leading up to the present, including lessons learned about OSS development.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As Bokeh rapidly approaches a 1.0 release, the project focus is shifting towards making Bokeh a solid, stable, minimal (but extensible) platform for interactive web-based visualization. This is great news for users, but also means that developers of higher-level or domain-specific tools can build upon it with confidence.&lt;/p&gt;
&lt;p&gt;This talk will briefly discuss some of the history of the project leading up to the present, including lessons learned about OSS development. After this quick status update, I will discuss and demonstrate two tools that are already building on and integrating with Bokeh:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Datashader is a sophisticated Python rendering pipeline that can be used together with Bokeh to effectively visualize large (billion+ point) data sets quickly.&lt;/li&gt;
&lt;li&gt;Holoviews is a very high level data language for slicing and dicing datasets that can automatically build interactive visualizations using Bokeh, based on the structure of your data.&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bryan Van de ven</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/bokeh-and-friends.html</guid></item><item><title>bqplot Interactive Data Visualization in Jupyter</title><link>https://pyvideo.org/pydata-seattle-2017/bqplot-interactive-data-visualization-in-jupyter.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;bqplot is an interactive plotting library for the Jupyter notebook in which every attribute of the plot is an interactive widget. bqplot can be linked with Jupyter widgets to create rich visualizations with just a few lines of Python code. These visualizations, which are based on D3.js and SVG, provide an unparalleled level of interactivity without a single line of JavaScript code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;bqplot is an interactive 2D plotting library for the Jupyter notebook in which every attribute of the plot is an interactive widget. bqplot can be linked with other Jupyter widgets to create rich visualizations from just a few lines of Python code. Since bqplot is built on top of the widgets framework of the notebook it leverages the widget infrastructure to provide the first plotting library that communicates between Python and JavaScript code. The visualizations are based on D3.js and SVG, enabling fast interactions and beautiful animations. In this talk, attendees will learn how to build interactive charts, dashboards and rich GUI applications using bqplot and ipywidgets.&lt;/p&gt;
&lt;p&gt;In the first part of the talk, we will walk the user through the bqplot API:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Grammar of Graphics based object model (axes, scales, marks etc) which lets users build custom visualizations&lt;/li&gt;
&lt;li&gt;Simple API similar to Matplotlib's pyplot, which provides sensible default choices for most parameters&lt;/li&gt;
&lt;li&gt;Interaction API which lets the user interact with the charts (selecting subset of data, panzoom etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will also review how bqplot ties into the new JupyterLab IDE, by demonstrating how the charts leverage the dashboarding, resizing and output mirroring tools of JupyterLab.&lt;/p&gt;
&lt;p&gt;In the second part of the talk, drawing examples from fields like Data Science and Finance we will show examples of building interactive charts and dashboards using bqplot and ipywidgets. We will work through examples of using bqplot with popular deep learning libraries to build custom visual dashboards directly in the notebook, including network visualizations and other novel interactive ways to control your training. We will visit the use of the novel selections offered by bqplot to show how they can be used to create advanced applications for time series data. Finally, we will visit the notion of templates - reusable interactive objects that enhance not only end user workflow, but provide developers and researchers seamless ways to incorporate interactivity into their development workflows.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dhruv Madeka</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/bqplot-interactive-data-visualization-in-jupyter.html</guid></item><item><title>Building a Brain Observatory for Visual Behavior</title><link>https://pyvideo.org/pydata-seattle-2017/building-a-brain-observatory-for-visual-behavior.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Link to Slides: &lt;a class="reference external" href="https://www.slideshare.net/secret/x8P1vnpGJIh31t"&gt;https://www.slideshare.net/secret/x8P1vnpGJIh31t&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Description
We are developing the next generation of the Allen Brain Observatory, in which we will release neuronal activity across multiple cell types and brain regions while mice are making decisions based on the images they see. In this talk, we provide an overview of our Python-based infrastructure built to integrate automated training of mice into our existing open neuroscience data generation pipeline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Allen Brain Observatory, the newest publicly available resource from the Allen Institute for Brain Science, presents the first standardized and freely downloadable survey of neuronal activity in the mouse visual cortex, featuring representations of the visual world among 27,154 neurons at the latest release. We are developing the second generation of this dataset, in which we will release neuronal activity across multiple cell types and brain regions while mice are making decisions based on the images they see. In this talk, we provide an overview of our Python-based infrastructure built to integrate automated training of mice into our existing open neuroscience data generation pipeline. This infrastructure includes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Automated training algorithms built on PsychoPy &amp;amp; PyDAQmx&lt;/li&gt;
&lt;li&gt;PyQT &amp;amp; Flask-based GUIs that allow technicians to manage training sessions &amp;amp; maintain mouse health records&lt;/li&gt;
&lt;li&gt;Real time monitoring &amp;amp; web-based data visualization with VEGA &amp;amp; pyzmq&lt;/li&gt;
&lt;li&gt;Posthoc analysis and visualization with pandas, matplotlib, &amp;amp; seaborn&lt;/li&gt;
&lt;li&gt;Automated progression through training stages using the transitions state machine framework&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Justin Kiggins</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/building-a-brain-observatory-for-visual-behavior.html</guid></item><item><title>Building a community fountain around your data stream</title><link>https://pyvideo.org/pydata-seattle-2017/building-a-community-fountain-around-your-data-stream.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With the trend towards data streams, building successful streaming analysis systems means building a community comfortable with streaming tech. But getting started with stream processing can be intimidating for anyone. In this talk, I’ll talk about designing and deploying a mini-testbed system to scale down the stream and how you can practice your favorite algorithm on an astronomical data stream.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The increasing availability of real-time data sources and the Internet of Things movement have pushed data analysis pipelines towards stream processing. But what does this really mean for my applications, and how do I have to change my code and workflow? In a new era of “Kappa architecture,” it’s easier than ever to use the same programming model for both batch and stream processing.&lt;/p&gt;
&lt;p&gt;For those interested in the design and operations side, I will cover high-level design considerations for architecting a modular and scalable stream processing infrastructure that can support the flexibility of different use cases and can welcome a community of users who are more familiar with batch processing.&lt;/p&gt;
&lt;p&gt;For the fast-batching Pythonistas, I’ll talk about some of the advantages of using streaming tech in a data processing pipeline and how to make your life easier with&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;built-in replication, scalability, and stream “rewind” for data distribution with Kafka,&lt;/li&gt;
&lt;li&gt;structured messages with strictly enforced schemas and dynamic typing for fast parsing with Avro, and&lt;/li&gt;
&lt;li&gt;a stream processing interface that is similar to batch with Spark that you can even use in a Jupyter notebook.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you’re ready to jump into the stream, or at least take a drink from the fountain, I’ll point you to an open source, containerized (with Docker), streaming ecosystem testbed that you can deploy to mock a stream of data and take your streaming analytics on a dry run over an astronomical data stream.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maria Patterson</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/building-a-community-fountain-around-your-data-stream.html</guid></item><item><title>Changing the Bureaucrat’s Mind Toward Data Driven Decision</title><link>https://pyvideo.org/pydata-seattle-2017/changing-the-bureaucrats-mind-toward-data-driven-decision.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Mr. Gary Dunow will discuss the National Geospatial-Intelligence Agency (NGA), its challenges with big data, and some of the ways that NGA is taking on these challenges.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Data-Driven Decisions&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gary Dunow</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/changing-the-bureaucrats-mind-toward-data-driven-decision.html</guid></item><item><title>Chatbots Past, Present and Future</title><link>https://pyvideo.org/pydata-seattle-2017/chatbots-past-present-and-future.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In the past few months, chatbots have made headlines and have become a much-required interface for communication. In this talk, we will discuss three aspects of chatbots:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;How chatbots have evolved from Natural Language Processing and Artificial Intelligence&lt;/li&gt;
&lt;li&gt;Internals and Technologies involved&lt;/li&gt;
&lt;li&gt;How to build chatbots yourself and the questions to ask yourself before building one&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the past few months, chatbots have made headlines and have become a much-required interface for communication. In this talk, we will discuss three aspects of chatbots:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;How chatbots have evolved from Natural Language Processing and Artificial Intelligence In this segment we will answer the following questions:&lt;/dt&gt;
&lt;dd&gt;&lt;ul class="first last"&gt;
&lt;li&gt;What research-areas have led to the evolution of chatbots?&lt;/li&gt;
&lt;li&gt;How important is it for a chatbot to appear human, and to pass the Turing test?&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;Internals and Technologies involved in Chatbots In this segment we will answer the following questions:&lt;/dt&gt;
&lt;dd&gt;&lt;ul class="first last"&gt;
&lt;li&gt;What are the tools and technologies do you need to know about in order to make computers understand human language?&lt;/li&gt;
&lt;li&gt;Which python libraries should one be familiar with to build Language Understanding interfaces?&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;How to build chatbots yourself and the questions to ask yourself before building one In this segment we will answer the following questions:&lt;/dt&gt;
&lt;dd&gt;&lt;ul class="first last"&gt;
&lt;li&gt;Which API's are available for building chatbots&lt;/li&gt;
&lt;li&gt;Pro/Con's for using API's vs building chatbots from scratch&lt;/li&gt;
&lt;li&gt;Where can one host their chatbots?&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally we will discuss where chatbots are heading to from here&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr. Rutu Mulkar-Mehta</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/chatbots-past-present-and-future.html</guid></item><item><title>Code First, Math Later: Learning Neural Nets Through Implementation and Examples</title><link>https://pyvideo.org/pydata-seattle-2017/code-first-math-later-learning-neural-nets-through-implementation-and-examples.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will cover learning about neural networks through programming and experimentation. In particular, we will use the Keras library as a straightforward way to quickly implement popular neural network architectures such as feed-forward, convolutional, and recurrent networks. We'll also focus on internal data transformations within these networks, as information is passed from layer to layer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Programming frameworks for implementing neural networks have become easy to use, and now allow for rapid prototyping and experimentation. These frameworks can also be used as teaching tools for those who are getting started in neural networks and deep learning. However, often students and practitioners start from textbooks and research papers in order to learn about these powerful techniques, and get bogged down in mathematical notation and jargon. This talk proposes a different approach through three high-level steps:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;learning the basics of neural network architectures and applications,&lt;/li&gt;
&lt;li&gt;experimenting with these models through code examples, and&lt;/li&gt;
&lt;li&gt;revisiting the math and theory behind these models with a more practical understanding of how they work.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will focus mainly on architectures for three popular types of neural networks (feed-forward, convolutional, and recurrent), setting aside the issue of optimizing these networks during training.&lt;/p&gt;
&lt;p&gt;This talk assumes some familiarity with supervised machine learning and classification, but assumes no prior knowledge of neural networks or deep learning. A familiarity with Python is beneficial, since this talk presents neural nets primarily from the perspective of programming using a high-level library. However, if you are familiar with another programming language or deep learning library, the concepts will likely make sense.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kyle Shaffer</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/code-first-math-later-learning-neural-nets-through-implementation-and-examples.html</guid></item><item><title>Data processing with Apache Beam</title><link>https://pyvideo.org/pydata-seattle-2017/data-processing-with-apache-beam.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, we present the new Python SDK for Apache Beam - a parallel programming model that allows one to implement batch and streaming data processing jobs that can run on a variety of execution engines like Apache Spark and Google Cloud Dataflow. We will use examples to discuss some of the interesting challenges in providing a Pythonic API and execution environment for distributed processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Currently some popular data processing frameworks such as Apache Spark consider batch and stream processing jobs independently. The APIs across different processing systems such as Apache Spark or Apache Flink are also different. This forces the end user to learn a potentially new system every time. Apache Beam [1] addresses this problem by providing a unified programming model that can be used for both batch and streaming pipelines. The Beam SDK allows the user to execute these pipelines against different execution engines. Currently Apache Beam provides a Java and Python SDK.&lt;/p&gt;
&lt;p&gt;In the talk, we start off by providing an overview of Apache Beam using the Python SDK and the problems it tries to address from an end user’s perspective. We cover the core programming constructs in the Beam model such as PCollections, ParDo, GroupByKey, windowing and triggers. We describe how these constructs make it possible for pipelines to be executed in a unified fashion in both batch and streaming. Then we use examples to demonstrate these capabilities. The examples showcase using Beam for stream processing and real time data analysis, and how Beam can be used for feature engineering in some Machine Learning applications using Tensorflow. Finally, we end with Beam's vision of creating runner and execution independent graphs using the Beam FnApi [2].&lt;/p&gt;
&lt;p&gt;Apache Beam [1] is a top level Apache project and is completely open source. The code for Beam can be found on Github [3].&lt;/p&gt;
&lt;p&gt;[1] &lt;a class="reference external" href="https://beam.apache.org/"&gt;https://beam.apache.org/&lt;/a&gt; [2] &lt;a class="reference external" href="http://s.apache.org/beam-fn-api"&gt;http://s.apache.org/beam-fn-api&lt;/a&gt; [3] &lt;a class="reference external" href="https://github.com/apache/beam"&gt;https://github.com/apache/beam&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sourabh Bajaj</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/data-processing-with-apache-beam.html</guid></item><item><title>Data Visualization and Exploration with Python</title><link>https://pyvideo.org/pydata-seattle-2017/data-visualization-and-exploration-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Visualization is an essential method in any data scientist’s toolbox and is a key data exploration method and is a powerful tool for presentation of results and understanding problems with analytics. Attendees are introduced to Python visualization packages, Matplotlib, Pandas, and Seaborn. The Jupyter notebook can be downloaded at &lt;a class="reference external" href="https://github.com/StephenElston/ExploringDataWithPython"&gt;https://github.com/StephenElston/ExploringDataWithPython&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Visualization of complex real-world datasets presents a number of challenges to data scientists. By developing skills in data visualization, data scientists can confidently explore and understand the relationships in complex data sets. Using the Python matplotlib, pandas plotting and seaborn packages attendees will learn to:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Explore complex data sets with visualization, to develop understanding of the inherent relationships.&lt;/li&gt;
&lt;li&gt;Create multiple views of data to highlight different aspects of the inherent relationships, with different graph types.&lt;/li&gt;
&lt;li&gt;Use plot aesthetics to project multiple dimensions.&lt;/li&gt;
&lt;li&gt;Apply conditioning or faceting methods to project multiple dimensions&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Stephen Elston</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/data-visualization-and-exploration-with-python.html</guid></item><item><title>Designing for Guidance in Machine Learning</title><link>https://pyvideo.org/pydata-seattle-2017/designing-for-guidance-in-machine-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How does the task of developing a machine learning system change when we not only have to predict outcomes from inputs, but also guide users to make their inputs better? Using practical examples in Python, I'll explore some of the lessons we've learned building an augmented writing platform at Textio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Machine learning systems are getting better and better at tasks that we used to think only humans could be good at. I can write a program to look at an image of an animal and tell you whether it’s a cat or dog, for example. The measure of my program’s quality is its performance on new inputs: how many images does it classify correctly without ever having seen them before? How I accomplish that—what exactly it is about the images that allows me to distinguish a cat from a dog—is almost irrelevant compared to how well my program does on the task.&lt;/p&gt;
&lt;p&gt;But what if I don’t just want my system to be good at recognizing that you’re showing it a picture of a cat, I also want to be able to give you clear instructions on the top n things you can do to turn that cat into a dog? Now all of a sudden I have to start thinking differently about the information that my model uses to do the classification. I need features that are still automatable (I can tell a computer how to measure them and use them as inputs to a machine learning algorithm) but are also explainable (a lay person can understand what they mean and how to change their values). I may also find issues in my training/test data that I might not have noticed if I hadn’t had to do the same amount of introspection.&lt;/p&gt;
&lt;p&gt;In this talk, I’ll walk through a toy machine learning problem end-to-end, showing how our approach has to change when we add the requirement of producing actionable guidance. The content will be targeted at people who have an interest in machine learning, but experience isn’t necessary.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Olivia Gunton</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/designing-for-guidance-in-machine-learning.html</guid></item><item><title>D’oh! Unevenly spaced time series analysis of The Simpsons in Pandas</title><link>https://pyvideo.org/pydata-seattle-2017/doh-unevenly-spaced-time-series-analysis-of-the-simpsons-in-pandas.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial will explore the use of tools in the Pandas data analysis library for analyzing unevenly spaced time series data. The tutorial will start off with a brief primer on Pandas and the data.world API, and demonstrate how to use Pandas tools for analyzing data from The Simpsons episodes from data.world.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Indeed data scientists occasionally analyze time series data in which the events of interest are unevenly spaced. For example, when we want to understand how a change to a user interface for Indeed Hire recruiters affects the time it takes them to review candidates, we might look at changes in time intervals between individual candidate dispositions in our logs. When we want to understand the ratio of new business to repeat business - or explore different definitions of repeat business - we analyze the intervals in the creation dates of new requisitions from the same client.&lt;/p&gt;
&lt;p&gt;The Pandas data analysis library offers powerful tools for conducting time series analysis. When working on unevenly spaced time series, we have found the shift() and transform() DataFrame methods particularly helpful. Many of the examples of using these methods that we found on the web were used only on small, artificial datasets. Determining how best to apply them to real datasets was not always as straightforward as we would have hoped.&lt;/p&gt;
&lt;p&gt;Rather than use internal proprietary data to illustrate examples of how these methods can be used effectively to analyze unevenly spaced time series data, we will instead use data from a publicly available dataset of episodes of The Simpsons at data.world (&lt;a class="reference external" href="https://data.world/data-society/the-simpsons-by-the-data"&gt;https://data.world/data-society/the-simpsons-by-the-data&lt;/a&gt;). In doing so, we will also provide an introduction on how to use the data.world API.&lt;/p&gt;
&lt;p&gt;The purpose of this tutorial is to&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Provide a brief, focused primer on some basic aspects of Pandas&lt;/li&gt;
&lt;li&gt;Provide an overview of data.world datasets and accessing them via the API&lt;/li&gt;
&lt;li&gt;Show how advanced Pandas tools can be used for analyzing unevenly spaced time series data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Participants will be best prepared for this tutorial if they&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Understand Python basics&lt;/li&gt;
&lt;li&gt;Have Python 2 or Python 3 installed on their computers&lt;/li&gt;
&lt;li&gt;Install the latest versions of Pandas and Jupyter Notebook (recommended: use Anaconda)&lt;/li&gt;
&lt;li&gt;Install the data.world Python API (&lt;tt class="docutils literal"&gt;pip install &lt;span class="pre"&gt;git+git://github.com/datadotworld/data.world-py.git&lt;/span&gt;&lt;/tt&gt;)&lt;/li&gt;
&lt;li&gt;Create a data.world account and an API key via the data.world Advanced Settings page&lt;/li&gt;
&lt;li&gt;Update: jupyter notebooks associated with the tutorial have been uploaded to a GitHub repository (&lt;a class="reference external" href="https://github.com/gumption/pydata-simpsons"&gt;https://github.com/gumption/pydata-simpsons&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joe McCarthy</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/doh-unevenly-spaced-time-series-analysis-of-the-simpsons-in-pandas.html</guid></item><item><title>Effective Visual Studio</title><link>https://pyvideo.org/pydata-seattle-2017/effective-visual-studio.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial will walk through Visual Studio 2017, including how to set it up for use with Python and R, and the features it provides that will make you a more efficient, effective, and happier data scientist or engineer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;According to Stack Overflow's developer survey for 2017, the majority of data scientists are working in Visual Studio, and with the latest release of Visual Studio 2017 including support for Python and R out of the box, there is a good reason for this. In this tutorial, we will walk through setting up Visual Studio 2017, its special features for Python and R developers, and the features it provides for all languages that will make you a more efficient, effective, and happier data scientist or engineer.&lt;/p&gt;
&lt;p&gt;Bring your Windows laptop or virtual machine, and to prepare ahead of time, install Visual Studio 2017 Community (or Professional/Enterprise if you have access) from visualstudio.com, and select the &amp;quot;Data Science and Analytical Applications&amp;quot; workload.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Steve Dower</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/effective-visual-studio.html</guid></item><item><title>Forecasting Time Series Data at scale with the TICK stack</title><link>https://pyvideo.org/pydata-seattle-2017/forecasting-time-series-data-at-scale-with-the-tick-stack.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Forecasting time series data across a variety of different time series comes with many challenges. Using the TICK stack we demonstrate a workflow that helps to overcome those challenges. Specifically we take a look at the Facebook Prophet procedure for forecasting business time series.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Forecasting time series data can require a significant amount of attention in order to get reliable results. As the number and variety of time series increases it becomes too expensive to manage each forecast individually. Using the TICK stack we demonstrate a workflow that helps to reduce the amount of attention each forecast needs. This is accomplished by using the procedure called Prophet, which was recently open sourced by Facebook.&lt;/p&gt;
&lt;p&gt;The basic idea of this procedure is two fold:&lt;/p&gt;
&lt;p&gt;Reduce the amount of effort to train and maintain a single forecast.
Automatically surface the forecasts that are performing poorly.
To reduce the amount of effort per forecast, the Facebook Prophet algorithm is a simple model with a few well understood parameters. By automatically surfacing forecasts that perform poorly, effort need only be spent when specific forecasts need attention.&lt;/p&gt;
&lt;p&gt;The last piece needed to make this process scale is a single set of tools that follow the workflow. We demonstrate how the TICK stack can be used to manage forecasting time series at scale, using InfluxDB to store the data and Kapacitor to manage and surface forecasts.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nathaniel Cook</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/forecasting-time-series-data-at-scale-with-the-tick-stack.html</guid></item><item><title>From Novice to Data Ninja</title><link>https://pyvideo.org/pydata-seattle-2017/from-novice-to-data-ninja.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial aims to introduce beginner Python learners to the diverse types of data a data scientist may face at work: time series, images, videos, text, graphs, geospatial data. In two hours participants will become comfortable handling each type of data and extracting interesting information from it. Participants will also learn tips and tricks how to handle large datasets of each kind.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data are all around us and come in all sizes and shapes. As a beginner Python learner you often dream to become a master of all data types and handle them with ease: but soon you get intimidated by piles of libraries and unfamiliar jargon surrounding specific data formats and processing tools. The good news is that once you overcome the initial hurdles, you realize that you can analyze these diverse datasets using similar methodology.&lt;/p&gt;
&lt;p&gt;The goal of this tutorial is to provide participants with hands-on experience working with diverse types of data: text, sound, video, graphs, GIS. Each data module will include an example with a small dataset to introduce the properties of the data type, and one with a larger dataset to equip the learners with tools to handle the vast data in the wild. Throughout the tutorial we will illustrate how same data mining techniques can be used on different types of data.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/valentina-s/Novice2DataNinja"&gt;https://github.com/valentina-s/Novice2DataNinja&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Valentina Staneva</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/from-novice-to-data-ninja.html</guid><category>tutorial</category></item><item><title>High Fidelity Web Crawling in Python</title><link>https://pyvideo.org/pydata-seattle-2017/high-fidelity-web-crawling-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python modules such as Requests make it easy for Python to pull HTML from a webpage which you can feed to your parsing function. What becomes difficult is converting that process into an autonomous process to crawl webpages to parse their HTML for data. This talk covers the lessons learns and solutions I’ve found to create high fidelity autonomous web crawling scripts in Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Josh Weissbock</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/high-fidelity-web-crawling-in-python.html</guid></item><item><title>High Performance Distributed Tensorflow</title><link>https://pyvideo.org/pydata-seattle-2017/high-performance-distributed-tensorflow.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this completely demo-based talk, Chris will demonstrate various techniques to post-process and optimize trained Tensorflow AI models to reduce deployment size and increase prediction performance.&lt;/p&gt;
&lt;p&gt;First, we'll use various techniques such as 8-bit quantization, weight-rounding, and batch-normalization folding, we will simplify the path of forward propagation and prediction.&lt;/p&gt;
&lt;p&gt;Next, we'll loadtest and compare our optimized and unoptimized models - in addition to enabling and disabling request batching.&lt;/p&gt;
&lt;p&gt;Last, we'll dive deep into Google's Tensorflow Graph Transform Tool to build custom model optimization functions.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Fregly</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/high-performance-distributed-tensorflow.html</guid></item><item><title>How diversity drives excellence in our data driven tech world</title><link>https://pyvideo.org/pydata-seattle-2017/how-diversity-drives-excellence-in-our-data-driven-tech-world.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In an age where decisions are increasingly powered by data, the benefits of being data-driven are undeniable. We no longer need to convince the data science community of the value of data. But can the same be said about the value of diversity? This panel discussion and QA will be framed around the benefits of inclusion efforts and how building a diverse data science community benefits everyone.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This session will be organized as a panel and QA discussion around the need for diversity, how diversity drives excellence, and incentivizing a diverse community. The panel participants will represent an array of diversity in backgrounds, experiences, current job positions, and career stages.&lt;/p&gt;
&lt;p&gt;The flow of the discussion will loosely follow this general outline: Brief panelist introductions What obstacles have you encountered to building a diverse community? What lessons can you share about overcoming these obstacles? What actionable advice do you have for someone who is trying to increase understanding of the value of building a diverse a community?&lt;/p&gt;
&lt;p&gt;The goals of the session are to build a discussion such that the audience will Learn about how others have overcome obstacles to inclusive environments. Have an increased understanding of how diversity benefits more than the minority. Connect with others and share resources for building up communities in which the importance of diversity is a foundational principle.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Various speakers</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/how-diversity-drives-excellence-in-our-data-driven-tech-world.html</guid></item><item><title>How to be a 10x Data Scientist</title><link>https://pyvideo.org/pydata-seattle-2017/how-to-be-a-10x-data-scientist.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Knowing the difference between your logistic and linear regression models or knowing how to train a model using a CNN won't make you a 10x data scientist, but there are other tips and tricks to becoming an even greater commodity to your employer than you already are. Bringing ideas from the developer community, I'll cover what you can do to increase productivity and level up your career.&lt;/p&gt;
&lt;p&gt;Using basic principles from the world of software development, this talk will cover ideas on how to become a more productive data scientist. This includes common principles such as not reinventing the wheel by using API's and libraries instead of writing your own code, writing tests to future-proof your code and be your own QA, how to make your models available to team members regardless of what language they use, how to write your code for production, versioning and automation.&lt;/p&gt;
&lt;p&gt;Data scientists will take away how they can become a 10x developer and increase their value and write better code by leveraging software developers common best practices.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Stephanie Kim</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/how-to-be-a-10x-data-scientist.html</guid></item><item><title>Implementing and Training Predictive Customer Lifetime Value Models in Python</title><link>https://pyvideo.org/pydata-seattle-2017/implementing-and-training-predictive-customer-lifetime-value-models-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Customer lifetime value models (CLVs) are powerful predictive models that allow analysts and data scientists to forecast how much customers are worth to a business. CLV models provide crucial inputs to inform marketing acquisition decisions, retention measures, customer care queuing, demand forecasting, etc. They are used and applied in a variety of verticals, including retail, gaming, and telecom.&lt;/p&gt;
&lt;p&gt;This tutorial is separated into two parts:&lt;/p&gt;
&lt;p&gt;In the first part, we will provide a brief overview of the ins and outs of probabilistic models, which can be used to quantify the future value of a customer, and demonstrate how e-commerce companies are using the outputs of these models to identify, retain, and target high-value customers.&lt;/p&gt;
&lt;p&gt;In the second part, we will implement, train, and validate predictive customer lifetime value models in a hands-on Python tutorial. Throughout the tutorial, we will use a real-world retail dataset and go over all the steps necessary to build a reliable customer lifetime value model: data exploration, feature engineering, model implementation, training, and validation. We will also use some of the probabilistic programming language packages available in Python (e.g. Stan, PyMC) to train these models.&lt;/p&gt;
&lt;p&gt;The resulting Python notebooks will lay out the foundation for more advanced models tailored to the specifics of each business setting. Throughout the tutorial, we will give the audience additional tips on how to tweak the models to fit different business settings.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jean-Rene Gauthier</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/implementing-and-training-predictive-customer-lifetime-value-models-in-python.html</guid></item><item><title>In database Machine Learning with Python in SQL Server</title><link>https://pyvideo.org/pydata-seattle-2017/in-database-machine-learning-with-python-in-sql-server.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;SQL Server 2017 has built in AI capabilities that bring Python based intelligence to where the data lives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this session, we will take a deeper look at the SQL Server Machine Learning Services with Python and its enterprise grade advantages. This enables you to use the latest innovations from the open source Python world on your data that is securely stored in SQL Server and provides additional scale, performance, operationalization and management capabilities. We will walk through a demo to build and deploy a Python based machine learning application in SQL Server to predict how long a patient is likely to stay in the hospital at the time of getting admitted to the hospital.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sumit Kumar</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/in-database-machine-learning-with-python-in-sql-server.html</guid></item><item><title>Interactive Data Analysis: Visualization and Beyond</title><link>https://pyvideo.org/pydata-seattle-2017/interactive-data-analysis-visualization-and-beyond.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data analysis is a complex process with frequent shifts among data formats, tools and models, as well as between symbolic and visual thinking. How might the design of improved tools accelerate people's exploration and understanding of data? Covering both interactive demos and principles from academic research, my talk will examine how to craft a careful balance of interactive and automated methods, combining concepts from data visualization, machine learning, and computer systems to design novel interactive analysis tools.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jeffrey Heer</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/interactive-data-analysis-visualization-and-beyond.html</guid></item><item><title>Introduction to data analytics with pandas</title><link>https://pyvideo.org/pydata-seattle-2017/introduction-to-data-analytics-with-pandas.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data analytics in Python benefits from the beautiful API offered by the pandas library. With it, manipulating and analysing data is fast and seamless. In this workshop, we'll take a hands-on approach to performing an exploratory analysis in pandas. We'll begin by importing some real data. Then, we'll clean it, transform it, and analyse it, finishing with some visualisations.&lt;/p&gt;
&lt;p&gt;Introduction
In this hands-on workshop, we'll walk through the exploratory analysis of real-world data. Datasets are often messy, full of holes and inconsistencies, and a data scientist or analyst may spend a large fraction of their time cleaning and preparing data.&lt;/p&gt;
&lt;p&gt;Fortunately, pandas makes a lot of this fairly trivial. It allows the user to import data from all sorts of different sources, and then manipulate the powerful DataFrame object. Analytics with pandas are human-friendly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Workshop&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pulling in the data
Starting with some data in CSV form, we'll look at the general properties of our dataset. What columns do we have; what kind of values are contained in them ? We'll identify problematic fields, and join two datasets to make one complete dataframe.&lt;/p&gt;
&lt;p&gt;Cleaning
We've identified problems with our data, and now it's time to correct them. We'll fill in missing values, drop irrelevant rows, and fix incorrect datatypes.&lt;/p&gt;
&lt;p&gt;Transforming the data
Next, we'll standardise some numerical fields where we're looking for deviations rather than absolute values, and derive some new columns based on the data we have.&lt;/p&gt;
&lt;p&gt;Visualisation
Throughout, we'll be generating visualisations, to guide us in where to go next.&lt;/p&gt;
&lt;p&gt;Prerequisites
You'll need to be fairly comfortable working with Python. We won't be doing anything overly complicated, but having a grasp of Python syntax is expected.&lt;/p&gt;
&lt;p&gt;Laptop
If you want to follow along, please have a working Python setup, with pandas and matplotlib installed. Aim for a recent version of pandas. If you're unsure what to install, I recommend getting Python 3 through Anaconda : &lt;a class="reference external" href="https://www.continuum.io/downloads"&gt;https://www.continuum.io/downloads&lt;/a&gt; - this distribution comes with everything you need and is very friendly.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Quentin Caudron</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/introduction-to-data-analytics-with-pandas.html</guid></item><item><title>JupyterLab+Real Time Collaboration</title><link>https://pyvideo.org/pydata-seattle-2017/jupyterlabreal-time-collaboration.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R.&lt;/p&gt;
&lt;p&gt;PyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Brian Granger</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/jupyterlabreal-time-collaboration.html</guid></item><item><title>Learn to be a painter using Neural Style Painting</title><link>https://pyvideo.org/pydata-seattle-2017/learn-to-be-a-painter-using-neural-style-painting.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Vincent Van Gogh was noticeably one of the most influential artistic figures of the Western art. Won't it be great, if one is able to teach machines to paint in a similar manner to create visually appealing images. In this talk, we will learn how to paint images with the help of Convolutional Neural Network - VGG-19 using TensorFlow, SparkMagic and Livy to imitate renowned artists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Humans have continuously mastered the art of painting images that are visually appealing. They are able mix multiple different styles to produce new styles which instantly catches our attention. Generating such high quality images using algorithms have been less explored in the past. With the advancement in computer vision and object recognition coupled by maturity of Deep Learning frameworks, recently it has become more convenient to generate high quality emotional intuitive artistic images. In 2015, Leon A. Gatys et al, published paper &amp;quot;Image Style Transfer Using Convolutional Neural Network&amp;quot; explaining how to generate artistic images using neural representation to separate and combine random input images using a very deep Convolutional Neural Network - VGG-VD. In this talk, we take a fun dive into learning to be a painter by extracting relevant feature representation from high performing Neural Network using TensoFlow, SparkMagic and Livy in a scalable manner.&lt;/p&gt;
&lt;p&gt;Take away for the audience:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Develop basic understanding of Convolutional Neural Network&lt;/li&gt;
&lt;li&gt;Realize benefits of using TensforFlow in building Deep Neural Networks&lt;/li&gt;
&lt;li&gt;Learn how to use SparkMagic and Livy to build scalable solutions&lt;/li&gt;
&lt;li&gt;Learn how to make machines paint like experts&lt;/li&gt;
&lt;li&gt;Learn how to apply this style of painting to create poster thumbnails which might have wide variety of applications&lt;/li&gt;
&lt;/ol&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Pramit Choudhary</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/learn-to-be-a-painter-using-neural-style-painting.html</guid></item><item><title>Machine Learning Infrastructure at Stripe: Bridging from Python JVM</title><link>https://pyvideo.org/pydata-seattle-2017/machine-learning-infrastructure-at-stripe-bridging-from-python-jvm.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Machine learning at Stripe has a foundation built on Python and the PyData stack, with scikit-learn and pandas continuing to be core components of an ML pipeline that feeds a production system written in Scala. This talk will cover the ML Infra team’s work to bridge the serialization and scoring gap between Python and the JVM, as well as how ML Engineers ship models to production.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Stripe Machine Learning Infrastructure team exists to help engineers, data scientists, and analysts at Stripe develop and ship models to production. They own and operate the primary service that provides an API for scoring models for applications such as fraud and NLP, and are always looking for ways to help internal Stripe customers ship ML for new applications or model types.&lt;/p&gt;
&lt;p&gt;ML models at Stripe are trained and evaluated in Python, with scikit-learn as an integral piece in our pipeline. However, the primary scoring service is written in Scala, which presents us with a problem: how do we serialize and export models from Python to the JVM? This talk will discuss our serialization framework for serializing and packaging machine learning components; by the end you will learn how we export models, transformers, encoders, and pipelines from the world of scikit to that of our Scala service.&lt;/p&gt;
&lt;p&gt;We'll then cover what happens after the model has been loaded by our Scala service, namely how we name models uniquely and use metadata we call &amp;quot;tags&amp;quot; to keep track of what model is currently running in production, history of production models, etc. We'll discuss how we score candidate models in parallel to the production model to evaluate them for promotion to production.&lt;/p&gt;
&lt;p&gt;By the end of the talk you should have a clear idea of how we serialize, package, promote, and evaluate candidate models across the entire machine learning infra stack, from the start of training in Python to the final scoring in Scala.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rob Story</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/machine-learning-infrastructure-at-stripe-bridging-from-python-jvm.html</guid></item><item><title>Make it Work, Make it Right, Make it Fast Debugging and Profiling in Dask</title><link>https://pyvideo.org/pydata-seattle-2017/make-it-work-make-it-right-make-it-fast-debugging-and-profiling-in-dask.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a pure python library for parallel and distributed computing. It's designed with flexibility in mind, making it easy to parallelize the complicated workflows often found in science. However, once you get something working, how do you debug or profile it? In this talk we'll cover the various tools Dask provides for diagnosing bugs and bottlenecks, as well as tips for resolving these issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dask is a pure python library for parallel and distributed computing. It's designed with simplicity and flexibility in mind, making it easy to parallelize the complicated workflows often found in science. However, once you get something working, how do you debug or profile it? Debugging and profiling parallel code is notoriously hard! In this talk we'll cover the various tools Dask provides for diagnosing bugs and performance bottlenecks, as well as tips and techniques for resolving these issues.&lt;/p&gt;
&lt;p&gt;Starting with an example single-threaded probram, we'll walk through adding Dask to parallelize it, and then iterate on this example to gradually improve performance throughout the talk. Attendees should leave having a better understanding of:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What tools Dask provides for debugging on a single machine&lt;/li&gt;
&lt;li&gt;How Dask uses IPython to make debugging distributed computations easy&lt;/li&gt;
&lt;li&gt;How to profile Dask code both on a single machine and on a cluster&lt;/li&gt;
&lt;li&gt;How to interpret the graphs presented in the Dask Dashboard&lt;/li&gt;
&lt;li&gt;Performance techniques for attacking bottlenecks once they've been identified.&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jim Crist</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/make-it-work-make-it-right-make-it-fast-debugging-and-profiling-in-dask.html</guid></item><item><title>Making packages and packaging "just work"</title><link>https://pyvideo.org/pydata-seattle-2017/making-packages-and-packaging-just-work.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python is a wonderful language, capable of gluing just about any number of other languages together, allowing us to reap the strengths of each language. The cost of this is myriad opportunities for library and binary incompatibility. This talk will cover some of the common ways that incompatibility arises, as well as several approaches that Continuum uses or is developing to reduce headache.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each language and each package has some things that it does better than others. One of Python's major strengths is its ability to readily interface with many other languages and harness their strengths. When it comes to speed, compiled, statically-typed languages are hard to beat. Because of this, data science libraries often harness compiled shared libraries for their compute-intensive operations. In a small environment with few of these dependencies, this often works quite well. However, as the number of binary dependencies grows, the risk of incompatibility increases.&lt;/p&gt;
&lt;p&gt;The Anaconda distribution and its package manager, Conda, have been very successful as platforms in part because the wide array of packages built for Anaconda by Continuum are built to be binary compatible. Unfortunately, as more people and more communities build their own Conda packages that are not binary compatible with Anaconda or each other, more users have expressed confusion and consternation when things don't just work in all cases.&lt;/p&gt;
&lt;p&gt;To address this issue, we have needed to develop tooling at both package build time, to better track and establish the actual limits of binary compatibility, and also at package install time, to allow conda to make better use of this information to inform users of what they can expect to work. We’ll talk about new features in our build tool, conda-build, that helps us choose our desired binary compatibility. We’ll also talk about our new automated build system that helps us flesh out a much larger range of compatibility by building more combinations of packages with limited binary compatibility. Finally, we’ll also talk about the extra data that you’ll soon be able to provide to Conda, so that you will have more power to say exactly what you want.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Michael Sarahan</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/making-packages-and-packaging-just-work.html</guid></item><item><title>Monitoring Displacement Crises with Python</title><link>https://pyvideo.org/pydata-seattle-2017/monitoring-displacement-crises-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data for Democracy is a civic engagement hub, for volunteer data scientists to carry out work with social impact. This talk will focus on the work of one team, who have built a web scraper and natural language processing pipeline to track and analyse online reports of people displaced by conflict and disaster. It will also reflect on the challenges faced using data in the humanitarian sector.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data for Democracy is a community of civic minded volunteer technologists, programmers and data scientists working on everything from understanding propaganda, to reducing urban traffic fatalities, to making election data available to the public. One of the main projects is focused on tracking online reports of internally displaced persons (IDPs) - people forced to flee from conflict or disaster but who remain within their original country of residence. The team has been been working in response to a call by the International Displacement Monitoring Centre, for solutions to the problem of collecting and analysing data from different news sources about situations involving IDPs.&lt;/p&gt;
&lt;p&gt;The group has developed a Python back-end that scrapes web pages, extracts content, tags and filters articles by topic, and retrieves key information such as the number of people displaced. The solution makes full use of the spectrum of packages available in the Python toolkit, including newspaper to parse online articles, gensim for powerful, efficient topic modelling, scikit-learn for article classification, and sqlalchemy for database handling. This talk will provide an overview to the technical approach used by the multidisciplinary and international team to tame the messy unstructured data and provide a prototype product that can be used by humanitarian analysts to monitor displacement crisis information.&lt;/p&gt;
&lt;p&gt;The presentation will also highlight the challenges and successes that come with working in a group of volunteers spread across multiple timezones, disciplines, and experience levels, to create a data product for a sector that has traditionally been slow to make use of technology. Through the story of this project, the motivations and wider efforts of the volunteer-led Data for Democracy community will also be highlighted, showing the power that data practitioners have to make a difference.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">George Richardson</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/monitoring-displacement-crises-with-python.html</guid></item><item><title>Mosaicking the Earth every day</title><link>https://pyvideo.org/pydata-seattle-2017/mosaicking-the-earth-every-day.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Planet's mission is to image the surface of the Earth every day. With over 140 Earth observation satellites currently in orbit imaging over 100 million square kilometers of land area per day, we are approaching that goal and expect to achieve it later this year. With a data pipeline built with open source tools, we process terabytes of data every data and manage an archive of petabytes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Global mosaics are created from Planet satellite images at regular intervals (quarterly, monthly, and weekly) by selecting the best quality scenes (e.g. cloud- and haze-free), color balancing, and seamlessly compositing millions of scenes to create continuous maps of the Earth for each time slice. As our data rate increases, we are scaling up the cadence of our mosaics, and plan to build a continuously updated &amp;quot;dynamic&amp;quot; mosaic of the most recent cloud-free images of the Earth. Daily data at 5 meter spatial resolution will open up new analysis techniques previously limited by the temporal or spatial resolution of existing instruments.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kelsey Jordahl</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/mosaicking-the-earth-every-day.html</guid></item><item><title>Moving notebooks into the cloud: challenges and lessons learned</title><link>https://pyvideo.org/pydata-seattle-2017/moving-notebooks-into-the-cloud-challenges-and-lessons-learned.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The product and engineering teams of Civis Analytics integrated Jupyter notebooks into our cloud-based platform, providing the ability to run multiple notebooks concurrently and share them. We'll discuss the technical challenges we encountered and how we solved them, and what we learned about notebook users and their user stories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In late 2016, we decided to make notebooks a core component of our data-science platform, giving each user the ability to run multiple notebooks concurrently in the cloud and share them. To do this, we had to tackle the problem from both product and engineering perspectives. Along the way, we learned about how notebooks fit into data-science work and how we could best leverage them to provide value to our users.&lt;/p&gt;
&lt;p&gt;We present major findings from user research that we conducted, including user surveys, a design sprint, and analysis of Civis data scientists' notebooks. For instance, we learned that in addition to providing an exploratory workspace, notebooks can be used as deliverables in two very different ways: to document a particular analysis, and to build reports or dashboards. The former requires that running the notebook generate the same results every time. The latter requires updated results every time. Our users often get their data by querying a live system, so we built an always-on data store of past queries that can be used when analyses must be reproducible. We also educated our users about various tools for building reports from notebooks.&lt;/p&gt;
&lt;p&gt;Another significant engineering challenge is collecting the dependencies for a notebook. Notebooks are typically used on local machines that accumulate state over time. This is in contrast to cloud instances, which are dynamically provisioned. Bundling dependencies with a notebook is a major value-add, as it allows dependencies to be changed and shared easily without affecting other notebooks. We were able to take advantage of Docker on Kubernetes to manage dependencies and bring provisioning delays down to acceptable levels, though this remains a significant technical challenge.&lt;/p&gt;
&lt;p&gt;Finally, we present ways that notebook products could improve to better integrate with enterprise cloud platforms. For example, better support for non-local filesystems or UI for observing the status of long-running operations (querying a remote data warehouse, for example) could help move cloud notebooks further into the enterprise.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Saranga Komanduri</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/moving-notebooks-into-the-cloud-challenges-and-lessons-learned.html</guid></item><item><title>Online Change Point Detection Using Spark Streaming</title><link>https://pyvideo.org/pydata-seattle-2017/online-change-point-detection-using-spark-streaming.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Scan Statistics is a distribution based methodology for detecting anomalies. This talk will explore the use of scan statistics to perform real time analysis on streaming data using Spark Streaming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Scan Statistics is a distribution based methodology for detecting anomalous data. Unlike simpler methodologies like moving average and exponential smoothing that rely on previous data, we can perform a hypothesis test regarding the distribution of the data and thus perform the analysis in real time. Spark Streaming is a framework that lends itself well to this use case. This talk will introduce a Python package built for Spark Streaming that performs real time anomaly detection using various distributions of count data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Michal Monselise</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/online-change-point-detection-using-spark-streaming.html</guid></item><item><title>Pandas, Pipelines, and Custom Transformers</title><link>https://pyvideo.org/pydata-seattle-2017/pandas-pipelines-and-custom-transformers.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Using pandas and scikit-learn together can be a bit clunky. For complex preprocessing, the scikit-learn Pipeline conveniently chains together transformers. But, it will convert your DataFrame to a numpy array. In this talk, we will walk through pandas DataFrames, scikit-learn preprocessing and Pipelines, and how to use custom transformers to stay in pandas land.&lt;/p&gt;
&lt;p&gt;GitHub Link: &lt;a class="reference external" href="https://github.com/jem1031/pandas-pipelines-custom-transformers"&gt;https://github.com/jem1031/pandas-pipelines-custom-transformers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For data science in python, the pandas DataFrame is a common choice to store and manipulate data sets. It has named columns, each of which can contain a different data type, and an index to identify rows and assist in joining. The scikit-learn package is the major machine learning library in python. It has implementations for a wide variety of popular feature engineering, supervised, and unsupervised machine learning algorithms. Perhaps even more importantly to its success, scikit-learn provides a uniform interface for these transformers and estimators, making it easy to swap out one for another.&lt;/p&gt;
&lt;p&gt;Many scikit-learn transformers will take and return pandas DataFrames, but some only return numpy arrays. This means losing the column names and row indices. A few important examples include the meta-transformers Pipeline and FeatureUnion. The Pipeline chains together transformers to be applied in order. The FeatureUnion combines the results of transformers that can be applied in parallel. With these, the entire feature engineering process can be stored in one object and easily applied to new data sets.&lt;/p&gt;
&lt;p&gt;Luckily, scikit-learn also provides the ability to write your own custom transformers. It is as simple as defining a new class that implements the fit and transform methods. We can use this to create pandas-friendly versions of the Pipeline and FeatureUnion, as well as add transformations that are not already provided.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Julie Michelman</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/pandas-pipelines-and-custom-transformers.html</guid></item><item><title>Parallelizing Scientific Python with Dask</title><link>https://pyvideo.org/pydata-seattle-2017/parallelizing-scientific-python-with-dask.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a flexible tool for parallelizing Python code on a single machine or across a cluster. It builds upon familiar tools in the PyData ecosystem (e.g. NumPy and Pandas) while allowing them to scale across multiple cores or machines. This tutorial will cover both the high-level use of dask collections, as well as the low-level use of dask graphs and schedulers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dask is a flexible tool for parallelizing Python code on a single machine or across a cluster.&lt;/p&gt;
&lt;p&gt;We can think of dask at a high and a low level&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;High level collections: Dask provides high-level Array, Bag, and DataFrame collections that mimic and build upon NumPy arrays, Python lists, and Pandas DataFrames, but that can operate in parallel on datasets that do not fit into main memory.&lt;/li&gt;
&lt;li&gt;Low Level schedulers: Dask provides dynamic task schedulers that execute task graphs in parallel. These execution engines power the high-level collections mentioned above but can also power custom, user-defined workloads to expose latent parallelism in procedural code. These schedulers are low-latency and run computations with a small memory footprint.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Different users operate at different levels but it is useful to understand both. This tutorial will cover both the high-level use of dask.array and dask.dataframe and the low-level use of dask graphs and schedulers. Attendees will come away&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Able to use dask.delayed to parallelize existing code&lt;/li&gt;
&lt;li&gt;Understanding the differences between the dask schedulers, and when to use one over another&lt;/li&gt;
&lt;li&gt;With a firm understanding of the different dask collections (dask.array and dask.dataframe) and how and when to use them.&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jim Crist</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/parallelizing-scientific-python-with-dask.html</guid></item><item><title>PixieDust make Jupyter Notebooks with Apache Spark Faster, Flexible, and Easier to use</title><link>https://pyvideo.org/pydata-seattle-2017/pixiedust-make-jupyter-notebooks-with-apache-spark-faster-flexible-and-easier-to-use.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PixieDust is a new Python open source library that helps data scientists and developers working in Jupyter Notebooks and Apache Spark to be more efficient. PixieDust speeds up data manipulation and visualisation with features like auto-visualisation, and integration with cloud services. Come along and learn how you can use this tool in your own projects to visualize and explore data effortlessly&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;PixieDust is a new Python open source library that helps data scientists and developers working in Jupyter Notebooks and Apache Spark to be more efficient. PixieDust speeds up data manipulation and display with features like: Automated local install of Python and Scala kernels running with Spark Realtime Spark Job progress monitoring directly from the Notebook Use Scala directly in your Python notebook. Variables are automatically transferred from Python to Scala and vice-versa Auto-visualisation of Spark DataFrames using popular chart engines like Matplotlib, Seaborn, Bokeh, or MapBox Seamless integration to cloud services Create embedded apps with your own visualisations or apps using the PixieDust extensibility APIs Come along and learn how you can use this tool in your own projects to visualise and explore data effortlessly with no coding. If you prefer working with a Scala Notebook, this session is also for you, as PixieDust can also run on a Scala Kernel. Imagine being able to visualise your favourite Python chart engines from a Scala Notebook! This session will end with a demo combining Twitter, Watson Tone Analyser, Spark Streaming, and some fun real-time visualisations - all running within a Notebook.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raj Singh</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/pixiedust-make-jupyter-notebooks-with-apache-spark-faster-flexible-and-easier-to-use.html</guid></item><item><title>pomegranate: fast and flexible probabilistic modeling in python</title><link>https://pyvideo.org/pydata-seattle-2017/pomegranate-fast-and-flexible-probabilistic-modeling-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;I will describe the python package pomegranate, which implements flexible probabilistic modeling in cython. I will highlight several supported models including mixtures, hidden Markov models, and Bayesian networks. At each step I will show that these models are both faster and more flexible than other implementations. In addition, I will describe the built-in out-of-core and parallel APIs.&lt;/p&gt;
&lt;p&gt;Link to slides: &lt;a class="reference external" href="http://noble.gs.washington.edu/~maxwl/2017-07-05%20pydata%20pomegranate.pdf"&gt;http://noble.gs.washington.edu/~maxwl/2017-07-05%20pydata%20pomegranate.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk I will give an full tutorial for the python package pomegranate, which is a flexible probabilistic modeling package implemented in cython for speed. I will highlight several models it supports, specifically probability distributions, mixture models, naive Bayes, Markov chains, hidden Markov models, and Bayesian networks. At each step I will show that these models are both faster and more flexible than other implementations in the open source community along with code examples. In addition, I will show how to utilize the underlying modularity of the code to stack these models to produce more complicated ones such as mixtures of Bayesian networks, or HMMs with complicated mixture emissions. Lastly, I will show how easy it is to use the built-in out-of-core and parallel APIs to allow for multithreaded training of complex models on massive amounts of data which can't fit in data-- all without the user having to think about any implementation details. An accompany Jupyter notebook will allow users to follow along, see code examples for all figures presented, and make modifications.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maxwell W Libbrecht</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/pomegranate-fast-and-flexible-probabilistic-modeling-in-python.html</guid></item><item><title>Practical Optimization for Stats Nerds</title><link>https://pyvideo.org/pydata-seattle-2017/practical-optimization-for-stats-nerds.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many models important to inferential statistics and machine learning use some form of optimization under the hood. For example, least squares regression and support vector machines are both implemented as simple optimization models. With the right tools in your hands, optimization can do so much more! This talk shows how to implement familiar statistical models directly using optimization solvers.&lt;/p&gt;
&lt;p&gt;Key take-aways&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Many statistical techniques are based on some sort of optimization.&lt;/li&gt;
&lt;li&gt;Optimization has lots of uses, such as solving decision models.&lt;/li&gt;
&lt;li&gt;Learning to structure problems you already know for optimization solvers is a great way to understand them!&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ryan J. O'Neil</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/practical-optimization-for-stats-nerds.html</guid></item><item><title>Provenance for Reproducible Data Science</title><link>https://pyvideo.org/pydata-seattle-2017/provenance-for-reproducible-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In science, results that are not reproducible by peer scientists are valueless and of no significance. Good practices for reproducible science are to publish used codes under Open Source licenses, perform code reviews, save the computational environments with containers (e.g., Docker), use open data formats, use a data management system, and record the provenance of all actions.&lt;/p&gt;
&lt;p&gt;The provenance of data provides detailed information about the origin of that data. That includes information about ownership and both actions and modifications performed on the data. With provenance information, data will be traceable and users can be confident in quality of the data. To specify and store provenance information, W3C has standardized the provenance model PROV. Using PROV and associated implementations, users can record provenance of data analytics processes. The provenance information are directed acyclic graphs that can be analysed to get insight into the data analytics processes.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;The talk covers&lt;/dt&gt;
&lt;dd&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;Introduction to provenance and PROV&lt;/li&gt;
&lt;li&gt;Modelling provenance for data processing&lt;/li&gt;
&lt;li&gt;Python APIs for provenance recording&lt;/li&gt;
&lt;li&gt;Provenance recording for Jupyter notebooks&lt;/li&gt;
&lt;li&gt;Storing provenance in graph databases&lt;/li&gt;
&lt;li&gt;Analysis of provenance information&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andreas Schreiber</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/provenance-for-reproducible-data-science.html</guid></item><item><title>Pydata 101</title><link>https://pyvideo.org/pydata-seattle-2017/pydata-101.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The PyData ecosystem is vast and powerful, but it can be overwhelming to newcomers. In this talk I outline some of the history of &lt;em&gt;why&lt;/em&gt; the Python data science space is the way it is, as well as &lt;em&gt;what&lt;/em&gt; tools and techniques you should focus on to get started for your own problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jake VanderPlas</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/pydata-101.html</guid><category>keynote</category></item><item><title>Python and IoT: From Chips and Bits to Data Science</title><link>https://pyvideo.org/pydata-seattle-2017/python-and-iot-from-chips-and-bits-to-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will take you through the design of a smart lighting system, including sensor hardware and software (based around MicroPython), data analysis (using NumPy, Pandas, and Jupyter), and lighting control (using Hidden Markov Models via Hmmlearn). From the talk, you should get a sense of how the hardware, software, and math fit together to create a solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ever want to know what is behind the &amp;quot;Internet of Things&amp;quot; hype? I wanted to as well, so I embarked on a side project to learn more. This talk is the story of my journey, using, of course, my favorite programming language, Python.&lt;/p&gt;
&lt;p&gt;In this talk, I will take you through my project, a lighting replay system. The application monitors the light levels in several rooms of a residence and then replays a similar pattern when the house is unoccupied. The goal is to make the house look occupied, with a lighting pattern that is different every day, but looks realistic. It accounts for the different patterns found in each individual room as well as seasonal factors (e.g. changing sunrise/sunset times). The full source code for the application is available on github &lt;a class="reference external" href="https://github.com/mpi-sws-rse/thingflow-examples/tree/master/lighting_replay_app"&gt;https://github.com/mpi-sws-rse/thingflow-examples/tree/master/lighting_replay_app&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jeff Fischer</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/python-and-iot-from-chips-and-bits-to-data-science.html</guid><category>iot</category><category>micropython</category></item><item><title>Python for NET or NET for Python</title><link>https://pyvideo.org/pydata-seattle-2017/python-for-net-or-net-for-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For long time .NET developers wanted to tap into the rich libraries from SciPy and PyData communities. Bridging .NET and CPython runtimes is the easiest approach to solving this problem. Python for .NET (pythonnet, Python.NET) makes this possible by wrapping CPython C-API from C# in .NET and Mono runtimes. This also allows two-way interoperability between both runtimes on Windows, Linux, and OSX (MacOS), and even Linux subsystem on Windows! This presentation is going to show how to use Python code from .NET and .NET assemblies from Python. Particular importance in Python.NET is given to installation options: Python wheels, conda and nuget binaries, docker images, and even distribution with WinPython. The deployment is also simplified with tools such as PyInstaller and cx_freeze. This presentation will show how to install and deploy Python.NET apps using these tools. In this talk we are going to show how to call numpy, scipy, pandas, matplotlib, sympy, and pyomo from .NET without much boilerplate code. The second part will show how to use .NET from Python, particularly C# magic cells (clrmagic) in ipython kernel with Jupyter Notebooks. The Python.NET tutorial was converted to Jupyter Notebook and C# code cells that are embedded within the same notebook. This presentation will show few demos with REPL experience both from C# and Python using IPython, scriptcs, and Visual Studio. All libraries used in this presentation are open-source and available on all major platforms. Python for .NET is a library developed since 2003, which &amp;quot;graduated&amp;quot; from Zope, moved to SourceForge and eventually to GitHub, where it became widely used and adopted. This talk will also demonstrate clrmagic - Jupyter extension, built on top of pythonnet and developed by authors of this presentation. Python.NET is built with a number of open-source technologies. It uses pycparser, ply, and clang (gcc and MSVC also work) for parsing internal CPython structures. .NET types are exposed to Python with Unmanaged Exports (DllExport) open-source &amp;quot;compiler&amp;quot; on Windows. On other platforms, C-API of Mono and CPython are used to &amp;quot;bootstrap&amp;quot; both runtimes. Python.NET is used by financial algorithmic trading platforms, engineering companies, and few open-source projects: QuantConnect Lean, pywebview, Pybee Beeware Toga cross-platform UI toolkit. Python.NET enables large .NET applications to embed numerical Python libraries without boilerplate code and without sacrificing for performance.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Denis Akhiyarov</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/python-for-net-or-net-for-python.html</guid></item><item><title>Robust Algorithms for Machine Learning</title><link>https://pyvideo.org/pydata-seattle-2017/robust-algorithms-for-machine-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Seattle 2017&lt;/p&gt;
&lt;p&gt;Many companies implementing Machine Learning (ML) have learned that noise and other errors in the data set can cause stability issues resulting in time loss and headache.&lt;/p&gt;
&lt;p&gt;Robust algorithms are under-appreciated, particularly by people new to data analysis. This talk will review the basic idea of robust or non-parametric algorithms and look at some of the more important named algorithms, as well as looking at how to apply the philosophy of robustness to any problem.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tom Radcliffe</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/robust-algorithms-for-machine-learning.html</guid></item><item><title>Robust Automated Forecasting in Python and R</title><link>https://pyvideo.org/pydata-seattle-2017/robust-automated-forecasting-in-python-and-r.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will cover how to make millions of time series forecasts in an automated fashion. We will be covering helpful heuristics to inform preprocessing, tradeoffs between contextual evaluation metrics (and meta-metrics), useful libraries for employing different forecasting techniques in Python and R, and how to choose the best hardware for forecasting given cost and runtime constraints.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Pranav Bahl</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/robust-automated-forecasting-in-python-and-r.html</guid></item><item><title>Scalable Data Science in Python and R on Apache Spark</title><link>https://pyvideo.org/pydata-seattle-2017/scalable-data-science-in-python-and-r-on-apache-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In the world of Data Science, Python and R are very popular. Apache Spark is a highly scalable data platform. How could a Data Scientist integrate Spark into their existing Data Science toolset? How does Python work with Spark? How could one leverage the rich 10000+ packages on CRAN for R?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the world of Data Science, Python and R are very popular. Apache Spark is a highly scalable data platform. How could a Data Scientist integrate Spark into their existing Data Science toolset? How does Python work with Spark? How could one leverage the rich 10000+ packages on CRAN for R?&lt;/p&gt;
&lt;p&gt;We will start with PySpark, beginning with a quick walkthrough of data preparation practices and an introduction to Spark MLLib Pipeline Model. We will also discuss how to integrate native Python packages with Spark.&lt;/p&gt;
&lt;p&gt;Compare to PySpark, SparkR is a new language binding for Apache Spark and it is designed to be familiar to native R users. In this talk we will walkthrough many examples how several new features in Apache Spark 2.x will enable scalable machine learning on Big Data. In addition to talking about the R interface to the ML Pipeline model, we will explore how SparkR support running user code on large scale data in a distributed manner, and give examples on how that could be used to work with your favorite R packages.&lt;/p&gt;
&lt;p&gt;Presentation: &lt;a class="reference external" href="https://www.slideshare.net/felixcss/scalable-data-science-in-python-and-r-on-apache-spark"&gt;https://www.slideshare.net/felixcss/scalable-data-science-in-python-and-r-on-apache-spark&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Felix Cheung</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/scalable-data-science-in-python-and-r-on-apache-spark.html</guid></item><item><title>Scaling Scikit Learn</title><link>https://pyvideo.org/pydata-seattle-2017/scaling-scikit-learn.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What do you do if you have a lot of models to fit, don’t want to spend all day with your laptop as a space heater, and have access to AWS? Take it to the cloud! I’ll share my experience setting up a system to take models coded with scikit-learn and run them in a cloud computing environment. This talk will focus on training data that fit in memory and data for prediction which maybe doesn’t.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I worked as part of a team to create software which moves data to and from scikit-learn models running in AWS’s EC2 service, and my talk will highlight some of the challenges we faced and the solutions we came up with. This project is possible because scikit-learn has a standardized API for all model types. No matter what algorithm you’re using, it has the same methods with the same arguments as any other algorithm.&lt;/p&gt;
&lt;p&gt;Data start and end either as tables in AWS’s Redshift (a postgres database) or CSVs stored in AWS’s S3 (a key-value store). The training data need to fit in memory, but we can make predictions on arbitrarily-large Redshift tables in roughly constant time, given a large enough pool of EC2 instances. The software and execution environment are packaged into Docker containers for reproducibility and speed in setting up on new EC2 instances.&lt;/p&gt;
&lt;p&gt;The challenges on the training side are in massaging input data to match the formats which scikit-learn models expect and in storing enough metadata to ensure that we can reproduce the arrays of features at prediction time. Predictions distribute chunks of data to their own EC2 instances. I’ll show off the custom backend for the joblib library that we use to manage the remote processes for predictions.&lt;/p&gt;
&lt;p&gt;Our software runs in Civis Analytics’s data science platform. For the application described in this talk, the platform mediates interactions with AWS services to provide security and permissioning. The principles I’ll discuss will be of general applicability to anyone interested in cloud-based production systems based on scikit-learn.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Stephen Hoover</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/scaling-scikit-learn.html</guid></item><item><title>Sirbarksalot Bark Detection in Python</title><link>https://pyvideo.org/pydata-seattle-2017/sirbarksalot-bark-detection-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dachshunds are notorious barkers and my dogs are no exception. Using a spare webcam, the Facebook Messenger API, and some python code, I built a simple app for sending notifications when my dogs are barking. In this talk, I'll cover the methodology and some surprising things I learned about my dogs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My wife and I have two dachshunds, Cody and Caylee. Cody in particular loves to bark and we do a lot to mitigate his barking when we are home. However, when we are at work, we have no idea how much barking he is inflicting on our neighbors. Out of curiosity for his daytime barking habits, I built an app that listens for barks and then sends a notification on Facebook. In this talk, I'll go over the methodology for bark detection from sample collection to creating a bark classifier. Then I will go over the basic structure of the app and the Facebook API component. Lastly, I'll discuss some interesting things I've learned about my dogs,&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicholas Kridler</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/sirbarksalot-bark-detection-in-python.html</guid></item><item><title>So you want to be a Python expert?</title><link>https://pyvideo.org/pydata-seattle-2017/so-you-want-to-be-a-python-expert.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R.&lt;/p&gt;
&lt;p&gt;PyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">James Powell</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/so-you-want-to-be-a-python-expert.html</guid></item><item><title>Unlocking the power of AI: A fundamentally different approach to building intelligent systems</title><link>https://pyvideo.org/pydata-seattle-2017/unlocking-the-power-of-ai-a-fundamentally-different-approach-to-building-intelligent-systems.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Keen Browne explains how Bonsai’s platform enables every developer to add intelligence to their software or hardware, regardless of AI expertise. Bonsai’s suite of tools—a new programming language, AI engine, and cloud service—abstracts away the lowest-level details of programming AI, allowing developers to focus on concepts they want a system to learn and how those concepts can be taught.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Building deep learning systems at present is part science, part art, and a whole lot of arcana. Rather than focusing on the concepts you want the system to learn and how those can be taught, you often find yourself dealing with low-level details like network topology and hyperparameters.&lt;/p&gt;
&lt;p&gt;Databases solved this problem for data by allowing users to program at a higher level of abstraction. Databases eschew low-level implementation details and instead build a model of the information (the schema) using a high-level declarative programming language (such as SQL). The database server actualizes this model and manages its usage with real data. Similarly, for artificial intelligence, one can build a model for conceptual understanding (the mental model) using a high-level declarative programming language (such as Inkling). A runtime server can then be used to actualize this model and manage its usage with real data.&lt;/p&gt;
&lt;p&gt;Keen Browne explains how Bonsai’s platform enables every developer to add intelligence to their software or hardware, regardless of AI expertise. Bonsai’s suite of tools—a new programming language, AI engine, and cloud service—abstracts away the lowest-level details of programming AI, allowing developers to focus on concepts they want a system to learn and how those concepts can be taught. Keen explores the underpinnings of this technique, details the Inkling programming language, and demonstrates how to build, debug, and iteratively refine models. To make things concrete and fun, Keen demonstrates how to create a system to play the video game Breakout using deep learning (but requiring codifying only the high-level concepts relevant for intelligent play) and offers a curriculum for how to teach this system.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Keen Browne</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/unlocking-the-power-of-ai-a-fundamentally-different-approach-to-building-intelligent-systems.html</guid></item><item><title>Upgrading Legacy Projects: Lessons Learned</title><link>https://pyvideo.org/pydata-seattle-2017/upgrading-legacy-projects-lessons-learned.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Stuck with a legacy system? Get unstuck! Learn how to design an upgrade path, overcome common obstacles, and reduce the risk of code rot in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Have a finicky legacy system you just can't seem to upgrade? Have you upgraded a tool just to watch it sink back into decay, despite your best intentions? We've been there, made it through to the other side, and want to share what we learned! Come learn about software rot, how to avoid it, how to reverse its effects, and what to do when you can't. This talk will include technical lessons (e.g., testing strategies, designing upgrade paths) and non-technical lessons (e.g., empathize with the people who wrote the code, understand the processes that lead to the current situation).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matt Braymer-Hayes</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/upgrading-legacy-projects-lessons-learned.html</guid></item><item><title>Using CNTKs Python Interface for Deep Learning</title><link>https://pyvideo.org/pydata-seattle-2017/using-cntks-python-interface-for-deep-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will review tutorial examples of using CNTK's python interface for image classification, speech recognition, and natural language processing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Topics to be covered include ...&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Cognitive Toolkit (CNTK) installation&lt;/li&gt;
&lt;li&gt;What is &amp;quot;machine learning&amp;quot;? [gradient descent example]&lt;/li&gt;
&lt;li&gt;What is &amp;quot;learning representations&amp;quot;?&lt;/li&gt;
&lt;li&gt;Why do Graphics Processing Units (GPUs) help?&lt;/li&gt;
&lt;li&gt;How do we prevent overfitting?&lt;/li&gt;
&lt;li&gt;CNTK Packages and Modules&lt;/li&gt;
&lt;li&gt;Deep Learning Examples [including Convolutional Neural Network -(CNN) and Long Short-Term Memory (LSTM) examples]&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dave DeBarr</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/using-cntks-python-interface-for-deep-learning.html</guid></item><item><title>Using Machine Learning and Brain Waves to Detect Errors in Human Problem Solving</title><link>https://pyvideo.org/pydata-seattle-2017/using-machine-learning-and-brain-waves-to-detect-errors-in-human-problem-solving.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Muse Headband is a simple to use EEG machine. Using this headband, a python model was developed using big data and machine learning techniques to interpret brain wave patterns to create a feedback system that helps the user understand their cognitive thinking while solving a problem.&lt;/p&gt;
&lt;p&gt;As more medical devices become cheaper and readily available to the public, they can be used in everyday life. The Muse Headband is an EEG machine that provides realtime measurements of brain waves. This talk will discuss the process of gathering and cleaning the data from the headband, using Keras to develop a model, and creating a basic real-time feedback end user program.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Katie Porterfield</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/using-machine-learning-and-brain-waves-to-detect-errors-in-human-problem-solving.html</guid></item><item><title>Using Scattertext and the Python NLP Ecosystem for Text Visualization</title><link>https://pyvideo.org/pydata-seattle-2017/using-scattertext-and-the-python-nlp-ecosystem-for-text-visualization.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Scattertext is a Python package that lets you compare and contrast how words are used differently in two types of documents, producing interactive, Javascript-based visualizations that can easily be embedded into Jupyter Notebooks. Using spaCy and Empath, Scattertext can also show how emotional states and words relating to a particular topic differ.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Notebooks and presentation for this talk are available from &lt;a class="reference external" href="https://github.com/JasonKessler/Scattertext-PyData"&gt;https://github.com/JasonKessler/Scattertext-PyData&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Motivation and introduction&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What's the matter with word clouds?&lt;/li&gt;
&lt;li&gt;How to read a plot made by Scattertext&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to make your own plots&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Preparing a Pandas data frame with your data set&lt;/li&gt;
&lt;li&gt;Plotting with Scattertext, and fine tuning plots for interpretability and speed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Scattertext and the Python NLP ecosystem&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Visualizing emotions using Empath.&lt;/li&gt;
&lt;li&gt;Using word vectors from spaCy and elsewhere see how topic-specific language differs.&lt;/li&gt;
&lt;li&gt;Visualizing topic models from scikit-learn.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Links&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Source code for the package is hosted on Github at &lt;a class="reference external" href="https://github.com/JasonKessler/scattertext"&gt;https://github.com/JasonKessler/scattertext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For more information, please see the paper which will appear as a 2017 ACL Demo at &lt;a class="reference external" href="https://arxiv.org/abs/1703.00565"&gt;https://arxiv.org/abs/1703.00565&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Kessler</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/using-scattertext-and-the-python-nlp-ecosystem-for-text-visualization.html</guid></item><item><title>Vocabulary Analysis of Job Descriptions</title><link>https://pyvideo.org/pydata-seattle-2017/vocabulary-analysis-of-job-descriptions.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will explore how to do some basic natural language processing (NLP) on the text including tokenization and stemming to combine word forms, and stop word removal and sentence detection to examine word sequences (n-grams). We will then look at the distribution of the vocabulary terms and n-grams in our data set using term frequency and inverse document frequency (TF.IDF).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the initial analysis of a data set it is useful to gather informative summaries. This includes evaluating the available fields, by finding unique counts or by calculating summary statistics such as averages for numerical fields. These summaries help in understanding what is in the data itself, the underlying quality, and illuminate potential paths for further exploration. In structured data, this a straightforward task, but for unstructured text, different types of summaries are needed. Some useful examples for text data include a count of the number of documents in which a term occurs, and the number of times a term occurs in a document. Since vocabulary terms often have variant forms, e.g. “performs” and “performing”, it is useful to pre-process and combine these forms before computing distributions. Oftentimes, we want to look at sequences of words, for example we may want to count the number of times “data science” occurs, and not just “data” and “science”. We will use the pandas Python Data Analysis Library and the Natural Language Toolkit (NLTK) to process a data set of job descriptions posted by employers in the United States, and look at the difference in vocabularies across different job segments.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Thomas</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/vocabulary-analysis-of-job-descriptions.html</guid></item><item><title>We came, we saw, we hacked How to win a Big Data hackathon</title><link>https://pyvideo.org/pydata-seattle-2017/we-came-we-saw-we-hacked-how-to-win-a-big-data-hackathon.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Hackathon prize money keeps rising each year as organizations learn to take advantage of their value. Runner-up cash prizes are now in the thousands. Hackathons are about bringing ideas to life in a very short time frame and pitches are typically short, 2-3 minutes. In this talk, we will walk through two hackathon examples using structured and unstructured data sets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Nowadays the main purpose of the hackathons is about using data science, programming and statistical analysis to solve Big Data challenges in order to deliver valuable business insights for leaders to take decision of important matters. At the essence of it, a good hack must be technically impressive and includes the theme as part of the project criteria. In this talk, we will discuss three aspects of a Big Data hackathon project.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Framework of your project: How to select your data set? What are the tools and technologies that you need to know for your project? How to come up with a hack that provides a solution to the theme/business question?&lt;/li&gt;
&lt;li&gt;How to build your Data Science/Analysis model your self by providing two examples of structured and unstructured data set cases.&lt;/li&gt;
&lt;li&gt;Finally, don’t let 3 minutes of stage time undo all the hard work &amp;amp; sleepness nights! We will give you a guidance of what is really important for your speech.&lt;/li&gt;
&lt;/ol&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eloisa Tran</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/we-came-we-saw-we-hacked-how-to-win-a-big-data-hackathon.html</guid></item><item><title>WorldRowing com: End To End Data Analysis</title><link>https://pyvideo.org/pydata-seattle-2017/worldrowing-com-end-to-end-data-analysis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Ever wonder if 10,000 hours is really the baseline? Did some set of events make the difference in someone getting to the olympics? WorldRowing has captured data on races and athletes for decades but little has been done to analyze the data across countries, athletes and races to the possible outcomes and where investment might be best to identify impact to rowing at a high performance level. Data for each race, for each athlete is stored on the WorldRowing website. Varying amounts of personal information, plus race information, is available for athletes spanning several decades. This talk investigates the athlete race data from WorldRowing.com and demonstrates an end to end walk through of a data analysis problem using Python.&lt;/p&gt;
&lt;p&gt;Introduction/Problem Statement&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;World Rowing Athlete Database&lt;/li&gt;
&lt;li&gt;Description of the data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data Analysis&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Scraping the data&lt;/li&gt;
&lt;li&gt;what to get, how to get it&lt;/li&gt;
&lt;li&gt;Data Munging and Wrangling&lt;/li&gt;
&lt;li&gt;Analysis&lt;/li&gt;
&lt;li&gt;Does the data show correlations?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Takeaways&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;How to identify a problem for data analysis&lt;/li&gt;
&lt;li&gt;Dealing with inconsistent data&lt;/li&gt;
&lt;li&gt;Drawing conclusions from that dataset&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lou Harwood</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/worldrowing-com-end-to-end-data-analysis.html</guid></item><item><title>Writing a Book in Jupyter Notebooks</title><link>https://pyvideo.org/pydata-seattle-2017/writing-a-book-in-jupyter-notebooks.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;I will describe an on-going project to write a book exploring mathematical and computational aspects of wave propagation problems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I will describe an on-going project to write a book exploring mathematical and computational aspects of wave propagation problems. Jointly with David Ketcheson and Mauricio del Razo, we are developing the book as a series of Jupyter notebooks that include Python code for the reader to experiment with, and that employ interactive widgets and animation tools to provide a more active experience for students or researchers grappling with this theory. We are still experimenting with the best approach to developing material that works well in the notebook and also translates well to static webpages and hardcopy, with the hope of expanding its accessibility and usefulness. The current state of the project can be found in the Github repository &lt;a class="reference external" href="https://github.com/clawpack/riemann_book"&gt;https://github.com/clawpack/riemann_book&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Randall J. LeVeque</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/writing-a-book-in-jupyter-notebooks.html</guid></item></channel></rss>