<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_ronert-obst.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-06-01T00:00:00+00:00</updated><entry><title>Smart Cars of Tomorrow: Real-Time Driving Patterns</title><link href="https://pyvideo.org/pydata-london-2015/smart-cars-of-tomorrow-real-time-driving-patterns.html" rel="alternate"></link><published>2015-06-20T00:00:00+00:00</published><updated>2015-06-20T00:00:00+00:00</updated><author><name>Ronert Obst</name></author><id>tag:pyvideo.org,2015-06-20:pydata-london-2015/smart-cars-of-tomorrow-real-time-driving-patterns.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;In recent years, the adoption of electric cars has resulted in a
desperate need from carmakers for accurate range prediction. In
addition, fuel efficiency is of increasing concern due to today’s
ever-rising fuel costs. In this talk, we will outline a machine
learning framework for real-time data analysis to demonstrate how
live data collected from cars can be used to provide valuable
information for range prediction and smart navigation.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In recent years, the adoption of electric cars has resulted in a
desperate need from carmakers for accurate range prediction. In
addition, fuel efficiency is of increasing concern due to today’s
ever-rising fuel costs. In this talk, we will outline a machine learning
framework for real-time data analysis to demonstrate how live data
collected from cars can be used to provide valuable information for
range prediction and smart navigation.&lt;/p&gt;
&lt;p&gt;For our solution, we use a Bluetooth dongle that connects to a standard
OBD II car diagnostics data port. Together with a self-developed iOS app
we can then stream OBD II data into our framework’s big data
infrastructure for long-term storage, batch training processes, and
subsequent real-time analysis. We will show how we used different
open-source technologies (Spark. Spring XD, Python and others) to
stream, store, and reason over this data in a scalable way.&lt;/p&gt;
&lt;p&gt;In particular, we will focus on how we designed the machine learning
framework to derive individual driver ‘fingerprints’ from variables such
as speed, acceleration, driving times, and location, taken from
historical data. These fingerprints are then used within the real-time
prediction framework to determine final journey destination and driving
behavior in real time during the journey. We will also look at how other
public and free data sources such as traffic information, weather, and
fuel station locations could be used to further improve the accuracy and
scope of our models.&lt;/p&gt;
&lt;p&gt;This talk is intended to demonstrate pioneering work in the space of big
data and the connected car. We will take into consideration the insights
we have gained from building this prototype, both into infrastructure
and analysis, to give our view on what such real-time driving
intelligence applications of tomorrow could look like.&lt;/p&gt;
</summary></entry><entry><title>PySpark in Practice</title><link href="https://pyvideo.org/pydata-berlin-2016/pyspark-in-practice.html" rel="alternate"></link><published>2016-06-01T00:00:00+00:00</published><updated>2016-06-01T00:00:00+00:00</updated><author><name>Ronert Obst</name></author><id>tag:pyvideo.org,2016-06-01:pydata-berlin-2016/pyspark-in-practice.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;In this talk we will share our best practices of using PySpark in numerous customer facing data science engagements. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Unit testing with PySpark&lt;/li&gt;
&lt;li&gt;Integration with SQL on Hadoop engines&lt;/li&gt;
&lt;li&gt;Data pipeline management and workflows&lt;/li&gt;
&lt;li&gt;Data Structures (RDDs vs. Data Frames vs. Data Sets)&lt;/li&gt;
&lt;li&gt;When to use MLlib vs scikit-learn&lt;/li&gt;
&lt;li&gt;Operationalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At Pivotal Labs we have many data science engagements on big data. Typical problems involve real-time data from sensors collected by telecom operators to GPS data produced by vehicle tracking systems. One widespread framework to solve those inherently difficult problems is Apache Spark. In this talk, we want to share our best practices with PySpark, Spark’s Python API, highlighting our experience as well as dos and dont's. In particular, we will focus on the whole data science pipeline from data ingestion, data munging and wrangling to the actual model building.&lt;/p&gt;
&lt;p&gt;Finally, many businesses have started to realise that there is no return on investment from data science if the models do not go into production. At Pivotal Labs, one our core principle is API first. Therefore, we will also talk how we put our models into production, sharing our hands-on knowledge in this field and also how this fits into test-driven development.&lt;/p&gt;
</summary><category term="pyspark"></category></entry><entry><title>PySpark in Practice</title><link href="https://pyvideo.org/pydata-london-2016/ronert-obst-dat-tran-pyspark-in-practice.html" rel="alternate"></link><published>2016-05-11T00:00:00+00:00</published><updated>2016-05-11T00:00:00+00:00</updated><author><name>Ronert Obst</name></author><id>tag:pyvideo.org,2016-05-11:pydata-london-2016/ronert-obst-dat-tran-pyspark-in-practice.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In this talk we will share our best practices of using PySpark in numerous customer facing data science engagements. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Unit testing with PySpark&lt;/li&gt;
&lt;li&gt;Integration with SQL on Hadoop engines&lt;/li&gt;
&lt;li&gt;Data pipeline management and workflows&lt;/li&gt;
&lt;li&gt;Data Structures (RDDs vs. Data Frames vs. Data Sets)&lt;/li&gt;
&lt;li&gt;When to use MLlib vs scikit-learn&lt;/li&gt;
&lt;li&gt;Operationalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At Pivotal Labs we have many data science engagements on big data. Typical problems involve real-time data from sensors collected by telecom operators to GPS data produced by vehicle tracking systems. One widespread framework to solve those inherently difficult problems is Apache Spark. In this talk, we want to share our best practices with PySpark, Spark’s Python API, highlighting our experience as well as dos and don'ts. In particular, we will focus on the whole data science pipeline from data ingestion, data munging and wrangling to the actual model building.&lt;/p&gt;
&lt;p&gt;Finally, many businesses have started to realise that there is no return on investment from data science if the models do not go into production. At Pivotal Labs, one our core principle is API first. Therefore, we will also talk how we put our models into production, sharing our hands-on knowledge in this field and also how this fits into test-driven development.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://pydata2016.cfapps.io/"&gt;http://pydata2016.cfapps.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/datitran/spark-tdd-example"&gt;https://github.com/datitran/spark-tdd-example&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Massively Parallel Processing with Procedural Python</title><link href="https://pyvideo.org/pydata-berlin-2014/massively-parallel-processing-with-procedural-pyt.html" rel="alternate"></link><published>2014-07-27T00:00:00+00:00</published><updated>2014-07-27T00:00:00+00:00</updated><author><name>Ronert Obst</name></author><id>tag:pyvideo.org,2014-07-27:pydata-berlin-2014/massively-parallel-processing-with-procedural-pyt.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Python data ecosystem has grown beyond the confines of single
machines to embrace scalability. Here we describe one of our approaches
to scaling, which is already being used in production systems. The goal
of in-database analytics is to bring the calculations to the data,
reducing transport costs and I/O bottlenecks. Using PL/Python we can run
parallel queries across terabytes of data using not only pure SQL but
also familiar PyData packages such as scikit-learn and nltk. This
approach can also be used with PL/R to make use of a wide variety of R
packages. We look at examples on Postgres compatible systems such as the
Greenplum Database and on Hadoop through Pivotal HAWQ. We will also
introduce MADlib, Pivotal’s open source library for scalable in-database
machine learning, which uses Python to glue SQL queries to low level C++
functions and is also usable through the PyMADlib package.&lt;/p&gt;
</summary></entry></feed>