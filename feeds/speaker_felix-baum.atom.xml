<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Felix Baum</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_felix-baum.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-09-18T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Lightning Talk: Efficient Inference at the Edge: Performance You Need at the Lowest Power You Deserve</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-efficient-inference-at-the-edge-performance-you-need-at-the-lowest-power-you-deserve.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Felix Baum</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-efficient-inference-at-the-edge-performance-you-need-at-the-lowest-power-you-deserve.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most AI algorithms created for edge applications are initially developed on workstations. Developers then often struggle to get these workloads running on edge devices and achieve performance levels required for new and innovative use cases. This holds true for a wide range of applications, from IoT to automotive to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most AI algorithms created for edge applications are initially developed on workstations. Developers then often struggle to get these workloads running on edge devices and achieve performance levels required for new and innovative use cases. This holds true for a wide range of applications, from IoT to automotive to XR to mobile to compute. In this session we would cover the results of the collaborative effort between PyTorch and Qualcomm teams to integrate the Qualcomm AI Stack into PyTorch 2.0 workflow and how we streamlined the path for developers from initial algorithm development to edge deployment. This would make it easy to re-target algorithms to edge hardware by supporting framework and data types that PyTorch developers are familiar with and we provide a set of tools that empower developers to extract the best performance and energy efficiency from their Android handsets to enable advanced use cases with premium features, performance boosts and power savings.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Running State-of-Art Gen AI Models on-Device with NPU Acceleration</title><link href="https://pyvideo.org/pytorch-conference-2024/running-state-of-art-gen-ai-models-on-device-with-npu-acceleration.html" rel="alternate"></link><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><author><name>Felix Baum</name></author><id>tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/running-state-of-art-gen-ai-models-on-device-with-npu-acceleration.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Since the boom of generative AI, the industry is now moving towards on-device AI inferencing, as it is not only a trend but a necessity now in order to save costs, achieve the best inference performance, ultra-low latency at the lowest power possible. In this session we go over …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Since the boom of generative AI, the industry is now moving towards on-device AI inferencing, as it is not only a trend but a necessity now in order to save costs, achieve the best inference performance, ultra-low latency at the lowest power possible. In this session we go over the new features added on the Qualcomm AI Stack and how it works with the public release of ExecuTorch 1.0. We will discuss how to run traditional workloads as well as GenAI use cases including the latest version of Llama on the Mobile device while using Qualcomm Hexagon NPU.&lt;/p&gt;
</content><category term="PyTorch Conference 2024"></category></entry></feed>