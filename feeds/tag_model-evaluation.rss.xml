<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Fri, 23 Oct 2015 00:00:00 +0000</lastBuildDate><item><title>How to evaluate a classifier in scikit-learn</title><link>https://pyvideo.org/data-school/scikit-learn-09-evaluating-classification-models.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, you'll learn how to properly evaluate a classification model using a variety of common tools and metrics, as well as how to adjust the performance of a classifier to best match your business objectives. I'll start by demonstrating the weaknesses of classification accuracy as an evaluation metric. I'll then discuss the confusion matrix, the ROC curve and AUC, and metrics such as sensitivity, specificity, and precision. By the end of the video, you will have a solid foundation for intelligently evaluating your own classification model.&lt;/p&gt;
&lt;p&gt;This is the ninth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Fri, 23 Oct 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-10-23:data-school/scikit-learn-09-evaluating-classification-models.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>model evaluation</category><category>classification</category><category>confusion matrix</category><category>ROC curve</category><category>AUC</category></item><item><title>How to find the best model parameters in scikit-learn</title><link>https://pyvideo.org/data-school/scikit-learn-08-parameter-tuning-with-grid-search.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, you'll learn how to efficiently search for the optimal tuning parameters (or &amp;quot;hyperparameters&amp;quot;) for your machine learning model in order to maximize its performance. I'll start by demonstrating an exhaustive &amp;quot;grid search&amp;quot; process using scikit-learn's GridSearchCV class, and then I'll compare it with RandomizedSearchCV, which can often achieve similar results in far less time.&lt;/p&gt;
&lt;p&gt;This is the eighth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Wed, 15 Jul 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-07-15:data-school/scikit-learn-08-parameter-tuning-with-grid-search.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>cross-validation</category><category>model evaluation</category><category>parameter tuning</category><category>grid search</category></item><item><title>Selecting the best model in scikit-learn using cross-validation</title><link>https://pyvideo.org/data-school/scikit-learn-07-model-evaluation-with-cross-validation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, we'll learn about K-fold cross-validation and how it can be used for selecting optimal tuning parameters, choosing between models, and selecting features. We'll compare cross-validation with the train/test split procedure, and we'll also discuss some variations of cross-validation that can result in more accurate estimates of model performance.&lt;/p&gt;
&lt;p&gt;This is the seventh video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Sun, 28 Jun 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-06-28:data-school/scikit-learn-07-model-evaluation-with-cross-validation.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>cross-validation</category><category>model evaluation</category><category>feature selection</category><category>parameter tuning</category></item><item><title>Data science in Python: pandas, seaborn, scikit-learn</title><link>https://pyvideo.org/data-school/scikit-learn-06-data-science-pipeline.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, we'll cover the data science pipeline from data ingestion (with pandas) to data visualization (with seaborn) to machine learning (with scikit-learn). We'll learn how to train and interpret a linear regression model, and then compare three possible evaluation metrics for regression problems. Finally, we'll apply the train/test split procedure to decide which features to include in our model.&lt;/p&gt;
&lt;p&gt;This is the sixth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 28 May 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-05-28:data-school/scikit-learn-06-data-science-pipeline.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>pandas</category><category>seaborn</category><category>linear regression</category><category>model evaluation</category><category>feature selection</category><category>visualization</category></item><item><title>Comparing machine learning models in scikit-learn</title><link>https://pyvideo.org/data-school/scikit-learn-05-comparing-machine-learning-models.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We've learned how to train different machine learning models and make predictions, but how do we actually choose which model is &amp;quot;best&amp;quot;? We'll cover the train/test split process for model evaluation, which allows you to avoid &amp;quot;overfitting&amp;quot; by estimating how well a model is likely to perform on new data. We'll use that same process to locate optimal tuning parameters for a KNN model, and then we'll re-train our model so that it's ready to make real predictions.&lt;/p&gt;
&lt;p&gt;This is the fifth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 14 May 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-05-14:data-school/scikit-learn-05-comparing-machine-learning-models.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>model evaluation</category><category>overfitting</category></item></channel></rss>