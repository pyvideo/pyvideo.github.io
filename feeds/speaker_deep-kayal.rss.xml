<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Fri, 14 Jul 2017 00:00:00 +0000</lastBuildDate><item><title>Large-scale data extraction, structuring and matching using Python and Spark</title><link>https://pyvideo.org/europython-2017/large-scale-data-extraction-structuring-and-matching-using-python-and-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Motivation - Matching data collections with the aim to augment and
integrate the information for any available data point that lies in
two or more of these collections, is a problem that nowadays arises
often. Notable examples of such data points are scientific
publications for which metadata and data are kept in various
repositories, and users’ profiles, whose metadata and data exist in
several social networks or platforms.&lt;/p&gt;
&lt;p&gt;In our case, collections were as follows: (1) A large dump of
compressed data files on s3 containing archives in the form of zips,
tars, bzips and gzips, which were expected to contain published
papers in the form of xmls and pdfs, amongst other files, and (2) A
large store of xmls in the form of xmls, some of which are to be
matched to Collection 1.&lt;/p&gt;
&lt;p&gt;Problem Statement - The problems, then, are: (1) How to best unzip
the compressed archives and extract the relevant files? (2) How to
extract meta-information from the xml or pdf files? (3) How to match
the meta-information from the two different collections? And all of
these must be done in a big-data environment.&lt;/p&gt;
&lt;p&gt;Presentation –
&lt;a class="reference external" href="https://drive.google.com/open?id=1hA9J80446Qh7nd8PMYZibtIR1WjMkdLXfDgwUlts7JM"&gt;https://drive.google.com/open?id=1hA9J80446Qh7nd8PMYZibtIR1WjMkdLXfDgwUlts7JM&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The presentation will describe the solution process and the use of
python and Spark in the large-scale unzipping and extraction of files
from archives, and how metadata was then extracted from the files to
perform the matches on.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Deep Kayal</dc:creator><pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-14:europython-2017/large-scale-data-extraction-structuring-and-matching-using-python-and-spark.html</guid></item></channel></rss>