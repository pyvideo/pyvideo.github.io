<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_dr-rebecca-bilbro.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-09-05T00:00:00+00:00</updated><entry><title>Visual Diagnostics at Scale</title><link href="https://pyvideo.org/euroscipy-2019/visual-diagnostics-at-scale.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Dr. Rebecca Bilbro</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/visual-diagnostics-at-scale.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Even with a modestly-sized dataset, the hunt for the most effective
machine learning model is &lt;em&gt;hard&lt;/em&gt;. Arriving at the optimal combination of
features, algorithm, and hyperparameters frequently requires significant
experimentation and iteration. This leads some of us to stay inside
algorithmic comfort zones, some to trail off on random walks, and others
to resort to automated processes like gridsearch. But whatever path we
take, we are often left in doubt about whether our final solution really
is the optimal one. And as our datasets grow in size and dimension, so
too does this ambiguity.&lt;/p&gt;
&lt;p&gt;Fortunately, many of us have developed strategies for steering model
search. Open source libraries like
&lt;a class="reference external" href="https://seaborn.pydata.org/"&gt;seaborn&lt;/a&gt;,
&lt;a class="reference external" href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt; and
&lt;a class="reference external" href="https://www.scikit-%20yb.org/en/latest/"&gt;yellowbrick&lt;/a&gt; can help make
machine learning more informed with visual diagnostic tools like
histograms, correlation matrices, parallel coordinates, manifold
embeddings, validation and learning curves, residuals plots, and
classification heatmaps. These tools enable us to tune our models with
visceral cues that allow us to be more strategic in our choices.
Visualizing feature transformations, algorithmic behavior,
cross-validation methods, and model performance allows us a peek into
the multi-dimensional realm in which our models operate.&lt;/p&gt;
&lt;p&gt;However, large, high-dimensional datasets can prove particularly
difficult to explore. Not only do the majority of people struggle to
visualize anything beyond two- or three-dimensional space, many of our
favorite open source Python tools are not designed to be performant with
arbitrarily big data. So how well &lt;em&gt;do&lt;/em&gt; our favorite visualization
techniques hold up to large, complex datasets?&lt;/p&gt;
&lt;p&gt;In this talk, we'll consider a suite of visual diagnostics — some
familiar and some new — and explore their strengths and weaknesses with
several publicly available datasets of varying size. Which suffer most
from the curse of dimensionality in face of increasingly big data? What
are the workarounds (e.g. sampling, brushing, filtering, etc.) and when
should we use them? And most importantly, how can we continue to steer
the machine learning process — not only purposefully but at scale?&lt;/p&gt;
&lt;p&gt;Machine learning is a search for the best combination of features,
model, and hyperparameters. But as data grow, so does the search space!
Fortunately, visual diagnostics can focus our search and allow us to
steer modeling purposefully, and at scale.&lt;/p&gt;
</summary></entry><entry><title>Words in Space</title><link href="https://pyvideo.org/pydata-miami-2019/words-in-space.html" rel="alternate"></link><published>2019-01-09T00:00:00+00:00</published><updated>2019-01-09T00:00:00+00:00</updated><author><name>Dr. Rebecca Bilbro</name></author><id>tag:pyvideo.org,2019-01-09:pydata-miami-2019/words-in-space.html</id><summary type="html"></summary><category term="pydata"></category></entry></feed>