<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_analytics.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-12T00:00:00+00:00</updated><entry><title>Enhancing Angklung Music Rehearsals with Python</title><link href="https://pyvideo.org/europython-2019/enhancing-angklung-music-rehearsals-with-python.html" rel="alternate"></link><published>2019-07-12T00:00:00+00:00</published><updated>2019-07-12T00:00:00+00:00</updated><author><name>Trapsilo Bumi</name></author><id>tag:pyvideo.org,2019-07-12:europython-2019/enhancing-angklung-music-rehearsals-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Angklung is a traditional musical instrument from Indonesia. This
instrument has a lot of variety in how it is performed; a common format
is the orchestral format in which 15-30 players gather to form a team.
Playing Angklung in this way is fun but also presents some challenges
that are hard to solve manually.&lt;/p&gt;
&lt;p&gt;First, I will introduce you to the instrument and how it works/how it is
played. Then, I will show you how Python-based algorithms can be used to
ease the pains of managing Angklung teams, by reading Angklung-specific
sheet music and calculating the most optimal distribution of Angklung
based on several important factors.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Analytics"></category><category term="Beginners"></category><category term="Case Study"></category><category term="Use Case"></category></entry><entry><title>Explaining AI to Managers üë®‚Äçüíºü§ñüë©‚Äçüíº</title><link href="https://pyvideo.org/europython-2019/explaining-ai-to-managers.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Alexander CS Hendorf</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/explaining-ai-to-managers.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Artificial intelligence promises great value. The technology is mostly
understood only by few, yet still unexplainable even for experts.&lt;/div&gt;
&lt;div class="line"&gt;In this talk I'll present how to narrow the hype down to real value
explainable to everyone in your organisation - without the math.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can you downloaded at &lt;a class="reference external" href="http://bit.ly/ai-executives-data-litteracy"&gt;http://bit.ly/ai-executives-data-litteracy&lt;/a&gt;-
europython&lt;/p&gt;
</summary><category term="Analytics"></category><category term="Business"></category><category term="Deep Learning"></category><category term="Management"></category></entry><entry><title>Bioinformatics pipeline for revealing tumour heterogeneity</title><link href="https://pyvideo.org/europython-2019/bioinformatics-pipeline-for-revealing-tumour-heterogeneity.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Mustafa Anil Tuncel</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/bioinformatics-pipeline-for-revealing-tumour-heterogeneity.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Reproducibility of research is a common issue in science, especially
in computationally expensive research fields e.g. cancer research.&lt;/div&gt;
&lt;div class="line"&gt;A comprehensive picture of the genomic aberrations that occur during
tumour progression and the resulting intra-tumour heterogeneity, is
essential for personalised and precise cancer therapies. With the
change in the tumour environment under treatment, heterogeneity allows
the tumour additional ways to evolve resistance, such that
intra-tumour genomic diversity is a cause of relapse and treatment
failure. Earlier bulk sequencing technologies were incapable of
determining the diversity in the tumour.&lt;/div&gt;
&lt;div class="line"&gt;Single-cell DNA sequencing - a recent sequencing technology - offers
resolution down to the level of individual cells and is playing an
increasingly important role in this field.&lt;/div&gt;
&lt;div class="line"&gt;We present a reproducible and scalable Python data analysis pipeline
that employs a statistical model and an MCMC algorithm to infer the
evolutionary history of copy number alterations of a tumour from
single cells. The pipeline is built using Python, Conda environment
management system and the Snakemake workflow management system. The
pipeline starts from the raw sequencing files and a settings file for
parameter configurations. After running the data analysis, pipeline
produces report and figures to inform the treatment decision of the
cancer patient.&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Algorithms"></category><category term="Analytics"></category><category term="C-Languages"></category><category term="Command-Line"></category><category term="Data Science"></category></entry><entry><title>Geospatial Analysis using Python and JupyterHub</title><link href="https://pyvideo.org/europython-2019/geospatial-analysis-using-python-and-jupyterhub.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Martin Christen</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/geospatial-analysis-using-python-and-jupyterhub.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Geospatial data is data containing a spatial component ‚Äì describing
objects with a reference to the planet's surface. This data usually
consists of a spatial component, of various attributes, and sometimes of
a time reference (where, what, and when). Efficient processing and
visualization of small to large-scale spatial data is a challenging
task.&lt;/p&gt;
&lt;p&gt;This talk describes how to process and visualize geospatial vector and
raster data using Python and the Jupyter Notebook.&lt;/p&gt;
&lt;p&gt;To process the data a high performance computer with 4 GPUS (NVidia
Tesla V100), 192 GB RAM, 44 CPU Cores is used to run JupyterHub.&lt;/p&gt;
&lt;p&gt;There are numerous modules available which help using geospatial data in
using low- and high-level interfaces, which are shown in this
presentation. In addition, it is shown how to use deep learning for
raster analysis using the high performance GPUs and several deep
learning frameworks.&lt;/p&gt;
</summary><category term="Analytics"></category><category term="Big Data"></category><category term="Deep Learning"></category><category term="GPU"></category><category term="Visualization"></category></entry><entry><title>A Worked Intro to Scikit-learn</title><link href="https://pyvideo.org/pycon-italia-2019/a-worked-intro-to-scikit-learn.html" rel="alternate"></link><published>2019-05-04T00:00:00+00:00</published><updated>2019-05-04T00:00:00+00:00</updated><author><name>Anders Bogsnes</name></author><id>tag:pyvideo.org,2019-05-04:pycon-italia-2019/a-worked-intro-to-scikit-learn.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A talk introducing the audience to Scikit-learn as a library aimed at
people who know Python at a beginner/intermediate level but are new to
machine learning concepts. The goal of the talk is for the audience to
leave with an understanding of the foundations of machine learning while
respecting how easy it is to make a wrong choice that invalidates your
model.&lt;/p&gt;
&lt;p&gt;It will be a short background on scikit-learn followed by a livecoding
demo where I demonstrate how scikit-learn works and detail common
pitfalls.&lt;/p&gt;
&lt;p&gt;I will demonstrate ways of coping with problems such as data leakage,
the importance of train-test splits, choosing metrics wisely, and
explain how cross-validation works and why we use it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1596"&gt;https://python.it/feedback-1596&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Saturday 4 May&lt;/strong&gt; at 10:45 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="pydata"></category><category term="Python"></category><category term="scikit-learn"></category><category term="Machine Learning"></category><category term="analytics"></category><category term="data"></category><category term="sklearn"></category></entry><entry><title>Sviluppare per Alexa in Python</title><link href="https://pyvideo.org/pycon-italia-2019/sviluppare-per-alexa-in-python.html" rel="alternate"></link><published>2019-05-04T00:00:00+00:00</published><updated>2019-05-04T00:00:00+00:00</updated><author><name>Alberto Anceschi</name></author><id>tag:pyvideo.org,2019-05-04:pycon-italia-2019/sviluppare-per-alexa-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In questo talk verr√† mostrato come sviluppare una Skill per Amazon Alexa
in italiano. Verranno presentati i concetti e le metodologie utili allo
sviluppo di un‚Äôinterfaccia vocale, le risorse e gli strumenti a
disposizione dello sviluppatore ed infine i requisiti qualitativi da
rispettare affinch√© la Skill sviluppata venga pubblicata sullo store. La
Skill fornir√† insight sul traffico del sito web, interfacciandosi con
Google Analytics attraverso autenticazione dell‚Äôutente. Requisiti:
serverless (AWS Lambda), Oauth.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1676"&gt;https://python.it/feedback-1676&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Saturday 4 May&lt;/strong&gt; at 11:30 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="#assistant"></category><category term="#Python"></category><category term="#google"></category><category term="#api"></category><category term="#voice"></category><category term="#alexa"></category><category term="#analytics"></category></entry><entry><title>Machine learning approaches for road scene video analysis</title><link href="https://pyvideo.org/pycon-italia-2019/machine-learning-approaches-for-road-scene-video-analysis.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Andrea Benericetti</name></author><id>tag:pyvideo.org,2019-05-03:pycon-italia-2019/machine-learning-approaches-for-road-scene-video-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dashboard cameras (dashcams), inward or front-facing cameras installed
in personal or commercial vehicles, are becoming increasingly popular
due to the pervasiveness of their applications: driver safety,
autonomous driving, fleet management systems, insurance, theft detection
are some examples. Focusing on safety, one of the main problems is to
analyze videos and automatically detect dangerous situations occurring
in them, such as the risk of a near crash with another vehicle or
pedestrian. In this talk, we show how we tackle this real- world problem
at Verizon Connect, using a mix of state-of-the-art deep learning
methods and traditional computer vision / machine learning techniques,
leveraging libraries such as scikit-learn, scipy/numpy, pandas, openCV,
and keras/tensorflow. We also describe how we use AWS and docker to
deploy, serve and scale the application to customers all over the world.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1603"&gt;https://python.it/feedback-1603&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 11:15 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="ComputerVision"></category><category term="analytics"></category><category term="scikit-learn"></category><category term="opencv"></category><category term="video"></category><category term="machine-learning"></category><category term="optical-flow"></category><category term="pandas"></category></entry><entry><title>Pandas ecosystem 2019</title><link href="https://pyvideo.org/pycon-italia-2019/pandas-ecosystem-2019.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Marc Garcia</name></author><id>tag:pyvideo.org,2019-05-03:pycon-italia-2019/pandas-ecosystem-2019.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas is more than 10 years old now. In this time, it became almost a
standard for building data pipelines and perform data analysis in
Python. As the popularity of the project grows, it also grows the number
of projects that depend or interact with pandas.&lt;/p&gt;
&lt;p&gt;This talk will cover this ecosystem of projects around pandas, mainly in
the prespective of scalability and performance. Discussing for example
how projects like Arrow are key for the future of pandas, or how Dask is
overcoming pandas limitations.&lt;/p&gt;
&lt;p&gt;In a first part, the talk will focus on pandas itself, its components,
and its architecture. This will give the required context for a second
part, that will explain related projects, how they interact with pandas,
and what the whole ecosystem can offer to users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1613"&gt;https://python.it/feedback-1613&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 12:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="pydata"></category><category term="analytics"></category><category term="data-analysis"></category><category term="Data Mining"></category><category term="data"></category><category term="pandas"></category></entry><entry><title>Traversing the land of graph computing and databases</title><link href="https://pyvideo.org/pycon-italia-2019/traversing-the-land-of-graph-computing-and-databases.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Akash Tandon</name></author><id>tag:pyvideo.org,2019-05-03:pycon-italia-2019/traversing-the-land-of-graph-computing-and-databases.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Graphs have long held a special place in the computer science‚Äôs history
(and codebases). With the advent of a new wave of the information age
characterized by a greater emphasis on linked data, graph computing and
databases have risen to prominence. Be it enterprise knowledge graphs or
graph-based analytics, there are a great number of potential
applications.&lt;/p&gt;
&lt;p&gt;To reap the benefits of graph databases and computing, one needs to
understand the basics as well as current technical landscape and
offerings. Also, it‚Äôs important to understand if a graph-based approach
suits your problem. This talk will touch upon these points. Be prepared
to learn some graph fundamentals and witness a live demo using Neo4j, a
popular graph database.&lt;/p&gt;
&lt;p&gt;Soft pre-requisites include familiarity with Python and experience
working with a SQL or NoSQL database. There are no hard pre-requisites.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1682"&gt;https://python.it/feedback-1682&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 12:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="nosql"></category><category term="graph"></category><category term="analytics"></category><category term="databases"></category><category term="neo4j"></category><category term="datascience"></category><category term="graphdatabase"></category></entry><entry><title>My code is not for you: Protecting Python developer‚Äôs identity in OSS</title><link href="https://pyvideo.org/pycon-ca-2018/my-code-is-not-for-you-protecting-python-developers-identity-in-oss.html" rel="alternate"></link><published>2018-11-11T00:00:00+00:00</published><updated>2018-11-11T00:00:00+00:00</updated><author><name>Alina Matyukhina</name></author><id>tag:pyvideo.org,2018-11-11:pycon-ca-2018/my-code-is-not-for-you-protecting-python-developers-identity-in-oss.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;OSS is open to anyone by design, whether it is developers or malicious users. Authors typically hide their identity through nicknames, however they have no protection against attribution techniques. This talk will present attacks on Python developers identity and discuss protection methods.&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;p&gt;The rapid increase in open source software is an attestation of a new standard in software development. Today over 80 percent of any software code is open source, according to a recent study by Sonatype. Python replaced Java as the second-most popular language on GitHub, with 40 percent more pull requests opened this year than last by GitHub Inc.'s annual report.&lt;/p&gt;
&lt;p&gt;As the popularity of open source software increases, so do privacy concerns of individual contributors that often wish to remain anonymous. The majority of open-source repositories (GitHub, Google Code, SourceForge, etc.) allow users to keep their identity private while sharing their code. While in the past it was often sufficient, with the evolution of author attribution technology the question of how private the identity of software developers is becoming increasingly important.&lt;/p&gt;
&lt;p&gt;Software author attribution aims to decide who wrote a computer program, given its source or binary code. The main premise of this technique lies in the assumption that programmers unconsciously tend to use the same coding patterns. These patterns comprised of a number of distinctive features allow to characterise a programmer‚Äôs style and uniquely identify his/her works. Applications of software author attribution are wide and include software forensics - where the analyst wants to determine the author of a program given a set of potential programmers, plagiarism detection - where the analyst wants to identify illicit code reuse, ghostwriting detection - where given a suspicious piece of code the analysis wants to determine if it has been plagiarized from one of the programs in a given set, and in general any scenario where software ownership needs to be determined.&lt;/p&gt;
&lt;p&gt;In this session we will show how analysts can identify the author of Python software and how this process can be deceiving. We present two attacks on current attribution systems: author imitation and author hiding. The first attack can be applied on Python developer's identity in open-source projects. The attack transforms syntactical representation of attacker‚Äôs source code to a version that mimics the victim‚Äôs coding style while retaining functionality of original code. This is particularly concerning for Python open-source contributors who are unaware of the fact that by contributing to open-source projects they reveal identifiable information that can be used to their disadvantage. For example, one can easily see that by imitating someone‚Äôs coding style it is possible to implicate any software developer in wrongdoing. To resist this attack we discuss multiple approaches of hiding a coding style of Python software author before contribute to open-source.&lt;/p&gt;
</summary><category term="analytics"></category><category term="identity protection"></category></entry><entry><title>Building Analytics Workflow using Airflow and Spark</title><link href="https://pyvideo.org/pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html" rel="alternate"></link><published>2019-02-23T00:00:00+00:00</published><updated>2019-02-23T00:00:00+00:00</updated><author><name>Yohei Onishi</name></author><id>tag:pyvideo.org,2019-02-23:pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Yohei had built and operates a data analytics system for global retail logistics operations using Airflow and Spark since the end of last year. In this session, He will talk about how you can build a scalable analytics workflow system based on Airflow (Python) and write extensible job using Python. GCP has provided fully managed Airflow service called Cloud Composer. So he will explain how you can easily build Airflow cluster compared to building your own Airflow cluster on the on-premise server or AWS EC2.&lt;/p&gt;
</summary><category term="airflow"></category><category term="spark"></category><category term="analytics"></category></entry><entry><title>GPU-accelerated data analysis in Python: a study case in Material Sciences</title><link href="https://pyvideo.org/pycon-italia-2018/gpu-accelerated-data-analysis-in-python-a-study-case-in-material-sciences.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Giuseppe Di Bernardo</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/gpu-accelerated-data-analysis-in-python-a-study-case-in-material-sciences.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Max Planck Computing and Data Facility is engaged in the development
and optimization of algorithms and applications for high performance
computing as well as for data-intensive projects. As programming
language in data science, Python is now used at MPCDF in the scientific
area of ‚Äúatom probe crystallography‚Äù (APT): a Fourier analysis in 3D
space can be simulated in order to reveal composition and
crystallographic structure at the atomic scale of billions APT
experimental data sets.&lt;/p&gt;
&lt;p&gt;The Python data ecosystem has proved to be well suited to this, as it
has grown beyond the confines of single machines to embrace scalability.
The talk aims to describe our approach to scaling across multiple GPUs,
and the role of visualization methods too.&lt;/p&gt;
&lt;p&gt;Our data workflow analysis relies on the GPU-accelerated Python software
package PyNX, an open source library which provides fast parallel
computation scattering. The code takes advantage of the high throughput
of GPUs, using the pyCUDA library.&lt;/p&gt;
&lt;p&gt;Exploratory data analysis, high productivity and rapid prototyping with
high performance are enabled through Jupyter Notebooks and Python
packages e.g., pandas, matplotlib/plotly. In production stage,
interactive visualization is realized by using standard scientific tool,
e.g. Paraview, an open-source 3D visualization program which requires
Python modules to generate visualization components within VTK files.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 14:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="GPUComputing"></category><category term="visualization"></category><category term="mathematical-modelling"></category><category term="image-processing"></category><category term="bigdata"></category><category term="matplotlib"></category><category term="analytics"></category><category term="data-visualization"></category><category term="data-analysis"></category><category term="Data Mining"></category><category term="scientific-computing"></category><category term="physics"></category><category term="python3"></category></entry><entry><title>Hacking Your Way Into Machine Learning</title><link href="https://pyvideo.org/pycon-italia-2018/hacking-your-way-into-machine-learning.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Laksh Arora</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/hacking-your-way-into-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You might have heard of Machine Learning from your co-worker or in a local meetup and are enticed to get started but not sure how to take that first step. Confused between different sources, where to start from or how to proceed given a particular problem statement or dataset, then this talk is for you. It is aimed at complete beginners ( maybe you? ) who are just starting in machine learning and are ready to commit.
The talk will go something like this - each of the following items will be explained how it‚Äôs useful and why we should use it. Then alongside showcase, that same step applied to the real example(dataset) of that particular item so that the audience will be able to grasp the idea. It will add to around 35 minutes leaving us with 10 minutes for Q&amp;amp;A.
1) Context ( 5 mins ):
Discuss why we need Machine Learning and how we can use Machine Learning in different domains.
2) Resources ( 3 mins):
Talks about the dataset availability, online competitions, and Open Source libraries such as Scikit-learn, Matplotlib, Keras.
3) Jupyter Notebook (25 mins):
This Jupyter notebook will be a great starting point for most Supervised Machine Learning projects that involve common tasks: a) Imports and data loading (2 mins )
b) Data Exploration (5 mins)
c) Data Cleaning (3 mins)
d) Feature Engineering (4 mins)
e) Model Exploration (6 mins)
f) Final Model Building and Prediction ( 5 mins)
4) Wrap up ( 2 mins ):
Finalizing my talk, sharing some tips etc.
5) Q&amp;amp;A ( 10 mins ):
Question and Answering with the Audience.
Hope to inspire the audience to get started with machine learning, explore different domains, to learn, to create and engage with the Machine Learning Community.&lt;/p&gt;
</summary><category term="data-analysis"></category><category term="data-visualization"></category><category term="Python"></category><category term="scikit-learn"></category><category term="matplotlib"></category><category term="analytics"></category><category term="scipy"></category><category term="machine-learning"></category><category term="data"></category><category term="Statistical Learning"></category></entry><entry><title>Come vanno gli affari? Visualizziamolo con Superset</title><link href="https://pyvideo.org/pycon-italia-2018/come-vanno-gli-affari-visualizziamolo-con-superset.html" rel="alternate"></link><published>2018-04-20T00:00:00+00:00</published><updated>2018-04-20T00:00:00+00:00</updated><author><name>Riccardo Magliocchetti</name></author><id>tag:pyvideo.org,2018-04-20:pycon-italia-2018/come-vanno-gli-affari-visualizziamolo-con-superset.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Superset √® una piattaforma Open Source di Business Intelligence incubata
nel progetto Apache. Superset permette, senza scrivere una riga di
codice, di creare, esplorare, visualizzare e condividere piccole o
grandi quantit√† di dati. Nel talk verr√† fatta una introduzione a
Superset, mostrandone le funzionalit√†. Quindi vedremo come usare questo
strumento per analizzare un database di una azienda di vendita online.
Partendo da un database creeremo delle visualizzazioni e quindi delle
dashboard. Senza scrivere una riga di Python :)&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;venerd√¨ 20 aprile&lt;/strong&gt; at 17:30 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="analytics"></category><category term="superset"></category><category term="#data"></category><category term="business"></category></entry><entry><title>Sports performance evaluation: from cognitive mechanisms to data-driven algorithms</title><link href="https://pyvideo.org/pycon-italia-2018/sports-performance-evaluation-from-cognitive-mechanisms-to-data-driven-algorithms.html" rel="alternate"></link><published>2018-04-20T00:00:00+00:00</published><updated>2018-04-20T00:00:00+00:00</updated><author><name>Luca Pappalardo</name></author><id>tag:pyvideo.org,2018-04-20:pycon-italia-2018/sports-performance-evaluation-from-cognitive-mechanisms-to-data-driven-algorithms.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;blockquote&gt;
Not everything that counts can be counted, and not everything that
can be counted counts. A. Einstein&lt;/blockquote&gt;
&lt;p&gt;Humans are routinely asked to evaluate the performance of other
individuals, separating success from failure and affecting outcomes from
science to education and sports. Yet, little is known about the aspects
that determine the human perception of performance. How do expert
reviewers, as well as ordinary people, arrive to their evaluations? To
what extent these evaluations are based on objective performance
features? How are they affected by subjective biases or contextual
influences?&lt;/p&gt;
&lt;p&gt;This talk will answer these fascinating questions focusing on &lt;em&gt;soccer&lt;/em&gt; ,
the most popular sport in the world. Firstly, we will show how machine
learning can accurately reproduce the mechanisms human judges use to
evaluate the performance of soccer players, uncovering limits and
characteristics of the human evaluation process. Second, we design a
Python package that allows, in a completely unsupervised and data-driven
way, to (i) evaluate the quality of a player‚Äôs performance and (ii) rank
soccer players based on their performances.&lt;/p&gt;
&lt;p&gt;The first part of the talk will show how soccer ratings assigned to
every player of a game by sport-specialized newspapers are associated
with a high- dimensional vector of features extracted by massive data
which describe any quantifiable aspects of soccer games. The talk will
show how, by using Scikit- learn, we can train an &lt;em&gt;artificial judge&lt;/em&gt;
which learns the relation between technical performance and soccer
ratings, hence &lt;em&gt;accurately reproducing&lt;/em&gt; the human evaluation process. By
inspecting the structure of the artificial judge, the talk will show
that the human evaluation criteria follow a simplistic cognitive process
based on a simple heuristic: judges first select a limited number of
features which attract their attention and then rate a performance based
on the presence of noticeable values, i.e., features values far from the
norm that can be easily brought to mind.&lt;/p&gt;
&lt;p&gt;The second part of the talk will show how to overcome the simplicity of
the human evaluation process presenting &lt;strong&gt;PlayeRank&lt;/strong&gt; , a Python package
which implements an unsupervised data-driven framework to evaluate the
performance of soccer players in the main European leagues. The talk
will show how to use PlayeRank to construct a data-driven ranking of
players and highlight the factors which determine why celebrated
players, like Messi and Cristiano Ronaldo, actually result to be the top
players in the world. The modules composing PlayeRank will be presented,
showing how they allow the user to define the features characterizing a
performance, to detect in an automatic way the relevance of each
player‚Äôs action to a game outcome, to detect the role of a player given
his game data, to rate every performance as well as to obtain a final
ranking of all players in Europe. A short demo will be provided during
the talk through a Jupyter notebook, exploiting interactive data
visualization with the Bokeh package.&lt;/p&gt;
&lt;p&gt;The audience will learn how to use Python to construct evaluation
algorithms entirely based on machine learning and big data, a step
forward to a thorough and objective evaluation of performance which
overcomes the biases and the limitations of human perception of
performance. Just a basic knowledge of Python and of data mining
principles is required for a full understanding of the talk.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;venerd√¨ 20 aprile&lt;/strong&gt; at 15:15 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="mathematical-modelling"></category><category term="Python"></category><category term="bigdata"></category><category term="sports-analytics"></category><category term="Machine Learning"></category><category term="analytics"></category><category term="Algorithms"></category><category term="Data Mining"></category><category term="sklearn"></category></entry><entry><title>Data Plumbing 101 - ETL Pipelines for Everyday Projects</title><link href="https://pyvideo.org/pycon-de-2017/data-plumbing-101-etl-pipelines-for-everyday-projects.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Eberhard Hansis</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/data-plumbing-101-etl-pipelines-for-everyday-projects.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Eberhard Hansis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have been writing code for more than two thirds of my life, mostly for scientific computing and data analysis. In the past couple of years, I have worked on a range of different data science projects, all using Python at their core. During this time, I repeatedly was tasked with making data usable by joining multiple sources into a clearly defined data model. Once you have done that, it is amazing how much real-life value you can generate with little more than a bit of statistics and visualization. The world is full of underused data, let‚Äôs change that!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There is no data science without ETL! This presentation is about implementing maintainable data integration for your projects. We will have a first look a ‚ÄòOzelot‚Äô, a library based on Luigi and SQLAlchemy that helps you get started with building ETL pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ETL, the hard way&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You are starting a new data science project, and you can‚Äôt wait to perform some machine learning magic. However, before getting to ML, you have to deal with its ugly sibling: ETL. Extracting, transforming and loading data (or, more generally, data integration) is an indispensable first step in almost any data project.&lt;/p&gt;
&lt;p&gt;In your project you will, most likely, have to extract data from various sources, clean it, link it and prepare it to your needs. You will start writing a first data integration script for some part of the process, then a second, then a third. At some point you will write an ugly ‚Äòmaster‚Äô script to keep your 17 import scripts in check and run them in just the right order. When you come back to the code later, you will have a hard time deciphering what you did and why, and what format the output data is in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pipelines to the rescue&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Implementing a proper data integration pipeline and a well-defined data model helps document your data flows and makes them traceable. More importantly, it simplifies the ETL development process, because it lets you easily re-run the whole process or parts of it. And you will have to modify and re-run your ETL, because your code changes, your output requirements change or the data changes.&lt;/p&gt;
&lt;p&gt;In this talk I propose a setup for building maintainable data integration pipelines for everyday projects. This setup is embodied by ‚ÄòOzelot‚Äô, my brand-new Python library for ETL. It is based on Luigi for pipeline management and SQLAlchemy as ORM layer. Ozelot gives you core functionality to quickly start building your own solution, including an ORM base class, database connection management and Luigi task classes that play nice with the ORM. It comes with extensively documented examples that walk you through various aspects of data integration.&lt;/p&gt;
&lt;p&gt;The proposed setup works well for many small- to medium-sized projects -- projects, for which you previously might not have implemented a proper data integration pipeline. For big-data projects or those requiring live streaming data you probably want to consider alternative solutions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Core principles of data integration&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Taking one step back, I propose the following core principles for maintainable data integration:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Any and all data manipulation happens in the pipeline, in a single code base.&lt;/li&gt;
&lt;li&gt;The pipeline represents all dependencies between data integration tasks.&lt;/li&gt;
&lt;li&gt;Each task has a method for rolling back its operations.&lt;/li&gt;
&lt;li&gt;Data is loaded into a single database, in a clearly defined, object-based data model that also encodes object relationships.&lt;/li&gt;
&lt;li&gt;The whole process is fully automatic and thereby reproducible and traceable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will discuss why I think that these principles are important, and how they are reflected in the proposed setup.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data-science"></category><category term="analytics"></category><category term="python"></category></entry><entry><title>Data Science Best Practices : From Proof of Concepts to Production</title><link href="https://pyvideo.org/pycon-de-2017/data-science-best-practices-from-proof-of-concepts-to-production.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Yasir Khan</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/data-science-best-practices-from-proof-of-concepts-to-production.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Yasir Khan&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Yasir Khan is working as a Data Science Manager. He has nearly 12 years of experience in consulting and R&amp;amp;D domains. He obtained his PhD in Applied Machine Learning and has been invited as a speaker at several leading international forums and conferences. He has nearly 12 research articles and journals to his credit and book chapters published by Cambridge University Press.&lt;/p&gt;
&lt;p&gt;In his spare time he loves flying WWII aircrafts at a local aeroclub and is an avid scuba diver. He is also an investor in 2 technology startups.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This presentation will benefit the audience as it brings forward the practical issues in the industry today as we move towards industrializing data science algorithms. We will discuss the best practices around organization, methodology and tools to integrate a data science project into production.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As the industry understands the importance of Data Science for transforming businesses an interesting trend is arising. We have started seeing a widening gap &amp;amp; increasing difficulty to move teams from Proof of Concepts to Production. Apart from searching for the best data scientists, the industry is now looking for answers to organize &amp;amp; federate the data teams around practical business use cases. In this talk, Yasir Khan will provide an overview of the bottlenecks, and hurdles involved in a practical data project. While citing lessons learned from industry, this presentation will focus on important aspects such as business centric, data culture, data pipelines, organization, methodology &amp;amp; tools all of which are important but seldom ignored in large corporations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="use-case"></category><category term="analytics"></category><category term="python"></category><category term="machine learning"></category><category term="ai"></category><category term="business"></category></entry><entry><title>Effective Data Analysis with Pandas Indexes</title><link href="https://pyvideo.org/pycon-de-2017/effective-data-analysis-with-pandas-indexes.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Alexander Hendorf</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/effective-data-analysis-with-pandas-indexes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is the Swiss-Multipurpose Knife for Data Analysis in Python. In this talk we will look deeper into how to gain productivity utilizing Pandas powerful indexing and make advanced analytics a piece of cake. Pandas features multiple index types. This talk will give you a deep insight into the Pandas indexes and showcase the handiness of special Indexes as the TimeSeriesIndex.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="machine learning"></category><category term="analytics"></category><category term="data-science"></category><category term="business analytics"></category></entry><entry><title>Flow is in the Air: Best Practices of Building Analytical Data Pipelines with Apache Airflow</title><link href="https://pyvideo.org/pycon-de-2017/flow-is-in-the-air-best-practices-of-building-analytical-data-pipelines-with-apache-airflow.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Dominik Benz</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/flow-is-in-the-air-best-practices-of-building-analytical-data-pipelines-with-apache-airflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dominik Benz&lt;/strong&gt; (&amp;#64;john_maverick)&lt;/p&gt;
&lt;p&gt;Dominik Benz holds a PhD from the University of Kassel in the field of Data Mining on the Social Web. Since 2012 he is working as a Big Data Engineer at Inovex GmbH. In this time, he was involved in several projects concerned with establishing analytical data platforms in various companies. He is most experienced in tools around the Hadoop Ecosystem like Apache Hive and Spark, and has hands-on experience with productionizing analytical applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apache Airflow is an Open-Source python project which facilitates an intuitive programmatic definition of analytical data pipelines. Based on 2+ years of productive experience, we summarize its core concepts, detail on lessons learned and set it in context with the Big Data Analytics Ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Motivation &amp;amp; Outline&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Creating, orchestrating and running multiple data processing or analysis steps may cover a substantial portion of a Data Engineer and Data Scientist business. A widely adopted notion for this process is a &amp;quot;data pipeline&amp;quot; - which consists mainly of a set of &amp;quot;operators&amp;quot; which perform a particular action on data, with the possibility to specify dependencies among those. Real-Life examples may include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Importing several files with different formats into a Hadoop platform, perform data cleansing, and training a machine learning model on the result&lt;/li&gt;
&lt;li&gt;perform feature extraction on a given dataset, apply an existing deep learning model to it, and write the results in the backend of a microservice&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Apache Airflow is an open-source Python project developed by AirBnB which facilitates the programmatic definition of such pipelines. Features which differentiate Airflow from similar projects like Apache Oozie, Luigi or Azkaban include (i) its pluggable architecture with several extension points (ii) the programmatic approach of &amp;quot;workflow is code&amp;quot; and (iii) its tight relationship with the the Python as well as the Big Data Analytics Ecosystem. Based on several years of productive usage, we briefly summarize the core concepts of Airflow, and detail in-depth on lessons learned and best practices from our experience. These include hints for getting efficient quickly with Airflow, approaches to structure workflows, integrating it in an enterprise landscape, writing plugins and extentions, and maintaining it in productive environment. We conclude with a comparison with other analytical workflow engines and summarize why we have chosen Airflow.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Questions answered by this talk&lt;/em&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What are the core concepts of Apache Airflow?&lt;/li&gt;
&lt;li&gt;How can Airflow help me with moving data pipelines from analytics to production?&lt;/li&gt;
&lt;li&gt;Which concepts of Airflow make it more slim and more efficient compared to Apache Oozie?&lt;/li&gt;
&lt;li&gt;How can I specify dynamic dependencies at runtime between my analytical data processing steps?&lt;/li&gt;
&lt;li&gt;Which facilities does Airflow offer to enable automation and orchestration of analytical tasks?&lt;/li&gt;
&lt;li&gt;How can I extend the built-in facilities of Airflow by writing Python plugins?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;People who benefit most from this talk&lt;/em&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Data Scientists who are looking for a slim library to automate and control their data processing steps&lt;/li&gt;
&lt;li&gt;Data Engineers who want to save time debugging static workflow definitions (e.g. in XML)&lt;/li&gt;
&lt;li&gt;Project leaders interested in tools which lower the burden of moving from analytics to production&lt;/li&gt;
&lt;li&gt;Hadoop Cluster administrators eager to save cluster resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="workflow"></category><category term="data pipeline"></category><category term="data-science"></category><category term="analytics"></category></entry><entry><title>Large-scale machine learning pipelines using Luigi, PySpark and scikit-learn</title><link href="https://pyvideo.org/pycon-de-2017/large-scale-machine-learning-pipelines-using-luigi-pyspark-and-scikit-learn.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Alexander Bauer</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/large-scale-machine-learning-pipelines-using-luigi-pyspark-and-scikit-learn.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Alexander Bauer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alexander Bauer holds a Ph.D. in computer science. He has around 10 years industry experience, currently leading a team of data scientists at Lidl, one of the largest global discount supermarket chains. He is a Kaggle Master and regular speaker at the Frankfurt Predictive Analytics Meetup. He believes in agile software development practices and promotes Python as a primary language for data science applications in production.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For prescriptive analytics applications, data science teams need to design, build and maintain complex machine learning pipelines. In this talk, we demonstrate how such pipelines can be implemented in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data science teams working on real-world prescriptive analytics applications face the challenge to design, build and maintain considerably complex machine learning pipelines on a daily basis. Such pipelines include parsing data from multiple data sources, extracting relevant predictive features, executing training, validation, prediction steps and finally optimizing actions to meet desired business outcome so that they can be shared and visualized to business users. In this talk, we demonstrate how such pipelines can be implemented end-to-end in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn. We will share our lessons learned from using this framework in a real-world demand forecasting use case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data-science"></category><category term="analytics"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Master 2.5 GB of unstructured specification documents with ease</title><link href="https://pyvideo.org/pycon-de-2017/master-25-gb-of-unstructured-specification-documents-with-ease.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Andreas Schilling</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/master-25-gb-of-unstructured-specification-documents-with-ease.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dr. Andreas Schilling&lt;/strong&gt; is Senior Software Engineer at eXXcellent solutions. In his job, he helps customers to develop software solutions from the early stage of defining the particular requirements to developing information systems which meet their needs.&lt;/p&gt;
&lt;p&gt;Before working at eXXcellent solutions Andreas Schilling studied Information Systems at the University of Bamberg focusing on distributed systems and information management. Thereafter, he pursued his PhD and studied collaboration dynamics in open source projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;How Do you kick start a project which is based on 2.5 GB files of unstructured specification documents? To answer this question, we present our lessons learned from developing a Python based knowledge management tool which provides a lightweight and intuitive browser frontend.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk, we present lessons learned from and practical advice on how to deal with a large body of specification documents in your next project. We introduce our approach as well as code excerpts from our powerful toolset to transform a large set of unstructured and partially corrupt specification documents into structured JSON Files. Finally, we showcase a simple, yet powerful Javascript frontend which requires no additional infrastructure to present the compiled artefacts in an intuitive and responsive user interface.&lt;/p&gt;
&lt;p&gt;In particular this talk covers the following topics:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;How to make use of pywin32 to access layout and content information from partially corrupt .doc and .docx files and create simple JSON files with UTF-8 encoding.&lt;/li&gt;
&lt;li&gt;Identify and categorize signal words in your specification.&lt;/li&gt;
&lt;li&gt;Use pandas to compile content based recommender functionality&lt;/li&gt;
&lt;li&gt;Use networkx and py2cytoscape to visualize call sequences and semantic relationships in your specification.&lt;/li&gt;
&lt;li&gt;Present the compiled artefacts and identified relationships in an easy-to-use and lightweight Javascript browser interface without any additional infrastructure (i.e. no webserver and no database server).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="networkx"></category><category term="pandas"></category><category term="visualization"></category><category term="knowledge-management"></category><category term="analytics"></category><category term="use-case"></category><category term="python"></category><category term="business"></category></entry><entry><title>Sport analysis with Python</title><link href="https://pyvideo.org/pycon-de-2017/sport-analysis-with-python.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Thuy Le</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/sport-analysis-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Give an example data of the IoT sport case for instance the information of football match of a team (the positions, velocities of each player with are recorded in every 20 millisecond).&lt;/li&gt;
&lt;li&gt;We use Python to analysis and processing data (calculate the match time, analyst the activities of each player such as time in the bench, time in the pitch, ... )&lt;/li&gt;
&lt;li&gt;We use Tableau to visualize data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="devops"></category><category term="analytics"></category><category term="data-science"></category><category term="python"></category></entry><entry><title>The Python Ecosystem for Data Science: A Guided Tour</title><link href="https://pyvideo.org/pycon-de-2017/the-python-ecosystem-for-data-science-a-guided-tour.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Christian Staudt</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/the-python-ecosystem-for-data-science-a-guided-tour.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Christian Staudt&lt;/strong&gt; (&amp;#64;C_L_Staudt)&lt;/p&gt;
&lt;p&gt;I am an independent data scientist with a background in computer science, in-depth in algorithms, data analysis, high-performance computing and software engineering. My current interests include machine learning and data visualization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pythonistas have access to an extensive collection of tools for data analysis. The space of tools is best understood as an ecosystem: Libraries build upon each other, and a good library fills an ecological niche by doing certain jobs well. This is a guided tour of the Python data science ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Python Ecosystem for Data Science: A Guided Tour&lt;/p&gt;
&lt;p&gt;Python is on its way to becoming the lingua franca of data science, and Pythonistas have access to an impressive and extensive collection of tools for data analysis. Here, a data scientist needs to see the forest for the trees: The space of tools is best understood as an ecosystem, where libraries build upon each other, and where a good library fills an ecological niche by doing certain jobs well. This talk is a guided tour of the Python data science ecosystem. More than a list of libraries, it aims to provide some structure, classing tools by type of data, size of data, and type of analysis. In our tour, we visit a number of areas, including working with tabular data (numpy, pandas, dask, ...) and graph data (e.g. networkx), statistics (e.g. statsmodels), machine learning (scikit-learn, ...), data visualization (matplotlib, seaborn, bokeh, ...). Aspiring data scientists, and everyone else working with data, should find this useful for selecting the right tools for their next data-driven project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="use-case"></category><category term="business"></category><category term="ai"></category><category term="analytics"></category><category term="data-science"></category><category term="python"></category><category term="machine learning"></category></entry><entry><title>Turbodbc: Turbocharged database access for data scientists</title><link href="https://pyvideo.org/pycon-de-2017/turbodbc-turbocharged-database-access-for-data-scientists.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Michael K√∂nig</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/turbodbc-turbocharged-database-access-for-data-scientists.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Michael K√∂nig&lt;/strong&gt; (&amp;#64;turbodbc)&lt;/p&gt;
&lt;p&gt;Michael is a senior software engineer at Blue Yonder GmbH. He holds a PhD in physics, practices test-driven development, and digs Clean Code in C++ and Python. In the last five years, he invested more money in table tennis gear than in smartphones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Python's database API 2.0 is well suited for transactional database workflows, but not so much for column-heavy data science. This talk explains how the ODBC-based turbodbc database module extends this API with first-class, efficient support for familiar NumPy and Apache Arrow data structures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk introduces the open source Python database module turbodbc. It uses standard ODBC drivers to connect with virtually any database and is a viable (and often faster) alternative to &amp;quot;native&amp;quot; Python drivers.&lt;/p&gt;
&lt;p&gt;Briefly recounting the painful story of how data scientists previously used our analytics database, I explain why turbodbc was created and what distinguishes it from other ODBC modules. Sketching the flow of data from databases via drivers and Python modules to consumable Python objects, I motivate a few extensions to the standard database API 2.0 that turbodbc has made. These extensions heavily use NumPy arrays and Apache Arrow tables to provide data scientists with both familiar and efficient binary data structures they can further work on. I conclude my talk with benchmark results for a few databases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="numpy"></category><category term="database"></category><category term="python"></category><category term="data-science"></category><category term="analytics"></category></entry><entry><title>Data Science &amp; Data Visualization in Python. How to harness power of Python for social good?</title><link href="https://pyvideo.org/pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Radovan Kavicky</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python as an Open Data Science tool offers many libraries for data visualization and I will show you how to use and combine the best. I strongly believe that power of data is not only in the information &amp;amp; insight that data can provide us, Data is and can be really beautiful and can not only transform our perception but also the world that we all live in.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my talk I will primarily focus on answering/offer the answer to these questions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Why we need data science and why more and more people should be really interested in analyzing data and data visualization? (motivation)&lt;/li&gt;
&lt;li&gt;What is data science and how to start doing it in Python? (introduction of procedures, tools, most popular IDE-s for Python, etc.)&lt;/li&gt;
&lt;li&gt;What tools for data analysis and data visualization Python offers? (in each stage of analysis the best libraries will be shown for the specific purpose; as for data visualization we will focus particularly on Bokeh, Seaborn, Plotly and use of Jupyter Notebook and Plotly)&lt;/li&gt;
&lt;li&gt;How to 'unlock' the insight hidden in data through Python and how to use it to transform not only public administration or business, but ultimately the transformation of the whole society and economy towards the insight &amp;amp; knowledge based? (potential of data science)&lt;/li&gt;
&lt;li&gt;Open Data, Open Government Partnership, Open Public Administration &amp;amp; all the advantages of Open Data Science &amp;amp; Python. Data-Driven Approach. Everywhere. Now. (the end of talk +vision)&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="python"></category><category term="data-science"></category><category term="data-visualization"></category><category term="analytics"></category><category term="PyData"></category><category term="PyDataBLN"></category><category term="PyDataBerlin"></category><category term="PyDataBA"></category><category term="PyDataBratislava"></category><category term="talk"></category><category term="Data"></category><category term="Bokeh"></category><category term="Social Good"></category><category term="datascience"></category><category term="jupyter"></category><category term="open science"></category><category term="open data science"></category><category term="DataVisualization"></category><category term="data-analysis"></category><category term="analysis"></category><category term="matplotlib"></category><category term="numpy"></category><category term="data wrangling"></category><category term="jupyter notebook"></category><category term="pandas"></category><category term="machine learning"></category><category term="deep learning"></category><category term="Open Data"></category><category term="Citizen Data Science"></category></entry></feed>