<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_giuseppe-di-bernardo.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-05-05T00:00:00+00:00</updated><entry><title>Deep Learning for brain MRI segmentation: Big Data, AI and HPC meet together</title><link href="https://pyvideo.org/pycon-italia-2019/deep-learning-for-brain-mri-segmentation-big-data-ai-and-hpc-meet-together.html" rel="alternate"></link><published>2019-05-05T00:00:00+00:00</published><updated>2019-05-05T00:00:00+00:00</updated><author><name>Giuseppe Di Bernardo</name></author><id>tag:pyvideo.org,2019-05-05:pycon-italia-2019/deep-learning-for-brain-mri-segmentation-big-data-ai-and-hpc-meet-together.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With ever-increasing advancements in technology, neuroscientists are
able to collect data in greater volumes and with finer resolution. There
has been a growing interest in leveraging this vast volume of data
across levels of analysis, measurement techniques, and experimental
paradigms to gain more insight into brain function. At multiple stages
and levels of neuroscience investigation, ML holds great promise as an
addition to the arsenal of analysis tools for discovering how the brain
works. As quantitative analysis of brain MRI is routine for many
neurological diseases and conditions, deep learning-based segmentation
approaches for brain Magnetic Resonance Imaging (MRI) are gaining
interest due to their self-learning and generalisation ability over
large amounts of data. On the other hand, High Performance Computing
(HPC) and AI will increasingly intertwine as we transition to an
exascale future using new computing, storage, and communications
technologies. In this talk I will walk you through fundamentals of
generating high- performance deep-learning models in TensorFlow platform
using Python on large computing system (e.g NVIDIA® Tesla® GPUs powered
by Tensor Cores), in order to infer and segment thousands of cell
centroids out of the brain objects of interest. From a more
technological perspective, although astonishing results have been
achieved concerning the distribution of training large convolutional
neural networks on big data, to date the Python scientific ecosystem is
still missing tools for an optimised and, above all, distributed
inference of deep learning models. In this talk I will show you how a
tiling-based inferencing approach could be a good solution to remedy the
problem. The talk is intended for intermediate PyData researchers and
practitioners. Basic to intermediate level experience in image
recognition/object detection deep learning applications is assumed.
Overall, a good proficiency with the Python language and with scientific
python libraries (e.g. numpy, TensorFlow, Keras) are required for the
entire talk.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1794"&gt;https://python.it/feedback-1794&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Sunday 5 May&lt;/strong&gt; at 11:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="GPUComputing"></category><category term="parallelization"></category><category term="bio-informatics"></category><category term="Machine Learning"></category><category term="ComputerVision"></category><category term="optimization"></category><category term="data-analysis"></category><category term="Artificial Intelligence"></category></entry><entry><title>GPU-accelerated data analysis in Python: a study case in Material Sciences</title><link href="https://pyvideo.org/pycon-italia-2018/gpu-accelerated-data-analysis-in-python-a-study-case-in-material-sciences.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Giuseppe Di Bernardo</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/gpu-accelerated-data-analysis-in-python-a-study-case-in-material-sciences.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Max Planck Computing and Data Facility is engaged in the development
and optimization of algorithms and applications for high performance
computing as well as for data-intensive projects. As programming
language in data science, Python is now used at MPCDF in the scientific
area of “atom probe crystallography” (APT): a Fourier analysis in 3D
space can be simulated in order to reveal composition and
crystallographic structure at the atomic scale of billions APT
experimental data sets.&lt;/p&gt;
&lt;p&gt;The Python data ecosystem has proved to be well suited to this, as it
has grown beyond the confines of single machines to embrace scalability.
The talk aims to describe our approach to scaling across multiple GPUs,
and the role of visualization methods too.&lt;/p&gt;
&lt;p&gt;Our data workflow analysis relies on the GPU-accelerated Python software
package PyNX, an open source library which provides fast parallel
computation scattering. The code takes advantage of the high throughput
of GPUs, using the pyCUDA library.&lt;/p&gt;
&lt;p&gt;Exploratory data analysis, high productivity and rapid prototyping with
high performance are enabled through Jupyter Notebooks and Python
packages e.g., pandas, matplotlib/plotly. In production stage,
interactive visualization is realized by using standard scientific tool,
e.g. Paraview, an open-source 3D visualization program which requires
Python modules to generate visualization components within VTK files.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 14:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="GPUComputing"></category><category term="visualization"></category><category term="mathematical-modelling"></category><category term="image-processing"></category><category term="bigdata"></category><category term="matplotlib"></category><category term="analytics"></category><category term="data-visualization"></category><category term="data-analysis"></category><category term="Data Mining"></category><category term="scientific-computing"></category><category term="physics"></category><category term="python3"></category></entry><entry><title>Big Data Analytics at the MPCDF: GPU Crystallography with Python</title><link href="https://pyvideo.org/europython-2017/big-data-analytics-at-the-mpcdf-gpu-crystallography-with-python.html" rel="alternate"></link><published>2017-07-12T00:00:00+00:00</published><updated>2017-07-12T00:00:00+00:00</updated><author><name>Giuseppe Di Bernardo</name></author><id>tag:pyvideo.org,2017-07-12:europython-2017/big-data-analytics-at-the-mpcdf-gpu-crystallography-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In close collaboration with scientists from MPG, the Max Planck
Computing and Data Facility is&amp;nbsp;engaged in the development and
optimization of algorithms and applications for high
performance&amp;nbsp;computing, as well as in the design and&amp;nbsp;implementation of
solutions for data-intensive projects.&amp;nbsp;Python is now used at MPCDF in
the emerging area of “atom probe crystallography” (APT): a Fourier
spectral analysis in 3D reciprocal space can be simulated in order to
reveal both composition and crystallographic structure at the atomic
scale of billions APT experimental data sets. The Python data
ecosystem has proved to be well suited to this, as it has grown
beyond the confines of single machines to embrace scalability. This
talk aims to describe our approach to scaling across multiple GPUs,
and the role of our visualization methods too. Our data workflow
analysis relies on the GPU-accelerated Python software package called
PyNX, an open source Python library which provides fast parallel
computation scattering. The code is well suited for GPU computing,
using both the pyCUDA and pyOpenCL libraries. Exploratory data
analysis and performance tests are initially carried on through
Jupyter notebooks and Python packages e.g., pandas, matplotlib,
plotly. In production stage, interactive visualization is realized by
using standard scientific tool, e.g. Paraview, an open-source 3D
visualization program which e.g. requires Python modules to generate
visualization components within VTK files.&lt;/p&gt;
</summary></entry></feed>