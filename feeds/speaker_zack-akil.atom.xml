<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_zack-akil.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-13T00:00:00+00:00</updated><entry><title>Safer Cycling with an Edge TPU watching your back</title><link href="https://pyvideo.org/pydata-london-2019/safer-cycling-with-an-edge-tpu-watching-your-back.html" rel="alternate"></link><published>2019-07-13T00:00:00+00:00</published><updated>2019-07-13T00:00:00+00:00</updated><author><name>Zack Akil</name></author><id>tag:pyvideo.org,2019-07-13:pydata-london-2019/safer-cycling-with-an-edge-tpu-watching-your-back.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Cycling in central London is an extreme sport! To give me the competitive edge I've built a machine vision model that runs on an Edge TPU that keeps me informed about the dangerous vehicles that are behind me in blistering real time. I'll show you the end-to-end process of developing and deploying the model to the recently released Edge TPU hardware from Google.&lt;/p&gt;
</summary></entry><entry><title>"Lights, camera, AI!" - Automated sports videography</title><link href="https://pyvideo.org/pydata-london-2017/lights-camera-ai-automated-sports-videography.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Zack Akil</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/lights-camera-ai-automated-sports-videography.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Amateur sports video quality usually depends on how good your friends are with their camera-phone. In this talk I walk through the stages of how I designed and developed a machine vision model for tracking where the action is happening in a rugby match, and how I made it lightweight enough to run on a Raspberry Pi in real-time .&lt;/p&gt;
&lt;p&gt;Abstract
Inspired by the shaky camera work of friends and parents alike in school sports events; I've build a Raspberry Pi controlled robot that automatically points a camera at where the action is happening in a rugby match.&lt;/p&gt;
&lt;p&gt;The talk will discuss the stages of developing an embedded machine vision model. Starting with a simple heuristic approach in order to collect the training data. Then how you can compress the feature space in order to reduce the computational load on the Raspberry Pi. Along with that, how using python multi-processing can help in sharing the load of the machine vision work and servo control.&lt;/p&gt;
&lt;p&gt;After the talk people should know a little about embedding machine learning using Raspberry Pi's as well as some insight in to SimpleCV (machine vision library for python) and multi-processing with python.&lt;/p&gt;
</summary></entry></feed>