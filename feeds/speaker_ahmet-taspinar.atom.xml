<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_ahmet-taspinar.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-04-09T00:00:00+00:00</updated><entry><title>Using deep learning in natural language processing: explaining Google's Neural Machine Translation</title><link href="https://pyvideo.org/pydata-amsterdam-2017/using-deep-learning-in-natural-language-processing-explaining-googles-neural-machine-translation.html" rel="alternate"></link><published>2017-04-09T00:00:00+00:00</published><updated>2017-04-09T00:00:00+00:00</updated><author><name>Rob Romijnders</name></author><id>tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/using-deep-learning-in-natural-language-processing-explaining-googles-neural-machine-translation.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Using deep learning in natural language processing: explaining Google's Neural Machine Translation&lt;/p&gt;
&lt;p&gt;Recent advancements in Natural Language Processing (NLP) use deep learning to improve performance. In September 2016, Google announced that Google Translate will shift from phrase-based to neural machine translation. Other fields of NLP are making a similar shift. This talk motivates and explains these algorithms and discusses implementations.&lt;/p&gt;
&lt;p&gt;Recent advancements in Natural Language Processing (NLP) use deep learning algorithms to improve performance. Google Translate shifts to neural machine translation, Baidu speech genetarion uses neural nets and question answering too. These neural networks share common architectures. They exploit recurrent computation to traverse the input and output. This talk will motivate the recurrent neural networks and discuss architectures. In the second half we discuss extensions such as attention mechanisms. Key words: RNN, seq2seq, attention, word vectors, data/model parallelism, low precision inference, TPU (explained in this order)&lt;/p&gt;
</summary></entry></feed>