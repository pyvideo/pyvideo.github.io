<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_parallel-programming.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-10-26T00:00:00+00:00</updated><entry><title>Concurrency in Python - concepts, frameworks and best practices</title><link href="https://pyvideo.org/pycon-de-2018/concurrency-in-python-concepts-frameworks-and-best-practices.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Stefan Schwarzer</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/concurrency-in-python-concepts-frameworks-and-best-practices.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you run in situations where concurrent execution could speed up
your Python code? Are you using a GUI toolkit?&lt;/p&gt;
&lt;p&gt;This talk gives you the background to use concurrency in your code
without shooting yourself in the foot - which is quite easy if you don't
understand how concurrent execution differs from linear execution!&lt;/p&gt;
&lt;p&gt;The presentation starts with explaining some concepts like concurrency,
parallelism, resources, atomic operations, race conditions and
deadlocks.&lt;/p&gt;
&lt;p&gt;Then we discuss the commonly-used approaches to concurrency:
multithreading with the &lt;tt class="docutils literal"&gt;threading&lt;/tt&gt; module, multiprocessing with the
&lt;tt class="docutils literal"&gt;multiprocessing&lt;/tt&gt; module, and event loops (which include the
&lt;tt class="docutils literal"&gt;asyncio&lt;/tt&gt; framework). Each of these approaches has its typical use
cases, which are explained.&lt;/p&gt;
&lt;p&gt;You can implement concurrency on a number of abstraction levels. The
lowest level consists of primitives like locks, events, semaphores and
so on. A higher abstraction level is using queues, typically with worker
threads or processes. Even higher abstraction levels are active objects
(hiding primitives or queues behind an API; this includes &amp;quot;actors&amp;quot; if
you heard of them), the thread and process pools in
&lt;tt class="docutils literal"&gt;concurrent.futures&lt;/tt&gt; and the &lt;tt class="docutils literal"&gt;asyncio&lt;/tt&gt; framework. Finally, you can
&amp;quot;outsource&amp;quot; concurrency by leaving it to a message broker, which is a
distinct process that receives and distributes messages.&lt;/p&gt;
&lt;p&gt;The talk closes with some tips and best practices, mainly:&lt;/p&gt;
</summary><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category></entry><entry><title>Strongly typed datasets in a weakly typed world</title><link href="https://pyvideo.org/pycon-de-2018/strongly-typed-datasets-in-a-weakly-typed-world.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Marco Neumann</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/strongly-typed-datasets-in-a-weakly-typed-world.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We at Blue Yonder use Pandas quite a lot during our daily data science
and engineering work. This choice, together with Python as an underlying
programming language gives us flexibility, a feature-rich interface, and
access to a large community and ecosystem. When it comes to preserving
the data and exchanging it with different software stacks, we rely on
Parquet Datasets / Hive Tables. During the write process, there is a
shift from a rather weakly typed world to a strongly typed one. For
example, Pandas may convert integers to floats for many operations
without asking, but parquet files and the schema information stored
alongside them dictate very precise types. The type situation may get
even more &amp;quot;colorful&amp;quot;, when datasets are written by multiple code
versions or different software solutions over time. This then results in
important questions regarding type compatibility.&lt;/p&gt;
&lt;p&gt;This talk will first represent an overview on types at different layers
(like NumPy, Pandas, Arrow and Parquet) and the transition between this
layers. The second part of the talk will present examples of type
compatibility we have seen and why+how we think they should be handled.
At the end there will be a Q+A, which can be seen as the start of a
potentially longer RFC process to align different software stacks (like
Hive and Dask) to handle types in a similar way.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category></entry><entry><title>Big Data Systems Performance: The Little Shop of Horrors</title><link href="https://pyvideo.org/pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Jens Dittrich</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The confusion around terms such as like NoSQL, Big Data, Data Science,
Spark, SQL, and Data Lakes often creates more fog than clarity. However,
clarity about the underlying technologies is crucial to designing the
best technical solution in any field relying on huge amounts of data
including data science, machine learning, but also more traditional
analytical systems such as data integration, data warehousing,
reporting, and OLAP.&lt;/p&gt;
&lt;p&gt;In my presentation, I will show that often at least three dimensions are
cluttered and confused in discussions when it comes to data management:
First, buzzwords (labels &amp;amp; terms like &amp;quot;big data&amp;quot;, &amp;quot;AI&amp;quot;, &amp;quot;data lake&amp;quot;);
second, data design patterns (principles &amp;amp; best practices like:
selection push-down, materialization, indexing); and Third, software
platforms (concrete implementations &amp;amp; frameworks like: Python, DBMS,
Spark, and NoSQL-systems).&lt;/p&gt;
&lt;p&gt;Only by keeping these three dimensions apart, it is possible to create
technically-sound architectures in the field of big data analytics.&lt;/p&gt;
&lt;p&gt;I will show concrete examples, which through a simple redesign and wise
choice of the right tools and technologies, run thereby up to 1000 times
faster. This in turn triggers tremendous savings in terms of development
time, hardware costs, and maintenance effort.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Infrastructure"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>Fulfilling Apache Arrow's Promises: Pandas on JVM memory without a copy</title><link href="https://pyvideo.org/pycon-de-2018/fulfilling-apache-arrows-promises-pandas-on-jvm-memory-without-a-copy.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/fulfilling-apache-arrows-promises-pandas-on-jvm-memory-without-a-copy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Arrow established a standard for columnar in-memory analytics to
redefine the performance and interoperability of most Big Data
technologies in early 2016. Since then implementations in Java, C++,
Python, Glib, Ruby, Go, JavaScript and Rust have been added. Although
Apache Arrow (&lt;tt class="docutils literal"&gt;pyarrow&lt;/tt&gt;) is already known to many Python/Pandas users
for reading Apache Parquet files, its main benefit is the cross-language
interoperability. With feather and PySpark, you can already benefit from
this in Python and R/Java via the filesystem or network. While they
improve data sharing and remove serialization overhead, data still needs
to be copied as it is passed between processes.&lt;/p&gt;
&lt;p&gt;In the 0.23 release of Pandas, the concept of ExtensionArrays was
introduced. They allow the extension of Pandas DataFrames and Series
with custom, user- defined typed. The most prominent example is
&lt;tt class="docutils literal"&gt;cyberpandas&lt;/tt&gt; which adds an IP dtype that is backed by the appropriate
representation using NumPy arrays. These ExtensionArrays are not limited
to arrays backed by NumPy but can take an arbitrary storage as long as
they fulfill a certain interfaces. Using Apache Arrow we can implement
ExtensionArrays that are of the same dtype as the built-in types of
Pandas but memory management is not tied to Pandas' internal
BlockManager. On the other hand Apache Arrow has a much more wider set
of efficient types that we can also expose as an ExtensionArray. These
types include a native string type as well as a arbitrarily nested types
such as &lt;tt class="docutils literal"&gt;list of …&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;struct of (…, …, …)&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;To show the real-world benefits of this, we take the example of a data
pipeline that pulls data from a relational store, transforms it and then
passes it into a machine learning model. A typical setup nowadays most
likely involves a data lake that is queried with a JVM based query
engine. The machine learning model is then normally implemented in
Python using popular frameworks like CatBoost or Tensorflow.&lt;/p&gt;
&lt;p&gt;While sometimes these query engines provide Python clients, their
performance is normally not optimized for large results sets. In the
case of a machine learning model, we will do some feature
transformations and possibly aggregations with the query engine but feed
as many rows as possible into the model. This will lead then to result
sets that have above a million rows. In contrast to the Python clients,
these engines often come with efficient JDBC drivers that can cope with
result sets of this size but then the conversion from Java objects to
Python objects in the JVM bridge will slow things down again. In our
example, we will show how to use Arrow to retrieve a large result in the
JVM and then pass it on to Python without running into these
bottlenecks.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category></entry><entry><title>Cython to speed up your Python code</title><link href="https://pyvideo.org/pycon-de-2018/cython-to-speed-up-your-python-code.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Stefan Behnel</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/cython-to-speed-up-your-python-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="http://cython.org"&gt;Cython&lt;/a&gt; is not only a very fast and comfortable
way to talk to native code and libraries, it is also a widely used tool
for speeding up Python code. The Cython compiler translates Python code
to C or C++ code, and applies many static optimisations that make Python
code run visibly faster than in the interpreter. But even better, it
supports static type annotations that allow direct use of C/C++ data
types and functions, which the compiler uses to convert and optimise the
code into fast, native C. The tight integration of all three languages,
Python, C and C++, makes it possible to freely mix Python features like
generators and comprehensions with C/C++ features like native data
types, pointer arithmetic or manually tuned memory management in the
same code.&lt;/p&gt;
&lt;p&gt;This talk by a core developer introduces the Cython compiler by
interactive code examples, and shows how you can use it to speed up your
real-world Python code. You will learn how you can profile a Python
module and use Cython to compile and optimise it into a fast binary
extension module. All of that, without losing the ability to run it
through common development tools like code checkers or coverage test
tools.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Infrastructure"></category><category term="Jupyter"></category><category term="Parallel Programming"></category></entry><entry><title>Pyccel, a Fortran static compiler for scientific High-Performance Computing</title><link href="https://pyvideo.org/pycon-de-2018/pyccel-a-fortran-static-compiler-for-scientific-high-performance-computing.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Dr. Ing. Ratnani Ahmed</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/pyccel-a-fortran-static-compiler-for-scientific-high-performance-computing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Pyccel&lt;/em&gt; is a new &lt;strong&gt;static compiler&lt;/strong&gt; for Python that uses &lt;strong&gt;Fortran&lt;/strong&gt;
as backend language while enabling High-Performance Computing &lt;strong&gt;HPC&lt;/strong&gt;
capabilities.&lt;/p&gt;
&lt;p&gt;Fortran is a computer language for scientific programming that is
tailored for efficient run-time execution on a wide variety of
processors. Even if the &lt;em&gt;2003&lt;/em&gt; and &lt;em&gt;2008&lt;/em&gt; standards added major
improvements like &lt;em&gt;OOP, Coarrays, Submodules, do concurrent&lt;/em&gt; , etc ...
they are not covered by all available compilers. Moreover, the Fortran
developer still suffers from the lack of &lt;strong&gt;meta-programming&lt;/strong&gt; compared
to &lt;strong&gt;C++&lt;/strong&gt; ones. Therefore, it is more and more difficult for applied
mathematicians and computational physicists to write applications at the
&lt;em&gt;state of art&lt;/em&gt; (targeting CPUs, GPUs, MICs) while implementing
complicated algorithms or numerical schemes.&lt;/p&gt;
&lt;p&gt;Pyccel can be used in two cases:&lt;/p&gt;
&lt;p&gt;In order to achieve the second point, we developed an internal DSL for
&lt;em&gt;types&lt;/em&gt; and &lt;em&gt;macros&lt;/em&gt;. The later is used to map sentences based on
&lt;em&gt;mpi4py&lt;/em&gt; , &lt;em&gt;scipy.linalg.blas or lapack&lt;/em&gt; onto the appropriate calls in
Fortran. Moreover, two parsers, for &lt;em&gt;OpenMP&lt;/em&gt; and &lt;em&gt;OpenACC&lt;/em&gt; , were added
too, allowing for explicit parallelism through the use of pragmas.&lt;/p&gt;
&lt;p&gt;Last but not least, Pyccel is an extension of &lt;strong&gt;Sympy&lt;/strong&gt;. Actually, it
converts a Python code to symbolic expressions/trees, from a Full Syntax
Tree ( &lt;em&gt;RedBaron&lt;/em&gt; ), then annotates the new AST using types or different
settings provided by the user.&lt;/p&gt;
&lt;p&gt;In this talk, after a brief description of Pyccel, I will show different
applications including Finite Elements (1d, 2d, 3d), Semi-Lagrangian
schemes (4d), Kronecker linear solvers, diagnostics for 5D kinetic
simulations and Machine Learning for Partial Differential Equations.&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Algorithms"></category><category term="Astronomy"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>Scalable Scientific Computing using Dask</title><link href="https://pyvideo.org/pycon-de-2018/scalable-scientific-computing-using-dask.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/scalable-scientific-computing-using-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category><category term="Python"></category></entry><entry><title>Selinon - dynamic distributed task flows</title><link href="https://pyvideo.org/pycon-de-2018/selinon-dynamic-distributed-task-flows.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Fridolín Pokorný</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/selinon-dynamic-distributed-task-flows.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough flexibility and dynamic features needed badly in complex
flows. As we discovered all the limitations, we decided to implement
Selinon.&lt;/p&gt;
&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough flexibility and dynamic features needed badly in complex
flows. As we discovered all the limitations, we decided to implement
Selinon.&lt;/p&gt;
&lt;p&gt;Selinon enhances Celery task flow management and allows you to create
and model task flows in your distributed environment that can
dynamically change behavior based on computed results in your cluster,
automatically resolve tasks that need to be executed in case of
selective task runs, automatic tracing mechanism and many others.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Infrastructure"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category></entry></feed>