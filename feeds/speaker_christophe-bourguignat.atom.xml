<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_christophe-bourguignat.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2015-04-14T00:00:00+00:00</updated><entry><title>Why and how to explain machine learning predictions</title><link href="https://pyvideo.org/pydata-paris-2015/why-and-how-to-explain-machine-learning-predictio.html" rel="alternate"></link><published>2015-04-14T00:00:00+00:00</published><updated>2015-04-14T00:00:00+00:00</updated><author><name>Bora Eang</name></author><id>tag:pyvideo.org,2015-04-14:pydata-paris-2015/why-and-how-to-explain-machine-learning-predictio.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Unfortunately, the predictive models that are most powerful are usually
the least interpretable. However in some cases, for example fraud
detection, end users need an understandable explanation of a particular
prediction, at observation level, and not only at population level (e.g.
: features importance). During this talk we will present different
approaches to tackle this issue, both for random forests and gradient
boosting trees. We will also demonstrate an implementation based on
scikit-learn.&lt;/p&gt;
</summary></entry></feed>