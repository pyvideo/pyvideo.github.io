<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_carsten-van-weelden.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-04-29T00:00:00+00:00</updated><entry><title>RNN sequence labeling for document parsing in Tensorflow</title><link href="https://pyvideo.org/pydata-london-2018/rnn-sequence-labeling-for-document-parsing-in-tensorflow.html" rel="alternate"></link><published>2018-04-29T00:00:00+00:00</published><updated>2018-04-29T00:00:00+00:00</updated><author><name>Carsten van Weelden</name></author><id>tag:pyvideo.org,2018-04-29:pydata-london-2018/rnn-sequence-labeling-for-document-parsing-in-tensorflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will show how we improved our CV parsing performance by
training RNN models using Tensorflow. We will demonstrate how to set up
and train a BLSTM sequence labeling model and discuss extensions such as
learning line representations, combining RNN and CRF layers and training
multilingual models.&lt;/p&gt;
</summary></entry><entry><title>Siamese LSTM in Keras: Learning Character-Based Phrase Representations</title><link href="https://pyvideo.org/pydata-amsterdam-2017/siamese-lstm-in-keras-learning-character-based-phrase-representations.html" rel="alternate"></link><published>2017-04-09T00:00:00+00:00</published><updated>2017-04-09T00:00:00+00:00</updated><author><name>Carsten van Weelden</name></author><id>tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/siamese-lstm-in-keras-learning-character-based-phrase-representations.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Siamese LSTM in Keras: Learning Character-Based Phrase Representations&lt;/p&gt;
&lt;p&gt;In this talk we will explain how we solved the problem of classifying job titles into a job ontology with more than 5000 different classes. We do this by learning a character-based representation of job titles with a B-LSTM encoder trained as a Siamese network. You will learn about the methods in theory and how these can be implemented with the Keras deep learning library.&lt;/p&gt;
&lt;p&gt;Learning representations of textual data is a crucial component in NLP systems. An important application is linking entities extracted from unstructured text to a knowledge base. In our use case, the entities are job titles extracted from resumes or vacancies, and the knowledge base is a hierarchical job title taxonomy. Successfully linking job titles is particularly important in our application, as it directly influences the performance of information retrieval- and data analytics solutions.&lt;/p&gt;
&lt;p&gt;In this talk we will explain how we solved the problem of classifying job titles into a job ontology with more than 5000 different classes. We do this by learning a character-based representation of job titles with a B-LSTM encoder trained as a Siamese network. You will learn about the methods in theory and how these can be implemented with the Keras deep learning library.&lt;/p&gt;
&lt;p&gt;We will walk you through how we constructed training examples in a domain where large-scale manual annotation is nearly impossible. We will show you how we built a framework to test invariances we would like to model in our data, such as extra words in automatically extracted phrases (e.g. &amp;quot;class 1 driver using own vehicle, london&amp;quot;) and spelling variation (e.g. “C Sharp” vs “C#”). Lastly we introduce a negative sampling strategy such that the network learns to recognize subtle differences between phrases (e.g. “pipe fitter” versus “ship fitter”).&lt;/p&gt;
</summary></entry></feed>