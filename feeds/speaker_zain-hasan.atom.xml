<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Zain Hasan</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_zain-hasan.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-07-08T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Building Scalable Multimodal Search Applications with Python</title><link href="https://pyvideo.org/europython-2024/building-scalable-multimodal-search-applications-with-python.html" rel="alternate"></link><published>2024-07-08T00:00:00+00:00</published><updated>2024-07-08T00:00:00+00:00</updated><author><name>Zain Hasan</name></author><id>tag:pyvideo.org,2024-07-08:/europython-2024/building-scalable-multimodal-search-applications-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;[EuroPython 2024 — North Hall on 2024-07-10]&lt;/p&gt;
&lt;p&gt;Building Scalable Multimodal Search Applications with Python by Zain Hasan&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://ep2024.europython.eu/session/building-scalable-multimodal-search-applications-with-python"&gt;https://ep2024.europython.eu/session/building-scalable-multimodal-search-applications-with-python&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Many real-world problems are inherently multimodal, from the communicative modalities humans use such as spoken language and gestures to the force, sensory, and visual sensors used in …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;[EuroPython 2024 — North Hall on 2024-07-10]&lt;/p&gt;
&lt;p&gt;Building Scalable Multimodal Search Applications with Python by Zain Hasan&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://ep2024.europython.eu/session/building-scalable-multimodal-search-applications-with-python"&gt;https://ep2024.europython.eu/session/building-scalable-multimodal-search-applications-with-python&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Many real-world problems are inherently multimodal, from the communicative modalities humans use such as spoken language and gestures to the force, sensory, and visual sensors used in robotics. For machine learning models to address these problems and interact more naturally and wholistically with the world around them and ultimately be more general and powerful reasoning engines, we need them to understand data across all of its corresponding images, video, text, audio, and tactile representations.&lt;/p&gt;
&lt;p&gt;In this talk, Zain Hasan will discuss how we can use open-source multimodal embedding models in conjunction with large generative multimodal models that can that can see, hear, read, and feel data(!), to perform cross-modal search(searching audio with images, videos with text etc.) and multimodal retrieval augmented generation (MM-RAG) at the billion-object scale with the help of open source vector databases. I will also demonstrate, with live code demos, how being able to perform this cross-modal retrieval in real-time can enables users to use LLMs that can reason over their enterprise multimodal data. This talk will revolve around how we can scale the usage of multimodal embedding and generative models in production.&lt;/p&gt;
&lt;p&gt;---&lt;/p&gt;
&lt;p&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License: &lt;a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;https://creativecommons.org/licenses/by-nc-sa/4.0/&lt;/a&gt;&lt;/p&gt;
</content><category term="EuroPython 2024"></category></entry></feed>