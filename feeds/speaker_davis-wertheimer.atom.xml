<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Davis Wertheimer</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_davis-wertheimer.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-09-18T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Lessons Learned in WatsonX Training: Scaling Cloud-Native PyTorch FSDP to 20B Parameters</title><link href="https://pyvideo.org/pytorch-conference-2023/lessons-learned-in-watsonx-training-scaling-cloud-native-pytorch-fsdp-to-20b-parameters.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Davis Wertheimer</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lessons-learned-in-watsonx-training-scaling-cloud-native-pytorch-fsdp-to-20b-parameters.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will cover lessons learned along our almost year-and-a-half journey scaling up the WatsonX.AI stack for foundation model pretraining. Starting from 100M parameters on bare metal, we scaled PyTorch training to 20B parameters on cloud-based multi-node systems. We'll discuss the challenges encountered along the way …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will cover lessons learned along our almost year-and-a-half journey scaling up the WatsonX.AI stack for foundation model pretraining. Starting from 100M parameters on bare metal, we scaled PyTorch training to 20B parameters on cloud-based multi-node systems. We'll discuss the challenges encountered along the way, as well as the solutions we employed. This includes working with the PyTorch team to field test Fully-Sharded and Hybrid-Shard Data Parallel update protocols (FSDP/HSDP), as well as handling the associated communication vs computation bottlenecks, which are not always straightforward. We'll also review our collaboration on cloud-native distributed checkpointing, and development of a stateful and scalable distributed dataloader, allowing us to restart unstable jobs mid-epoch without revisiting stale data. And finally, we'll cover ongoing and upcoming challenges, like maintaining job stability and tensor parallelism integration.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>A Distributed Stateful Dataloader for Large-Scale Pretraining</title><link href="https://pyvideo.org/pytorch-conference-2024/a-distributed-stateful-dataloader-for-large-scale-pretraining.html" rel="alternate"></link><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><author><name>Davis Wertheimer</name></author><id>tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/a-distributed-stateful-dataloader-for-large-scale-pretraining.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A Distributed Stateful Dataloader for Large-Scale Pretraining - Davis Wertheimer, IBM &amp;amp; Linsong Chu, IBM Research&lt;/p&gt;
&lt;p&gt;Large-scale model pretraining crucially relies on specialized and dedicated dataloaders that can, for example, partition and stream data asynchronously across multiple processes and physical nodes. In this talk we discuss one of the torch-native dataloaders …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A Distributed Stateful Dataloader for Large-Scale Pretraining - Davis Wertheimer, IBM &amp;amp; Linsong Chu, IBM Research&lt;/p&gt;
&lt;p&gt;Large-scale model pretraining crucially relies on specialized and dedicated dataloaders that can, for example, partition and stream data asynchronously across multiple processes and physical nodes. In this talk we discuss one of the torch-native dataloaders we built and use at IBM Research for addressing these needs. Intended for use in large-scale model pretraining, particularly in research settings where rapid iteration between datasets may be required, our dataloader is distributed, stateful, checkpointable, composable and rescalable – while remaining a simple extension of the existing PyTorch dataloading framework. It automatically and invisibly handles data sharding, shuffling, subdataset weighting, checkpoint saving and loading, and custom user-defined preprocessing functions, with minimal overhead and high throughput. We discuss these properties and how we achieved them, such as reducing overhead by implementing a custom LCG random number generator, and demonstrate proof of concept on production-scale training of a 7B parameter Llama model over 4 trillion tokens.&lt;/p&gt;
</content><category term="PyTorch Conference 2024"></category></entry></feed>