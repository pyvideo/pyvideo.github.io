<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_paul-agapow.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2015-06-20T00:00:00+00:00</updated><entry><title>Rescuing and Exploring Complex Life Science Data</title><link href="https://pyvideo.org/pydata-london-2015/rescuing-and-exploring-complex-life-science-data.html" rel="alternate"></link><published>2015-06-20T00:00:00+00:00</published><updated>2015-06-20T00:00:00+00:00</updated><author><name>Paul Agapow</name></author><id>tag:pyvideo.org,2015-06-20:pydata-london-2015/rescuing-and-exploring-complex-life-science-data.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Often we have no choice but to work with messy, difficult data. I
describe the Python-based approaches used to rescue and repair a
complex malformed dataset (using csvkit and a rule-driven
sanitisation approach), mount it in a new user-friendly db (using
pycap) before exploration (using py4neo). I finish by reflecting on
Python’s “gaps” as concerns life science/ biomedical analytical
tools.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Everyone complains about messy, difficult datasets but often we have no
choice but to work with them. In 2014, I was charged with the
“informatic rescue” of the data for a large trans-European
epidemiological trial, where the challenges were (1) to extract and make
useable the complicated but malformed patient data in a remote and
idiosyncratic database, (2) make this available and regularly updated in
a user-friendly system, (3) integrate several other data sources and
finally (4) explore the data for research purposes.&lt;/p&gt;
&lt;p&gt;Here I describe the Python-based approach I used. Starting with &lt;em&gt;csvkit&lt;/em&gt;
to recreate the original legacy database for direct examination and
manipulation, malformed data was transformed by a pipeline of
rule-driven sanitisation before being subjected to validation via
another pipeline of rules. I describe how &lt;em&gt;pycap&lt;/em&gt; and &lt;em&gt;REDCap&lt;/em&gt; were used
to make an easily updatable and user- friendly database, and how this
was leveraged in merging other datasets. I show how this data was
integrated with associated and complex datasets (analytical and genomic)
and explored in a graph database using &lt;em&gt;py4neo&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Finally, I reflect on the gaps in Python’s life science and biomedical
analytical offerings, including why Excel spreadsheets are here to stay,
if our current IDEs are good enough and whether developers are the enemy
of the good enough.&lt;/p&gt;
</summary></entry></feed>