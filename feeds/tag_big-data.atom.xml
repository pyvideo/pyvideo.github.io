<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_big-data.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-12T00:00:00+00:00</updated><entry><title>Running a Synchrotron on Open Source Python</title><link href="https://pyvideo.org/europython-2019/running-a-synchrotron-on-open-source-python.html" rel="alternate"></link><published>2019-07-12T00:00:00+00:00</published><updated>2019-07-12T00:00:00+00:00</updated><author><name>Clinton Roy</name></author><id>tag:pyvideo.org,2019-07-12:europython-2019/running-a-synchrotron-on-open-source-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A synchrotron is a large research facility that has a large software
stack to keep things running, fortunately a large chunk of the stack is
Open Source and fair chunk of it is Python to boot. By the end of the
talk attendees will understand the scale of the infrastructure (both
physical and software) that is required, and have an idea of what sort
of problems a synchrotron could help them solve.&lt;/p&gt;
</summary><category term="ASYNC / Concurrency"></category><category term="Architecture"></category><category term="Big Data"></category><category term="Engineering"></category><category term="Hardware/IoT"></category></entry><entry><title>Building Data Workflows with Luigi and Kubernetes</title><link href="https://pyvideo.org/europython-2019/building-data-workflows-with-luigi-and-kubernetes.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Nar Kumar Chhantyal</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/building-data-workflows-with-luigi-and-kubernetes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will focus on how one can build complex data pipelines in
Python. I will introduce Luigi and show how it solves problems while
running multiple chain of batch jobs like dependency resolution,
workflow management, visualisation, failure handling etc.&lt;/p&gt;
&lt;p&gt;After that, I will present how to package Luigi pipelines as Docker
image for easier testing and deployment. Finally, I will go through way
to deploy them on Kubernetes cluster, thus making it possible to scale
Big Data pipelines on- demand and reduce infrastructure costs. I will
also give tips and tricks to make Luigi Scheduler play well with
Kubernetes batch execution feature.&lt;/p&gt;
&lt;p&gt;This talk will be accompanied by demo project. It will be very
beneficial for audience who have some experience in running batch jobs
(not necessarily in Python), typically people who work in Big Data
sphere like data scientists, data engineers, BI devs and software
developers. Familiarity with Python is helpful but not needed.&lt;/p&gt;
</summary><category term="Architecture"></category><category term="Big Data"></category><category term="Data"></category><category term="Distributed Systems"></category><category term="Scaling"></category></entry><entry><title>How software can feed the world üå±</title><link href="https://pyvideo.org/europython-2019/how-software-can-feed-the-world.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Christian Barra</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/how-software-can-feed-the-world.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Infarm is a FaaS, Farming as a Service, and whether you believe it or
not, our business is in-house farming at scale.&lt;/p&gt;
&lt;p&gt;We design and build our farms, grow vegetables and sell them, and the
backbone of our infrastructure is based on Python.&lt;/p&gt;
&lt;p&gt;You can check this video to see what we do -&amp;gt;
&lt;a class="reference external" href="https://twitter.com/christianbarra/status/1096399602159439874"&gt;https://twitter.com/christianbarra/status/1096399602159439874&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More than 10 million observations are recorded from our farms, feeding
our farm management system that allows operators, plant scientists, and
supervisors to monitor each farm in real-time.&lt;/p&gt;
&lt;p&gt;During this talk I will briefly introduce the world's problems we are
trying to resolve at Infarm and then talk about our IoT farms,
infrastructure, how we use Python and how we plan to improve the
capabilities of our farms by adding edge machine learning.&lt;/p&gt;
&lt;p&gt;Agenda&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;- What are the problems we are trying to solve at Infarm&lt;/div&gt;
&lt;div class="line"&gt;- Our 4 tech pillars&lt;/div&gt;
&lt;div class="line"&gt;- How we started with Python&lt;/div&gt;
&lt;div class="line"&gt;- Issues we are facing while scaling our Python infrastructure to
support &amp;gt; 400 farms&lt;/div&gt;
&lt;div class="line"&gt;- How we plan to evolve our software and infrastructure on 4 different
levels: consolidate, architecture, cloud native and observability&lt;/div&gt;
&lt;div class="line"&gt;- How Python is going to support our automated farms and its role in
making the farms smarter (edge computing with AI)&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Big Data"></category><category term="Hardware/IoT"></category><category term="Internet of Things (IoT)"></category><category term="Machine-Learning"></category><category term="Python general"></category></entry><entry><title>Machine learning on non curated data</title><link href="https://pyvideo.org/europython-2019/machine-learning-on-non-curated-data.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Gael Varoquaux</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/machine-learning-on-non-curated-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;According to industry surveys [1], the number one hassle of data
scientists is cleaning the data to analyze it. Textbook statistical
modeling is sufficient for noisy signals, but errors of a discrete
nature break standard tools of machine learning. I will discuss how to
easily run machine learning on data tables with two common dirty-data
problems: missing values and non-normalized entries. On both problems, I
will show how to run standard machine-learning tools such as
scikit-learn in the presence of such errors. The talk will be didactic
and will discuss simple software solutions. It will build on the latest
improvements to scikit-learn for missing values and the DirtyCat package
[2] for non normalized entries. I will also summarize theoretical
analyses in recent machine learning publications.&lt;/p&gt;
&lt;p&gt;This talk targets data practitioners. Its goal are to help data
scientists to be more efficient analysing data with such errors and
understanding their impacts.&lt;/p&gt;
&lt;p&gt;With missing values, I will use simple arguments and examples to outline
how to obtain asymptotically good predictions [3]. Two components are
key: imputation and adding an indicator of missingness. I will explain
theoretical guidelines for these, and I will show how to implement these
ideas in practice, with scikit-learn as a learner, or as a preprocesser.&lt;/p&gt;
&lt;p&gt;For non-normalized categories, I will show that using their string
representations to ‚Äúvectorize‚Äù them, creating vectorial representations
gives a simple but powerful solution that can be plugged in standard
statistical analysis tools [4].&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;[1] Kaggle, the state of ML and data science 2017
&lt;a class="reference external" href="https://www.kaggle.com/surveys/2017"&gt;https://www.kaggle.com/surveys/2017&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[2] &lt;a class="reference external" href="https://dirty-cat.github.io/stable/"&gt;https://dirty-cat.github.io/stable/&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[3] Josse Julie, Prost Nicolas, Scornet Erwan, and Varoquaux Ga√´l
(2019). ‚ÄúOn the consistency of supervised learning with missing
values‚Äù. &lt;a class="reference external" href="https://arxiv.org/abs/1902.06931"&gt;https://arxiv.org/abs/1902.06931&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[4] Cerda Patricio, Varoquaux Ga√´l, and K√©gl Bal√°zs. &amp;quot;Similarity
encoding for learning with dirty categorical variables.&amp;quot; Machine
Learning 107.8-10 (2018): 1477 &lt;a class="reference external" href="https://arxiv.org/abs/1806.00979"&gt;https://arxiv.org/abs/1806.00979&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Big Data"></category><category term="Data"></category><category term="Data Science"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Tips for the scientific programmer</title><link href="https://pyvideo.org/europython-2019/tips-for-the-scientific-programmer.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Michele Simionato</name></author><id>tag:pyvideo.org,2019-07-11:europython-2019/tips-for-the-scientific-programmer.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This is a talk for people who need to perform large numeric
calculations. They could be scientists, developers working in close
contact with scientists, or even people working on finance and other
quantitative fields. Such people are routinely confronted with issues
like&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;1 parallelism: how to parallelize calculations efficiently&lt;/div&gt;
&lt;div class="line"&gt;2 data: how to store and manage large amounts of data efficiently&lt;/div&gt;
&lt;div class="line"&gt;3 memory: how to avoid running out of memory&lt;/div&gt;
&lt;div class="line"&gt;4 performance: how to be fast&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The goal of the talk is to teach some lessons learned after several
years of doing numeric simulations in a context were micro-optimizations
are the least important factor, while overall architecture, design
choices and good algorithms are of paramount importance.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Architecture"></category><category term="Big Data"></category><category term="Case Study"></category><category term="Performance"></category></entry><entry><title>Geospatial Analysis using Python and JupyterHub</title><link href="https://pyvideo.org/europython-2019/geospatial-analysis-using-python-and-jupyterhub.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Martin Christen</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/geospatial-analysis-using-python-and-jupyterhub.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Geospatial data is data containing a spatial component ‚Äì describing
objects with a reference to the planet's surface. This data usually
consists of a spatial component, of various attributes, and sometimes of
a time reference (where, what, and when). Efficient processing and
visualization of small to large-scale spatial data is a challenging
task.&lt;/p&gt;
&lt;p&gt;This talk describes how to process and visualize geospatial vector and
raster data using Python and the Jupyter Notebook.&lt;/p&gt;
&lt;p&gt;To process the data a high performance computer with 4 GPUS (NVidia
Tesla V100), 192 GB RAM, 44 CPU Cores is used to run JupyterHub.&lt;/p&gt;
&lt;p&gt;There are numerous modules available which help using geospatial data in
using low- and high-level interfaces, which are shown in this
presentation. In addition, it is shown how to use deep learning for
raster analysis using the high performance GPUs and several deep
learning frameworks.&lt;/p&gt;
</summary><category term="Analytics"></category><category term="Big Data"></category><category term="Deep Learning"></category><category term="GPU"></category><category term="Visualization"></category></entry><entry><title>Getting Your Data Joie De Vivre Back!</title><link href="https://pyvideo.org/europython-2019/getting-your-data-joie-de-vivre-back.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Lynn Cherny</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/getting-your-data-joie-de-vivre-back.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most of us work too much and play too little. When was the last time you
smiled at something you made? Playing with fun datasets, especially big
data sets, opens up weird new forms of technical recreation. Why not
train an amusing model in a browser tab while you're waiting for that
day-job Spark query to finish? I'll show you some data toys I've built
using AI and interesting data sets: Most of them involve both backend
data science and front-end visualization tricks. They range from
poetry-composition helpers to game log analysis to image deconstruction
and reconstruction. All of them taught me something, often about myself
and what I like artistically, and sometimes about what &amp;quot;big data&amp;quot;
actually means.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Deep Learning"></category><category term="Visualization"></category></entry><entry><title>What about recommendation engines?</title><link href="https://pyvideo.org/europython-2019/what-about-recommendation-engines.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Adriana Dorneles</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/what-about-recommendation-engines.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;How recommendation engines are taking part in our daily routine and
how companies as Netflix and Amazon implement it?&lt;/div&gt;
&lt;div class="line"&gt;This talk aims to show the elements that compound a recommendation
engine to people who have never been in touch with the matter or want
to know a bit more. At the end of this session, you might be able to
reproduce your own recommendation system and also know where to find
more about it.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Talk structure:&lt;/div&gt;
&lt;div class="line"&gt;1. What is and why use a recommendation engine?&lt;/div&gt;
&lt;div class="line"&gt;2. Recommendation engine importance&lt;/div&gt;
&lt;div class="line"&gt;3. Steps of a recommendation&lt;/div&gt;
&lt;div class="line"&gt;4. Recommendation algorithms&lt;/div&gt;
&lt;div class="line"&gt;5. Basic Statistics for distance and correlation&lt;/div&gt;
&lt;div class="line"&gt;6. Example&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Business"></category><category term="Data Science"></category><category term="Python 3"></category></entry><entry><title>Machine Learning for Inventory Management</title><link href="https://pyvideo.org/pycon-italia-2019/machine-learning-for-inventory-management.html" rel="alternate"></link><published>2019-05-04T00:00:00+00:00</published><updated>2019-05-04T00:00:00+00:00</updated><author><name>Laura De Stefanis</name></author><id>tag:pyvideo.org,2019-05-04:pycon-italia-2019/machine-learning-for-inventory-management.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Material forecast is the process of deciding which items to stock in the
inventory, how much, and when. Aim of the forecast is to increase parts
availability with the less possible impact on inventory, having enough
stock in the warehouse to ensure the business keeps moving but not
enough stock to drain its limited cash reserves. This decision process
is being profoundly revised in its foundational concepts, thanks to new
classification methodologies enabled by Machine Learning. We integrated
domain knowledge and ML to create a new classification and level setting
process, leveraging on 6 years of data and new statistical indicators
for demand patter. These new features are used to run the machine
learning algorithm that classify Make To Stock / Make To Order items in
a single flow approach. The validation phase is reduced at each
iteration as ML model can be re-trained to incorporate past validations,
increasing efficiency and performances. Level setting problem is
addressed benchmarking ML methods (Reinforcement Learning), Montecarlo
simulations and traditional statistical methodologies. Regarding RL and
Montecarlo we established punishments for letting an particular
inventory item run out of stock and we also punish the model for stock
too higher value for too long. For rewards, we primarily focus on
ordering items within a safe window before the demand. First application
of this new methodology brings a 20% reduction of inventory, without
impact on sales, and a workload reduction of about 70%.&lt;/p&gt;
&lt;p&gt;Feedback form: &lt;a class="reference external" href="https://python.it/feedback-1554"&gt;https://python.it/feedback-1554&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Saturday 4 May&lt;/strong&gt; at 12:15 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="logistics"></category><category term="clustering"></category><category term="statistics"></category><category term="Python"></category><category term="Big-Data"></category><category term="gestionali"></category><category term="django"></category><category term="supplychain"></category><category term="sql"></category><category term="Artificial Intelligence"></category></entry><entry><title>Meet dask and distributed: the unsung heroes of Python scientific data ecosystem.</title><link href="https://pyvideo.org/pycon-italia-2019/meet-dask-and-distributed-the-unsung-heroes-of-python-scientific-data-ecosystem.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Alessandro Amici</name></author><id>tag:pyvideo.org,2019-05-03:pycon-italia-2019/meet-dask-and-distributed-the-unsung-heroes-of-python-scientific-data-ecosystem.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Thanks to its world-class data tools and libraries, like Numpy, Pandas,
Jupyter, Matplotlib and xarray, Python is becoming the language of
choice in many scientific communities from Physics to Climate Science,
from Earth Observation to Economy.&lt;/p&gt;
&lt;p&gt;A turn-key but less-know component of the scientific ecosystem is the
dask library that enable seamless parallel, distributed and GPU
computing in most cases without code changes.&lt;/p&gt;
&lt;p&gt;We will use climate science as an typical example of a discipline where
simple tasks become easily big data problems and where mastering xarray,
dask and dask.distributed is the key to turn them back into simple
tasks, possibly on a large cluster of VMs (that you can easily provision
from your preferred cloud provider).&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://gitpitch.com/alexamici/talks/master?p=PyConX-2019"&gt;https://gitpitch.com/alexamici/talks/master?p=PyConX-2019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1704"&gt;https://python.it/feedback-1704&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 17:15 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="Jupyter"></category><category term="dask.distributed"></category><category term="Big-Data"></category><category term="xarray"></category><category term="dask"></category><category term="climate-change"></category><category term="earth-obeservation"></category><category term="pandas"></category></entry><entry><title>Strongly typed datasets in a weakly typed world</title><link href="https://pyvideo.org/pycon-de-2018/strongly-typed-datasets-in-a-weakly-typed-world.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Marco Neumann</name></author><id>tag:pyvideo.org,2018-10-26:pycon-de-2018/strongly-typed-datasets-in-a-weakly-typed-world.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We at Blue Yonder use Pandas quite a lot during our daily data science
and engineering work. This choice, together with Python as an underlying
programming language gives us flexibility, a feature-rich interface, and
access to a large community and ecosystem. When it comes to preserving
the data and exchanging it with different software stacks, we rely on
Parquet Datasets / Hive Tables. During the write process, there is a
shift from a rather weakly typed world to a strongly typed one. For
example, Pandas may convert integers to floats for many operations
without asking, but parquet files and the schema information stored
alongside them dictate very precise types. The type situation may get
even more &amp;quot;colorful&amp;quot;, when datasets are written by multiple code
versions or different software solutions over time. This then results in
important questions regarding type compatibility.&lt;/p&gt;
&lt;p&gt;This talk will first represent an overview on types at different layers
(like NumPy, Pandas, Arrow and Parquet) and the transition between this
layers. The second part of the talk will present examples of type
compatibility we have seen and why+how we think they should be handled.
At the end there will be a Q+A, which can be seen as the start of a
potentially longer RFC process to align different software stacks (like
Hive and Dask) to handle types in a similar way.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category></entry><entry><title>Big Data Systems Performance: The Little Shop of Horrors</title><link href="https://pyvideo.org/pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Jens Dittrich</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The confusion around terms such as like NoSQL, Big Data, Data Science,
Spark, SQL, and Data Lakes often creates more fog than clarity. However,
clarity about the underlying technologies is crucial to designing the
best technical solution in any field relying on huge amounts of data
including data science, machine learning, but also more traditional
analytical systems such as data integration, data warehousing,
reporting, and OLAP.&lt;/p&gt;
&lt;p&gt;In my presentation, I will show that often at least three dimensions are
cluttered and confused in discussions when it comes to data management:
First, buzzwords (labels &amp;amp; terms like &amp;quot;big data&amp;quot;, &amp;quot;AI&amp;quot;, &amp;quot;data lake&amp;quot;);
second, data design patterns (principles &amp;amp; best practices like:
selection push-down, materialization, indexing); and Third, software
platforms (concrete implementations &amp;amp; frameworks like: Python, DBMS,
Spark, and NoSQL-systems).&lt;/p&gt;
&lt;p&gt;Only by keeping these three dimensions apart, it is possible to create
technically-sound architectures in the field of big data analytics.&lt;/p&gt;
&lt;p&gt;I will show concrete examples, which through a simple redesign and wise
choice of the right tools and technologies, run thereby up to 1000 times
faster. This in turn triggers tremendous savings in terms of development
time, hardware costs, and maintenance effort.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Infrastructure"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>Data science complexity and solutions in real industrial projects</title><link href="https://pyvideo.org/pycon-de-2018/data-science-complexity-and-solutions-in-real-industrial-projects.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Artur Miller</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/data-science-complexity-and-solutions-in-real-industrial-projects.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As data scientists we usually like to apply fancy machine learning
models to well-groomed datasets. Everyone working on industrial problems
will eventually learn, that this does not reflect reality. The amount of
time spent on modeling is small compared to data gathering, -warehousing
and -cleaning. Even after training and deployment of the model, the work
is not done. Continuous monitoring of the performance and input data is
still necessary.&lt;/p&gt;
&lt;p&gt;In this talk I discuss how important data handling is for successful
data science projects. Each milestone, from finding the business case to
continuously monitoring the performance of the solution, is addressed.
This is exemplary shown on a project, with the goal of improving a
productive system.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Infrastructure"></category><category term="Machine Learning"></category></entry><entry><title>Data Science meets Data Protection: Keeping your data secure while learning from it.</title><link href="https://pyvideo.org/pycon-de-2018/data-science-meets-data-protection-keeping-your-data-secure-while-learning-from-it.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Andreas Dewes</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/data-science-meets-data-protection-keeping-your-data-secure-while-learning-from-it.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will discuss anonymization and pseudonymization techniques that you
can apply to your data to keep it secure and comply with the law(s)
while still being able to gain useful insights from it.&lt;/p&gt;
&lt;p&gt;We will show concrete Python implementations of various techniques and
use example data sets to show how applying pseudonymization and
anonymization will affect our ability to do machine learning / data
science.&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Business &amp; Start-Ups"></category><category term="Big Data"></category><category term="Data Science"></category></entry><entry><title>Fulfilling Apache Arrow's Promises: Pandas on JVM memory without a copy</title><link href="https://pyvideo.org/pycon-de-2018/fulfilling-apache-arrows-promises-pandas-on-jvm-memory-without-a-copy.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/fulfilling-apache-arrows-promises-pandas-on-jvm-memory-without-a-copy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Arrow established a standard for columnar in-memory analytics to
redefine the performance and interoperability of most Big Data
technologies in early 2016. Since then implementations in Java, C++,
Python, Glib, Ruby, Go, JavaScript and Rust have been added. Although
Apache Arrow (&lt;tt class="docutils literal"&gt;pyarrow&lt;/tt&gt;) is already known to many Python/Pandas users
for reading Apache Parquet files, its main benefit is the cross-language
interoperability. With feather and PySpark, you can already benefit from
this in Python and R/Java via the filesystem or network. While they
improve data sharing and remove serialization overhead, data still needs
to be copied as it is passed between processes.&lt;/p&gt;
&lt;p&gt;In the 0.23 release of Pandas, the concept of ExtensionArrays was
introduced. They allow the extension of Pandas DataFrames and Series
with custom, user- defined typed. The most prominent example is
&lt;tt class="docutils literal"&gt;cyberpandas&lt;/tt&gt; which adds an IP dtype that is backed by the appropriate
representation using NumPy arrays. These ExtensionArrays are not limited
to arrays backed by NumPy but can take an arbitrary storage as long as
they fulfill a certain interfaces. Using Apache Arrow we can implement
ExtensionArrays that are of the same dtype as the built-in types of
Pandas but memory management is not tied to Pandas' internal
BlockManager. On the other hand Apache Arrow has a much more wider set
of efficient types that we can also expose as an ExtensionArray. These
types include a native string type as well as a arbitrarily nested types
such as &lt;tt class="docutils literal"&gt;list of ‚Ä¶&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;struct of (‚Ä¶, ‚Ä¶, ‚Ä¶)&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;To show the real-world benefits of this, we take the example of a data
pipeline that pulls data from a relational store, transforms it and then
passes it into a machine learning model. A typical setup nowadays most
likely involves a data lake that is queried with a JVM based query
engine. The machine learning model is then normally implemented in
Python using popular frameworks like CatBoost or Tensorflow.&lt;/p&gt;
&lt;p&gt;While sometimes these query engines provide Python clients, their
performance is normally not optimized for large results sets. In the
case of a machine learning model, we will do some feature
transformations and possibly aggregations with the query engine but feed
as many rows as possible into the model. This will lead then to result
sets that have above a million rows. In contrast to the Python clients,
these engines often come with efficient JDBC drivers that can cope with
result sets of this size but then the conversion from Java objects to
Python objects in the JVM bridge will slow things down again. In our
example, we will show how to use Arrow to retrieve a large result in the
JVM and then pass it on to Python without running into these
bottlenecks.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category></entry><entry><title>Introduction to Docker for Pythonistas</title><link href="https://pyvideo.org/pycon-de-2018/introduction-to-docker-for-pythonistas.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Jan Wagner</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/introduction-to-docker-for-pythonistas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;My Talk aims to introduce you to Docker and how it works, how you can
use prebuild Images from the Docker-Hub and how you can make your own
Images.&lt;/div&gt;
&lt;div class="line"&gt;In more Detail, the following Points will be covered:&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Jupyter"></category><category term="Machine Learning"></category></entry><entry><title>Processing Geodata using Python</title><link href="https://pyvideo.org/pycon-de-2018/processing-geodata-using-python.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Martin Christen</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/processing-geodata-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a large amount of Python modules available suitable for spatial
data processing. In this talk, it is shown how to analyze, manipulate
and visualize geospatial data by using open source modules. The
following modules will be introduced:&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="Jupyter"></category><category term="Python"></category><category term="Visualisation"></category></entry><entry><title>Solving Data Science Problems using a Jupyter Notebook and SAP HANA's in-database Machine Learning Libraries</title><link href="https://pyvideo.org/pycon-de-2018/solving-data-science-problems-using-a-jupyter-notebook-and-sap-hanas-in-database-machine-learning-libraries.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Dr Frank Gottfried</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/solving-data-science-problems-using-a-jupyter-notebook-and-sap-hanas-in-database-machine-learning-libraries.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Companies store their data in databases with highly restricted access
regulations. The latest regulatorily changes enforces the need to work
on the datasets in this controlled environment without created
additional external copies. However Data Scientists prefer to work with
tools they are most familiar like Python, R and Jupyter Notebooks using
to a large amount of open- source packages (numpy, matplotlib, pandas,
..). SAP HANA provides highly optimized in-database machine learning
libraries. In this talk we will present how a Data Scientist can work in
an environment he/she is most familiar with and access the data stored
in SAP HANA using SAP HANA machine learning libraries with a
scikit-learn type interface. Data will remain in the database and will
be exposed as dataframes (similar to Pandas dataframes). We will explain
the software architecture and present a complete end-to-end use case by
using a Jupyter Notebook.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="Jupyter"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>Stretchy - NoSQL Database behind REST API</title><link href="https://pyvideo.org/pycon-de-2018/stretchy-nosql-database-behind-rest-api.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Artur Scholz</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/stretchy-nosql-database-behind-rest-api.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Stretchy is built as a microservice that provides a simple and intuitive
REST API with a NoSQL database as backend. No need for database
migrations or upfront schema design. The basic CRUD (create, read,
update, delete) operations are available for getting data in and out
from the database.&lt;/p&gt;
&lt;p&gt;Stretchy is free and open source software built with Python 3, using
Flask web framework. It currently uses MongoDB as its backend database.
Since it is interfaced through the REST API however, Stretchy is
technology agnostic and developers can create bindings to other
databases, including SQL databases.&lt;/p&gt;
&lt;p&gt;This presentation reviews the reasons for creating Stretchy, its current
applications, an short tutorial on how to use it, and tips on how to
deploy it.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="Web"></category></entry><entry><title>Where the heck is my memory?</title><link href="https://pyvideo.org/pycon-de-2018/where-the-heck-is-my-memory.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Florian Jetter</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/where-the-heck-is-my-memory.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Memory management is something the common Python user doesn‚Äôt need to
bother with because the gory details of it are hidden deep within the
interpreter itself. The garbage collector takes out the trash and we can
spend our precious time bothering with more important things on our
minds. Living in this encapsulated utopia is nice but sometimes it is
worth it to peak behind the curtains to unleash the full power of your
application. In this talk I want to show you when it is necessary to
face this harsh world and convince you that it is in fact not as scary
as it may seem. Using real life examples, I‚Äôm going to show you how to
use the garbage collector and open source tooling to get control over
the memory you might not even know you had at your disposal.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="DevOps"></category><category term="Infrastructure"></category></entry><entry><title>Cython to speed up your Python code</title><link href="https://pyvideo.org/pycon-de-2018/cython-to-speed-up-your-python-code.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Stefan Behnel</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/cython-to-speed-up-your-python-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="http://cython.org"&gt;Cython&lt;/a&gt; is not only a very fast and comfortable
way to talk to native code and libraries, it is also a widely used tool
for speeding up Python code. The Cython compiler translates Python code
to C or C++ code, and applies many static optimisations that make Python
code run visibly faster than in the interpreter. But even better, it
supports static type annotations that allow direct use of C/C++ data
types and functions, which the compiler uses to convert and optimise the
code into fast, native C. The tight integration of all three languages,
Python, C and C++, makes it possible to freely mix Python features like
generators and comprehensions with C/C++ features like native data
types, pointer arithmetic or manually tuned memory management in the
same code.&lt;/p&gt;
&lt;p&gt;This talk by a core developer introduces the Cython compiler by
interactive code examples, and shows how you can use it to speed up your
real-world Python code. You will learn how you can profile a Python
module and use Cython to compile and optimise it into a fast binary
extension module. All of that, without losing the ability to run it
through common development tools like code checkers or coverage test
tools.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Infrastructure"></category><category term="Jupyter"></category><category term="Parallel Programming"></category></entry><entry><title>Distributed Hyperparameter search with sklearn and kubernetes</title><link href="https://pyvideo.org/pycon-de-2018/distributed-hyperparameter-search-with-sklearn-and-kubernetes.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Jakob Karalus</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/distributed-hyperparameter-search-with-sklearn-and-kubernetes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While sklearn provides a good interface to do hyperparameter search on
large &amp;amp; complex model (pipelines), doing these can take up a lot of
time. The traditional way usually includes one beefy machine and a lot
of waiting. In other cases, people tend to ‚Äúmanually‚Äù schedule parameter
ranges between nodes, but that can also be problematic since these won't
talk to each other. Kubernetes itself is currently the most prominent
scheduler and shines at distributing task, but is a pretty complex
system in itself.&lt;/p&gt;
&lt;p&gt;In this talk, I will show how you can harness the scheduling of
kubernetes for distributing hyperparameter search with sklearn onto a
cluster of nodes. This can be achieved quite easily and with just a few
changes to the original code, so the Data Scientist won't be bothered by
complex kubernetes internals.&lt;/p&gt;
</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Infrastructure"></category><category term="Machine Learning"></category></entry><entry><title>reticulate: R interface to Python</title><link href="https://pyvideo.org/pycon-de-2018/reticulate-r-interface-to-python.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Jens Bruno Wittek</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/reticulate-r-interface-to-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python and R are the preferred languages for data science. In 2018,
RStudio introduced its package reticulate and clearly demonstrates that
it favours to join forces. Both languages have strengths and weaknesses.
Tools to combine the strengths will enable easier collaboration in
projects and more possibilities to succeed. Using Python from R gives R
users wider access to functions and makes it easier for Python beginners
to just run scripts and being able to collaborate in Python projects.
The talk will show the possibilities of reticulate: The main part starts
with demonstrating the Python interpreter within R. It will show how to
source Python scripts as well as install and import modules. Then it
will deal with the most important types of Python objects, how they are
represented in R and how to further manipulate them. Thereby, a special
focus is on using Python for data science. In addition, it will be
presented how Conda environments can be created and used from R. A
further application will be the creation of reports with Markdown and
LaTeX where R and Python can be used within one document and share
objects. A last topic is about showing the possibilities for easier
development in RStudio (help regarding Python functions, auto
completion).&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="NLP"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>Scalable Scientific Computing using Dask</title><link href="https://pyvideo.org/pycon-de-2018/scalable-scientific-computing-using-dask.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/scalable-scientific-computing-using-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;</summary><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category><category term="Python"></category></entry><entry><title>Selinon - dynamic distributed task flows</title><link href="https://pyvideo.org/pycon-de-2018/selinon-dynamic-distributed-task-flows.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Fridol√≠n Pokorn√Ω</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/selinon-dynamic-distributed-task-flows.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough flexibility and dynamic features needed badly in complex
flows. As we discovered all the limitations, we decided to implement
Selinon.&lt;/p&gt;
&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough flexibility and dynamic features needed badly in complex
flows. As we discovered all the limitations, we decided to implement
Selinon.&lt;/p&gt;
&lt;p&gt;Selinon enhances Celery task flow management and allows you to create
and model task flows in your distributed environment that can
dynamically change behavior based on computed results in your cluster,
automatically resolve tasks that need to be executed in case of
selective task runs, automatic tracing mechanism and many others.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Infrastructure"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category></entry><entry><title>Scaling your Data infrastructure</title><link href="https://pyvideo.org/pycon-italia-2018/scaling-your-data-infrastructure.html" rel="alternate"></link><published>2018-04-20T00:00:00+00:00</published><updated>2018-04-20T00:00:00+00:00</updated><author><name>Christian Barra</name></author><id>tag:pyvideo.org,2018-04-20:pycon-italia-2018/scaling-your-data-infrastructure.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;This talk aims to answer a few questions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What do you do when you need to move your model from your laptop to
production?&lt;/li&gt;
&lt;li&gt;Is &lt;tt class="docutils literal"&gt;big data == I need to use JVM&lt;/tt&gt; the right assumption?&lt;/li&gt;
&lt;li&gt;How can I put my jupyter notebook in production?&lt;/li&gt;
&lt;li&gt;How do you apply the best software engineering practices (testing and
ci for example) inside your data science process?&lt;/li&gt;
&lt;li&gt;How do you ‚Äúdecouple‚Äù your data scientists, developers and devops
teams?&lt;/li&gt;
&lt;li&gt;How do you guarantee the reproducibility of your models?&lt;/li&gt;
&lt;li&gt;How do you scale your training process when does not fit in memory
anymore?&lt;/li&gt;
&lt;li&gt;How do you serve your models and provide an easy rollback system?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Agenda:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The Data Science workflow&lt;/li&gt;
&lt;li&gt;Scaling is not just a matter of the size of your Data&lt;/li&gt;
&lt;li&gt;Scaling when the size of your Data matters&lt;/li&gt;
&lt;li&gt;DDS, Dockerized Data Science&lt;/li&gt;
&lt;li&gt;Cassiny&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I‚Äôll share my experience highlighting some of the challenges I faced and
the solutions I came up to answer these questions.&lt;/p&gt;
&lt;p&gt;During this presentation I will mention libraries like jupyter, atom,
scikit- learn, dask, ray, parquet, arrow and many others.&lt;/p&gt;
&lt;p&gt;The principles and best practices I will share are something that you
can apply, more or less easily, if you are running or in the process to
run a production system based on the Python stack.&lt;/p&gt;
&lt;p&gt;This talk will focus on (my) best practices to run the Python Data stack
together and I will also talk about Cassiny, an open source project I
started, that aims to simplify your life if you want to use a completely
Python based solution in your data science workflow.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 20 April&lt;/strong&gt; at 11:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="Jupyter"></category><category term="CloudComputing"></category><category term="pydata"></category><category term="#lessonslearned"></category><category term="Big-Data"></category><category term="S3"></category><category term="Data-Scientist"></category><category term="#amicodialessia"></category><category term="java"></category><category term="docker"></category><category term="cloud"></category></entry><entry><title>The Truth about Mastering Big Data</title><link href="https://pyvideo.org/pycon-sk-2018/the-truth-about-mastering-big-data.html" rel="alternate"></link><published>2018-03-10T00:00:00+00:00</published><updated>2018-03-10T00:00:00+00:00</updated><author><name>Anton Caceres</name></author><id>tag:pyvideo.org,2018-03-10:pycon-sk-2018/the-truth-about-mastering-big-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What do you think is the most essential skill a data scientist should
master? Knowledge of deep learning tools? Hadoop? SciPy?&lt;/p&gt;
&lt;p&gt;This talk reveals the cornerstone of data science: nothing is as
important as asking data the right questions. To make it work, we need
some tools and curiosity.&lt;/p&gt;
&lt;p&gt;While focusing on tools, we will first go over the data science subject
as a whole, define our goals, continue with an overview of the essential
Python packages like Pandas and Jupyter Notebook, and conclude with a
live demo. The purpose of this talk is to understand our data: read it,
visualize, and formulate right questions, as well as to endorse your
imagination as a data scientist.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="PyCon SK"></category><category term="Python"></category></entry><entry><title>Scaling up to Big Data Devops for Data Science</title><link href="https://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Marck Vaisman</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</summary><category term="big data"></category><category term="Data"></category><category term="data science"></category><category term="devops"></category><category term="scaling"></category><category term="science"></category></entry><entry><title>Tratando datos m√°s all√° de los l√≠mites de la memoria</title><link href="https://pyvideo.org/pycon-es-2015/tratando-datos-mas-alla-de-los-limites-de-la-memoria.html" rel="alternate"></link><published>2016-02-02T00:00:00+00:00</published><updated>2016-02-02T00:00:00+00:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2016-02-02:pycon-es-2015/tratando-datos-mas-alla-de-los-limites-de-la-memoria.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la era del 'Big Data' se necesitan cantidades cada vez m√°s grandes de memoria (RAM) para tratar y analizar estos datos. Pero tarde o temprano se llega a unos l√≠mites por encima de los cuales no se puede (o es muy caro) pasar.&lt;/p&gt;
&lt;p&gt;El compresor Blosc (blosc.org) y el contenedor de datos bcolz (bcolz.blosc.org), usan las capacidades de los ordenadores modernos (caches, procesadores multihilo y SSDs) para permitir tratar datos m√°s all√° los l√≠mites de la memoria.&lt;/p&gt;
</summary><category term="workshop"></category><category term="big data"></category><category term="blosc"></category><category term="bcolz"></category></entry><entry><title>Usando contenedores para Big Data</title><link href="https://pyvideo.org/pycon-es-2015/usando-contenedores-para-big-data.html" rel="alternate"></link><published>2016-02-02T00:00:00+00:00</published><updated>2016-02-02T00:00:00+00:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2016-02-02:pycon-es-2015/usando-contenedores-para-big-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la actualidad existe una variedad bastante grande de contenedores de datos para almacenar grandes cantidades de datos en Python, tanto en memoria como en disco. En mi taller pasaremos revista a unos cuantos de los m√°s √∫tiles, empezando por los m√°s b√°sicos y generales (listas, diccionarios, NumPy/ndarray, pandas/DataFrames) a los m√°s especializados (RDBMS, PyTables/Table/HDF5, bcolz/carray/ctable). Durante el camino se dar√°n pistas de cuando usar unos u otros dependiendo del caso de uso.&lt;/p&gt;
</summary><category term="workshop"></category><category term="big data"></category><category term="numpy"></category><category term="pandas"></category><category term="pytables"></category><category term="bcolz"></category></entry><entry><title>MapReduce mit Disco</title><link href="https://pyvideo.org/pycon-de-2013/mapreduce-mit-disco.html" rel="alternate"></link><published>2013-10-17T00:00:00+00:00</published><updated>2013-10-17T00:00:00+00:00</updated><author><name>Dr. Jan Morlock</name></author><id>tag:pyvideo.org,2013-10-17:pycon-de-2013/mapreduce-mit-disco.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Mit dem MapReduce-Verfahren k√∂nnen massive Datenmengen auf einem
Rechencluster verarbeitet werden. Namensgeber und wichtige Bestandteile
sind eine Map- und eine Reduce-Phase. Diese werden jeweils
parallelisiert ausgef√ºhrt und erm√∂glichen somit eine optimale Auslastung
der vorhandenen Ressourcen. Im Vergleich zu einer entsprechenden
sequentiellen Implementierung k√∂nnen dadurch gro√üe Zeiteinsparungen
erreicht werden.&lt;/p&gt;
&lt;p&gt;Mit dem freien Disco-Framework k√∂nnen MapReduce-Aufgaben leicht in
Python erstellt werden. Beim Zugriff auf die Eingabedaten werden
verschiedene Protokolle unterst√ºtzt. W√§hrend der Ausf√ºhrung kann der
Zustand des Rechenclusters sowie der Fortschritt der einzelnen Aufgaben
mit Hilfe einer Weboberfl√§che √ºberwacht werden. Ein verteiltes
Dateisystem, das Disco Distributed Filesystem (DDFS), wird zur
Speicherung der Zwischen- und Endergebnisse verwendet.&lt;/p&gt;
</summary><category term="big data"></category><category term="disco"></category><category term="mapreduce"></category><category term="parallelisierung"></category></entry></feed>