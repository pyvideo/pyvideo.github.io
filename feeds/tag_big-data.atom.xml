<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - big data</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_big-data.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2022-06-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Running a Synchrotron on Open Source Python</title><link href="https://pyvideo.org/europython-2019/running-a-synchrotron-on-open-source-python.html" rel="alternate"></link><published>2019-07-12T00:00:00+00:00</published><updated>2019-07-12T00:00:00+00:00</updated><author><name>Clinton Roy</name></author><id>tag:pyvideo.org,2019-07-12:/europython-2019/running-a-synchrotron-on-open-source-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A synchrotron is a large research facility that has a large software
stack to keep things running, fortunately a large chunk of the stack is
Open Source and fair chunk of it is Python to boot. By the end of the
talk attendees will understand the scale of the ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A synchrotron is a large research facility that has a large software
stack to keep things running, fortunately a large chunk of the stack is
Open Source and fair chunk of it is Python to boot. By the end of the
talk attendees will understand the scale of the infrastructure (both
physical and software) that is required, and have an idea of what sort
of problems a synchrotron could help them solve.&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="ASYNC / Concurrency"></category><category term="Architecture"></category><category term="Big Data"></category><category term="Engineering"></category><category term="Hardware/IoT"></category></entry><entry><title>Building Data Workflows with Luigi and Kubernetes</title><link href="https://pyvideo.org/europython-2019/building-data-workflows-with-luigi-and-kubernetes.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Nar Kumar Chhantyal</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/building-data-workflows-with-luigi-and-kubernetes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will focus on how one can build complex data pipelines in
Python. I will introduce Luigi and show how it solves problems while
running multiple chain of batch jobs like dependency resolution,
workflow management, visualisation, failure handling etc.&lt;/p&gt;
&lt;p&gt;After that, I will present how to package Luigi ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will focus on how one can build complex data pipelines in
Python. I will introduce Luigi and show how it solves problems while
running multiple chain of batch jobs like dependency resolution,
workflow management, visualisation, failure handling etc.&lt;/p&gt;
&lt;p&gt;After that, I will present how to package Luigi pipelines as Docker
image for easier testing and deployment. Finally, I will go through way
to deploy them on Kubernetes cluster, thus making it possible to scale
Big Data pipelines on- demand and reduce infrastructure costs. I will
also give tips and tricks to make Luigi Scheduler play well with
Kubernetes batch execution feature.&lt;/p&gt;
&lt;p&gt;This talk will be accompanied by demo project. It will be very
beneficial for audience who have some experience in running batch jobs
(not necessarily in Python), typically people who work in Big Data
sphere like data scientists, data engineers, BI devs and software
developers. Familiarity with Python is helpful but not needed.&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Architecture"></category><category term="Big Data"></category><category term="Data"></category><category term="Distributed Systems"></category><category term="Scaling"></category></entry><entry><title>How software can feed the world üå±</title><link href="https://pyvideo.org/europython-2019/how-software-can-feed-the-world.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Christian Barra</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/how-software-can-feed-the-world.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Infarm is a FaaS, Farming as a Service, and whether you believe it or
not, our business is in-house farming at scale.&lt;/p&gt;
&lt;p&gt;We design and build our farms, grow vegetables and sell them, and the
backbone of our infrastructure is based on Python.&lt;/p&gt;
&lt;p&gt;You can check this video to ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Infarm is a FaaS, Farming as a Service, and whether you believe it or
not, our business is in-house farming at scale.&lt;/p&gt;
&lt;p&gt;We design and build our farms, grow vegetables and sell them, and the
backbone of our infrastructure is based on Python.&lt;/p&gt;
&lt;p&gt;You can check this video to see what we do -&amp;gt;
&lt;a class="reference external" href="https://twitter.com/christianbarra/status/1096399602159439874"&gt;https://twitter.com/christianbarra/status/1096399602159439874&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More than 10 million observations are recorded from our farms, feeding
our farm management system that allows operators, plant scientists, and
supervisors to monitor each farm in real-time.&lt;/p&gt;
&lt;p&gt;During this talk I will briefly introduce the world's problems we are
trying to resolve at Infarm and then talk about our IoT farms,
infrastructure, how we use Python and how we plan to improve the
capabilities of our farms by adding edge machine learning.&lt;/p&gt;
&lt;p&gt;Agenda&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;- What are the problems we are trying to solve at Infarm&lt;/div&gt;
&lt;div class="line"&gt;- Our 4 tech pillars&lt;/div&gt;
&lt;div class="line"&gt;- How we started with Python&lt;/div&gt;
&lt;div class="line"&gt;- Issues we are facing while scaling our Python infrastructure to
support &amp;gt; 400 farms&lt;/div&gt;
&lt;div class="line"&gt;- How we plan to evolve our software and infrastructure on 4 different
levels: consolidate, architecture, cloud native and observability&lt;/div&gt;
&lt;div class="line"&gt;- How Python is going to support our automated farms and its role in
making the farms smarter (edge computing with AI)&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Big Data"></category><category term="Hardware/IoT"></category><category term="Internet of Things (IoT)"></category><category term="Machine-Learning"></category><category term="Python general"></category></entry><entry><title>Machine learning on non curated data</title><link href="https://pyvideo.org/europython-2019/machine-learning-on-non-curated-data.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Gael Varoquaux</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/machine-learning-on-non-curated-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;According to industry surveys [1], the number one hassle of data
scientists is cleaning the data to analyze it. Textbook statistical
modeling is sufficient for noisy signals, but errors of a discrete
nature break standard tools of machine learning. I will discuss how to
easily run machine learning on ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;According to industry surveys [1], the number one hassle of data
scientists is cleaning the data to analyze it. Textbook statistical
modeling is sufficient for noisy signals, but errors of a discrete
nature break standard tools of machine learning. I will discuss how to
easily run machine learning on data tables with two common dirty-data
problems: missing values and non-normalized entries. On both problems, I
will show how to run standard machine-learning tools such as
scikit-learn in the presence of such errors. The talk will be didactic
and will discuss simple software solutions. It will build on the latest
improvements to scikit-learn for missing values and the DirtyCat package
[2] for non normalized entries. I will also summarize theoretical
analyses in recent machine learning publications.&lt;/p&gt;
&lt;p&gt;This talk targets data practitioners. Its goal are to help data
scientists to be more efficient analysing data with such errors and
understanding their impacts.&lt;/p&gt;
&lt;p&gt;With missing values, I will use simple arguments and examples to outline
how to obtain asymptotically good predictions [3]. Two components are
key: imputation and adding an indicator of missingness. I will explain
theoretical guidelines for these, and I will show how to implement these
ideas in practice, with scikit-learn as a learner, or as a preprocesser.&lt;/p&gt;
&lt;p&gt;For non-normalized categories, I will show that using their string
representations to ‚Äúvectorize‚Äù them, creating vectorial representations
gives a simple but powerful solution that can be plugged in standard
statistical analysis tools [4].&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;[1] Kaggle, the state of ML and data science 2017
&lt;a class="reference external" href="https://www.kaggle.com/surveys/2017"&gt;https://www.kaggle.com/surveys/2017&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[2] &lt;a class="reference external" href="https://dirty-cat.github.io/stable/"&gt;https://dirty-cat.github.io/stable/&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[3] Josse Julie, Prost Nicolas, Scornet Erwan, and Varoquaux Ga√´l
(2019). ‚ÄúOn the consistency of supervised learning with missing
values‚Äù. &lt;a class="reference external" href="https://arxiv.org/abs/1902.06931"&gt;https://arxiv.org/abs/1902.06931&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[4] Cerda Patricio, Varoquaux Ga√´l, and K√©gl Bal√°zs. &amp;quot;Similarity
encoding for learning with dirty categorical variables.&amp;quot; Machine
Learning 107.8-10 (2018): 1477 &lt;a class="reference external" href="https://arxiv.org/abs/1806.00979"&gt;https://arxiv.org/abs/1806.00979&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Big Data"></category><category term="Data"></category><category term="Data Science"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Tips for the scientific programmer</title><link href="https://pyvideo.org/europython-2019/tips-for-the-scientific-programmer.html" rel="alternate"></link><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><author><name>Michele Simionato</name></author><id>tag:pyvideo.org,2019-07-11:/europython-2019/tips-for-the-scientific-programmer.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This is a talk for people who need to perform large numeric
calculations. They could be scientists, developers working in close
contact with scientists, or even people working on finance and other
quantitative fields. Such people are routinely confronted with issues
like&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;1 parallelism: how to parallelize calculations efficiently ‚Ä¶&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This is a talk for people who need to perform large numeric
calculations. They could be scientists, developers working in close
contact with scientists, or even people working on finance and other
quantitative fields. Such people are routinely confronted with issues
like&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;1 parallelism: how to parallelize calculations efficiently&lt;/div&gt;
&lt;div class="line"&gt;2 data: how to store and manage large amounts of data efficiently&lt;/div&gt;
&lt;div class="line"&gt;3 memory: how to avoid running out of memory&lt;/div&gt;
&lt;div class="line"&gt;4 performance: how to be fast&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The goal of the talk is to teach some lessons learned after several
years of doing numeric simulations in a context were micro-optimizations
are the least important factor, while overall architecture, design
choices and good algorithms are of paramount importance.&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Algorithms"></category><category term="Architecture"></category><category term="Big Data"></category><category term="Case Study"></category><category term="Performance"></category></entry><entry><title>Geospatial Analysis using Python and JupyterHub</title><link href="https://pyvideo.org/europython-2019/geospatial-analysis-using-python-and-jupyterhub.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Martin Christen</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/geospatial-analysis-using-python-and-jupyterhub.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Geospatial data is data containing a spatial component ‚Äì describing
objects with a reference to the planet's surface. This data usually
consists of a spatial component, of various attributes, and sometimes of
a time reference (where, what, and when). Efficient processing and
visualization of small to large-scale spatial data is ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Geospatial data is data containing a spatial component ‚Äì describing
objects with a reference to the planet's surface. This data usually
consists of a spatial component, of various attributes, and sometimes of
a time reference (where, what, and when). Efficient processing and
visualization of small to large-scale spatial data is a challenging
task.&lt;/p&gt;
&lt;p&gt;This talk describes how to process and visualize geospatial vector and
raster data using Python and the Jupyter Notebook.&lt;/p&gt;
&lt;p&gt;To process the data a high performance computer with 4 GPUS (NVidia
Tesla V100), 192 GB RAM, 44 CPU Cores is used to run JupyterHub.&lt;/p&gt;
&lt;p&gt;There are numerous modules available which help using geospatial data in
using low- and high-level interfaces, which are shown in this
presentation. In addition, it is shown how to use deep learning for
raster analysis using the high performance GPUs and several deep
learning frameworks.&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Analytics"></category><category term="Big Data"></category><category term="Deep Learning"></category><category term="GPU"></category><category term="Visualization"></category></entry><entry><title>Getting Your Data Joie De Vivre Back!</title><link href="https://pyvideo.org/europython-2019/getting-your-data-joie-de-vivre-back.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Lynn Cherny</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/getting-your-data-joie-de-vivre-back.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most of us work too much and play too little. When was the last time you
smiled at something you made? Playing with fun datasets, especially big
data sets, opens up weird new forms of technical recreation. Why not
train an amusing model in a browser tab while you're ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most of us work too much and play too little. When was the last time you
smiled at something you made? Playing with fun datasets, especially big
data sets, opens up weird new forms of technical recreation. Why not
train an amusing model in a browser tab while you're waiting for that
day-job Spark query to finish? I'll show you some data toys I've built
using AI and interesting data sets: Most of them involve both backend
data science and front-end visualization tricks. They range from
poetry-composition helpers to game log analysis to image deconstruction
and reconstruction. All of them taught me something, often about myself
and what I like artistically, and sometimes about what &amp;quot;big data&amp;quot;
actually means.&lt;/p&gt;
</content><category term="EuroPython 2019"></category><category term="Big Data"></category><category term="Deep Learning"></category><category term="Visualization"></category></entry><entry><title>What about recommendation engines?</title><link href="https://pyvideo.org/europython-2019/what-about-recommendation-engines.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Adriana Dorneles</name></author><id>tag:pyvideo.org,2019-07-10:/europython-2019/what-about-recommendation-engines.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;How recommendation engines are taking part in our daily routine and
how companies as Netflix and Amazon implement it?&lt;/div&gt;
&lt;div class="line"&gt;This talk aims to show the elements that compound a recommendation
engine to people who have never been in touch with the matter or want
to know a bit more ‚Ä¶&lt;/div&gt;&lt;/div&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;How recommendation engines are taking part in our daily routine and
how companies as Netflix and Amazon implement it?&lt;/div&gt;
&lt;div class="line"&gt;This talk aims to show the elements that compound a recommendation
engine to people who have never been in touch with the matter or want
to know a bit more. At the end of this session, you might be able to
reproduce your own recommendation system and also know where to find
more about it.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Talk structure:&lt;/div&gt;
&lt;div class="line"&gt;1. What is and why use a recommendation engine?&lt;/div&gt;
&lt;div class="line"&gt;2. Recommendation engine importance&lt;/div&gt;
&lt;div class="line"&gt;3. Steps of a recommendation&lt;/div&gt;
&lt;div class="line"&gt;4. Recommendation algorithms&lt;/div&gt;
&lt;div class="line"&gt;5. Basic Statistics for distance and correlation&lt;/div&gt;
&lt;div class="line"&gt;6. Example&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Business"></category><category term="Data Science"></category><category term="Python 3"></category></entry><entry><title>Automating machine learning workflow with DVC</title><link href="https://pyvideo.org/europython-2020/automating-machine-learning-workflow-with-dvc.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Hongjoo Lee</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/automating-machine-learning-workflow-with-dvc.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What data scientist / ML engineer wants to do while software engineers are busy with CI/CD.&lt;/p&gt;
&lt;p&gt;As software engineers work on CI/CD process as soon as they start a new project, data scientists and ML engineers define a pipeline for data as it flows through a typical workflow ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What data scientist / ML engineer wants to do while software engineers are busy with CI/CD.&lt;/p&gt;
&lt;p&gt;As software engineers work on CI/CD process as soon as they start a new project, data scientists and ML engineers define a pipeline for data as it flows through a typical workflow. Each step of the pipeline is fed data processed from its preceding step as CI/CD process starts from code changes.&lt;/p&gt;
&lt;p&gt;&amp;quot;Pipelining ML project&amp;quot; is sometimes misleading as it implies a large project with a group of engineers working on some large systems , being considered to be hard for an individual and unnecessary for a small project. Regardless of its size, having well organized pipelines for any ML projects is essential to succeed and actually it could be done easily with utilizing a proper tool.&lt;/p&gt;
&lt;p&gt;In this talk, we will go through a machine learning workflow divided into a few steps composing a ML pipeline from data ingestion to model deployment. Each step depends on data produced by previous step, which are controlled by DVC. DVC is open-source version control system for data scientist and ML engineer helping them to organize data, models and experiments for some ML projects. The presentation will not only introduce how to use the tool but also show how to organize a ML pipeline with some examples.&lt;/p&gt;
&lt;p&gt;The goal of this talk is to motivate data scientists and ML engineer to start building machine learning pipeline with DVC. Audience might expect a guide to using DVC  for automating the pipeline. Also I will give some explanation about concepts of machine learning related techniques necessary for understanding the pipeline.&lt;/p&gt;
&lt;p&gt;This session is designed to be accessible to everyone in beginners level. Understandings of basic concepts of machine learning and version control system (preferably, Git) might be helpful but not mandatory for the audience.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Big Data"></category><category term="Data"></category><category term="Data Science"></category><category term="Deployment/Continuous Integration and Delivery"></category></entry><entry><title>Building reproducible distributed applications at scale</title><link href="https://pyvideo.org/europython-2020/building-reproducible-distributed-applications-at-scale.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Fabian H√∂ring</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/building-reproducible-distributed-applications-at-scale.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Packaging in Python is hard. Packaging is particularly hard when code needs to run in a distributed computing environment where it is difficult to know what runs where and which parts of the code are available to run there.&lt;/p&gt;
&lt;p&gt;In this talk we will present different ways to ship ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Packaging in Python is hard. Packaging is particularly hard when code needs to run in a distributed computing environment where it is difficult to know what runs where and which parts of the code are available to run there.&lt;/p&gt;
&lt;p&gt;In this talk we will present different ways to ship Python code to a compute cluster, what Python's &amp;quot;pickling&amp;quot; feature has to do with this, what self contained executables are and the challenges we met when shipping Python code to a cluster with 1000s of nodes running 1000s of jobs like TensorFlow or Spark.&lt;/p&gt;
&lt;p&gt;As an example, we will show how one can run a PySpark job on top of S3 storage using PEX as a self contained executable artifact. Finally we will explain how those ideas generalize for different Jobs (like Tensorflow, Dask), different virtual environments (like Anaconda or vanilla Python virtual envs) and different distributed storage's (like S3 or HDFS).&lt;/p&gt;
&lt;p&gt;The auditor will get an overview of the challenges of Python packaging for distributed applications and see code samples that can be applied in his own project.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Big Data"></category><category term="Distributed Systems"></category><category term="Packaging"></category><category term="Virtual Env"></category></entry><entry><title>Everything You Know About MongoDB is Wrong!</title><link href="https://pyvideo.org/europython-2020/everything-you-know-about-mongodb-is-wrong.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Mark Smith</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/everything-you-know-about-mongodb-is-wrong.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;(Probably)&lt;/p&gt;
&lt;p&gt;MongoDB is webscale, right? It's a JSON database, it's eventually consistent, and you use map-reduce to query it. Oh, and it's insecure.&lt;/p&gt;
&lt;p&gt;Let me clear up some things: MongoDB is an ACID-compliant database with transactions, schemas &amp;amp; relationships. It includes a powerful aggregation query language; map-reduce has been deprecated ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;(Probably)&lt;/p&gt;
&lt;p&gt;MongoDB is webscale, right? It's a JSON database, it's eventually consistent, and you use map-reduce to query it. Oh, and it's insecure.&lt;/p&gt;
&lt;p&gt;Let me clear up some things: MongoDB is an ACID-compliant database with transactions, schemas &amp;amp; relationships. It includes a powerful aggregation query language; map-reduce has been deprecated for some time now. MongoDB doesn't speak or store JSON, and nowadays it comes with pretty good security defaults (we think).&lt;/p&gt;
&lt;p&gt;There are many myths around about MongoDB - what it is, how it works, and what it does wrong. Like any database product, you need to know its capabilities and how to get the best out of it. On top of this, the product has changed a lot over the years, but lots of information out there hasn't caught up.&lt;/p&gt;
&lt;p&gt;I'll cover 8 myths around MongoDB, explain how they're wrong, why the myth originated in the first place (some of them weren't originally myths).&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What exactly is MongoDB?&lt;/li&gt;
&lt;li&gt;What is the current release of MongoDB?&lt;/li&gt;
&lt;li&gt;MongoDB is not a JSON database.&lt;/li&gt;
&lt;li&gt;MongoDB has transactions.&lt;/li&gt;
&lt;li&gt;MongoDB allows relationships.&lt;/li&gt;
&lt;li&gt;You should only consider sharding if you must.&lt;/li&gt;
&lt;li&gt;MongoDB is secure.&lt;/li&gt;
&lt;li&gt;MongoDB stores your data reliably.&lt;/li&gt;
&lt;li&gt;MongoDB is a big product, with lots to learn.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Along the way, I'll explain some of MongoDB's best-kept secrets, and provide practical tips and tricks for using it. The audience will leave with a good idea of what MongoDB is, what it isn't, and how to best develop with it.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Big Data"></category><category term="Data"></category><category term="Databases"></category><category term="Development"></category><category term="MongoDB"></category></entry><entry><title>Making Pandas Fly</title><link href="https://pyvideo.org/europython-2020/making-pandas-fly.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Ian Ozsvald</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/making-pandas-fly.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Process bigger-than-RAM data using Pandas, Dask and Vaex&lt;/p&gt;
&lt;p&gt;Larger datasets can't fit into RAM - suddenly you can't use Pandas any more - but we need to analyse that data! First we'll review techniques to compress our data (maybe cutting our DataFrame RAM usage in half!) so we can process more ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Process bigger-than-RAM data using Pandas, Dask and Vaex&lt;/p&gt;
&lt;p&gt;Larger datasets can't fit into RAM - suddenly you can't use Pandas any more - but we need to analyse that data! First we'll review techniques to compress our data (maybe cutting our DataFrame RAM usage in half!) so we can process more rows using regular Pandas. Next we'll look at clever ways to make common operations run faster on DataFrames including dropping down to numpy, compiling with Numba and running multi-core. Finally for still-larger datasets we'll review Dask on Pandas and the new Vaex competitor solution. You'll leave with new techniques to make your DataFrames smaller and ideas for processing your data faster.
This talk is inspired by Ian's work updating his O'Reilly book High Performance Python to the 2nd edition for 2020. With over 10 years of evolution the Pandas DataFrame library has gained a huge amount of functionality and it is used by millions of Pythonistas - but the most obvious way to solve a task isn't always the fastest or most RAM efficient. This talk will help any Pandas user (beginner or beyond) process more data faster, making them more effective at their jobs.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Multi-Processing"></category><category term="Performance"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>Mastering a data pipeline with Python: 6 years of learned lessons from mistakes</title><link href="https://pyvideo.org/europython-2020/mastering-a-data-pipeline-with-python-6-years-of-learned-lessons-from-mistakes.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Robson Junior</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/mastering-a-data-pipeline-with-python-6-years-of-learned-lessons-from-mistakes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Building data pipelines are a consolidated task, there are a vast number of tools that automate and help developers to create data pipelines with few clicks on the cloud. It might solve non-complex or well-defined standard problems. This presentation is a demystification of years of experience and painful mistakes ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Building data pipelines are a consolidated task, there are a vast number of tools that automate and help developers to create data pipelines with few clicks on the cloud. It might solve non-complex or well-defined standard problems. This presentation is a demystification of years of experience and painful mistakes using Python as a core to create reliable data pipelines and manage insanely amount of valuable data. Let's cover how each piece fits into this puzzle: data acquisition, ingestion, transformation, storage, workflow management and serving. Also, we'll walk through best practices and possible issues. We'll cover PySpark vs Dask and Pandas, Airflow, and Apache Arrow as a new approach.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Beginners"></category><category term="Big Data"></category><category term="Case Study"></category><category term="Data Science"></category><category term="Open-Source"></category></entry><entry><title>Radio Astronomy with Python</title><link href="https://pyvideo.org/europython-2020/radio-astronomy-with-python.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Priscila Gutierres</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/radio-astronomy-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Gaussian Processes and Neural Networks applied to photometric redshift reconstruction&lt;/p&gt;
&lt;p&gt;Looking at higher redshifts is equivalent to looking back in time: they improve the studies of cosmology, expanding our knowledge of the universe. It allows us to study various physical phenomena like the power spectrum of galaxies which describes ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Gaussian Processes and Neural Networks applied to photometric redshift reconstruction&lt;/p&gt;
&lt;p&gt;Looking at higher redshifts is equivalent to looking back in time: they improve the studies of cosmology, expanding our knowledge of the universe. It allows us to study various physical phenomena like the power spectrum of galaxies which describes the distribution of galaxies on a range of scales, galaxy clustering, and large scales, the detection of the Baryon Acoustic Oscillation feature.
As a result,  a significant amount of work has been done to increase the efficiency and accuracy of the process via new algorithms and optimization of existing ones.
Astronomical datasets are undergoing a rapid growth in size and complexity as past, ongoing and future surveys produce massive multi-temporal and multi-wavelength datasets, with huge information to be extracted and analyzed.
The alternative to a full spectroscopic survey is to obtain multi-color images of the sky and perform photometric redshift estimates for the galaxies we have available.
When dealing with this problem, there are two main approaches: model-driven data analysis (template fitting methods) and data-driven analysis, which can use machine learning methods.
To solve this problem, we use data-driven analysis, more specifically GPz (which uses Gaussian processes)  and  ANNz2 (which mainly uses neural networks), both python software.&lt;/p&gt;
&lt;p&gt;Prerequisites: machine learning and basic math knowledge&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Big Data"></category><category term="Data"></category><category term="Machine-Learning"></category><category term="Physics"></category></entry><entry><title>Real Time Stream Processing for Machine Learning at Massive Scale</title><link href="https://pyvideo.org/europython-2020/real-time-stream-processing-for-machine-learning-at-massive-scale.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Alejandro Saucedo</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/real-time-stream-processing-for-machine-learning-at-massive-scale.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Processing Massively Parallel Stream of Data with Python (+ Kafka, SKlearn, SpaCy and Seldon)&lt;/p&gt;
&lt;p&gt;This talk will provide a practical insight on how to build scalable data streaming machine learning pipelines to process large datasets in real time using Python and popular frameworks such as Kafka, SpaCy and Seldon.&lt;/p&gt;
&lt;p&gt;We ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Processing Massively Parallel Stream of Data with Python (+ Kafka, SKlearn, SpaCy and Seldon)&lt;/p&gt;
&lt;p&gt;This talk will provide a practical insight on how to build scalable data streaming machine learning pipelines to process large datasets in real time using Python and popular frameworks such as Kafka, SpaCy and Seldon.&lt;/p&gt;
&lt;p&gt;We will be covering a case study performing automated content moderation on Reddit comments in real time. Our dataset will consist of 200k reddit comments from /r/science, 50,000 of which have been removed by moderators. We will be handling the stream data in a Kubernetes cluster, and the stream processing will be handled using the stream processing library Kafka. We will be running the end-to-end pipeline in Kubernetes with various components legeraging SKLearn, SpaCy and Seldon.&lt;/p&gt;
&lt;p&gt;We will then dive into fundamental concepts on stream processing such as windows, watermarking and checkponting, and we will show how to use each of these frameworks to build complex data streaming pipelines that can perform real time processing at scale by building, deploying and monitoring a machine learning model which will process production incoming data..&lt;/p&gt;
&lt;p&gt;Finally we will show best practices when using these frameworks, as well as a high level overview of tools that can be used for monitoring in-depth.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="ASYNC / Concurreny"></category><category term="Best Practice"></category><category term="Big Data"></category><category term="Distributed Systems"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>The Painless Route in Python to Fast and Scalable Machine Learning</title><link href="https://pyvideo.org/europython-2020/the-painless-route-in-python-to-fast-and-scalable-machine-learning.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>David Liu</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/the-painless-route-in-python-to-fast-and-scalable-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python is the lingua franca for data analytics and machine learning. Its superior productivity makes it the preferred tool for prototyping. However, traditional Python packages are not necessarily designed to provide high performance and scalability for large datasets.
We start our tutorial with a short introduction on how to ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python is the lingua franca for data analytics and machine learning. Its superior productivity makes it the preferred tool for prototyping. However, traditional Python packages are not necessarily designed to provide high performance and scalability for large datasets.
We start our tutorial with a short introduction on how to get close-to-native performance with Intel-optimized packages, such as numpy, scipy, and scikit-learn. The next part of the tutorial is focused on getting high performance and scalability from multi-cores on a single machine to large clusters of workstations. We will demonstrate that with Python it is possible to achieve the same performance and scalability as with hand-tuned C++/MPI code:
-       Scalable Dataframe Compiler (SDC) is used to compile analytics code using pandas/Python and scale it to bare-metal cluster performance. It compiles a subset of Python code into efficient parallel binaries that use message passing to perform collective communications.
-       A convenient Python API to data analytics and machine learning primitives (daal4py). While its interface is scikit-learn-like, its MPI-based engine allows to scale machine learning algorithms to bare-metal cluster performance.
-       In the tutorial, we will use SDC and daal4py together to build an end-to-end analytics pipeline that scales to clusters, requiring only minimal code changes.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Analytics"></category><category term="Big Data"></category><category term="Distributed Systems"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>MapReduce mit Disco</title><link href="https://pyvideo.org/pycon-de-2013/mapreduce-mit-disco.html" rel="alternate"></link><published>2013-10-17T00:00:00+00:00</published><updated>2013-10-17T00:00:00+00:00</updated><author><name>Dr. Jan Morlock</name></author><id>tag:pyvideo.org,2013-10-17:/pycon-de-2013/mapreduce-mit-disco.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Mit dem MapReduce-Verfahren k√∂nnen massive Datenmengen auf einem
Rechencluster verarbeitet werden. Namensgeber und wichtige Bestandteile
sind eine Map- und eine Reduce-Phase. Diese werden jeweils
parallelisiert ausgef√ºhrt und erm√∂glichen somit eine optimale Auslastung
der vorhandenen Ressourcen. Im Vergleich zu einer entsprechenden
sequentiellen Implementierung k√∂nnen dadurch gro√üe Zeiteinsparungen
erreicht werden.&lt;/p&gt;
&lt;p&gt;Mit ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Mit dem MapReduce-Verfahren k√∂nnen massive Datenmengen auf einem
Rechencluster verarbeitet werden. Namensgeber und wichtige Bestandteile
sind eine Map- und eine Reduce-Phase. Diese werden jeweils
parallelisiert ausgef√ºhrt und erm√∂glichen somit eine optimale Auslastung
der vorhandenen Ressourcen. Im Vergleich zu einer entsprechenden
sequentiellen Implementierung k√∂nnen dadurch gro√üe Zeiteinsparungen
erreicht werden.&lt;/p&gt;
&lt;p&gt;Mit dem freien Disco-Framework k√∂nnen MapReduce-Aufgaben leicht in
Python erstellt werden. Beim Zugriff auf die Eingabedaten werden
verschiedene Protokolle unterst√ºtzt. W√§hrend der Ausf√ºhrung kann der
Zustand des Rechenclusters sowie der Fortschritt der einzelnen Aufgaben
mit Hilfe einer Weboberfl√§che √ºberwacht werden. Ein verteiltes
Dateisystem, das Disco Distributed Filesystem (DDFS), wird zur
Speicherung der Zwischen- und Endergebnisse verwendet.&lt;/p&gt;
</content><category term="PyCon DE 2013"></category><category term="big data"></category><category term="disco"></category><category term="mapreduce"></category><category term="parallelisierung"></category></entry><entry><title>Strongly typed datasets in a weakly typed world</title><link href="https://pyvideo.org/pycon-de-2018/strongly-typed-datasets-in-a-weakly-typed-world.html" rel="alternate"></link><published>2018-10-26T00:00:00+00:00</published><updated>2018-10-26T00:00:00+00:00</updated><author><name>Marco Neumann</name></author><id>tag:pyvideo.org,2018-10-26:/pycon-de-2018/strongly-typed-datasets-in-a-weakly-typed-world.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We at Blue Yonder use Pandas quite a lot during our daily data science
and engineering work. This choice, together with Python as an underlying
programming language gives us flexibility, a feature-rich interface, and
access to a large community and ecosystem. When it comes to preserving
the data and ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We at Blue Yonder use Pandas quite a lot during our daily data science
and engineering work. This choice, together with Python as an underlying
programming language gives us flexibility, a feature-rich interface, and
access to a large community and ecosystem. When it comes to preserving
the data and exchanging it with different software stacks, we rely on
Parquet Datasets / Hive Tables. During the write process, there is a
shift from a rather weakly typed world to a strongly typed one. For
example, Pandas may convert integers to floats for many operations
without asking, but parquet files and the schema information stored
alongside them dictate very precise types. The type situation may get
even more &amp;quot;colorful&amp;quot;, when datasets are written by multiple code
versions or different software solutions over time. This then results in
important questions regarding type compatibility.&lt;/p&gt;
&lt;p&gt;This talk will first represent an overview on types at different layers
(like NumPy, Pandas, Arrow and Parquet) and the transition between this
layers. The second part of the talk will present examples of type
compatibility we have seen and why+how we think they should be handled.
At the end there will be a Q+A, which can be seen as the start of a
potentially longer RFC process to align different software stacks (like
Hive and Dask) to handle types in a similar way.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category></entry><entry><title>Big Data Systems Performance: The Little Shop of Horrors</title><link href="https://pyvideo.org/pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Jens Dittrich</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The confusion around terms such as like NoSQL, Big Data, Data Science,
Spark, SQL, and Data Lakes often creates more fog than clarity. However,
clarity about the underlying technologies is crucial to designing the
best technical solution in any field relying on huge amounts of data
including data science ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The confusion around terms such as like NoSQL, Big Data, Data Science,
Spark, SQL, and Data Lakes often creates more fog than clarity. However,
clarity about the underlying technologies is crucial to designing the
best technical solution in any field relying on huge amounts of data
including data science, machine learning, but also more traditional
analytical systems such as data integration, data warehousing,
reporting, and OLAP.&lt;/p&gt;
&lt;p&gt;In my presentation, I will show that often at least three dimensions are
cluttered and confused in discussions when it comes to data management:
First, buzzwords (labels &amp;amp; terms like &amp;quot;big data&amp;quot;, &amp;quot;AI&amp;quot;, &amp;quot;data lake&amp;quot;);
second, data design patterns (principles &amp;amp; best practices like:
selection push-down, materialization, indexing); and Third, software
platforms (concrete implementations &amp;amp; frameworks like: Python, DBMS,
Spark, and NoSQL-systems).&lt;/p&gt;
&lt;p&gt;Only by keeping these three dimensions apart, it is possible to create
technically-sound architectures in the field of big data analytics.&lt;/p&gt;
&lt;p&gt;I will show concrete examples, which through a simple redesign and wise
choice of the right tools and technologies, run thereby up to 1000 times
faster. This in turn triggers tremendous savings in terms of development
time, hardware costs, and maintenance effort.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Infrastructure"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>Data science complexity and solutions in real industrial projects</title><link href="https://pyvideo.org/pycon-de-2018/data-science-complexity-and-solutions-in-real-industrial-projects.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Artur Miller</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/data-science-complexity-and-solutions-in-real-industrial-projects.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As data scientists we usually like to apply fancy machine learning
models to well-groomed datasets. Everyone working on industrial problems
will eventually learn, that this does not reflect reality. The amount of
time spent on modeling is small compared to data gathering, -warehousing
and -cleaning. Even after training and ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As data scientists we usually like to apply fancy machine learning
models to well-groomed datasets. Everyone working on industrial problems
will eventually learn, that this does not reflect reality. The amount of
time spent on modeling is small compared to data gathering, -warehousing
and -cleaning. Even after training and deployment of the model, the work
is not done. Continuous monitoring of the performance and input data is
still necessary.&lt;/p&gt;
&lt;p&gt;In this talk I discuss how important data handling is for successful
data science projects. Each milestone, from finding the business case to
continuously monitoring the performance of the solution, is addressed.
This is exemplary shown on a project, with the goal of improving a
productive system.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Infrastructure"></category><category term="Machine Learning"></category></entry><entry><title>Data Science meets Data Protection: Keeping your data secure while learning from it.</title><link href="https://pyvideo.org/pycon-de-2018/data-science-meets-data-protection-keeping-your-data-secure-while-learning-from-it.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Andreas Dewes</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/data-science-meets-data-protection-keeping-your-data-secure-while-learning-from-it.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will discuss anonymization and pseudonymization techniques that you
can apply to your data to keep it secure and comply with the law(s)
while still being able to gain useful insights from it.&lt;/p&gt;
&lt;p&gt;We will show concrete Python implementations of various techniques and
use example data sets to ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will discuss anonymization and pseudonymization techniques that you
can apply to your data to keep it secure and comply with the law(s)
while still being able to gain useful insights from it.&lt;/p&gt;
&lt;p&gt;We will show concrete Python implementations of various techniques and
use example data sets to show how applying pseudonymization and
anonymization will affect our ability to do machine learning / data
science.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Artificial Intelligence"></category><category term="Business &amp; Start-Ups"></category><category term="Big Data"></category><category term="Data Science"></category></entry><entry><title>Fulfilling Apache Arrow's Promises: Pandas on JVM memory without a copy</title><link href="https://pyvideo.org/pycon-de-2018/fulfilling-apache-arrows-promises-pandas-on-jvm-memory-without-a-copy.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/fulfilling-apache-arrows-promises-pandas-on-jvm-memory-without-a-copy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Arrow established a standard for columnar in-memory analytics to
redefine the performance and interoperability of most Big Data
technologies in early 2016. Since then implementations in Java, C++,
Python, Glib, Ruby, Go, JavaScript and Rust have been added. Although
Apache Arrow (&lt;tt class="docutils literal"&gt;pyarrow&lt;/tt&gt;) is already known to many Python ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Arrow established a standard for columnar in-memory analytics to
redefine the performance and interoperability of most Big Data
technologies in early 2016. Since then implementations in Java, C++,
Python, Glib, Ruby, Go, JavaScript and Rust have been added. Although
Apache Arrow (&lt;tt class="docutils literal"&gt;pyarrow&lt;/tt&gt;) is already known to many Python/Pandas users
for reading Apache Parquet files, its main benefit is the cross-language
interoperability. With feather and PySpark, you can already benefit from
this in Python and R/Java via the filesystem or network. While they
improve data sharing and remove serialization overhead, data still needs
to be copied as it is passed between processes.&lt;/p&gt;
&lt;p&gt;In the 0.23 release of Pandas, the concept of ExtensionArrays was
introduced. They allow the extension of Pandas DataFrames and Series
with custom, user- defined typed. The most prominent example is
&lt;tt class="docutils literal"&gt;cyberpandas&lt;/tt&gt; which adds an IP dtype that is backed by the appropriate
representation using NumPy arrays. These ExtensionArrays are not limited
to arrays backed by NumPy but can take an arbitrary storage as long as
they fulfill a certain interfaces. Using Apache Arrow we can implement
ExtensionArrays that are of the same dtype as the built-in types of
Pandas but memory management is not tied to Pandas' internal
BlockManager. On the other hand Apache Arrow has a much more wider set
of efficient types that we can also expose as an ExtensionArray. These
types include a native string type as well as a arbitrarily nested types
such as &lt;tt class="docutils literal"&gt;list of ‚Ä¶&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;struct of (‚Ä¶, ‚Ä¶, ‚Ä¶)&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;To show the real-world benefits of this, we take the example of a data
pipeline that pulls data from a relational store, transforms it and then
passes it into a machine learning model. A typical setup nowadays most
likely involves a data lake that is queried with a JVM based query
engine. The machine learning model is then normally implemented in
Python using popular frameworks like CatBoost or Tensorflow.&lt;/p&gt;
&lt;p&gt;While sometimes these query engines provide Python clients, their
performance is normally not optimized for large results sets. In the
case of a machine learning model, we will do some feature
transformations and possibly aggregations with the query engine but feed
as many rows as possible into the model. This will lead then to result
sets that have above a million rows. In contrast to the Python clients,
these engines often come with efficient JDBC drivers that can cope with
result sets of this size but then the conversion from Java objects to
Python objects in the JVM bridge will slow things down again. In our
example, we will show how to use Arrow to retrieve a large result in the
JVM and then pass it on to Python without running into these
bottlenecks.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category></entry><entry><title>Introduction to Docker for Pythonistas</title><link href="https://pyvideo.org/pycon-de-2018/introduction-to-docker-for-pythonistas.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Jan Wagner</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/introduction-to-docker-for-pythonistas.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;My Talk aims to introduce you to Docker and how it works, how you can
use prebuild Images from the Docker-Hub and how you can make your own
Images.&lt;/div&gt;
&lt;div class="line"&gt;In more Detail, the following Points will be covered:&lt;/div&gt;
&lt;/div&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Jupyter"></category><category term="Machine Learning"></category></entry><entry><title>Processing Geodata using Python</title><link href="https://pyvideo.org/pycon-de-2018/processing-geodata-using-python.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Martin Christen</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/processing-geodata-using-python.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a large amount of Python modules available suitable for spatial
data processing. In this talk, it is shown how to analyze, manipulate
and visualize geospatial data by using open source modules. The
following modules will be introduced:&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Jupyter"></category><category term="Python"></category><category term="Visualisation"></category></entry><entry><title>Solving Data Science Problems using a Jupyter Notebook and SAP HANA's in-database Machine Learning Libraries</title><link href="https://pyvideo.org/pycon-de-2018/solving-data-science-problems-using-a-jupyter-notebook-and-sap-hanas-in-database-machine-learning-libraries.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Dr Frank Gottfried</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/solving-data-science-problems-using-a-jupyter-notebook-and-sap-hanas-in-database-machine-learning-libraries.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Companies store their data in databases with highly restricted access
regulations. The latest regulatorily changes enforces the need to work
on the datasets in this controlled environment without created
additional external copies. However Data Scientists prefer to work with
tools they are most familiar like Python, R and Jupyter ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Companies store their data in databases with highly restricted access
regulations. The latest regulatorily changes enforces the need to work
on the datasets in this controlled environment without created
additional external copies. However Data Scientists prefer to work with
tools they are most familiar like Python, R and Jupyter Notebooks using
to a large amount of open- source packages (numpy, matplotlib, pandas,
..). SAP HANA provides highly optimized in-database machine learning
libraries. In this talk we will present how a Data Scientist can work in
an environment he/she is most familiar with and access the data stored
in SAP HANA using SAP HANA machine learning libraries with a
scikit-learn type interface. Data will remain in the database and will
be exposed as dataframes (similar to Pandas dataframes). We will explain
the software architecture and present a complete end-to-end use case by
using a Jupyter Notebook.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Jupyter"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>Stretchy - NoSQL Database behind REST API</title><link href="https://pyvideo.org/pycon-de-2018/stretchy-nosql-database-behind-rest-api.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Artur Scholz</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/stretchy-nosql-database-behind-rest-api.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Stretchy is built as a microservice that provides a simple and intuitive
REST API with a NoSQL database as backend. No need for database
migrations or upfront schema design. The basic CRUD (create, read,
update, delete) operations are available for getting data in and out
from the database.&lt;/p&gt;
&lt;p&gt;Stretchy ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Stretchy is built as a microservice that provides a simple and intuitive
REST API with a NoSQL database as backend. No need for database
migrations or upfront schema design. The basic CRUD (create, read,
update, delete) operations are available for getting data in and out
from the database.&lt;/p&gt;
&lt;p&gt;Stretchy is free and open source software built with Python 3, using
Flask web framework. It currently uses MongoDB as its backend database.
Since it is interfaced through the REST API however, Stretchy is
technology agnostic and developers can create bindings to other
databases, including SQL databases.&lt;/p&gt;
&lt;p&gt;This presentation reviews the reasons for creating Stretchy, its current
applications, an short tutorial on how to use it, and tips on how to
deploy it.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Web"></category></entry><entry><title>Where the heck is my memory?</title><link href="https://pyvideo.org/pycon-de-2018/where-the-heck-is-my-memory.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Florian Jetter</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/where-the-heck-is-my-memory.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Memory management is something the common Python user doesn‚Äôt need to
bother with because the gory details of it are hidden deep within the
interpreter itself. The garbage collector takes out the trash and we can
spend our precious time bothering with more important things on our
minds ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Memory management is something the common Python user doesn‚Äôt need to
bother with because the gory details of it are hidden deep within the
interpreter itself. The garbage collector takes out the trash and we can
spend our precious time bothering with more important things on our
minds. Living in this encapsulated utopia is nice but sometimes it is
worth it to peak behind the curtains to unleash the full power of your
application. In this talk I want to show you when it is necessary to
face this harsh world and convince you that it is in fact not as scary
as it may seem. Using real life examples, I‚Äôm going to show you how to
use the garbage collector and open source tooling to get control over
the memory you might not even know you had at your disposal.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="DevOps"></category><category term="Infrastructure"></category></entry><entry><title>Cython to speed up your Python code</title><link href="https://pyvideo.org/pycon-de-2018/cython-to-speed-up-your-python-code.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Stefan Behnel</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/cython-to-speed-up-your-python-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="http://cython.org"&gt;Cython&lt;/a&gt; is not only a very fast and comfortable
way to talk to native code and libraries, it is also a widely used tool
for speeding up Python code. The Cython compiler translates Python code
to C or C++ code, and applies many static optimisations that make Python
code ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="http://cython.org"&gt;Cython&lt;/a&gt; is not only a very fast and comfortable
way to talk to native code and libraries, it is also a widely used tool
for speeding up Python code. The Cython compiler translates Python code
to C or C++ code, and applies many static optimisations that make Python
code run visibly faster than in the interpreter. But even better, it
supports static type annotations that allow direct use of C/C++ data
types and functions, which the compiler uses to convert and optimise the
code into fast, native C. The tight integration of all three languages,
Python, C and C++, makes it possible to freely mix Python features like
generators and comprehensions with C/C++ features like native data
types, pointer arithmetic or manually tuned memory management in the
same code.&lt;/p&gt;
&lt;p&gt;This talk by a core developer introduces the Cython compiler by
interactive code examples, and shows how you can use it to speed up your
real-world Python code. You will learn how you can profile a Python
module and use Cython to compile and optimise it into a fast binary
extension module. All of that, without losing the ability to run it
through common development tools like code checkers or coverage test
tools.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="Infrastructure"></category><category term="Jupyter"></category><category term="Parallel Programming"></category></entry><entry><title>Distributed Hyperparameter search with sklearn and kubernetes</title><link href="https://pyvideo.org/pycon-de-2018/distributed-hyperparameter-search-with-sklearn-and-kubernetes.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Jakob Karalus</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/distributed-hyperparameter-search-with-sklearn-and-kubernetes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While sklearn provides a good interface to do hyperparameter search on
large &amp;amp; complex model (pipelines), doing these can take up a lot of
time. The traditional way usually includes one beefy machine and a lot
of waiting. In other cases, people tend to ‚Äúmanually‚Äù schedule parameter
ranges between nodes ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While sklearn provides a good interface to do hyperparameter search on
large &amp;amp; complex model (pipelines), doing these can take up a lot of
time. The traditional way usually includes one beefy machine and a lot
of waiting. In other cases, people tend to ‚Äúmanually‚Äù schedule parameter
ranges between nodes, but that can also be problematic since these won't
talk to each other. Kubernetes itself is currently the most prominent
scheduler and shines at distributing task, but is a pretty complex
system in itself.&lt;/p&gt;
&lt;p&gt;In this talk, I will show how you can harness the scheduling of
kubernetes for distributing hyperparameter search with sklearn onto a
cluster of nodes. This can be achieved quite easily and with just a few
changes to the original code, so the Data Scientist won't be bothered by
complex kubernetes internals.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Infrastructure"></category><category term="Machine Learning"></category></entry><entry><title>reticulate: R interface to Python</title><link href="https://pyvideo.org/pycon-de-2018/reticulate-r-interface-to-python.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Jens Bruno Wittek</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/reticulate-r-interface-to-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python and R are the preferred languages for data science. In 2018,
RStudio introduced its package reticulate and clearly demonstrates that
it favours to join forces. Both languages have strengths and weaknesses.
Tools to combine the strengths will enable easier collaboration in
projects and more possibilities to succeed. Using ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python and R are the preferred languages for data science. In 2018,
RStudio introduced its package reticulate and clearly demonstrates that
it favours to join forces. Both languages have strengths and weaknesses.
Tools to combine the strengths will enable easier collaboration in
projects and more possibilities to succeed. Using Python from R gives R
users wider access to functions and makes it easier for Python beginners
to just run scripts and being able to collaborate in Python projects.
The talk will show the possibilities of reticulate: The main part starts
with demonstrating the Python interpreter within R. It will show how to
source Python scripts as well as install and import modules. Then it
will deal with the most important types of Python objects, how they are
represented in R and how to further manipulate them. Thereby, a special
focus is on using Python for data science. In addition, it will be
presented how Conda environments can be created and used from R. A
further application will be the creation of reports with Markdown and
LaTeX where R and Python can be used within one document and share
objects. A last topic is about showing the possibilities for easier
development in RStudio (help regarding Python functions, auto
completion).&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Artificial Intelligence"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Data Science"></category><category term="NLP"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>Scalable Scientific Computing using Dask</title><link href="https://pyvideo.org/pycon-de-2018/scalable-scientific-computing-using-dask.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/scalable-scientific-computing-using-dask.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;</content><category term="PyCon DE 2018"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Parallel Programming"></category><category term="Python"></category></entry><entry><title>Selinon - dynamic distributed task flows</title><link href="https://pyvideo.org/pycon-de-2018/selinon-dynamic-distributed-task-flows.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Fridol√≠n Pokorn√Ω</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/selinon-dynamic-distributed-task-flows.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough flexibility and dynamic features needed badly in complex
flows. As we discovered all the limitations, we decided to implement
Selinon.&lt;/p&gt;
&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough flexibility and dynamic features needed badly in complex
flows. As we discovered all the limitations, we decided to implement
Selinon.&lt;/p&gt;
&lt;p&gt;Selinon enhances Celery task flow management and allows you to create
and model task flows in your distributed environment that can
dynamically change behavior based on computed results in your cluster,
automatically resolve tasks that need to be executed in case of
selective task runs, automatic tracing mechanism and many others.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="Infrastructure"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category></entry><entry><title>Tratando datos m√°s all√° de los l√≠mites de la memoria</title><link href="https://pyvideo.org/pycon-es-2015/tratando-datos-mas-alla-de-los-limites-de-la-memoria.html" rel="alternate"></link><published>2016-02-02T00:00:00+00:00</published><updated>2016-02-02T00:00:00+00:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2016-02-02:/pycon-es-2015/tratando-datos-mas-alla-de-los-limites-de-la-memoria.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la era del 'Big Data' se necesitan cantidades cada vez m√°s grandes de memoria (RAM) para tratar y analizar estos datos. Pero tarde o temprano se llega a unos l√≠mites por encima de los cuales no se puede (o es muy caro) pasar.&lt;/p&gt;
&lt;p&gt;El compresor Blosc (blosc.org ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la era del 'Big Data' se necesitan cantidades cada vez m√°s grandes de memoria (RAM) para tratar y analizar estos datos. Pero tarde o temprano se llega a unos l√≠mites por encima de los cuales no se puede (o es muy caro) pasar.&lt;/p&gt;
&lt;p&gt;El compresor Blosc (blosc.org) y el contenedor de datos bcolz (bcolz.blosc.org), usan las capacidades de los ordenadores modernos (caches, procesadores multihilo y SSDs) para permitir tratar datos m√°s all√° los l√≠mites de la memoria.&lt;/p&gt;
</content><category term="PyCon ES 2015"></category><category term="workshop"></category><category term="big data"></category><category term="blosc"></category><category term="bcolz"></category></entry><entry><title>Usando contenedores para Big Data</title><link href="https://pyvideo.org/pycon-es-2015/usando-contenedores-para-big-data.html" rel="alternate"></link><published>2016-02-02T00:00:00+00:00</published><updated>2016-02-02T00:00:00+00:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2016-02-02:/pycon-es-2015/usando-contenedores-para-big-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la actualidad existe una variedad bastante grande de contenedores de datos para almacenar grandes cantidades de datos en Python, tanto en memoria como en disco. En mi taller pasaremos revista a unos cuantos de los m√°s √∫tiles, empezando por los m√°s b√°sicos y generales (listas, diccionarios, NumPy/ndarray ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la actualidad existe una variedad bastante grande de contenedores de datos para almacenar grandes cantidades de datos en Python, tanto en memoria como en disco. En mi taller pasaremos revista a unos cuantos de los m√°s √∫tiles, empezando por los m√°s b√°sicos y generales (listas, diccionarios, NumPy/ndarray, pandas/DataFrames) a los m√°s especializados (RDBMS, PyTables/Table/HDF5, bcolz/carray/ctable). Durante el camino se dar√°n pistas de cuando usar unos u otros dependiendo del caso de uso.&lt;/p&gt;
</content><category term="PyCon ES 2015"></category><category term="workshop"></category><category term="big data"></category><category term="numpy"></category><category term="pandas"></category><category term="pytables"></category><category term="bcolz"></category></entry><entry><title>Scaling your Data infrastructure</title><link href="https://pyvideo.org/pycon-italia-2018/scaling-your-data-infrastructure.html" rel="alternate"></link><published>2018-04-20T00:00:00+00:00</published><updated>2018-04-20T00:00:00+00:00</updated><author><name>Christian Barra</name></author><id>tag:pyvideo.org,2018-04-20:/pycon-italia-2018/scaling-your-data-infrastructure.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;This talk aims to answer a few questions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What do you do when you need to move your model from your laptop to
production?&lt;/li&gt;
&lt;li&gt;Is &lt;tt class="docutils literal"&gt;big data == I need to use JVM&lt;/tt&gt; the right assumption?&lt;/li&gt;
&lt;li&gt;How can I put my jupyter notebook in production?&lt;/li&gt;
&lt;li&gt;How do you apply the ‚Ä¶&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;This talk aims to answer a few questions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What do you do when you need to move your model from your laptop to
production?&lt;/li&gt;
&lt;li&gt;Is &lt;tt class="docutils literal"&gt;big data == I need to use JVM&lt;/tt&gt; the right assumption?&lt;/li&gt;
&lt;li&gt;How can I put my jupyter notebook in production?&lt;/li&gt;
&lt;li&gt;How do you apply the best software engineering practices (testing and
ci for example) inside your data science process?&lt;/li&gt;
&lt;li&gt;How do you ‚Äúdecouple‚Äù your data scientists, developers and devops
teams?&lt;/li&gt;
&lt;li&gt;How do you guarantee the reproducibility of your models?&lt;/li&gt;
&lt;li&gt;How do you scale your training process when does not fit in memory
anymore?&lt;/li&gt;
&lt;li&gt;How do you serve your models and provide an easy rollback system?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Agenda:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The Data Science workflow&lt;/li&gt;
&lt;li&gt;Scaling is not just a matter of the size of your Data&lt;/li&gt;
&lt;li&gt;Scaling when the size of your Data matters&lt;/li&gt;
&lt;li&gt;DDS, Dockerized Data Science&lt;/li&gt;
&lt;li&gt;Cassiny&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I‚Äôll share my experience highlighting some of the challenges I faced and
the solutions I came up to answer these questions.&lt;/p&gt;
&lt;p&gt;During this presentation I will mention libraries like jupyter, atom,
scikit- learn, dask, ray, parquet, arrow and many others.&lt;/p&gt;
&lt;p&gt;The principles and best practices I will share are something that you
can apply, more or less easily, if you are running or in the process to
run a production system based on the Python stack.&lt;/p&gt;
&lt;p&gt;This talk will focus on (my) best practices to run the Python Data stack
together and I will also talk about Cassiny, an open source project I
started, that aims to simplify your life if you want to use a completely
Python based solution in your data science workflow.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 20 April&lt;/strong&gt; at 11:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon Italia 2018"></category><category term="Jupyter"></category><category term="CloudComputing"></category><category term="pydata"></category><category term="#lessonslearned"></category><category term="Big-Data"></category><category term="S3"></category><category term="Data-Scientist"></category><category term="#amicodialessia"></category><category term="java"></category><category term="docker"></category><category term="cloud"></category></entry><entry><title>Machine Learning for Inventory Management</title><link href="https://pyvideo.org/pycon-italia-2019/machine-learning-for-inventory-management.html" rel="alternate"></link><published>2019-05-04T00:00:00+00:00</published><updated>2019-05-04T00:00:00+00:00</updated><author><name>Laura De Stefanis</name></author><id>tag:pyvideo.org,2019-05-04:/pycon-italia-2019/machine-learning-for-inventory-management.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Material forecast is the process of deciding which items to stock in the
inventory, how much, and when. Aim of the forecast is to increase parts
availability with the less possible impact on inventory, having enough
stock in the warehouse to ensure the business keeps moving but not
enough ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Material forecast is the process of deciding which items to stock in the
inventory, how much, and when. Aim of the forecast is to increase parts
availability with the less possible impact on inventory, having enough
stock in the warehouse to ensure the business keeps moving but not
enough stock to drain its limited cash reserves. This decision process
is being profoundly revised in its foundational concepts, thanks to new
classification methodologies enabled by Machine Learning. We integrated
domain knowledge and ML to create a new classification and level setting
process, leveraging on 6 years of data and new statistical indicators
for demand patter. These new features are used to run the machine
learning algorithm that classify Make To Stock / Make To Order items in
a single flow approach. The validation phase is reduced at each
iteration as ML model can be re-trained to incorporate past validations,
increasing efficiency and performances. Level setting problem is
addressed benchmarking ML methods (Reinforcement Learning), Montecarlo
simulations and traditional statistical methodologies. Regarding RL and
Montecarlo we established punishments for letting an particular
inventory item run out of stock and we also punish the model for stock
too higher value for too long. For rewards, we primarily focus on
ordering items within a safe window before the demand. First application
of this new methodology brings a 20% reduction of inventory, without
impact on sales, and a workload reduction of about 70%.&lt;/p&gt;
&lt;p&gt;Feedback form: &lt;a class="reference external" href="https://python.it/feedback-1554"&gt;https://python.it/feedback-1554&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Saturday 4 May&lt;/strong&gt; at 12:15 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon Italia 2019"></category><category term="logistics"></category><category term="clustering"></category><category term="statistics"></category><category term="Python"></category><category term="Big-Data"></category><category term="gestionali"></category><category term="django"></category><category term="supplychain"></category><category term="sql"></category><category term="Artificial Intelligence"></category></entry><entry><title>Meet dask and distributed: the unsung heroes of Python scientific data ecosystem.</title><link href="https://pyvideo.org/pycon-italia-2019/meet-dask-and-distributed-the-unsung-heroes-of-python-scientific-data-ecosystem.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Alessandro Amici</name></author><id>tag:pyvideo.org,2019-05-03:/pycon-italia-2019/meet-dask-and-distributed-the-unsung-heroes-of-python-scientific-data-ecosystem.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Thanks to its world-class data tools and libraries, like Numpy, Pandas,
Jupyter, Matplotlib and xarray, Python is becoming the language of
choice in many scientific communities from Physics to Climate Science,
from Earth Observation to Economy.&lt;/p&gt;
&lt;p&gt;A turn-key but less-know component of the scientific ecosystem is the
dask library ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Thanks to its world-class data tools and libraries, like Numpy, Pandas,
Jupyter, Matplotlib and xarray, Python is becoming the language of
choice in many scientific communities from Physics to Climate Science,
from Earth Observation to Economy.&lt;/p&gt;
&lt;p&gt;A turn-key but less-know component of the scientific ecosystem is the
dask library that enable seamless parallel, distributed and GPU
computing in most cases without code changes.&lt;/p&gt;
&lt;p&gt;We will use climate science as an typical example of a discipline where
simple tasks become easily big data problems and where mastering xarray,
dask and dask.distributed is the key to turn them back into simple
tasks, possibly on a large cluster of VMs (that you can easily provision
from your preferred cloud provider).&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://gitpitch.com/alexamici/talks/master?p=PyConX-2019"&gt;https://gitpitch.com/alexamici/talks/master?p=PyConX-2019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1704"&gt;https://python.it/feedback-1704&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 17:15 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon Italia 2019"></category><category term="Jupyter"></category><category term="dask.distributed"></category><category term="Big-Data"></category><category term="xarray"></category><category term="dask"></category><category term="climate-change"></category><category term="earth-obeservation"></category><category term="pandas"></category></entry><entry><title>Building data pipelines with Apache Airflow</title><link href="https://pyvideo.org/pycon-italia-2022/building-data-pipelines-with-apache-airflow.html" rel="alternate"></link><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><author><name>Ricardo Sueiras</name></author><id>tag:pyvideo.org,2022-06-03:/pycon-italia-2022/building-data-pipelines-with-apache-airflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Building data pipelines with Apache Airflow - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Apache Airflow is a popular orchestration tool that uses Python to
author workflows, but what is it, what are the architectural components,
why do I need this, and how do you get started? If you care about
reliably getting your ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Building data pipelines with Apache Airflow - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Apache Airflow is a popular orchestration tool that uses Python to
author workflows, but what is it, what are the architectural components,
why do I need this, and how do you get started? If you care about
reliably getting your data from source to your big data platforms, this
is the talk for you. Moving, refining and enriching data in order to get
actionable insights is hard. Apache Airflow, an open source project that
makes it easy for data engineers to create repeatable and reliable
workflows in Python. This session will be a hands on getting started
looking at creating your first workflows and understanding how you can
use the features of Apache Airflow to orchestrate your own data
pipelines.&lt;/p&gt;
&lt;p&gt;Slides:&lt;/p&gt;
&lt;p&gt;Speaker: Ricardo Sueiras&lt;/p&gt;
</content><category term="PyCon Italia 2022"></category><category term="aws"></category><category term="big data"></category><category term="open source"></category></entry><entry><title>Data Engineering and Python</title><link href="https://pyvideo.org/pycon-italia-2022/data-engineering-and-python.html" rel="alternate"></link><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><author><name>Prakhar Srivastava</name></author><id>tag:pyvideo.org,2022-06-03:/pycon-italia-2022/data-engineering-and-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data Engineering and Python - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Data Engineering is the backbone of all analytics that happens in any
organization. This marks data engineering as the central role in any
data-driven organization. This talk aims to introduce the fundamentals
of data engineering with python apps driving the core concepts ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data Engineering and Python - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Data Engineering is the backbone of all analytics that happens in any
organization. This marks data engineering as the central role in any
data-driven organization. This talk aims to introduce the fundamentals
of data engineering with python apps driving the core concepts. The aim
of this talk is to introduce the audience to the world of big data
analytics with pythonic tools and libraries at its core. One can expect
the talk to cover the basics of Extract, Transform and Load(ETL)
pipelines using python scripts and then Airflow with PySpark. These ETL
pipelines are core to any data infrastructure. There will be a discourse
on how we can use cloud providers and design an entire system that is
responsible for analytics and ML in an organization. Finally a short
outro into how one can start their journey to become a data engineer.&lt;/p&gt;
&lt;p&gt;Speaker: Prakhar Srivastava&lt;/p&gt;
</content><category term="PyCon Italia 2022"></category><category term="architecture"></category><category term="big data"></category><category term="databases"></category><category term="distributed systems"></category></entry><entry><title>Event Driven Applications with Flask and Kafka</title><link href="https://pyvideo.org/pycon-italia-2022/event-driven-applications-with-flask-and-kafka.html" rel="alternate"></link><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><author><name>Francesco Tisiot</name></author><id>tag:pyvideo.org,2022-06-03:/pycon-italia-2022/event-driven-applications-with-flask-and-kafka.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Event Driven Applications with Flask and Kafka - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Let‚Äôs build an event driven application together! Using Flask as
framework we‚Äôll add the powerful Apache Kafka, a streaming platform, as
backend‚Ä¶ all live on stage! Join to understand how to integrate your
application with Apache Kafka ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Event Driven Applications with Flask and Kafka - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Let‚Äôs build an event driven application together! Using Flask as
framework we‚Äôll add the powerful Apache Kafka, a streaming platform, as
backend‚Ä¶ all live on stage! Join to understand how to integrate your
application with Apache Kafka! Solid event driven applications are based
on a widely adopted coding framework and a resilient data streaming
technology. While Flask is a common choice for the framework, a wise
selection of the backend data systems can bring benefits in terms of
ease of integration and performances.&lt;/p&gt;
&lt;p&gt;In this session we will explore Apache Kafka, a data streaming platform,
that enables reliable real-time data integration for your event driven
application. We will look at the types of problems that Kafka is best at
solving, and show how to use it in your own applications. We‚Äôll then
take a look at Kafka in action, by creating a Flask web application.&lt;/p&gt;
&lt;p&gt;After understanding how to produce and consume events from Apache Kafka,
we‚Äôll then introduce Kafka Connect, a selection of pre-built connectors
making Apache Kafka your only backend interface, no matter where your
data originally sits.&lt;/p&gt;
&lt;p&gt;If you‚Äôre using Flask, and want to understand how to make your
application event driven by integrating it with the most used streaming
technology, this session is for you!&lt;/p&gt;
&lt;p&gt;Speaker: Francesco Tisiot&lt;/p&gt;
</content><category term="PyCon Italia 2022"></category><category term="big data"></category><category term="development"></category><category term="flask"></category><category term="open source"></category></entry><entry><title>Generazione file PDF per affinamento dell'Object detection</title><link href="https://pyvideo.org/pycon-italia-2022/generazione-file-pdf-per-affinamento-dellobject-detection.html" rel="alternate"></link><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><author><name>Lorenzo Pisaneschi</name></author><id>tag:pyvideo.org,2022-06-03:/pycon-italia-2022/generazione-file-pdf-per-affinamento-dellobject-detection.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Generazione di file PDF con Transformers per affinamento dell‚Äôobject
detection in documenti testuali - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;De-costruire un file PDF √® un task di sempre maggior interesse; questo
talk mostrer√† come sfruttare l‚Äôinformazione non strutturata dei dati per
la generazione di PDF sintetici da utilizzare in sistemi ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Generazione di file PDF con Transformers per affinamento dell‚Äôobject
detection in documenti testuali - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;De-costruire un file PDF √® un task di sempre maggior interesse; questo
talk mostrer√† come sfruttare l‚Äôinformazione non strutturata dei dati per
la generazione di PDF sintetici da utilizzare in sistemi di
apprendimento pi√π controllabili, con l‚Äôuso di Transformers ed NLP su
dataset ristretti di PDF. Il Portable Document Format (meglio noto come
PDF) √® il formato di file pi√π utilizzato al mondo. All‚Äôinterno di un
file PDF possiamo trovare un‚Äôampia gamma di elementi diversi, non solo
semplice testo: elementi grafici come immagini ed algoritmi, tabelle,
didascalie, addirittura oggetti interattivi o firme digitali. La
decostruzione di un file PDF √® dunque un task difficile, per l‚Äôalto
livello di eterogeneit√† dei dati e perch√© il formato stesso non √® stato
pensato per fornire in modo strutturato la disposizione delle diverse
entit√† che costituiscono le pagine dei documenti. Tuttavia, questo tipo
di analisi sta attirando sempre pi√π l‚Äôinteresse di ricercatori e
aziende, perch√© le informazioni sul layout e i contenuti dei file di
testo possono essere utilizzate per aumentare i dataset esistenti per
ottenere miglioramenti qualitativi nell‚Äôespletamento delle comuni
operazioni di data mining, sfruttando le tecniche di deep learning pi√π
innovative.&lt;/p&gt;
&lt;p&gt;Questo talk, descrive un sistema che, partendo da un insieme ristretto
di file PDF, √® in grado di generarne un numero arbitrariamente grande
per costituire un dataset di immagini da utilizzare per addestrare reti
neurali a svolgere le classiche operazioni di object detection e
recognition; per fare questo, sar√† mostrata una pipeline capace di
annotare automaticamente il contenuto di un PDF; dopodich√©, sar√†
spiegato come utilizzare le annotazioni precedentemente ottenute come
input di una architettura a Transformers per generare layout artificiali
di documenti; infine, si passer√† all‚Äôeffettiva sintesi dei PDF: verr√†
indicato come popolare le parti testuali dei layout generati grazie a
tecniche di NLP (Natural Language Processing), come popolare immagini,
tabelle e formule e, infine, come utilizzare il dataset sintetico cos√¨
ottenuto.&lt;/p&gt;
&lt;p&gt;Speaker: Lorenzo Pisaneschi&lt;/p&gt;
</content><category term="PyCon Italia 2022"></category><category term="big data"></category><category term="deep learning"></category><category term="machine learning"></category><category term="predictions"></category></entry><entry><title>Nowcasting financial crisis with deep learning techniques</title><link href="https://pyvideo.org/pycon-italia-2022/nowcasting-financial-crisis-with-deep-learning-techniques.html" rel="alternate"></link><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><author><name>Pelucchi Mauro</name></author><id>tag:pyvideo.org,2022-06-03:/pycon-italia-2022/nowcasting-financial-crisis-with-deep-learning-techniques.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Nowcasting financial crisis with deep learning techniques - PyCon Italia
2022&lt;/p&gt;
&lt;p&gt;In this tutorial, I‚Äôll show how train machine learning models to predict
a financial crisis event in the next 20 days. Specifically, we‚Äôll speak
about: - preparation of the dataset - selection of variables with
RandonForest and Boruta - training ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Nowcasting financial crisis with deep learning techniques - PyCon Italia
2022&lt;/p&gt;
&lt;p&gt;In this tutorial, I‚Äôll show how train machine learning models to predict
a financial crisis event in the next 20 days. Specifically, we‚Äôll speak
about: - preparation of the dataset - selection of variables with
RandonForest and Boruta - training and the predictions - measure of our
performances This tutorial shows how apply Regression Models and Deep
Learning Models to nowcasting stock markets crisis events. Specifically,
we‚Äôll how the transmission mechanisms across stock markets can be used
to train machine learning models to predict crisis events. The
tutorial‚Äôll show the entire pipeline: from the preparation of the
dataset, how balance observations and how measure our performances. Here
the git repo of the talk:&lt;/p&gt;
&lt;p&gt;Speaker: Pelucchi Mauro&lt;/p&gt;
</content><category term="PyCon Italia 2022"></category><category term="big data"></category><category term="machine learning"></category></entry><entry><title>Processing and analysing streaming data with Python and Apache Flink</title><link href="https://pyvideo.org/pycon-italia-2022/processing-and-analysing-streaming-data-with-python-and-apache-flink.html" rel="alternate"></link><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><author><name>Javier Ramirez</name></author><id>tag:pyvideo.org,2022-06-03:/pycon-italia-2022/processing-and-analysing-streaming-data-with-python-and-apache-flink.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Processing and analysing streaming data with Python and Apache Flink -
PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Data used to be a batch thing, but more and more we get unbounded
streams of data, fast or slow, that we need to process and analyse in
near real time.&lt;/p&gt;
&lt;p&gt;In this talk I‚Äôll ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Processing and analysing streaming data with Python and Apache Flink -
PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Data used to be a batch thing, but more and more we get unbounded
streams of data, fast or slow, that we need to process and analyse in
near real time.&lt;/p&gt;
&lt;p&gt;In this talk I‚Äôll show you how you can use Apache Flink and QuestDB to
build reliable streaming data pipelines that can grow as much as you
need. Data used to be a batch thing, but more and more we get unbounded
streams of data, fast or slow, that we need to process and analyse in
near real time.&lt;/p&gt;
&lt;p&gt;In this talk I‚Äôll show you how you can use Apache Flink and QuestDB to
build reliable streaming data pipelines that can grow as much as your
Python application needs.&lt;/p&gt;
&lt;p&gt;Speaker: Javier Ramirez&lt;/p&gt;
</content><category term="PyCon Italia 2022"></category><category term="analytics"></category><category term="big data"></category><category term="databases"></category><category term="distributed systems"></category><category term="open source"></category></entry><entry><title>The Truth about Mastering Big Data</title><link href="https://pyvideo.org/pycon-sk-2018/the-truth-about-mastering-big-data.html" rel="alternate"></link><published>2018-03-10T00:00:00+00:00</published><updated>2018-03-10T00:00:00+00:00</updated><author><name>Anton Caceres</name></author><id>tag:pyvideo.org,2018-03-10:/pycon-sk-2018/the-truth-about-mastering-big-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What do you think is the most essential skill a data scientist should
master? Knowledge of deep learning tools? Hadoop? SciPy?&lt;/p&gt;
&lt;p&gt;This talk reveals the cornerstone of data science: nothing is as
important as asking data the right questions. To make it work, we need
some tools and curiosity ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What do you think is the most essential skill a data scientist should
master? Knowledge of deep learning tools? Hadoop? SciPy?&lt;/p&gt;
&lt;p&gt;This talk reveals the cornerstone of data science: nothing is as
important as asking data the right questions. To make it work, we need
some tools and curiosity.&lt;/p&gt;
&lt;p&gt;While focusing on tools, we will first go over the data science subject
as a whole, define our goals, continue with an overview of the essential
Python packages like Pandas and Jupyter Notebook, and conclude with a
live demo. The purpose of this talk is to understand our data: read it,
visualize, and formulate right questions, as well as to endorse your
imagination as a data scientist.&lt;/p&gt;
</content><category term="PyCon SK 2018"></category><category term="Big Data"></category><category term="PyCon SK"></category><category term="Python"></category></entry><entry><title>Scaling up to Big Data Devops for Data Science</title><link href="https://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Marck Vaisman</name></author><id>tag:pyvideo.org,2016-10-08:/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</content><category term="PyData DC 2016"></category><category term="big data"></category><category term="Data"></category><category term="data science"></category><category term="devops"></category><category term="scaling"></category><category term="science"></category></entry></feed>