<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Jodie Burchell</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_jodie-burchell.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-05-18T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Reproducible Research in Python</title><link href="https://pyvideo.org/pycon-au-2016/reproducible-research-in-python.html" rel="alternate"></link><published>2016-08-15T00:00:00+00:00</published><updated>2016-08-15T00:00:00+00:00</updated><author><name>Jodie Burchell</name></author><id>tag:pyvideo.org,2016-08-15:/pycon-au-2016/reproducible-research-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jodie Burchell
&lt;a class="reference external" href="https://2016.pycon-au.org/schedule/127/view_talk"&gt;https://2016.pycon-au.org/schedule/127/view_talk&lt;/a&gt;
You’ve seen a great idea on someone’s blog that you think would really push that old analysis you did 6 months ago to the next level. You open up the Dropbox folder you have with all of your …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jodie Burchell
&lt;a class="reference external" href="https://2016.pycon-au.org/schedule/127/view_talk"&gt;https://2016.pycon-au.org/schedule/127/view_talk&lt;/a&gt;
You’ve seen a great idea on someone’s blog that you think would really push that old analysis you did 6 months ago to the next level. You open up the Dropbox folder you have with all of your scripts, and … you’re lost. Which script did you start with? What does this random chunk of code do? Where is the original data file? You finally sort out your scripts, but then your code fails every second line because you don't even remember which packages you used before. Frustrated, you give up.&lt;/p&gt;
&lt;p&gt;What if I told you that there is a better way to keep track of your analyses, and that it is easier than you think to do so? In this talk I will show you how using a reproducible research approach to your analyses can save you hours of time when revisiting or updating old projects, and demonstrate some of the tools that Python has available to make this possible. This talk will cover how to manage your packages using virtualenvs, how to thoroughly document your analysis using Jupyter Notebook, how to keep track of any changes using source control systems like Git and how to collaborate effectively using GitHub. By the end you will wonder why you’ve ever done your analyses any other way, and will be happily maintaining and improving your projects for many years to come!&lt;/p&gt;
</content><category term="PyCon AU 2016"></category></entry><entry><title>Vectorise all the things! How basic linear algebra can speed up your data science code</title><link href="https://pyvideo.org/pycon-uk-2022/vectorise-all-the-things-how-basic-linear-algebra-can-speed-up-your-data-science-code.html" rel="alternate"></link><published>2022-09-17T00:00:00+00:00</published><updated>2022-09-17T00:00:00+00:00</updated><author><name>Jodie Burchell</name></author><id>tag:pyvideo.org,2022-09-17:/pycon-uk-2022/vectorise-all-the-things-how-basic-linear-algebra-can-speed-up-your-data-science-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Do you feel like your data science code is horribly inefficient, but you don’t know how to make things faster? Fear not! In this talk, we’ll speed up some common operations using tricks from linear algebra - all within the comfort of the Python ecosystem.&lt;/p&gt;
&lt;p&gt;Have you found …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Do you feel like your data science code is horribly inefficient, but you don’t know how to make things faster? Fear not! In this talk, we’ll speed up some common operations using tricks from linear algebra - all within the comfort of the Python ecosystem.&lt;/p&gt;
&lt;p&gt;Have you found that your data science code works beautifully on a few dozen test rows, but leaves you wondering how to spend the next couple of hours after you start looping through your full data set? Are you only familiar with Python, and wish there was a way to speed things up without subjecting yourself to learning C? In this talk, I will show you some simple tricks, borrowed from linear algebra, which can give you significant performance gains in your Python data science code. I will gently take you through the basics of linear algebra, explaining core operations such as matrix addition, subtraction and multiplication, scalar multiplication and the dot product. I will then show you some examples of how you can easily utilise these concepts in your machine learning code to speed up common data science operations such as distance calculations, classification tasks and finding nearest neighbours.&lt;/p&gt;
</content><category term="PyCon UK 2022"></category></entry><entry><title>Vectorize all the things! Using linear algebra and NumPy to make your Python code lightning fast.</title><link href="https://pyvideo.org/pycon-us-2023/vectorize-all-the-things-using-linear-algebra-and-numpy-to-make-your-python-code-lightning-fast.html" rel="alternate"></link><published>2023-04-22T00:00:00+00:00</published><updated>2023-04-22T00:00:00+00:00</updated><author><name>Jodie Burchell</name></author><id>tag:pyvideo.org,2023-04-22:/pycon-us-2023/vectorize-all-the-things-using-linear-algebra-and-numpy-to-make-your-python-code-lightning-fast.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you found that your code works beautifully on a few dozen examples,
but leaves you wondering how to spend the next couple of hours after you
start looping through all of your data? Are you only familiar with
Python, and wish there was a way to speed things …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you found that your code works beautifully on a few dozen examples,
but leaves you wondering how to spend the next couple of hours after you
start looping through all of your data? Are you only familiar with
Python, and wish there was a way to speed things up without subjecting
yourself to learning C?&lt;/p&gt;
&lt;p&gt;In this talk, you'll see some simple tricks, borrowed from linear
algebra, which can give you significant performance gains in your Python
code, and how you can implement these in NumPy. We'll start exploring an
inefficient implementation of an algorithm that relies heavily on loops
and lists. Throughout the talk, we'll iteratively replace bottlenecks
with NumPy vectorized operations.&lt;/p&gt;
&lt;p&gt;At each stage, you'll learn the linear algebra behind why these
operations are more efficient so that you'll be able to utilize these
concepts in your own code. You'll see how straightforward it can be to
make your code many times faster, all without losing readability or
needing to understand complex coding concepts.&lt;/p&gt;
</content><category term="PyCon US 2023"></category></entry><entry><title>Lies, damned lies and large language models</title><link href="https://pyvideo.org/pycon-us-2024/lies-damned-lies-and-large-language-models.html" rel="alternate"></link><published>2024-05-18T00:00:00+00:00</published><updated>2024-05-18T00:00:00+00:00</updated><author><name>Jodie Burchell</name></author><id>tag:pyvideo.org,2024-05-18:/pycon-us-2024/lies-damned-lies-and-large-language-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Would you like to use large language models (LLMs) in your own project,
but are troubled by their tendency to frequently “hallucinate”, or
produce incorrect information? Have you ever wondered if there was a way
to easily measure an LLM’s hallucination rate, and compare this against
other models …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Would you like to use large language models (LLMs) in your own project,
but are troubled by their tendency to frequently “hallucinate”, or
produce incorrect information? Have you ever wondered if there was a way
to easily measure an LLM’s hallucination rate, and compare this against
other models? And would you like to learn how to help LLMs produce more
accurate information?&lt;/p&gt;
&lt;p&gt;In this talk, we’ll have a look at some of the main reasons that
hallucinations occur in LLMs, and then focus on how we can measure one
specific type of hallucination: the tendency of models to regurgitate
misinformation that they have learned from their training data. We’ll
explore how we can easily measure this type of hallucination in LLMs
using a dataset called &lt;em&gt;TruthfulQA&lt;/em&gt; in conjunction with Python tooling
including Hugging Face’s &lt;tt class="docutils literal"&gt;datasets&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;transformers&lt;/tt&gt; packages, and
the &lt;tt class="docutils literal"&gt;langchain&lt;/tt&gt; package.&lt;/p&gt;
&lt;p&gt;We’ll end by looking at recent initiatives to reduce hallucinations in
LLMs, using a technique called retrieval augmented generation (RAG).
We’ll look at how and why RAG makes LLMs less likely to hallucinate, and
how this can help make these models more reliable and usable in a range
of contexts.&lt;/p&gt;
</content><category term="PyCon US 2024"></category></entry></feed>