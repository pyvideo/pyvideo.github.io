<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Priyank Agrawal</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_priyank-agrawal.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Learning by Repetition: Stochastic Multi-armed Bandits under Priming Effect</title><link href="https://pyvideo.org/uai-2020/learning-by-repetition-stochastic-multi-armed-bandits-under-priming-effect.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Priyank Agrawal</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/learning-by-repetition-stochastic-multi-armed-bandits-under-priming-effect.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Learning by Repetition: Stochastic Multi-armed Bandits under Priming Effect&lt;/p&gt;
&lt;p&gt;Priyank Agrawal (University of Illinois at Urbana-Champaign)*; Theja Tulabandula (University of Illinois at Chicago)&lt;/p&gt;
&lt;p&gt;We study the effect of persistence of engagement on learning in a stochastic multi-armed bandit setting. In advertising and recommendation systems, repetition effect includes a wear-in â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Learning by Repetition: Stochastic Multi-armed Bandits under Priming Effect&lt;/p&gt;
&lt;p&gt;Priyank Agrawal (University of Illinois at Urbana-Champaign)*; Theja Tulabandula (University of Illinois at Chicago)&lt;/p&gt;
&lt;p&gt;We study the effect of persistence of engagement on learning in a stochastic multi-armed bandit setting. In advertising and recommendation systems, repetition effect includes a wear-in period, where the user's propensity to reward the platform via a click or purchase depends on how frequently they see the recommendation in the recent past. It also includes a counteracting wear-out period, where the user's propensity to respond positively is dampened if the recommendation was shown too many times recently. Priming effect can be naturally modelled as a temporal constraint on the strategy space, since the reward for the current action depends on historical actions taken by the platform. We provide novel algorithms that achieves sublinear regret in time and the relevant wear-in/wear-out parameters. The effect of priming on the regret upper bound is also additive, and we get back a guarantee that matches popular algorithms such as the UCB1 and Thompson sampling when there is no priming effect. Our work complements recent work on modeling time varying rewards, delays and corruptions in bandits, and extends the usage of rich behavior models in sequential decision making settings.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>