<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_matthew-honnibal.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-07-26T00:00:00+00:00</updated><entry><title>Building new NLP solutions with spaCy and Prodigy</title><link href="https://pyvideo.org/europython-2018/building-new-nlp-solutions-with-spacy-and-prodigy.html" rel="alternate"></link><published>2018-07-26T00:00:00+00:00</published><updated>2018-07-26T00:00:00+00:00</updated><author><name>Matthew Honnibal</name></author><id>tag:pyvideo.org,2018-07-26:europython-2018/building-new-nlp-solutions-with-spacy-and-prodigy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Commercial machine learning projects are currently like start-ups: many
projects fail, but some are extremely successful, justifying the total
investment. While some people will tell you to “embrace failure”, I say
failure sucks — so what can we do to fight it? In this talk, I will
discuss how to address some of the most likely causes of failure for new
Natural Language Processing (NLP) projects. My main recommendation is to
take an iterative approach: don’t assume you know what your pipeline
should look like, let alone your annotation schemes or model
architectures. I will also discuss a few tips for figuring out what’s
likely to work, along with a few common mistakes. To keep the advice
well-grounded, I will refer specifically to our open-source library
spaCy, and our commercial annotation tool Prodigy.&lt;/p&gt;
</summary></entry><entry><title>Building new NLP solutions with spaCy and Prodigy</title><link href="https://pyvideo.org/pydata-berlin-2018/building-new-nlp-solutions-with-spacy-and-prodigy.html" rel="alternate"></link><published>2018-07-07T00:00:00+00:00</published><updated>2018-07-07T00:00:00+00:00</updated><author><name>Matthew Honnibal</name></author><id>tag:pyvideo.org,2018-07-07:pydata-berlin-2018/building-new-nlp-solutions-with-spacy-and-prodigy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, I will discuss how to address some of the most likely
causes of failure for new Natural Language Processing (NLP) projects. My
main recommendation is to take an iterative approach: don't assume you
know what your pipeline should look like, let alone your annotation
schemes or model architectures.&lt;/p&gt;
</summary></entry><entry><title>Designing spaCy: Industrial-strength NLP</title><link href="https://pyvideo.org/pydata-berlin-2016/designing-spacy-industrial-strength-nlp.html" rel="alternate"></link><published>2016-05-31T00:00:00+00:00</published><updated>2016-05-31T00:00:00+00:00</updated><author><name>Matthew Honnibal</name></author><id>tag:pyvideo.org,2016-05-31:pydata-berlin-2016/designing-spacy-industrial-strength-nlp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;The spaCy natural language processing (NLP) library features state-of-the-art performance, and a high-level Python API. Efficiency is crucial for NLP, because job sizes are constantly increasing. This talk describes how we’ve met these challenges in spaCy, by implementing the library in Cython.&lt;/p&gt;
&lt;p&gt;The spaCy natural language processing (NLP) library features state-of-the-art performance, and a high-level Python API. Efficiency is crucial for NLP, because job sizes are constantly increasing. The key algorithms are also relatively complicated, and frequently subject to change, as new research is published. This talk describes how we’ve met these challenges in spaCy, by implementing the library in Cython. Unlike many Cython users, we did not write the library in Python first, and then optimize it. Instead, we designed the library as a C extension from the start, and added the Python API on top. This allows us to build the library on top of efficient, memory-managed data structures, without having to maintain a separate C or C++ codebase. The result is the fastest NLP library in the world, support for GIL-free multithreading, in a concise readable codebase, and with no compromise on user friendliness.&lt;/p&gt;
</summary><category term="spacy"></category><category term="npl"></category></entry></feed>