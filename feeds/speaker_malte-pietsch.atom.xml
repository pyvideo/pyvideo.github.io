<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_malte-pietsch.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-12-13T00:00:00+00:00</updated><entry><title>Keynote: Transfer Learning - Entering a new era in NLP</title><link href="https://pyvideo.org/pydata-warsaw-2019/keynote-transfer-learning-entering-a-new-era-in-nlp.html" rel="alternate"></link><published>2019-12-13T00:00:00+00:00</published><updated>2019-12-13T00:00:00+00:00</updated><author><name>Malte Pietsch</name></author><id>tag:pyvideo.org,2019-12-13:pydata-warsaw-2019/keynote-transfer-learning-entering-a-new-era-in-nlp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Transfer learning has been changing the NLP landscape tremendously since
the release of BERT one year ago. Transformers of all kinds have
emerged, dominate most research leaderboards and have made their way
into industrial applications. In this talk we will dissect the paradigm
of transfer learning and its effects on pipelines, modelling and the
engineers mindset.&lt;/p&gt;
&lt;p&gt;Sufficient training data is often a bottleneck for real-world machine
learning applications. The computer vision community mitigated this
problem by pretraining models on ImageNet and transferring knowledge to
the desired task. Thanks to an emerging new class of deep language
models, transfer learning has also become the new standard in NLP. In
this talk we will share strategies, tips &amp;amp; tricks along all model
phases: Pretraining a language model from scratch, adjusting it for
domain specific language and fine-tuning it for the desired down-stream
task. We will demonstrate the practical implications by showing how
models like BERT caused major breakthroughs for the task of Question
Answering.&lt;/p&gt;
</summary><category term="keynote"></category></entry></feed>