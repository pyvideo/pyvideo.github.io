<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Sebastian W. Ober</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_sebastian-w-ober.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-07-31T00:00:00+00:00</updated><subtitle></subtitle><entry><title>An Improved Variational Approximate Posterior for the Deep Wishart</title><link href="https://pyvideo.org/uai-2023/an-improved-variational-approximate-posterior-for-the-deep-wishart.html" rel="alternate"></link><published>2023-07-31T00:00:00+00:00</published><updated>2023-07-31T00:00:00+00:00</updated><author><name>Sebastian W. Ober</name></author><id>tag:pyvideo.org,2023-07-31:/uai-2023/an-improved-variational-approximate-posterior-for-the-deep-wishart.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;An Improved Variational Approximate Posterior for the Deep Wishart Process&amp;quot;
Sebastian W. Ober, Ben Anson, Edward Milsom, Laurence Aitchison
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/ober23a.html"&gt;https://proceedings.mlr.press/v216/ober23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
Deep kernel processes are a recently introduced class of deep Bayesian models that have the flexibility of neural networks, but work entirely …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;An Improved Variational Approximate Posterior for the Deep Wishart Process&amp;quot;
Sebastian W. Ober, Ben Anson, Edward Milsom, Laurence Aitchison
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/ober23a.html"&gt;https://proceedings.mlr.press/v216/ober23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
Deep kernel processes are a recently introduced class of deep Bayesian models that have the flexibility of neural networks, but work entirely with Gram matrices. They operate by alternately sampling a Gram matrix from a distribution over positive semi-definite matrices, and applying a deterministic transformation. When the distribution is chosen to be Wishart, the model is called a deep Wishart process (DWP). This particular model is of interest because its prior is equivalent to a deep Gaussian process (DGP) prior, but at the same time it is invariant to rotational symmetries, leading to a simpler posterior distribution. Practical inference in the DWP was made possible in recent work (“A variational approximate posterior for the deep Wishart process” Ober and Aitchison, 2021a) where the authors used a generalisation of the Bartlett decomposition of the Wishart distribution as the variational approximate posterior. However, predictive performance in that paper was less impressive than one might expect, with the DWP only beating a DGP on a few of the UCI datasets used for comparison. In this paper, we show that further generalising their distribution to allow linear combinations of rows and columns in the Bartlett decomposition results in better predictive performance, while incurring negligible additional computation cost.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://www.auai.org/uai2023/oral_slides/402-oral-slides.pdf"&gt;https://www.auai.org/uai2023/oral_slides/402-oral-slides.pdf&lt;/a&gt;&lt;/p&gt;
</content><category term="UAI 2023"></category></entry></feed>