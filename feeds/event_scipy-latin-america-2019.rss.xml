<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Thu, 10 Oct 2019 00:00:00 +0000</lastBuildDate><item><title>Applying GANs for data augmentation and image segmentation for brain MRI</title><link>https://pyvideo.org/scipy-latin-america-2019/applying-gans-for-data-augmentation-and-image-segmentation-for-brain-mri.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Using GANs for different steps in medical image analysis: 1) Image generation for data augmentation, this is an important step since neural network algorithms require huge amounts of data to help them generalize in an optimal solution. In the other hand, anonymization of data has became lately a very sensitive field, retrieving information from the dataset that could compromise the privacy of a patient is dangerous and by generating artificial samples that would look like the original from patients we can avoid thisproblem 2) Image segmentation for magnetic resonance image: image to image translation are often used to generate neural style transfer in order to make a picture look more like a famous painting but in this case I would like to provide important information for a physician, that includes the location of tumors in the brain, gray matter, white matter and cerebrospinal fluid. Valuable information to automate diagnostics and decision making when time is key element.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Arturo Polanco</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/applying-gans-for-data-augmentation-and-image-segmentation-for-brain-mri.html</guid></item><item><title>BackCLIP: a tool to identify common background presence in PAR-CLIP datasets</title><link>https://pyvideo.org/scipy-latin-america-2019/backclip-a-tool-to-identify-common-background-presence-in-par-clip-datasets.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PAR-CLIP, a CLIP-seq protocol, derives a transcriptome wide set of binding sites for RNA-binding proteins. Even though the protocol uses stringent washing to remove experimental noise, some of it remains. A recent study measured three sets of non-specific RNA backgrounds which are present in several PAR-CLIP datasets. However, a tool to identify the presence of common background in PAR-CLIP datasets is not yet available. In this talk I will introduce a tool that uses this score to identify the amount of common backgrounds present in a PAR-CLIP dataset, and we provide the user the option to use or remove it. My team used the proposed strategy in 30 PAR-CLIP datasets from nine proteins. It is possible to identify the presence of common backgrounds in a dataset and identify differences in datasets for the same protein. This method is the first step in the process of completely removing such backgrounds.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carlos Sierra</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/backclip-a-tool-to-identify-common-background-presence-in-par-clip-datasets.html</guid></item><item><title>Behavioral Biometrics: Re-inventing Authentication using Python</title><link>https://pyvideo.org/scipy-latin-america-2019/behavioral-biometrics-re-inventing-authentication-using-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In the past 50 years, passwords have dominated the authentication methods. This approach is based on secrecy and the illusion of security rooted on the password's entropy. However, secrets can be stolen and passwords can be guessed, with no way to prevent malicious activity that is made on our behalf from occurring. In this presentation, we will discuss how we implemented machine learning to strengthen the identification process by utilizing behavioral biometrics analysis in multi-modal authentication. We will show how to combine both mouse dynamics and typing patterns to validate users’ identities in a frictionless scenario during login phase. Moreover, we make the most of Python to preprocessing mouse movements and typing traces to create valuable features which define unique behavioral patterns. Our proposal aims to learn from the users' unique interactions in order to complement password authentication in web applications. This presentation is about a practical application inspired on a paper published by us at the 1st International Workshop on Security in Machine Learning and its Applications co-located with the 17th International Conference on Applied Cryptography and Network Security ( &lt;a class="reference external" href="https://www.acns19.com/wp-content/uploads/2019/06/SiMLA-1.pdf"&gt;https://www.acns19.com/wp-content/uploads/2019/06/SiMLA-1.pdf&lt;/a&gt; ).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jesús Solano</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/behavioral-biometrics-re-inventing-authentication-using-python.html</guid></item><item><title>Beyond Jupyter: Other environments to do scientific programming in Python</title><link>https://pyvideo.org/scipy-latin-america-2019/beyond-jupyter-other-environments-to-do-scientific-programming-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The easiness with which you can mix code, text, equations and graphics in Jupyter has made it the preferred tool of data scientists, engineers and scientists to do scientific programming in Python. Jupyter certainly excels at data exploration and teaching but lacks many features necessary to create large and modular programs, which can be found in other well-known scientific environments such as Matlab. Spyder is an alternative to Jupyter which seeks to provide most of these features in an integrated, easy to use environment that works as a desktop application in all major operating systems (Windows, Linux and macOS). In particular, Spyder comes with a powerful editor that provides code completion, signaling of warnings and errors and the ability to go where a function, class or method is defined. With the next Spyder version, these facilities will be available not only for Python but for several other programming languages as well (e.g. C++ or Julia). After writing your code on the editor, you can execute it in Spyder's integrated IPython console. Spyder supports several types of evaluation modes, according to your needs: you can run the whole file, only portions of the it (called cells) or a single line. This allows for quick iteration and incremental code development: every change can be reevaluated and feedback about its results obtained immediately thanks to the console's presence. To further support this process, Spyder also offers and integrated documentation viewer called 'Help'. After pressing 'Ctrl+I' next to any object used in the editor or the console, the documentation viewer will display its associated help in a beautifully formatted way, which avoids constantly consulting the web to remember how to use that object. After a code execution is completed, Spyder also gives the possibility to explore all objects created by it in a pane called 'Variable Explorer'. There users can inspect and modify the contents of lists, dictionaries, sets, 2D and 3D Numpy arrays, Pandas Dataframes and any other kind of Python object through specialized viewers for each of them. Besides these four main components (Editor, Console, Help and Variable Explorer), Spyder comes with a host of other facilities aimed at increasing productivity and allowing users to easily embark in small to mid-size programming projects. Among them we would like to mention the following: the ability to create projects to quickly switch between different coding efforts, find any text snippet contained in plain text files of a given directory, and explore all files present on that directory. As the current Spyder maintainer, it would be my pleasure to present these and several other components and nifty tricks and features to the SciPy Latin America community.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carlos Córdoba</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/beyond-jupyter-other-environments-to-do-scientific-programming-in-python.html</guid></item><item><title>Capturing knowledge in code</title><link>https://pyvideo.org/scipy-latin-america-2019/capturing-knowledge-in-code.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lindsey Heagy</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/capturing-knowledge-in-code.html</guid><category>keynote</category></item><item><title>Dendron: a library to assess stability of clustering patterns on HCA</title><link>https://pyvideo.org/scipy-latin-america-2019/dendron-a-library-to-assess-stability-of-clustering-patterns-on-hca.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Durante este charla presentaré la forma en que implementamos nuestro enfoque del análisis de agrupamiento jerárquico (HCA), el cual, a pesar de su amplio uso en diversas áreas del conocimiento, aún mantiene sutilezas que afectan la forma en que se pueden interpretar sus resultados. En la charla presentaré formalmente el HCA y el problema de los llamados «ties in proximity», que pueden producir una gran cantidad de posibles dendrogramas resultantes. Las implementaciones computacionales disponibles en Python y en R-package usualmente arrojan un único dendrograma como resultado del análisis, por lo que resulta imperativo asegurar la validez de las conclusiones que se derivan de los patrones de agrupamiento. De igual manera, pequeñas variaciones en los objetos analizados o cambios en las metodologías de agrupameinto pueden afectar drásticamente los patrones encontrados, por lo que también se hace indispensable asegurar la estabilidad de los patrones de agrupamiento resultantes. Por estas razones se necesita una herramienta que permita analizar la estabilidad de los resultados de HCA y que garantice que los resultados encontrados no dependen del instrumento empleado para explorar los datos.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eugenio Llanos</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/dendron-a-library-to-assess-stability-of-clustering-patterns-on-hca.html</guid></item><item><title>Diagnóstico de epilepsia: Utilizando Python para analizar señales cerebrales</title><link>https://pyvideo.org/scipy-latin-america-2019/diagnostico-de-epilepsia-utilizando-python-para-analizar-senales-cerebrales.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;La Ingeniería no es para mantenerla guardada en un computador. Es una herramienta poderosa para hacer del mundo un lugar mejor. Con ayuda de Python, que por lo sencillo de su sintaxis permite realizar cosas increíbles de forma fácil y rápida, veremos como analizar las señales que produce el cerebro, para saber si sufre de epilepsia, con el fin de poder actuar sobre el paciente y ofrecerle una mejor calidad de vida. Utilizando el análisis en frecuencia formulado hace casi dos siglos, podemos obtener las características que indican si un paciente sufre o no sufre de epilepsia, y después, con ayuda de potentes algoritmos de machine learning como la regresión lineal, las redes neuronales o las maquinas de soporte vectorial, se determina si una persona está enferma o no. Finalmente, utilizando métricas, se descubre si realmente los algoritmos están arrojando datos coherentes o no, y de no ser así, veremos que técnicas se pueden aplicar para mejorar los resultados obtenidos.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Parada</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/diagnostico-de-epilepsia-utilizando-python-para-analizar-senales-cerebrales.html</guid></item><item><title>High-Performance Pandas part 2: Time Based Analysis Using Pandas</title><link>https://pyvideo.org/scipy-latin-america-2019/high-performance-pandas-part-2-time-based-analysis-using-pandas.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Time series data is usually a difficult type of data to work with. Because of the later, it is a common practice to avoid the use of libraries such as pandas to do this sort of data analysis. However, the power of pandas goes beyond wrapping numpy or allow a friendly data tables visualization. In this talk I will show, in a practical way, how to make the most of the full functionality of pandas library by exploiting datetime API and applying vectorized operations over a time series dataframe. From this presentation you will learn how to efficiently manipulate dates within the same pandas dataframe in a vectorized way, how to gruop time data and how to aggregate your data all in the same place saving you time and coding. For this talk you only need passion for data and minimum experience using pandas library in any kind of project.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Luis Camacho</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/high-performance-pandas-part-2-time-based-analysis-using-pandas.html</guid></item><item><title>La R[*]volución de Jupyter</title><link>https://pyvideo.org/scipy-latin-america-2019/la-rvolucion-de-jupyter.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Damián Avila</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/la-rvolucion-de-jupyter.html</guid><category>keynote</category></item><item><title>Python in algotrading lectures</title><link>https://pyvideo.org/scipy-latin-america-2019/python-in-algotrading-lectures.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;I teach Algorithmic trading in Master in quantitative finance at the universidad del Rosario. In this short talk I will share my experience teaching the elective lecture in algorithmic trading with the use of python, more specifically with Anaconda plus Spider. I will give a small recap of the lecture including: Dataframe structures, datereaders for scrapping information online on the web, graphical technical analysis, a small reference to trading techniques as: momentum, crossover and pairs trading strategies, how to test the algorithms or robots with zipline backtesting and finally a small implementation with the neural networks of sklearn. From this lecture, some tesis projects arose and where implemented in python, so to end, I will talk shortly about the computational importance in financial research, such as neural networks in finance, Support vector machine in clustering financial information, finite difference to solve financial partial differential equations and applications to option pricing.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hugo Ramirez</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/python-in-algotrading-lectures.html</guid></item><item><title>The intraday market at Bolsa de Valores de Colombia</title><link>https://pyvideo.org/scipy-latin-america-2019/the-intraday-market-at-bolsa-de-valores-de-colombia.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We develop a set of tools to explore and visualize historical high frequency data for stocks and bonds that trade at Bolsa de Valores de Colombia. We use trade and quote data on the most liquid assets that are observable during the trading day for a period of six months. We show that the Pandas library provides enough flexibility in sampling the time stamp of transactions and hence an important advantage for processing high frequency financial data. After we process the information we develop tools to estimate observable and non-observable market quality parameters for a selection of stocks and bonds. Market quality in financial markets is measured along different dimensions of the prices and the volume of transactions: volatility, liquidity, transaction activity, jump activity and price impact. These measurements are important for market participants and researchers interested in empirical finance, algorithmic trading and market microstructure; however, they are not directly available from financial information providers because of the amount of processing power required to work with intraday data. Some preliminary results with stocks indicate, that as observed in other markets at the beginning of trading hours, liquidity (measured as bid ask spread and depth) is the lowest and price volatility is high. Additionally, we observe that stocks with lower levels of price impact also have the highest levels of depth.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carlos Castro</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/the-intraday-market-at-bolsa-de-valores-de-colombia.html</guid></item><item><title>The pandas of the future</title><link>https://pyvideo.org/scipy-latin-america-2019/the-pandas-of-the-future.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Since the start of the project 10 years ago, pandas has grown in popularity, to become almost a standard for data wrangling and analysis in Python. While pandas has served well the needs of many of its users, several new projects have been started in the last years to respond to needs that pandas is not able to address. For example, Dask provides a pandas-like API to distribute jobs over a cluster. Vaex provides a pandas-like API to perform out-of-core computation. cuDF is reimplementing a pandas-like dataframe for GPUs. Koalas implements a pandas-like API for Apache Spark. And there are even more projects like Modin or static-frame. At the same time, pandas itself has been trying to address new needs from the users, like adding the ability to use third-party data types (besides the original numeric and datetime ones from NumPy). For example CyberPandas extends pandas with an efficient IP address type. And GeoPandas does the same with geolocations. Other work has been done to break parts of pandas, so it can be better extended, and used to solve new problems. For example, pandas 0.25 decoupled all plotting code in pandas, to allow the use of third-party plotting libraries. This allows for example to generate the same plots pandas is able to generate, but interactive, using Bokeh, HoloViews, Altair or others. The future of pandas and its ecosystem is uncertain. In this talk I'll give an insider point of view on what can be broken in pandas, so many projects are being implemented to address the same needs. How pandas can be broken even more, to cover more user needs. What are the current and planned developments, and what users can expect from pandas in the future.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marc García</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/the-pandas-of-the-future.html</guid></item><item><title>Uplift Modelling for selecting maximal impact treatment for customers</title><link>https://pyvideo.org/scipy-latin-america-2019/uplift-modelling-for-selecting-maximal-impact-treatment-for-customers.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As a marketing strategy, Rappi grants different types of incentives to users in order to generate additional purchases and revenue. As the company has a limited budget for marketing campaigns, we need to wisely choose to which users to grant incentives with the objective of bringing about incremental impact while reducing costs. Thus, one of our greatest challenges is to avoid granting incentives to users who are so inclined to purchasing that would make an order even without contacting them. For this, we developed an uplift modeling methodology to predict the buying behavior of customers when we give them an incentive, and when we don’t. The methodology consists in two machine learning models: one for scoring each user for expected buying after receiving an incentive, and another for a passive treatment without incentive. With both models we can observe the influence of the treatment on any customer, what allows us to grant incentives just to users whose likelihood of buying increases with the incentive. This technique has an important business value as it reduced wasted marketing costs.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maria Cortes</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/uplift-modelling-for-selecting-maximal-impact-treatment-for-customers.html</guid></item><item><title>Visualizing genomic data with python</title><link>https://pyvideo.org/scipy-latin-america-2019/visualizing-genomic-data-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;La visualización de los datos permite realizar análisis de la información por lo tanto para los datos de NGS es un valor vital dado que no solo es para el uso de personas con conocimientos en programación y análisis de datos si no para diferentes disciplinas como la medicina. La visualización de datos genómicos a través de un “tablero” permite obtener información relevante que impacta en la toma de decisiones basada en información relevante. En esta propuesta se pretende mostrar app usando dash y python para evaluar la calidad del mapeo en un proceso de secuenciación de diferentes regiones genómicas en un paciente y la importancia de realizar dicha evaluación con la finalidad de posteriormente buscar variaciones genéticas o de evaluar otras opciones diagnosticas en caso de que el proceso de secuenciamiento falle por falta de cubrimiento o por sobrelapamiento de lecturas ya que los datos de secuenciación tienen una probabilidad de error debido a la metodología utilizada en laboratorio.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jennifer Velez</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/visualizing-genomic-data-with-python.html</guid></item><item><title>What to Wear? Recomendación de outfits usando visión por computador</title><link>https://pyvideo.org/scipy-latin-america-2019/what-to-wear-recomendacion-de-outfits-usando-vision-por-computador.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Steven Pineda</dc:creator><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-10:scipy-latin-america-2019/what-to-wear-recomendacion-de-outfits-usando-vision-por-computador.html</guid></item><item><title>Application of the Metropolis–Hastings algorithm in 2D and 3D nano-materials with magnetic properties.</title><link>https://pyvideo.org/scipy-latin-america-2019/application-of-the-metropolis-hastings-algorithm-in-2d-and-3d-nano-materials-with-magnetic-properties.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk is aimed to show the potential of Monte Carlo methods in physics and other fields. in other to accomplish this statement it will be discussed an specific application that is the work I've been doing with the nano-magnetism group in the last year. Here it will be shown how to part from a hamiltonian, and simulate the dynamics of the system using monte-carlo methods. it will be showed how to extract information of this measures as it's the presence of a core-shell nanoparticle from the FORC diagrams. in order to aim this goal, the talk will begin with a review of the canonical ensemble formalism. after it will be analyzed how to extrapolate the formalism into a metropolis-hasting method that is based in Monte Carlo simulations. Eventually the talk will go to an specific topic that is the simulation of 2D and 3D materials that present nano-magnetic properties, it will be discussed the state of the art of how experimental physicist deal with this materials, what they measured and what they look for in the data. for this it will be presented the FORC diagrams that make easier the analysis of the properties of the materials. Then it will be seen how to simulate those measures and how to display the data. In order to have a decent behavior of the algorithm, it has to be overcome a couple of problems as it's the escalation of the computational time and the numeric looseness. meanwhile, it will be discussed how to parallelize the code with Numba and how to make and statistical analysis in order to reduce the standard deviation of the data. Finally it will be shown the process of validation of the simulation with the experimental data and the theoretical predictions, and the interpretation of the obtained data, as it's the core-shell.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicolás Vergara</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/application-of-the-metropolis-hastings-algorithm-in-2d-and-3d-nano-materials-with-magnetic-properties.html</guid></item><item><title>Black Holes and Gravitational Theories beyond General Relativity Research using Python</title><link>https://pyvideo.org/scipy-latin-america-2019/black-holes-and-gravitational-theories-beyond-general-relativity-research-using-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Black holes are some of the most mysterious objects studied by physics in the past 50 years. Their intriguing properties make them the main topic of investigation for many researchers around the world and the advances in both astronomical observational techniques and image processing capabilities permitted to confirm the existence of these astrophysical objects in the Universe this year with the presentation of the photography of the black hole in the galaxy M87. Black holes are described in terms of Einstein’s General Theory of Relativity and therefore the mathematical complexity involved is enormous. Some specialized and proprietary software such as Mathematica and Maple have been used in the past to help in the mathematical calculations. However, the growing use of Python in the academic community and the development of the appropriate tools is changing the panorama and it is becoming the main language involved in research as well as in education. In this talk we will show how we incorporated Python both in pre and postgraduate education and in our research on black holes and alternative gravitational theories such as string theory, providing a powerful tool to perform analytic calculations and image/data visualization. We will show some practical applications such as event horizon visualization, motion of particles around black holes, gravitational lensing images and embedding of hypersurfaces in spacetime.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eduard Larrañaga</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/black-holes-and-gravitational-theories-beyond-general-relativity-research-using-python.html</guid></item><item><title>Classifying scientific papers using topic models with Python</title><link>https://pyvideo.org/scipy-latin-america-2019/classifying-scientific-papers-using-topic-models-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The amount of information generated every year by the scientific community has been steadily increasing, making it almost impossible for scientists to stay up-to-date with all the data that may impact their research. There is a need for methods and tools that automate knowledge extraction to help scientist understand high level trends and topics around a particular field. Topic modelling is a well known technique in the natural language processing world that addresses part of this problem. This talk presents the results of using probabilistic topic modelling to analyze 10 years of scientific papers submitted to the European Geophysical Union yearly meeting (EGU). The first step in advancing scientific research is to understand what is in a text document. We need to know which topics are included and how they are distributed. We might not fully solve your homework but topic modelling will give you a good idea of the content of a document without you having to read it. Our prototype for text classification started as a side project for the NSF EarthCube project, the original idea was to create a “Google” for scientific data. We succeeded in implementing a focused crawler (derived from Apache Nutch) that indexed billions of links with scientific information. In order to analyze this vast data set we started exploring the idea of using Natural Language Processing tools to gain insights on it. We used NLP techniques on scientific papers, conference abstracts and websites to build topic models and create useful interactive data visualizations. All this work would not have been possible without the use of NLP libraries in Python such as PyLDAvis, scikit-learn, ScatterText, NLTK and others. This talk will cover what data we used for this research and how to recreate our results with topic modelling using Jupyter notebooks. Lastly, I will discuss how these techniques can be applied to other domains.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Luis Alberto Lopez</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/classifying-scientific-papers-using-topic-models-with-python.html</guid></item><item><title>Del POC a producción - Infraestructura para machine-learning en Mercadolibre</title><link>https://pyvideo.org/scipy-latin-america-2019/del-poc-a-produccion-infraestructura-para-machine-learning-en-mercadolibre.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Para tratar el tema de la distancia entre el desarrollo experimental de un modelo de machine-learning y los desafíos de servirlo como un servicio production-ready, voy a aprovechar una experiencia de la vida real. En Mercadolibre existe Fury: una plataforma de infraestructura y desarrollo, para el despliegue de microservicios. La misma provee features para build, deploy, gestión de entornos de desarrollo, automatización de la creación de infraestructura, monitoreo, métricas, servicios, etc. Es uno de los principales factores que permitió crecer de 400 desarrolladores a los ~3000 que somos ahora, en pocos años.  El problema es que dicha infraestructura no fue diseñada para lidiar con las particularidades asociadas a los sistemas de machine-learning: entornos de experimentación para data-scientists, procesos de ETL, entrenamiento de modelos, acceso a infraestructura específica como GPU, etc. Entonces, en un esfuerzo transversal a la organización, con la colaboración de varios equipos, desarrollamos Fury Data Apps: una extensión a Fury que provee herramientas y servicios para que cualquiera pueda diseñar, experimentar, desarrollar y desplegar sistemas basados en machine-learning, a la escala que Mercadolibre necesita. En esta charla voy a presentar: las características principales de la plataforma los problemas que soluciona el stack tecnológico que la soporta en general, cómo cubre el gap entre un POC y un sistema production-ready&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carlos De la Torre</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/del-poc-a-produccion-infraestructura-para-machine-learning-en-mercadolibre.html</guid></item><item><title>Extracción y análisis de información de accidentes de tránsito desde redes sociales</title><link>https://pyvideo.org/scipy-latin-america-2019/extraccion-y-analisis-de-informacion-de-accidentes-de-transito-desde-redes-sociales.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;La detección de accidentes de tránsito es una estrategia importante para que los gobiernos implementen políticas que reduzcan este fenómeno. Existen diferentes técnicas para detectar accidentes de tráfico, como el procesamiento de imágenes, dispositivos RFID, redes sociales y otros. Sin embargo, algunas estrategias tecnológicas pueden ser difíciles y costosas de implementar. Por esta razón, el análisis de datos de redes sociales es una técnica importante a tener en cuenta en los sistemas inteligentes de transporte. Sin embargo, los datos de redes sociales relevantes para accidentes de tráfico pueden ser difíciles de filtrar y caracterizar a partir de otros datos. El lenguaje, el estilo de escritura, la ortografía son algunos factores que deben considerarse al analizar los datos de las redes sociales. Esta presentación propone un método para clasificar datos relacionados con accidentes de tránsito de twitter. Tres etapas componen el método. La primera etapa establece la forma en que se pueden obtener los datos de la red social. El segundo propone dos técnicas basadas en TF-IDF y Doc2vec para limpiar y normalizar los datos. Finalmente, la tercera etapa presenta un modelo de clasificación para obtener datos relacionados con accidentes de tránsito.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nestor Suat</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/extraccion-y-analisis-de-informacion-de-accidentes-de-transito-desde-redes-sociales.html</guid></item><item><title>Fundamentals of Bayesian Analysis with PyMC3 and TensorFlow Probability</title><link>https://pyvideo.org/scipy-latin-america-2019/fundamentals-of-bayesian-analysis-with-pymc3-and-tensorflow-probability.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While lots of cutting-edge ML/DL algorithms are yielding amazing results, the APIs and environments which wrap them up tend to be so high-level that practitioners do not always get to understand the logic underneath. This talk is intended to take a direct look into a specific branch of statistical analysis which is very used from years ago in probabilistic learning issues, by explaining the core concepts and exploring such tools as PyMC3 and TensorFlow Probability (TFP). Even though several cutting-edge frameworks allow to abstract the way 'intelligent' algorithms make decisions at a high-level, probabilistic and bayesian modeling often provide an increase in the ability to interpret model parameters and results, along with a reasonable handling for uncertainty. Nowadays, advances in computational statistics and software capacities help the average user enhance their decision making processes through specialized software libraries such as PyMC3 and TFP, which provide data scientists/analysts with a robust set of tools for probabilistic modeling and inference.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sebastián Arango</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/fundamentals-of-bayesian-analysis-with-pymc3-and-tensorflow-probability.html</guid></item><item><title>Get your hyperparameters right! How to tune your machine learning models with SciPy</title><link>https://pyvideo.org/scipy-latin-america-2019/get-your-hyperparameters-right-how-to-tune-your-machine-learning-models-with-scipy.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I would like to tackle the problem of hyperparameter tuning in Machine Learning (ML). In particular, I would like to show how SciPy and its optimization tools can help in determining the best combination of parameters for our particular ML model. The idea is to perform an optimization routine over the hyperparameter space, where the objective function is a metric that evaluates the model's results. As an example, I will analyze text data from different sources (press, books, tweets, ...) and perform a Part-Of-Speech Tagging implemented with an Averaged Perceptron. I will then tune the hyperparameters of the algorithm by optimizing over different metrics and comparing the prediction results. In conclusion, the goal of this talk is to show the outstanding capabilities of the SciPy optimization modules, as well as to exemplify the multidisciplinarity of this scientific python library by extending its reach to artificial intelligence applications.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">María Remolina</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/get-your-hyperparameters-right-how-to-tune-your-machine-learning-models-with-scipy.html</guid></item><item><title>Julia: a next generation language</title><link>https://pyvideo.org/scipy-latin-america-2019/julia-a-next-generation-language.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Is often the case that we build our numerical and data pipelines on very safe and fast libraries that communicate with Python. New types of hardware accelerators such as GPU and the new software stack 2.0 requires breathtaking approaches that were not contemplated in the past. In this talk, we will talk about Julia, the Python-Julia ecosystem, and numerical problems that requires us, to write more close to the hardware and how Julia allows to maintain expressiveness without sacrificing speed, we walk over, programming GPGPUs on Julia, and sample Neural Ordinary Differential Equations and take a fresh look on the Julia Language. We will explore how is the performance obtained on very simple tasks and (if time permits) we will look at the emitted code on the LLVM IR, the CUDA paradigm on Julia, and how Julia can help you to write sample code on foreign architectures such as Google TPUs.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Cardozo</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/julia-a-next-generation-language.html</guid></item><item><title>Keynote: SciPy and NumFOCUS communities: past, present, and future</title><link>https://pyvideo.org/scipy-latin-america-2019/keynote-scipy-and-numfocus-communities-past-present-and-future.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Travis Oliphant</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/keynote-scipy-and-numfocus-communities-past-present-and-future.html</guid></item><item><title>Learning to classify the heavens...and helping physics along the way</title><link>https://pyvideo.org/scipy-latin-america-2019/learning-to-classify-the-heavensand-helping-physics-along-the-way.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The talk is aimed at disseminating the work done over the past 2 years in constructing a data set for astronomical transient events (objects in the sky with aperiodic behaviour...more in the talk :D) and classifying them using state of the art machine learning methods. The abstract of the paper that summarizes our work can be found below: We introduce MANTRA, an annotated dataset of 4869 transient and 16940 non-transient object lightcurves built from the Catalina Real Time Transient Survey. We provide public access to this dataset as a plain text file to facilitate standardized quantitative comparison of astronomical transient event recognition algorithms. Some of the classes included in the dataset are: supernovae, cataclismic variables, active galactic nuclei, high proper motion stars, blazars and flares. As a complement to the dataset, we experiment with multiple data pre-processing methods, feature selection techniques and popular machine learning algorithms (Support Vector Machines, Random Forests and Neural Networks). We assess quantitative performance in two classification tasks: binary (transient/non-transient) and eight-class classification. The best performing algorithm in both task is the Random Forest Classifier. It achieves an F1-score of 86.61%in the binary classification and 50.38%in the eight-class classification. For the latter, the class with the highest F1-score are non-transients (87.12%)and the lowest corresponds to flares (11.96%); for supernovae it achieves a value of 50.07%, close to the average across classes.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mauricio Neira</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/learning-to-classify-the-heavensand-helping-physics-along-the-way.html</guid></item><item><title>Máquinas de aprendizaje para análisis de datos geoespaciales en procesos de clasificación de cobertura terrestre</title><link>https://pyvideo.org/scipy-latin-america-2019/maquinas-de-aprendizaje-para-analisis-de-datos-geoespaciales-en-procesos-de-clasificacion-de-cobertura-terrestre.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;El procesamiento digital de imágenes está estrictamente ligado a los software y se ha convertido en una herramienta necesaria para procesar, analizar y presentar resultados, por este motivo, diferentes empresas y públicas han desarrollado proyectos para analizar y gestionar de una manera eficiente este tipo de información, una de ellas es el IDEAM que ha desarrollado la adaptación del OPEN DATACUBE de imágenes de satélite para Colombia, esta plataforma permite realizar el procesamiento, análisis y clasificación de las imágenes del cubo en lenguaje Python, por medio de algoritmos desarrollados que se conectan a la base de datos del cubo de datos de imágenes de satélite de Colombia, este cuenta con datos de sensores remotos ópticos y de radar tomadas periódicamente desde hace 19 años. Los datos del cubo cuentan con una estructura estándar y las bandas de las imágenes se pueden consultar como matrices en formato xarray , lo cual permite realizar operaciones y generar algoritmos sobre las imágenes digitales de una manera más sencilla. La industria del procesamiento digital de imágenes de satélite ha venido desarrollando una serie de algoritmos específicos para el tratamiento de las mismas en busca de la optimización de resultados, dentro de la propuesta actual se presenta una clasificación supervisada semiautomática en jupyter notebook con máquinas de aprendizaje (Machine Learning), utilizando librerías como scikit-learn y numpy. La metodología se desarrolla con la selección y apilamiento de imágenes consultadas del cubo de datos manteniendo la estructura de los distintos espectros(bandas espectrales) ejecutando algunas mascaras que permiten la mejora de los datos, posteriormente se emplean las maquinas de aprendizaje con una incorporación de modelos de entrenamiento o datos vector que servirán como insumo para la clasificación, en esta oportunidad se realiza la incorporación de varios clasificadores para obtener una imagen clasificada optimizada controlando cada paso en el procedimiento, finalmente con la imagen clasificada con coberturas se realiza una validación cruzada de resultados y una validación de exactitud temática, en comparación con los procesamientos de clasificación de otros tipos software, con esta metodología podemos emplear más de un insumo de entrada (diferentes imágenes o dem) y clasificar por más de un algoritmo, obteniendo mejores resultados temáticos sin perder la información de localización espacial de los insumos empleando la librería gdal. En conclusión el manejo de datos en plataformas como OPEN DATACUBE permiten una trazabilidad de los procesos y errores obtenidos en los mismos, obteniendo una identificación de la cobertura más acertada a la realidad con un seguimiento temporal de resultados realizando operaciones sobre los datos base que favorecen una la metodología de trabajo particular y resultante respecto al área específica de investigación.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yilsey Benavides</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/maquinas-de-aprendizaje-para-analisis-de-datos-geoespaciales-en-procesos-de-clasificacion-de-cobertura-terrestre.html</guid></item><item><title>Maximizing Churn Campaign Profitability with Cost-Sensitive Machine Learning</title><link>https://pyvideo.org/scipy-latin-america-2019/maximizing-churn-campaign-profitability-with-cost-sensitive-machine-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Customer churn predictive modeling deals with predicting the probability of a customer defecting using historical, behavioral and socio-economical information. This tool is of great benefit to subscription based companies allowing them to maximize the results of retention campaigns. The problem of churn predictive modeling has been widely studied by the data mining and machine learning communities. It is usually tackled by using classification algorithms in order to learn the different patterns of both the churners and non-churners. Nevertheless, current state-of-the-art classification algorithms are not well aligned with commercial goals, in the sense that, the models miss to include the real financial costs and benefits during the training and evaluation phases. In the case of churn, evaluating a model based on a traditional measure such as accuracy or predictive power, does not yield the best results when measured by the actual financial cost, ie. investment per subscriber on a loyalty campaign and the financial impact of failing to detect a real churner versus wrongly predicting a non-churner as a churner. In this talk, I will present a cost-sensitive framework for customer churn predictive modeling using CostCla a cost-sensitive classification python library. First we define a financial based measure for evaluating the effectiveness of a churn campaign taking into account the available portfolio of offers, their individual financial cost and probability of offer acceptance depending on the customer profile. Then, using a real-world churn dataset from Rappi, we compared different cost-insensitive and cost-sensitive classification algorithms and measure their effectiveness based on their predictive power and also the cost optimization. The results show that using a cost-sensitive approach yields to an increase in cost savings of up to 26.4 %.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alejandro Correa</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/maximizing-churn-campaign-profitability-with-cost-sensitive-machine-learning.html</guid></item><item><title>Optimization-based design of wireless sensor networks for gas monitoring systems in underground mines</title><link>https://pyvideo.org/scipy-latin-america-2019/optimization-based-design-of-wireless-sensor-networks-for-gas-monitoring-systems-in-underground-mines.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The process of extracting coal from underground mines produces toxic and explosive gases that cause negative health effects of the miners. Monitoring the concentration of gases in underground coal mines is a mandatory process that may save lives. The health and safety of miners improves when technologies are implemented to monitor gas concentrations and provide early alarms. Wireless Sensor Networks (WSNs) can alert mine personnel when dangerous levels of gases are detected, such as methane, carbon monoxide and hydrogen sulfide, which are highly explosive and highly toxic to humans. However, the design of a WSN is not trivial, it is necessary to find the number of nodes, node location, and data routing to address two important objectives: maximizing the WSN lifetime and minimizing the costs related with the number of sensors deployed. We propose an integer optimization approach to assist the design of WSNs for gas monitoring, that allows users to address both the cost of installation and the WSN lifetime. The proposed methodology was implemented in Python, using Gurobi as our optimization solver. We developed a software to graphically represent the obtained solutions in order to facilitate stakeholders' analysis and decision making. The software uses images of the mine's layout and use the Pillow library to draw nodes and edges of the WSN. The algorithm has been applied to three case studies of mines located in Boyacá (Colombia). We also use the Networkx Python package to design random graphs and simulate larger mines to test the performance of the algorithm.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Iván David Alfonso</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/optimization-based-design-of-wireless-sensor-networks-for-gas-monitoring-systems-in-underground-mines.html</guid></item><item><title>¿Qué tanto se mueven nuestros ríos?</title><link>https://pyvideo.org/scipy-latin-america-2019/que-tanto-se-mueven-nuestros-rios.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A través de esta charla se podrán descubrir cuáles han sido los patrones que el río más importante de Colombia, el río Magdalena, ha tenido en su migración lateral a través del análisis de más de 40 años de imágenes satelitales. Para descifrar lo que el río tiene por decirnos en este ámbito, se utilizó un enfoque distinto a los estudios convencionales que tratan sobre el tema. Mediante la aplicación de la técnica de Random Forest y la visualización de datos interactivos, se llegó a la conclusión que el río ha presentado en la ventana de tiempo analizada, tasas máximas anuales de movimiento lateral cercanas a los 220 m/año de erosión y de 273 m/año de sedimentación. Además, se tiene la posibilidad de observar cuales han sido los comportamientos año a año del tramo de estudio y se comprueba la eficacia de nuevas técnicas en la clasificación y predicción de fenómenos fluviales.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alexander Montalvo</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/que-tanto-se-mueven-nuestros-rios.html</guid></item><item><title>Recommendations at Rappi</title><link>https://pyvideo.org/scipy-latin-america-2019/recommendations-at-rappi.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will introduce the recommendation system topic, what are the main goals, requirements, best practices and benchmarks. During the talk we will go over all the data pipeline architecture we use at Rappi required to make different kind of recommendations depending on which step of the funnel the user is. Starting from data collection going through Machine Learning model, system architecture and how we mix Data Science and UX, to improve them. Data collection requires an analytics infrastructure and transactional data processed, aggregated and available to the Machine Learning Models. Depending on the funnel step, there are different objectives for the recommendations and so different metrics to evaluate the model. Operationalizing an ML product is not trivial, so we will also talk about how to expose our ML model to system, including software engineering best practices. Differences between offline / batch predictions and real time predictions. Hypothesis and experimentations, A/B Testing. Finally we will show the impact of User Experience Designer in a Data Product, specially for recommendations.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ariel Wolfmann</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/recommendations-at-rappi.html</guid></item><item><title>SpectralNET: redes neuronales e hidrocarburos</title><link>https://pyvideo.org/scipy-latin-america-2019/spectralnet-redes-neuronales-e-hidrocarburos.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En este estudio se exploran técnicas de aprendizaje computacional profundo con el fin de predecir las propiedades físicas de los hidrocarburos a partir de firmas espectrales, este tipo particular de información se describe como un vector continuo que mide la reflectancia de la luz sobre cualquier objeto en función del espectro electromagnético, este tipo de dato es usado por la NASA para la identificación de agujeros negros. Por lo que su comprensión y uso en la industria petrolera es fundamental para la identificación y caracterización de manifestaciones de hidrocarburo en superficie (rezumaderos). Por consiguiente se quiere aprovechar el potencial de las firmas espectrales para predecir la propiedad física de la gravedad API, la cual describe la relación de densidades entre el hidrocarburo y el agua. A partir de dicha relación, se puede estimar cómo será el comportamiento de los fluidos en el yacimiento, construyendo las bases para la planeación estratégica de los campos petroleros. La presente propuesta se concibe como una herramienta predictiva compuesta de dos partes. La primera parte, especializada en el pre-procesamiento de firmas espectrales, donde se extraen las bandas más relevantes de acuerdo con técnicas de selección de variables, se realiza un proceso de limpieza e imputación de datos; y la segunda, donde se crean dos modelos de aprendizaje profundo, uno para las firmas espectrales de campo y otro para las firmas de las imágenes del satélite WorldView3. De esta manera, la herramienta tiene el objetivo de aprender los patrones de absorción del hidrocarburo dentro de una firma espectral, generalizando este aprendizaje para predecir dicha propiedad en los rezumaderos de la cuenca en estudio. SpectralNET, fue desarrollada en Python sobre la plataforma Google-Colab usando todo el potencial de este lenguaje en inteligencia artificial, la elegancia de su sintaxis para lograr las mejores eficiencias en el desempeño de los modelos y por su puesto todo el estado del arte en librerías de ETL, modelaje y visualización de datos que han permitido simplicidad y robustez en la investigación. Como resultado se consigue un RMSE de 2.6 para las firmas de campo y RMSE 5.1 para las firmas extraídas de las imágenes, siendo muy satisfactorio si se compara con el error teórico bayesiano calculado en laboratorio de 1.5 grados API, estos resultados permiten la identificación y comprensión de nuevas estructuras petrolíferas, la prevención de desastres ambientales dada la migración vertical de crudo a superficie permitiend o monitorear anticipadamente las microfugas, con ahorros que pueden llegar hasta $2MUSD. Como investigación futura, se propone probar SpectralNet sobre imágenes satelitales de Aster y Landsat, así mismo extrapolar los modelos a otras propiedades físicas e investigar la resolución espectral que puedan llevar a obtener un modelo más parsimonioso y posiblemente más preciso.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sergio Castelblanco</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/spectralnet-redes-neuronales-e-hidrocarburos.html</guid></item><item><title>Testing in machine-learning based systems</title><link>https://pyvideo.org/scipy-latin-america-2019/testing-in-machine-learning-based-systems.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There's a big difference between a machine-learning model in a Jupyter Notebook and a 24/7, high-performance, highly-available, high-throughput, online service. You won't find easily how to go from one to the other: not in tutorials nor in scientific publications. In this talk we'll review some recommendations and industry's best-practices, for a robust lifecycle of a production-ready machine-learning related project. We will cover some taxonomy of topics to pay attention, with some suggested paths of action. We are going to start by defining the phases we identified in which a machine learning project can be: discovery, MVP building, or production-improving-and-maintenance. Later, describe a rich and deep set of different practices that can help with targeting risks, reducing, of even eliminating them. By the end, there is a section of recommendations about which of these practices, in our experience, fit better for each of the previously mentioned phases of a project.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Javier Mansilla</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/testing-in-machine-learning-based-systems.html</guid></item><item><title>Un modelo de posicionamiento previo de suministros de emergencia para respuesta a sismos en la ciudad de Bogotá utilizando Python</title><link>https://pyvideo.org/scipy-latin-america-2019/un-modelo-de-posicionamiento-previo-de-suministros-de-emergencia-para-respuesta-a-sismos-en-la-ciudad-de-bogota-utilizando-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;El posicionamiento previo de los suministros de emergencia es un mecanismo para aumentar la preparación ante desastres naturales. Para ello es de gran importancia la selección de proveedores en logística humanitaria y la planeación de la posible distribución de estos artículos después de que ocurra el suceso. En este trabajo se implementa un modelo de optimización estocástica de dos etapas que proporciona una estrategia de pre-posicionamiento de respuesta de emergencia para terremotos en la ciudad de Bogotá y un algoritmo heurístico para resolver instancias del problema a gran escala, ambos métodos utilizando los paquetes Pandas y Gurobi de Python. El desarrollo del proyecto permite ver la flexibilidad que ofrece Python para replicar rápidamente modelos complejos de la literatura para evaluar y comparar su desempeño. Se presenta un caso de estudio para la ciudad de Bogotá en donde se tienen escenarios estadísticamente distribuidos, evidenciando el impacto que genera el modelo y proporcionando insumos para la toma de decisiones. El objetivo de la charla es brindar a la audiencia el contexto necesario sobre problemas operativos en el manejo de riesgo de desastres, así como algunas nociones de optimización para desarrollar las ideas y modelos utilizados en la investigación. Posteriormente se discutirá la implementación computacional en Python, los resultados del proyecto. Particularmente, se discutirá el potencial de los paquetes de Python (e.g., Gurobi) para facilitar y agilizar el desarrollo de prototipos de modelos de optimización, simulación, entre otros.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Cuellar</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/un-modelo-de-posicionamiento-previo-de-suministros-de-emergencia-para-respuesta-a-sismos-en-la-ciudad-de-bogota-utilizando-python.html</guid></item><item><title>Visor Ambiental. Herramienta para integración de datos científicos en la intervención territorial sostenible.</title><link>https://pyvideo.org/scipy-latin-america-2019/visor-ambiental-herramienta-para-integracion-de-datos-cientificos-en-la-intervencion-territorial-sostenible.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Hoy en día la enorme cantidad de datos científicos y documentación específica a menudo resulta inaccesible. Estos grandes volúmenes de información producida por investigadores y consultores, abren oportunidades para que operadores, empresas y entes fiscalizadores compartan, integren y transfieran información valiosa a una base de datos común y dinámica, ayudando y fortaleciendo al desarrollo de intervenciones territoriales que consideren la conservación del ambiente. Los actuales desarrollos en tecnologías web (software) facilitan crear aplicaciones web exploratorias, vinculando datos científicos dentro de una herramienta interactiva logrando la accesibilidad de la información, garantizando la exploración y difusión de datos científicos apropiadamente. Los Visores de datos geográficos y ambientales pueden ser herramientas útiles para investigaciones, estudios de impacto ambiental y ordenamiento del territorio. El objetivo del Visor Web Ambiental es No depender de software complejo como las plataformas SIG. Pensamos en su diseño práctico, así cualquier persona maneja e investiga sus contenidos, sin conocimiento previo de geoinformática. El usuario accede intuitivamente e interactúa de manera simple, obteniendo información geográfica y ambiental fiable. Desarrollamos una plataforma de datos para condensar y sistematizar información geográfica y ambiental. Nuestro enfoque está destinado a fortalecer la consulta y transferencia de información de manera interactiva, dinámica y atractiva, destinada a investigadores y organismos que necesiten integrar datos confiables para desarrollar o fiscalizar intervenciones territoriales en un contexto de sostenibilidad ambiental promoviendo además la interacción de los sectores público y privado. Concomitantemente, establecimos procedimientos de actualización y verificación continua del Visor de datos proporcionando a los usuarios un mejor respaldo en sus intervenciones o toma de decisiones en la planificación e implementación de proyectos.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marcos Vaira</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:scipy-latin-america-2019/visor-ambiental-herramienta-para-integracion-de-datos-cientificos-en-la-intervencion-territorial-sostenible.html</guid></item><item><title>Charlas relámpago 2019-10-08</title><link>https://pyvideo.org/scipy-latin-america-2019/charlas-relampago-2019-10-08.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Unknown</dc:creator><pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-08:scipy-latin-america-2019/charlas-relampago-2019-10-08.html</guid><category>lightning talks</category></item></channel></rss>