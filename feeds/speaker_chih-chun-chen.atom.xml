<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_chih-chun-chen.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2015-06-19T00:00:00+00:00</updated><entry><title>How “good” is your model, and how can you make it better?</title><link href="https://pyvideo.org/pydata-london-2015/how-good-is-your-model-and-how-can-you-make-it-better.html" rel="alternate"></link><published>2015-06-19T00:00:00+00:00</published><updated>2015-06-19T00:00:00+00:00</updated><author><name>Chih-Chun Chen</name></author><id>tag:pyvideo.org,2015-06-19:pydata-london-2015/how-good-is-your-model-and-how-can-you-make-it-better.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;This hands-on tutorial will show you how to use scikit-learn’s model
evaluation functions to evaluate different models in terms of
accuracy and generalisability, and search for optimal parameter
configurations.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The objective of this tutorial is to give participants the skills
required to validate, evaluate and fine-tune models using scikit-learn’s
evaluation metrics and parameter search capabilities. It will combine
both the theoretical rationale behind these methods and their code
implementation.&lt;/p&gt;
&lt;p&gt;The session will be structured as follows (rough timings in
parentheses):&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Explanation of over-fitting and the bias-variance trade-off, followed
by a brief conceptual overview of cross-validation, bootstrapping,
and ensemble methods, in particular with respect to bias and
variance. Pointers to the corresponding scikit-learn functions will
also be given. (20 minutes)&lt;/li&gt;
&lt;li&gt;Implementation of cross-validation and grid-search method for
parameter tuning, using KNN classification as an illustrative
example. Participants will train two KNN neighbours with different
numbers of neighbours on preprocessed data (provided). They will then
be guided through cross-validation, plotting of results, and
grid-search to find the best neighbour and weight configuration(s).
(30 minutes)&lt;/li&gt;
&lt;li&gt;Comparison of different classification models using cross-validation.
Participants will implement a logistic regression, linear and
non-linear support vector machine (SVM) or neural network model and
apply the same cross-validation and grid search method as in the
guided KNN example. Participants will then compare their plots,
evaluate their results and discuss which model they might choose for
different objectives, trading off generalisability, accuracy, speed
and randomness. (70 minutes)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We assume participants will be familiar with numpy, matplotlib, and at
least the intuition behind some of the main classification algorithms.
Before the tutorial, participants with github accounts should fork from
&lt;a class="reference external" href="https://github.com/cambridgecoding/pydata-tutorial"&gt;https://github.com/cambridgecoding/pydata-tutorial&lt;/a&gt; or download the files
and iPython notebook so they can participate in the hands on activities.
Required libraries: numpy, scikit-learn, matplotlib, pandas, scipy,
multilayer_perceptron (provided)&lt;/p&gt;
</summary><category term="tutorial"></category></entry></feed>