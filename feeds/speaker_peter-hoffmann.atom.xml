<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_peter-hoffmann.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-07-25T00:00:00+00:00</updated><entry><title>Using Pandas and Dask to work with large columnar datasets in Apache Parquet</title><link href="https://pyvideo.org/europython-2018/using-pandas-and-dask-to-work-with-large-columnar-datasets-in-apache-parquet.html" rel="alternate"></link><published>2018-07-25T00:00:00+00:00</published><updated>2018-07-25T00:00:00+00:00</updated><author><name>Peter Hoffmann</name></author><id>tag:pyvideo.org,2018-07-25:europython-2018/using-pandas-and-dask-to-work-with-large-columnar-datasets-in-apache-parquet.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="section" id="apache-parquet-data-format"&gt;
&lt;h4&gt;Apache Parquet Data Format&lt;/h4&gt;
&lt;p&gt;Apache Parquet is a binary, efficient columnar data format. It uses
various techniques to store data in a CPU and I/O efficient way like row
groups, compression for pages in column chunks or dictionary encoding
for columns. Index hints and statistics to quickly skip over chunks of
irrelevant data enable efficient queries on large amount of data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="apache-parquet-with-pandas-dask"&gt;
&lt;h4&gt;Apache Parquet with Pandas &amp;amp; Dask&lt;/h4&gt;
&lt;p&gt;Apache Parquet files can be read into Pandas DataFrames with the two
libraries fastparquet and Apache Arrow. While Pandas is mostly used to
work with data that fits into memory, Apache Dask allows us to work with
data larger then memory and even larger than local disk space. Data can
be split up into partitions and stored in cloud object storage systems
like Amazon S3 or Azure Storage.&lt;/p&gt;
&lt;p&gt;Using Metadata from the partiton filenames, parquet column statistics
and dictonary filtering allows faster performance for selective queries
without reading all data. This talk will show how use partitioning, row
group skipping and general data layout to speed up queries on large
amount of data.&lt;/p&gt;
&lt;/div&gt;
</summary></entry><entry><title>12 Factor Apps for Data-Science with Python</title><link href="https://pyvideo.org/swiss-python-summit-2018/12-factor-apps-for-data-science-with-python.html" rel="alternate"></link><published>2018-02-16T00:00:00+00:00</published><updated>2018-02-16T00:00:00+00:00</updated><author><name>Peter Hoffmann</name></author><id>tag:pyvideo.org,2018-02-16:swiss-python-summit-2018/12-factor-apps-for-data-science-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Heroku distilled their principles to build modern cloud applications to maximize developer productivity and application maintainability in the in the &lt;a class="reference external" href="https://12factor.net"&gt;https://12factor.net&lt;/a&gt; manifesto. These principles have influenced many of our design decisions at Blue Yonder.&lt;/p&gt;
&lt;p&gt;While our data scientists care about machine learning models and statistics, we want to free them of being concerned with technicalities like maintenance of network equipment, operating system updates or even hardware failures. In order to save our data scientists from these tasks, we have invested into a data science platform.&lt;/p&gt;
&lt;p&gt;This talk will give an insight how we use Apache Mesos, Devpi, Graylog and Prometheus/Graphana to provide a developer-friendly environment for data scientists to build their own distributed applications in Python without having to care about servers or scaling.&lt;/p&gt;
</summary></entry><entry><title>Infrastructure as Python Code: Deploying your Web Services on Microsoft Azure</title><link href="https://pyvideo.org/pyconweb-2017/infrastructure-as-python-code-deploying-your-web-services-on-microsoft-azure.html" rel="alternate"></link><published>2017-05-28T00:00:00+00:00</published><updated>2017-05-28T00:00:00+00:00</updated><author><name>Peter Hoffmann</name></author><id>tag:pyvideo.org,2017-05-28:pyconweb-2017/infrastructure-as-python-code-deploying-your-web-services-on-microsoft-azure.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will give an overview on how to deploy web services on the Azure Cloud with different tools like Azure Resource Manager Templates, the Azure SDK for Python and the Azure module for Ansible and present best practices learned while moving a company into the Azure Cloud.&lt;/p&gt;
</summary></entry><entry><title>Infrastructure as Python Code: Run your Services on Microsoft Azure</title><link href="https://pyvideo.org/europython-2017/infrastructure-as-python-code-run-your-services-on-microsoft-azure.html" rel="alternate"></link><published>2017-07-11T00:00:00+00:00</published><updated>2017-07-11T00:00:00+00:00</updated><author><name>Peter Hoffmann</name></author><id>tag:pyvideo.org,2017-07-11:europython-2017/infrastructure-as-python-code-run-your-services-on-microsoft-azure.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Using Infrastructure-as-Code principles with configuration through
machine processable definition files in combination with the adoption
of cloud computing provides faster feedback cycles in
development/testing and less risk in deployment to production. The
Microsoft Azure Cloud (&lt;a class="reference external" href="https://azure.microsoft.com/"&gt;https://azure.microsoft.com/&lt;/a&gt;) allows different
ways to provision, deploy and run your python  service:&lt;/p&gt;
&lt;p&gt;The Azure Resource Manger Templates
(&lt;a class="reference external" href="https://azure.microsoft.com/en-us/resources/templates/"&gt;https://azure.microsoft.com/en-us/resources/templates/&lt;/a&gt;) allows you
to provision your application using a declarative template. With
parameters, variables and Azure template functions, the same template
can be used to deploy your application in different stages (dev,
test, production) and environments for different customers. We open
sourced the tropo library (&lt;a class="reference external" href="https://pypi.python.org/pypi/tropo/"&gt;https://pypi.python.org/pypi/tropo/&lt;/a&gt;) to
create Azure Resource Templates from python.&lt;/p&gt;
&lt;p&gt;Azure SDK for Python (&lt;a class="reference external" href="http://azure-sdk-for-python.readthedocs.io"&gt;http://azure-sdk-for-python.readthedocs.io&lt;/a&gt;) for
a low level access to manage resources in the Azure Cloud.&lt;/p&gt;
&lt;p&gt;An Azure Ansible Module
(&lt;a class="reference external" href="https://docs.ansible.com/ansible/guide_azure.html"&gt;https://docs.ansible.com/ansible/guide_azure.html&lt;/a&gt;) based on the
Azure SDK to automate software provisioning, configuration
management, and application deployment in a single environment.&lt;/p&gt;
&lt;p&gt;Each of the alternatives has different strengths and drawbacks.
Presenting our learnings from migrating our infrastructure into the
Azrue Cloud will help to avoid common pitfalls and show deployment
patterns that will ease the live of devops.&lt;/p&gt;
</summary></entry><entry><title>PySpark - Data processing in Python on top of Apache Spark.</title><link href="https://pyvideo.org/europython-2015/pyspark-data-processing-in-python-on-top-of-apache-spark.html" rel="alternate"></link><published>2015-08-03T00:00:00+00:00</published><updated>2015-08-03T00:00:00+00:00</updated><author><name>Peter Hoffmann</name></author><id>tag:pyvideo.org,2015-08-03:europython-2015/pyspark-data-processing-in-python-on-top-of-apache-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Peter Hoffmann - PySpark - Data processing in Python on top of Apache Spark.
[EuroPython 2015]
[22 July 2015]
[Bilbao, Euskadi, Spain]&lt;/p&gt;
&lt;p&gt;[Apache Spark][1] is a computational engine for large-scale data processing. It
is responsible for scheduling, distribution and monitoring applications which
consist of many computational task across many worker machines on a computing
cluster.&lt;/p&gt;
&lt;p&gt;This Talk will give an overview of PySpark with a focus on Resilient
Distributed Datasets and the DataFrame API. While Spark Core itself is written
in Scala and runs on the JVM, PySpark exposes the Spark programming model to
Python. It defines an API for Resilient Distributed Datasets (RDDs). RDDs are a
distributed memory abstraction that lets programmers perform in-memory
computations on large clusters in a fault-tolerant manner. RDDs are immutable,
partitioned collections of objects. Transformations construct a new RDD from a
previous one. Actions compute a result based on an RDD. Multiple
computation steps
are expressed as directed acyclic graph (DAG). The DAG execution model is
a generalization of the Hadoop MapReduce computation model.&lt;/p&gt;
&lt;p&gt;The Spark DataFrame API was introduced in Spark 1.3. DataFrames envolve Spark's
RDD model and are inspired by Pandas and R data frames. The API provides
simplified operators for filtering, aggregating, and projecting over large
datasets. The DataFrame API supports diffferent data sources like JSON
datasources, Parquet files, Hive tables and JDBC database connections.&lt;/p&gt;
&lt;p&gt;Resources:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;[An Architecture for Fast and General Data Processing on Large Clusters][2] Matei Zaharia&lt;/li&gt;
&lt;li&gt;[Spark][6] Cluster Computing with Working Sets - Matei Zaharia et al.&lt;/li&gt;
&lt;li&gt;[Resilient Distributed Datasets][5] A Fault-Tolerant Abstraction for In-Memory Cluster Computing -Matei Zaharia et al.&lt;/li&gt;
&lt;li&gt;[Learning Spark][3] Lightning Fast Big Data Analysis - Oreilly&lt;/li&gt;
&lt;li&gt;[Advanced Analytics with Spark][4] Patterns for Learning from Data at Scale - Oreilly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[1]: &lt;a class="reference external" href="https://spark.apache.org"&gt;https://spark.apache.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2]: &lt;a class="reference external" href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf"&gt;http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3]: &lt;a class="reference external" href="http://shop.oreilly.com/product/0636920028512.do"&gt;http://shop.oreilly.com/product/0636920028512.do&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4]: &lt;a class="reference external" href="http://shop.oreilly.com/product/0636920035091.do"&gt;http://shop.oreilly.com/product/0636920035091.do&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5]: &lt;a class="reference external" href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf"&gt;https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6]: &lt;a class="reference external" href="http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf"&gt;http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf&lt;/a&gt;&lt;/p&gt;
</summary><category term="pyspark"></category></entry><entry><title>SQLAlchemy as the backbone of a Data Science company</title><link href="https://pyvideo.org/europython-2016/sqlalchemy-as-the-backbone-of-a-data-science-company.html" rel="alternate"></link><published>2016-08-05T00:00:00+00:00</published><updated>2016-08-05T00:00:00+00:00</updated><author><name>Peter Hoffmann</name></author><id>tag:pyvideo.org,2016-08-05:europython-2016/sqlalchemy-as-the-backbone-of-a-data-science-company.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Peter Hoffmann - SQLAlchemy as the backbone of a Data Science company
[EuroPython 2016]
[20 July 2016]
[Bilbao, Euskadi, Spain]
(&lt;a class="reference external" href="https://ep2016.europython.eu//conference/talks/sqlalchemy-as-the-backbone-of-a-data-science-company"&gt;https://ep2016.europython.eu//conference/talks/sqlalchemy-as-the-backbone-of-a-data-science-company&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;In times of NoSQL databases and Map Reduce Algorithms it's surprising how far
you can scale the relational data model. At &lt;a class="reference external" href="http://blue-yonder.com"&gt;Blue Yonder&lt;/a&gt;
we use SQLAlchemy in all stages of our data science workflows and handle tenth
of billions of records to feed our predictive algorithms. This talk will dive
into SQLAlchemy beyond the Object Relational Mapping (ORM) parts and conentrate
on the SQLAlchemy Core API, the Expression Language and Database Migrations
with Alembic.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;p&gt;In times of NoSQL databases and Map Reduce Algorithms it's surprising how far
you can scale the relational data model. At &lt;a class="reference external" href="http://blue-yonder.com"&gt;Blue Yonder&lt;/a&gt;
we use SQLAlchemy in all stages of our data science workflows and handle tenth
of billions of records to feed our predictive algorithms. This talk will dive
into SQLAlchemy beyond the Object Relational Mapping (ORM) parts and conentrate
on the SQLAlchemy Core API and the Expression Language:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Database Abstraction&lt;/strong&gt;: Statements are generated properly for different database vendor and type without you having to think about it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Database input is escaped and sanitized prior to beeing commited to the database. This prevents against common SQL injection attacks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Composability and Reuse&lt;/strong&gt;: Common building blocks of queries are expressed as SQLAlchemy selectables and can be reuesd in other queries.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Testability&lt;/strong&gt;: SQLAlchemy allows you to perform functional tests against a database or mock out queries and connections.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reflection&lt;/strong&gt;: Reflection is a technique that allows you to generate a SQLAlchemy repesentation from an existing database. You can reflect tables, views, indexes, and foreign keys.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result of the usage of SQLAlchemy in Blue Yonder, we have implemented and
open sourced a SQLAlchemy dialect for the in memory, column-oriented database
system &lt;a class="reference external" href="https://github.com/blue-yonder/sqlalchemy_exasol"&gt;EXASolution&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>log everything with logstash and elasticsearch</title><link href="https://pyvideo.org/europython-2014/log-everything-with-logstash-and-elasticsearch.html" rel="alternate"></link><published>2014-07-22T00:00:00+00:00</published><updated>2014-07-22T00:00:00+00:00</updated><author><name>Peter Hoffmann</name></author><id>tag:pyvideo.org,2014-07-22:europython-2014/log-everything-with-logstash-and-elasticsearch.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;When your application grows beyond one machine you need a central space
to log, monitor and analyze what is going on. Logstash and elasticsearch
let you store your logs in a structured way. Kibana is a web fronted to
search and aggregate your logs. The talk gives an overview on how to add
centralized, structured logging to a python application running on
multiple servers. It focuses on useful patterns and shows the benefits
from structured logging.&lt;/p&gt;
</summary></entry></feed>