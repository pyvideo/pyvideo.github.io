<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - infrastructure</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_infrastructure.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2022-06-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Snakes on a cloud: the OpenStack project</title><link href="https://pyvideo.org/europython-2011/snakes-on-a-cloud-the-openstack-project.html" rel="alternate"></link><published>2011-07-24T00:00:00+00:00</published><updated>2011-07-24T00:00:00+00:00</updated><author><name>Thierry Carrez</name></author><id>tag:pyvideo.org,2011-07-24:/europython-2011/snakes-on-a-cloud-the-openstack-project.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Thierry Carrez - 23 June 2011 in &amp;quot;Track Lasagne&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;OpenStack is an innovative open source project written in Python, backed
by Rackspace Hosting and NASA, building a massively-scalable and
reliable cloud computing platform.&lt;/p&gt;
&lt;p&gt;The first part of this talk will clarify the place of OpenStack in the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Thierry Carrez - 23 June 2011 in &amp;quot;Track Lasagne&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;OpenStack is an innovative open source project written in Python, backed
by Rackspace Hosting and NASA, building a massively-scalable and
reliable cloud computing platform.&lt;/p&gt;
&lt;p&gt;The first part of this talk will clarify the place of OpenStack in the
general &amp;quot;cloud&amp;quot; landscape and explain why a fully open cloud
infrastructure stack is necessary to avoid vendor lock-in. We'll then
focus on the OpenStack project goals, its developer community, its open
design and release processes, and the developer tools it chose.&lt;/p&gt;
&lt;p&gt;The second part of the talk will present into more technical details the
different components of OpenStack: Nova (compute) and Swift (storage),
including the Python libraries that are used (libvirt, SQLAlchemy,
eventlet…). A Q&amp;amp;A session at the end of the talk will give the audience
a chance to clear any remaining dark area.&lt;/p&gt;
</content><category term="EuroPython 2011"></category><category term="cloud"></category><category term="design"></category><category term="hosting"></category><category term="infrastructure"></category><category term="openstack"></category><category term="python,"></category></entry><entry><title>What is Google App Engine?</title><link href="https://pyvideo.org/europython-2011/what-is-google-app-engine.html" rel="alternate"></link><published>2011-07-24T00:00:00+00:00</published><updated>2011-07-24T00:00:00+00:00</updated><author><name>Wesley J. Chun</name></author><id>tag:pyvideo.org,2011-07-24:/europython-2011/what-is-google-app-engine.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] wesley chun - 23 June 2011 in &amp;quot;Track Tagliatelle&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Google App Engine is a unique hosting platform that lets you build
applications and run them in Google's data centers using the massive
global infrastructure built to run the Internet's most powerful company.
App Engine offers a development …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] wesley chun - 23 June 2011 in &amp;quot;Track Tagliatelle&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Google App Engine is a unique hosting platform that lets you build
applications and run them in Google's data centers using the massive
global infrastructure built to run the Internet's most powerful company.
App Engine offers a development environment that uses familiar
technologies (Java and Python) and provides a powerful and robust set of
APIs to users while maintaining security and independence from other
apps running in the cloud. It is always free to get started so you can
try it out with no risk, and if you need additional computing resources,
you can purchase additional computing resources beyond the free quota
limits. (If you enable billing and trust us with your credit card, we
will extend your free quotas even further; you won't get charged until
you exceed those &lt;em&gt;extended&lt;/em&gt; quotas.) Scale your application to millions
of users and pay only for what you use at competitive market pricing.&lt;/p&gt;
&lt;p&gt;In this session, we provide an update of the newest features found in
the most recent releases of the App Engine platform. We also share some
suggestions for best practices to existing App Engine developers.&lt;/p&gt;
&lt;p&gt;Beginners to the App Engine platform will be interested in the
introductory workshop which may be offered (see description below).&lt;/p&gt;
&lt;p&gt;Google App Engine workshop&lt;/p&gt;
&lt;p&gt;In this tutorial, we'll give you a comprehensive introduction to the
platform in two/three components:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1-hour Introduction to Cloud computing and Google App Engine seminar&lt;/li&gt;
&lt;li&gt;3-hour App Engine hands-on workshop/codelab&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the first hour, we review Cloud Computing as an industry and where
Google App Engine fits into the picture. Specifically, we discuss App
Engine as a PaaS solution because of the inherent challenges of building
web and other applications. We'll outline the architecture of App
Engine, what it's major components are, introduce its features and APIs,
discuss the service and how it works (including information on the free
quotas), present some information about current users and usage,
including integration with Google Apps, and finally, give an overview of
its enterprise edition called Google App Engine for Business.&lt;/p&gt;
&lt;p&gt;After the approximately one-hour lecture, we'll show you how to create
applications that run on App Engine by building a simple but real web
application from the ground up via a hands-on coding laboratory.
Although based on the online tutorial, this codelab goes up and beyond
what's in the documentation: you will get a more detailed step-by-step
instructions to replicate that example as well as have the opportunity
to extend your application with some of the newer APIs that come with
App Engine. The codelab will cover the Users service, non-relational
Datastore, and Memcache APIs. Time-permitting, we'll also discuss some
of the newest features found in recent App Engine releases.&lt;/p&gt;
</content><category term="EuroPython 2011"></category><category term="architecture"></category><category term="cloud"></category><category term="google"></category><category term="hosting"></category><category term="infrastructure"></category><category term="memcache"></category><category term="security"></category><category term="web"></category></entry><entry><title>Playing tasks with Django-Celery</title><link href="https://pyvideo.org/europython-2011/playing-tasks-with-django-celery.html" rel="alternate"></link><published>2011-07-21T00:00:00+00:00</published><updated>2011-07-21T00:00:00+00:00</updated><author><name>Mauro Rocco</name></author><id>tag:pyvideo.org,2011-07-21:/europython-2011/playing-tasks-with-django-celery.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Mauro Rocco - 22 June 2011 in &amp;quot;Track Tagliatelle &amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Celery is an open source task queueing system based on distributed
message passing.&lt;/p&gt;
&lt;p&gt;I will talk about the tools that Celery offers for task distribution and
how to monitor and manage the system using a Django web interface …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Mauro Rocco - 22 June 2011 in &amp;quot;Track Tagliatelle &amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Celery is an open source task queueing system based on distributed
message passing.&lt;/p&gt;
&lt;p&gt;I will talk about the tools that Celery offers for task distribution and
how to monitor and manage the system using a Django web interface. This
talk will also focus on how we use Celery at Jamendo and our real
solutions to some common issues you may encounter when developing a
back-office based on Celery.&lt;/p&gt;
&lt;p&gt;The talk will cover the following topics:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A brief overview of Celery and the AMPQ protocol AMPQ protocol
overview, Celery introduction: Celery, RabbitMQ code examples&lt;/li&gt;
&lt;li&gt;The impact of Celery on the Jamendo work-flow; examples with real
tasks. Here I will talk about the Jamendo back-office infrastructure
and some of our common tasks. I will discuss the improvements made by
introducing a new back-office system based on Celery. I will show
some code snippets and go over some real scenarios.&lt;/li&gt;
&lt;li&gt;Overview of the Django Celery admin interface and some Jamendo
extensions. Let's talk about the Django-Celery interface that allows
one to monitor or schedule tasks directly from the Django admin. I
will explain which common additional features are necessary and how
to add them.&lt;/li&gt;
&lt;li&gt;Common &amp;quot;gotchas&amp;quot; we encountered while working with Celery and how we
solved them.&lt;/li&gt;
&lt;li&gt;Global task locks&lt;/li&gt;
&lt;li&gt;Centralized logging: be able to read all the logs of all celery
workers on different servers and filter them for real-time debugging&lt;/li&gt;
&lt;/ul&gt;
</content><category term="EuroPython 2011"></category><category term="celery"></category><category term="distributed"></category><category term="django"></category><category term="infrastructure"></category><category term="queueing"></category><category term="rabbitmq"></category><category term="real-time"></category><category term="web"></category></entry><entry><title>Building a hosting platform with Python</title><link href="https://pyvideo.org/europython-2011/building-a-hosting-platform-with-python.html" rel="alternate"></link><published>2011-07-20T00:00:00+00:00</published><updated>2011-07-20T00:00:00+00:00</updated><author><name>Andrew Godwin</name></author><id>tag:pyvideo.org,2011-07-20:/europython-2011/building-a-hosting-platform-with-python.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Andrew Godwin - 20 June 2011 in &amp;quot;Track Lasagne&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At ep.io we built a Python hosting platform from the ground up, designed
to run large numbers of web applications on a small number of physical
machines both securely and in a reasonably scalable way. This talk …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Andrew Godwin - 20 June 2011 in &amp;quot;Track Lasagne&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At ep.io we built a Python hosting platform from the ground up, designed
to run large numbers of web applications on a small number of physical
machines both securely and in a reasonably scalable way. This talk will
show you how we built our infrastructure - using Redis, eventlet,
PostgreSQL and more - and what lessons we learnt from our first few
thousand deploys.&lt;/p&gt;
&lt;p&gt;See how we split services into multiple processes and greenthreads; the
pains of building a cooperatively-multitasking PTY module; how Redis
isn't the answer to everything, but is still very useful; how to
persuade third-party software to work securely in a shared environment;
and how important it is to have good logging, especially when you have
more than five servers.&lt;/p&gt;
</content><category term="EuroPython 2011"></category><category term="ep.io"></category><category term="hosting"></category><category term="infrastructure"></category><category term="postgresql"></category><category term="redis"></category><category term="scalable"></category><category term="web"></category></entry><entry><title>Best Practices for Python in the Cloud</title><link href="https://pyvideo.org/europython-2011/best-practices-for-python-in-the-cloud.html" rel="alternate"></link><published>2011-07-18T00:00:00+00:00</published><updated>2011-07-18T00:00:00+00:00</updated><author><name>Gisle Aas</name></author><id>tag:pyvideo.org,2011-07-18:/europython-2011/best-practices-for-python-in-the-cloud.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Gisle Aas - 21 June 2011 in &amp;quot;Track Lasagne&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Abstract: Whether you’re an independent developer or development manager
in a large company, “the cloud” is on everyone’s mind. But just because
it’s in the cloud, doesn’t mean development and deployment is
effortless. The …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Gisle Aas - 21 June 2011 in &amp;quot;Track Lasagne&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Abstract: Whether you’re an independent developer or development manager
in a large company, “the cloud” is on everyone’s mind. But just because
it’s in the cloud, doesn’t mean development and deployment is
effortless. The cloud presents infrastructure and development challenges
in a new way.&lt;/p&gt;
&lt;p&gt;In this presentation, ActiveState's Gisle Aas will share best practices
in building and deploying a Python-centric LAMP stack(s) on the cloud
for a range of web-based applications from simple Django site to HPC GPU
Clusters.&lt;/p&gt;
&lt;p&gt;Based on ActiveState’s experiences, Gisle will discuss the challenges
faced and lessons learned in building an infrastructure to deploy web
applications to the cloud with Python.&lt;/p&gt;
&lt;p&gt;You will learn about:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Which packages are critical for a secure, Python-centric LAMP stack
(and what it takes to build them)!&lt;/li&gt;
&lt;li&gt;Tips for developing, deploying, and scaling Python applicaitons in
the cloud&lt;/li&gt;
&lt;li&gt;How to use Python to connect and build infrastructure to support and
manage your deployment&lt;/li&gt;
&lt;/ul&gt;
</content><category term="EuroPython 2011"></category><category term="cloud"></category><category term="deploy"></category><category term="deployment"></category><category term="django"></category><category term="gpu"></category><category term="hpc"></category><category term="infrastructure"></category><category term="lamp"></category><category term="packages"></category><category term="scaling"></category><category term="web"></category></entry><entry><title>Ubuntu and the opportunistic programming</title><link href="https://pyvideo.org/europython-2011/ubuntu-and-the-opportunistic-programming.html" rel="alternate"></link><published>2011-07-14T00:00:00+00:00</published><updated>2011-07-14T00:00:00+00:00</updated><author><name>Paolo Sammicheli</name></author><id>tag:pyvideo.org,2011-07-14:/europython-2011/ubuntu-and-the-opportunistic-programming.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Paolo Sammicheli - 20 June 2011 in &amp;quot;Track Tagliatelle&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will show the tools and the infrastructure that makes easy creating
own python project in Ubuntu and distributing it to millions of users.
It will be shown several tools: Launchpad, Quickly and and the Ubuntu's
PPA (personal …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Paolo Sammicheli - 20 June 2011 in &amp;quot;Track Tagliatelle&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will show the tools and the infrastructure that makes easy creating
own python project in Ubuntu and distributing it to millions of users.
It will be shown several tools: Launchpad, Quickly and and the Ubuntu's
PPA (personal package archiving).&lt;/p&gt;
</content><category term="EuroPython 2011"></category><category term="infrastructure"></category></entry><entry><title>How we run GraphQL APIs in production on our Kubernetes cluster</title><link href="https://pyvideo.org/europython-2019/how-we-run-graphql-apis-in-production-on-our-kubernetes-cluster.html" rel="alternate"></link><published>2019-07-12T00:00:00+00:00</published><updated>2019-07-12T00:00:00+00:00</updated><author><name>Alexys Jacob</name></author><id>tag:pyvideo.org,2019-07-12:/europython-2019/how-we-run-graphql-apis-in-production-on-our-kubernetes-cluster.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I would like to share the workflow and tools we use to
build, deploy and operate GraphQL APIs on our on-premise Kubernetes
cluster.&lt;/p&gt;
&lt;p&gt;I will share code and command examples explaining how we are operating
our applications since our recent transition from REST APIs on Web …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I would like to share the workflow and tools we use to
build, deploy and operate GraphQL APIs on our on-premise Kubernetes
cluster.&lt;/p&gt;
&lt;p&gt;I will share code and command examples explaining how we are operating
our applications since our recent transition from REST APIs on Web
servers to GraphQL APIs containers on Kubernetes.&lt;/p&gt;
&lt;p&gt;This talk will not be about the difference between REST and GraphQL but
focus on the workflow, tools and experience we gained in switching our
run time environments and API models.&lt;/p&gt;
&lt;p&gt;At Numberly, we have built and are operating our own on-premise
Kubernetes cluster so we will also be talking about its capabilities and
share some of the experience we gained in doing so.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Proposed agenda:&lt;/div&gt;
&lt;div class="line"&gt;- Our previous workflow and its limitations&lt;/div&gt;
&lt;div class="line"&gt;- How we designed our Kubernetes cluster, its capabilities and the
choices we made&lt;/div&gt;
&lt;div class="line"&gt;- Developer workflow, environments management and deployment&lt;/div&gt;
&lt;div class="line"&gt;- Our GraphQL stack, featuring a sample application&lt;/div&gt;
&lt;div class="line"&gt;- What we're still working on to improve&lt;/div&gt;
&lt;/div&gt;
</content><category term="EuroPython 2019"></category><category term="APIs"></category><category term="Best Practice"></category><category term="Case Study"></category><category term="Docker"></category><category term="Infrastructure"></category></entry><entry><title>Advanced Infrastructure Management in Kubernetes using Python</title><link href="https://pyvideo.org/europython-2020/advanced-infrastructure-management-in-kubernetes-using-python.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Gautam Prajapati</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/advanced-infrastructure-management-in-kubernetes-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Automate managing complex applications in a cloud native way using Operators written in Python&lt;/p&gt;
&lt;p&gt;Many of us are using Kubernetes in production. A Kubernetes Operator is a way to automate packaging, deploying, and managing of a Kubernetes Application. It is a software alternative to a human operator who has …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Automate managing complex applications in a cloud native way using Operators written in Python&lt;/p&gt;
&lt;p&gt;Many of us are using Kubernetes in production. A Kubernetes Operator is a way to automate packaging, deploying, and managing of a Kubernetes Application. It is a software alternative to a human operator who has deep knowledge of how to set up, deploy, and manage a particular piece of infrastructure and what to do if it isn’t behaving correctly. Let’s see how we can automate all of this while staying in the Python ecosystem.&lt;/p&gt;
&lt;p&gt;It will be helpful to know some basic concepts of Kubernetes(Deployments, Services, Pods, Configmap etc.) and Celery(docs.celeryproject.org) to get the most out of this talk.&lt;/p&gt;
&lt;p&gt;Talk is divided into four phases.&lt;/p&gt;
&lt;p&gt;Phase I - Problems and Opportunities
We're going to see some simple examples/problems where a lot of manual effort is involved so as to connect audience to the problem.
We're going to discuss problems with configuration management, database cluster setup and introduce the focus problem of the talk which is around automating the setup of a Celery cluster.&lt;/p&gt;
&lt;p&gt;Phase II - Incrementally Approaching the Solution
We're going to incrementally approach the automation each of the manual steps involved in running a Celery cluster in Production. We're going to discuss the extension capabilities in Kubernetes using CRDs and Custom Controllers which are going to help us manage our Celery cluster automagically.&lt;/p&gt;
&lt;p&gt;Phase III - Celery Operator in action
We're going to see the code of custom controller and the whole operator in action. We create the newly defined celery resource and see how the operator works on bringing up the worker and flower deployments and handles autoscaling based on queue length.&lt;/p&gt;
&lt;p&gt;Phase IV - Conclusion and Q&amp;amp;A
We're going to talk about different use-cases and what is world doing with Operators. We'll discuss the next steps for the Celery operator and some resources to help build operators. We'll end the talk with a Q&amp;amp;A.&lt;/p&gt;
&lt;p&gt;Slides for the talk are available on - &lt;a class="reference external" href="https://bit.ly/europython20-ppt"&gt;https://bit.ly/europython20-ppt&lt;/a&gt;
Celery Operator POC I built for this talk is open source - &lt;a class="reference external" href="https://github.com/brainbreaker/Celery-Kubernetes-Operator"&gt;https://github.com/brainbreaker/Celery-Kubernetes-Operator&lt;/a&gt;&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="DevOps general"></category><category term="Distributed Systems"></category><category term="Infrastructure"></category><category term="Messaging and Job Queues (RabbitMQ/Redis/...)"></category><category term="python"></category></entry><entry><title>Durable Functions: A More Durable Azure Function</title><link href="https://pyvideo.org/europython-2020/durable-functions-a-more-durable-azure-function.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Joseph Song</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/durable-functions-a-more-durable-azure-function.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Serverless functions-as-a-service lets you run event-triggered code without having to explicitly provision or manage infrastructure. While they are powerful, they also have limitations. In this talk, we’ll discuss how Durable Functions can overcome the limitation by extending Azure Functions to let you write stateful functions in a serverless …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Serverless functions-as-a-service lets you run event-triggered code without having to explicitly provision or manage infrastructure. While they are powerful, they also have limitations. In this talk, we’ll discuss how Durable Functions can overcome the limitation by extending Azure Functions to let you write stateful functions in a serverless compute environment.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Infrastructure"></category><category term="serverless"></category></entry><entry><title>Painless Machine Learning in Production</title><link href="https://pyvideo.org/europython-2020/painless-machine-learning-in-production.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Chase Stevens</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/painless-machine-learning-in-production.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Developing machine learning models is easy; training, deploying, monitoring, scaling, and maintaining them in an automated fashion - all while maintaining your sanity - is hard.&lt;/p&gt;
&lt;p&gt;In this session, I'll discuss the infrastructure and tooling my small team of data science practitioners and engineers is using to manage and orchestrate the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Developing machine learning models is easy; training, deploying, monitoring, scaling, and maintaining them in an automated fashion - all while maintaining your sanity - is hard.&lt;/p&gt;
&lt;p&gt;In this session, I'll discuss the infrastructure and tooling my small team of data science practitioners and engineers is using to manage and orchestrate the machine learning model lifecycle, including pitfalls we've encountered along the way. Particular attention will be paid to where we've opted to use off-the-shelf solutions versus developing our own, the importance of developer ergonomics, and how to maximally empower data scientists to get their work into production without the need for a dedicated MLOps team.&lt;/p&gt;
&lt;p&gt;The talk will cover our ML stack as it exists in production today, and will touch on our application of a number of technologies and techniques, including:
- AWS SageMaker
- Airflow
- Docker
- Cookiecutter
- Property-based testing
- Jsonschema
- Linting
- Slack integration
- Model artifacts and diagnostics
- Automated deployments and rollbacks
- Healthchecks
- Autoscaling
- DBT&lt;/p&gt;
&lt;p&gt;At the end of the session, attendees should expect to leave with new insights that they can apply immediately to their own ML systems and infrastructure, as well as a better understanding of how to minimize engineering and ops overhead, in the real world, across data science teams of any size and composition.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Case Study"></category><category term="Data Science"></category><category term="DevOps general"></category><category term="Infrastructure"></category><category term="Machine-Learning"></category></entry><entry><title>Beyond Jupyter Notebooks - Building your own Data Science platform with Python &amp; Docker</title><link href="https://pyvideo.org/pycon-de-2018/beyond-jupyter-notebooks-building-your-own-data-science-platform-with-python-docker.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Joshua Görner</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/beyond-jupyter-notebooks-building-your-own-data-science-platform-with-python-docker.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Interactive notebooks like Jupyter have become more and more popular in
the recent past and build the core of many data scientist's workplace.
Being accessed via web browser they allow scientists to easily structure
their work by combining code and documentation.&lt;/p&gt;
&lt;p&gt;Yet notebooks often lead to isolated and disposable …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Interactive notebooks like Jupyter have become more and more popular in
the recent past and build the core of many data scientist's workplace.
Being accessed via web browser they allow scientists to easily structure
their work by combining code and documentation.&lt;/p&gt;
&lt;p&gt;Yet notebooks often lead to isolated and disposable analysis artefacts.
Keeping the computation inside those notebooks does not allow for
convenient concurrent model training, model exposure or scheduled model
retraining.&lt;/p&gt;
&lt;p&gt;Those issues can be addressed by taking advantage of recent developments
in the discipline of software engineering. Over the past years
containerization became the technology of choice for crafting and
deploying applications. Building a data science platform that allows for
easy access (via notebooks), flexibility and reproducibility (via
containerization) combines the best of both worlds and addresses Data
Scientist's hidden needs.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Artificial Intelligence"></category><category term="Algorithms"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Infrastructure"></category><category term="Jupyter"></category><category term="Machine Learning"></category><category term="Programming"></category><category term="Python"></category></entry><entry><title>Big Data Systems Performance: The Little Shop of Horrors</title><link href="https://pyvideo.org/pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Jens Dittrich</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/big-data-systems-performance-the-little-shop-of-horrors.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The confusion around terms such as like NoSQL, Big Data, Data Science,
Spark, SQL, and Data Lakes often creates more fog than clarity. However,
clarity about the underlying technologies is crucial to designing the
best technical solution in any field relying on huge amounts of data
including data science …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The confusion around terms such as like NoSQL, Big Data, Data Science,
Spark, SQL, and Data Lakes often creates more fog than clarity. However,
clarity about the underlying technologies is crucial to designing the
best technical solution in any field relying on huge amounts of data
including data science, machine learning, but also more traditional
analytical systems such as data integration, data warehousing,
reporting, and OLAP.&lt;/p&gt;
&lt;p&gt;In my presentation, I will show that often at least three dimensions are
cluttered and confused in discussions when it comes to data management:
First, buzzwords (labels &amp;amp; terms like &amp;quot;big data&amp;quot;, &amp;quot;AI&amp;quot;, &amp;quot;data lake&amp;quot;);
second, data design patterns (principles &amp;amp; best practices like:
selection push-down, materialization, indexing); and Third, software
platforms (concrete implementations &amp;amp; frameworks like: Python, DBMS,
Spark, and NoSQL-systems).&lt;/p&gt;
&lt;p&gt;Only by keeping these three dimensions apart, it is possible to create
technically-sound architectures in the field of big data analytics.&lt;/p&gt;
&lt;p&gt;I will show concrete examples, which through a simple redesign and wise
choice of the right tools and technologies, run thereby up to 1000 times
faster. This in turn triggers tremendous savings in terms of development
time, hardware costs, and maintenance effort.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Infrastructure"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>Cloud chat bot for lazy people</title><link href="https://pyvideo.org/pycon-de-2018/cloud-chat-bot-for-lazy-people.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Björn Meier</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/cloud-chat-bot-for-lazy-people.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At work we established Slack years ago as our chat application and by
now quite a percentage of communication goes through it. As a result it
got much easier to contact one person or a group simultaneously. And
this is good as we can share our knowledge save each …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At work we established Slack years ago as our chat application and by
now quite a percentage of communication goes through it. As a result it
got much easier to contact one person or a group simultaneously. And
this is good as we can share our knowledge save each other time. But it
also introduced a category of questions in the chat which only require
simple tedious tasks to get the answer and then post it as a response.
One possibility is to educate and point others to the place where they
can find the answer or what tasks they have to do. The other one is use
a chat bot for this. Both ways have advantages and for the bot it is
that you can import a specific type of response more easily into a
conversation without first gathering the information and copy and paste
it. I am a developer and service operator and one category of questions
which fits this is the category of service health questions, like &amp;quot;Does
service X has a problem right now?&amp;quot;. Hence, I will use a bot to answer
them. First I will show you how you can create a python bot for the
Azure bot service. With it the questioner then can either directly use
the bot to answer his question or you can just create the response for
him without going to the service health monitoring. In this case the
service health information has to be obtained from a Prometheus
monitoring service and then transformed into a chat message.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="DevOps"></category><category term="Infrastructure"></category></entry><entry><title>Data science complexity and solutions in real industrial projects</title><link href="https://pyvideo.org/pycon-de-2018/data-science-complexity-and-solutions-in-real-industrial-projects.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Artur Miller</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/data-science-complexity-and-solutions-in-real-industrial-projects.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As data scientists we usually like to apply fancy machine learning
models to well-groomed datasets. Everyone working on industrial problems
will eventually learn, that this does not reflect reality. The amount of
time spent on modeling is small compared to data gathering, -warehousing
and -cleaning. Even after training and …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As data scientists we usually like to apply fancy machine learning
models to well-groomed datasets. Everyone working on industrial problems
will eventually learn, that this does not reflect reality. The amount of
time spent on modeling is small compared to data gathering, -warehousing
and -cleaning. Even after training and deployment of the model, the work
is not done. Continuous monitoring of the performance and input data is
still necessary.&lt;/p&gt;
&lt;p&gt;In this talk I discuss how important data handling is for successful
data science projects. Each milestone, from finding the business case to
continuously monitoring the performance of the solution, is addressed.
This is exemplary shown on a project, with the goal of improving a
productive system.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="Infrastructure"></category><category term="Machine Learning"></category></entry><entry><title>Observe all your applications</title><link href="https://pyvideo.org/pycon-de-2018/observe-all-your-applications.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Christoph Heer</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/observe-all-your-applications.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You just deployed your new version of an application or micro-service;
how do you know everything works as expected? You run your comprehensive
test suite to verify functional correctness for known scenarios and
performance tests before deploying, but does your application really
work at the moment or is it …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You just deployed your new version of an application or micro-service;
how do you know everything works as expected? You run your comprehensive
test suite to verify functional correctness for known scenarios and
performance tests before deploying, but does your application really
work at the moment or is it just responding with error messages to all
incoming requests?&lt;/p&gt;
&lt;p&gt;I’m part of the team that runs a huge infrastructure for the SAP HANA
development. This infrastructure is vital for nearly all development &amp;amp;
testing activities of SAP HANA. As this infrastructure is powered by
multiple in-house developed applications, we immediately want to know if
an application starts to fail and we need to be able to quickly diagnose
what caused the failure.&lt;/p&gt;
&lt;p&gt;This talk will give you an overview how we monitor our full stack from
the 2000 physical machines up to the 10,000 parallel running Python
application processes, micro-service instances and batch processing
jobs. It includes a review about the used tools, bad and good examples
of instrumentation in Python code, the resulting visualisation and an
outlook on upcoming improvements.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="DevOps"></category><category term="Infrastructure"></category><category term="Networks"></category><category term="Programming"></category><category term="Python"></category></entry><entry><title>Python on the blockchain: Triumphs and tribulations in a crypto startup</title><link href="https://pyvideo.org/pycon-de-2018/python-on-the-blockchain-triumphs-and-tribulations-in-a-crypto-startup.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Danny McDonald</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/python-on-the-blockchain-triumphs-and-tribulations-in-a-crypto-startup.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While many cryptographic components of blockchain protocols can be
extremely complex, blockchain systems themselves are relatively easy to
understand when viewed from a distance. To take the example of Bitcoin,
users store digital currency in hardware or software wallets, and use
private keys to sign and broadcast transactions. Broadcasted …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While many cryptographic components of blockchain protocols can be
extremely complex, blockchain systems themselves are relatively easy to
understand when viewed from a distance. To take the example of Bitcoin,
users store digital currency in hardware or software wallets, and use
private keys to sign and broadcast transactions. Broadcasted
transactions are grouped together into a block through a cryptographic
process known as mining, with miners rewarded through the collection of
transaction fees and the issuance of new coins. The mined block of
transactions is appended to the existing chain, and verified by a global
network of nodes. This process repeats in perpetuity, with each newly
added block adding to the trustedness and security of data stored on the
chain.&lt;/p&gt;
&lt;p&gt;Increased interest in and demand for cryptocurrencies has brought about
a need for places where digital assets can easily be bought, sold or
traded. Our platform, Bitpanda, accomplishes this with a backend written
in Python, and relying heavily on Django and MySQL databases. In our
presentation, we begin by providing a brief overview of how blockchains
work. Following this, we describe the Python architecture that (e.g.)
generates cryptocurrency wallets, builds, signs and sends transactions,
and monitors blockchains for new, relevant data. Key challenges,
solutions and failures encountered during the development of the system,
and growth of our team, are presented.&lt;/p&gt;
&lt;p&gt;Throughout our talk, we also highlight a number of broader social
implications of blockchains, and our work with them. More specifically,
we describe the need for open-innovation based approaches to blockchain
development, the value of open-source within the blockchain community,
and the current lack of critical discourse surrounding the potential
uses of blockchains as mechanisms of surveillance and control.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Business &amp; Start-Ups"></category><category term="Community"></category><category term="Django"></category><category term="Infrastructure"></category><category term="Python"></category></entry><entry><title>Python with and without Pants</title><link href="https://pyvideo.org/pycon-de-2018/python-with-and-without-pants.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Stephan Erb</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/python-with-and-without-pants.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What is the best outfit for comfortable, but highly productive
programming at home? While this is definitely an important question,
this talk will focus on a topic that is slightly more controversial:
monorepos and their build tools. Specifically, the talk will have a
closer look at Pants (&lt;a class="reference external" href="https://www.pantsbuild.org"&gt;https://www …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What is the best outfit for comfortable, but highly productive
programming at home? While this is definitely an important question,
this talk will focus on a topic that is slightly more controversial:
monorepos and their build tools. Specifically, the talk will have a
closer look at Pants (&lt;a class="reference external" href="https://www.pantsbuild.org"&gt;https://www.pantsbuild.org&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Pants is a build system for large or rapidly growing code bases. It
supports all stages of a typical build ( bootstrapping, dependency
resolution, compilation, linting, ...) and allows users to organize
their code via targets for binaries, libraries, and tests. For Python
programmers, pants is especially interesting, as it makes the
manipulation and distribution of hermetically sealed Python environments
painless - so called PEXes.&lt;/p&gt;
&lt;p&gt;The talk will motivate Pants and its usage in the context of a large
company- wide monorepo. It will then focus on important Python-centric
features, and shortly explain how those work under the hood. The talk
will conclude with a discussion of usecases for Pants outside of a
monorepo, i.e. for the rest of us.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="DevOps"></category><category term="Infrastructure"></category></entry><entry><title>Where the heck is my memory?</title><link href="https://pyvideo.org/pycon-de-2018/where-the-heck-is-my-memory.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Florian Jetter</name></author><id>tag:pyvideo.org,2018-10-25:/pycon-de-2018/where-the-heck-is-my-memory.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Memory management is something the common Python user doesn’t need to
bother with because the gory details of it are hidden deep within the
interpreter itself. The garbage collector takes out the trash and we can
spend our precious time bothering with more important things on our
minds …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Memory management is something the common Python user doesn’t need to
bother with because the gory details of it are hidden deep within the
interpreter itself. The garbage collector takes out the trash and we can
spend our precious time bothering with more important things on our
minds. Living in this encapsulated utopia is nice but sometimes it is
worth it to peak behind the curtains to unleash the full power of your
application. In this talk I want to show you when it is necessary to
face this harsh world and convince you that it is in fact not as scary
as it may seem. Using real life examples, I’m going to show you how to
use the garbage collector and open source tooling to get control over
the memory you might not even know you had at your disposal.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="DevOps"></category><category term="Infrastructure"></category></entry><entry><title>Cython to speed up your Python code</title><link href="https://pyvideo.org/pycon-de-2018/cython-to-speed-up-your-python-code.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Stefan Behnel</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/cython-to-speed-up-your-python-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="http://cython.org"&gt;Cython&lt;/a&gt; is not only a very fast and comfortable
way to talk to native code and libraries, it is also a widely used tool
for speeding up Python code. The Cython compiler translates Python code
to C or C++ code, and applies many static optimisations that make Python
code …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="http://cython.org"&gt;Cython&lt;/a&gt; is not only a very fast and comfortable
way to talk to native code and libraries, it is also a widely used tool
for speeding up Python code. The Cython compiler translates Python code
to C or C++ code, and applies many static optimisations that make Python
code run visibly faster than in the interpreter. But even better, it
supports static type annotations that allow direct use of C/C++ data
types and functions, which the compiler uses to convert and optimise the
code into fast, native C. The tight integration of all three languages,
Python, C and C++, makes it possible to freely mix Python features like
generators and comprehensions with C/C++ features like native data
types, pointer arithmetic or manually tuned memory management in the
same code.&lt;/p&gt;
&lt;p&gt;This talk by a core developer introduces the Cython compiler by
interactive code examples, and shows how you can use it to speed up your
real-world Python code. You will learn how you can profile a Python
module and use Cython to compile and optimise it into a fast binary
extension module. All of that, without losing the ability to run it
through common development tools like code checkers or coverage test
tools.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="Infrastructure"></category><category term="Jupyter"></category><category term="Parallel Programming"></category></entry><entry><title>Distributed Hyperparameter search with sklearn and kubernetes</title><link href="https://pyvideo.org/pycon-de-2018/distributed-hyperparameter-search-with-sklearn-and-kubernetes.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Jakob Karalus</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/distributed-hyperparameter-search-with-sklearn-and-kubernetes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While sklearn provides a good interface to do hyperparameter search on
large &amp;amp; complex model (pipelines), doing these can take up a lot of
time. The traditional way usually includes one beefy machine and a lot
of waiting. In other cases, people tend to “manually” schedule parameter
ranges between nodes …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While sklearn provides a good interface to do hyperparameter search on
large &amp;amp; complex model (pipelines), doing these can take up a lot of
time. The traditional way usually includes one beefy machine and a lot
of waiting. In other cases, people tend to “manually” schedule parameter
ranges between nodes, but that can also be problematic since these won't
talk to each other. Kubernetes itself is currently the most prominent
scheduler and shines at distributing task, but is a pretty complex
system in itself.&lt;/p&gt;
&lt;p&gt;In this talk, I will show how you can harness the scheduling of
kubernetes for distributing hyperparameter search with sklearn onto a
cluster of nodes. This can be achieved quite easily and with just a few
changes to the original code, so the Data Scientist won't be bothered by
complex kubernetes internals.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Algorithms"></category><category term="Big Data"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Infrastructure"></category><category term="Machine Learning"></category></entry><entry><title>How type annotations make your code better</title><link href="https://pyvideo.org/pycon-de-2018/how-type-annotations-make-your-code-better.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Igor Davydenko</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/how-type-annotations-make-your-code-better.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Type annotations still not received that amount of popularity, that
should. People still finding them hard and sometimes ambiguous to use.
But if you start new project in Python in 2018 you should consider using
type annotations in your code and this short talk describes why.&lt;/p&gt;
&lt;p&gt;I'll go over …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Type annotations still not received that amount of popularity, that
should. People still finding them hard and sometimes ambiguous to use.
But if you start new project in Python in 2018 you should consider using
type annotations in your code and this short talk describes why.&lt;/p&gt;
&lt;p&gt;I'll go over examples, where type annotations helped my team to create
less complex code, and how using type annotations changing your mind for
projecting &amp;amp; implementing features for your project.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Infrastructure"></category><category term="Web"></category></entry><entry><title>IoT using Python on Linux: Lessons Learned</title><link href="https://pyvideo.org/pycon-de-2018/iot-using-python-on-linux-lessons-learned.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Thomas Keppler</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/iot-using-python-on-linux-lessons-learned.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In a distributed sensor network system with a Java based Cloud
application, mobile apps and a proprietary radio protocol accompanying
it we developed an IoT appliance that connects the existing radio
infrastructure to the Cloud service developed in-house.&lt;/p&gt;
&lt;p&gt;Using CPython 3.5 + Debian GNU/Linux 9 on an ARMv7 …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In a distributed sensor network system with a Java based Cloud
application, mobile apps and a proprietary radio protocol accompanying
it we developed an IoT appliance that connects the existing radio
infrastructure to the Cloud service developed in-house.&lt;/p&gt;
&lt;p&gt;Using CPython 3.5 + Debian GNU/Linux 9 on an ARMv7 platform, we
developed the following features:&lt;/p&gt;
&lt;p&gt;Over the course of this project, we learned a lot about Test Driven
Development of Python apps in teams and DevOps in the IoT space. We
would now like to share our experience developing a Python application
for a headless IoT device and the things we would liked to have known
upfront.&lt;/p&gt;
&lt;p&gt;The talk is held both by Matthias Schmidt (Senior Architect at diva-e)
and Thomas Keppler (Software Developer at diva-e).&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="DevOps"></category><category term="Infrastructure"></category><category term="Networks"></category><category term="Programming"></category><category term="Python"></category></entry><entry><title>Python Dependency Management</title><link href="https://pyvideo.org/pycon-de-2018/python-dependency-management.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Patrick Muehlbauer</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/python-dependency-management.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For a long time there were &lt;tt class="docutils literal"&gt;pip&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;virtualenv&lt;/tt&gt; which were used
together with &lt;tt class="docutils literal"&gt;requirements.txt&lt;/tt&gt; files to manage Python dependencies.
Nowadays there are various other tools that help you improve the
workflow.&lt;/p&gt;
&lt;p&gt;We will have a look at popular projects like&lt;/p&gt;
&lt;p&gt;After the talk you will be able …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For a long time there were &lt;tt class="docutils literal"&gt;pip&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;virtualenv&lt;/tt&gt; which were used
together with &lt;tt class="docutils literal"&gt;requirements.txt&lt;/tt&gt; files to manage Python dependencies.
Nowadays there are various other tools that help you improve the
workflow.&lt;/p&gt;
&lt;p&gt;We will have a look at popular projects like&lt;/p&gt;
&lt;p&gt;After the talk you will be able to decide for yourself which approach
suits your usecases best and don't have to rely on rants postet on
reddit.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Infrastructure"></category></entry><entry><title>Selinon - dynamic distributed task flows</title><link href="https://pyvideo.org/pycon-de-2018/selinon-dynamic-distributed-task-flows.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Fridolín Pokorný</name></author><id>tag:pyvideo.org,2018-10-24:/pycon-de-2018/selinon-dynamic-distributed-task-flows.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough flexibility and dynamic features needed badly in complex
flows. As we discovered all the limitations, we decided to implement
Selinon.&lt;/p&gt;
&lt;p&gt;Have you ever tried to define and process complex workflows for data
processing? If the answer is yes, you might have struggled to find the
right framework for that. You've probably came across Celery - popular
task flow management for Python. Celery is great, but it does not
provide enough flexibility and dynamic features needed badly in complex
flows. As we discovered all the limitations, we decided to implement
Selinon.&lt;/p&gt;
&lt;p&gt;Selinon enhances Celery task flow management and allows you to create
and model task flows in your distributed environment that can
dynamically change behavior based on computed results in your cluster,
automatically resolve tasks that need to be executed in case of
selective task runs, automatic tracing mechanism and many others.&lt;/p&gt;
</content><category term="PyCon DE 2018"></category><category term="Big Data"></category><category term="Infrastructure"></category><category term="Parallel Programming"></category><category term="Programming"></category><category term="Python"></category></entry><entry><title>Infrastructure as Code with Terraform</title><link href="https://pyvideo.org/pycon-italia-2017/infrastructure-as-code-with-terraform.html" rel="alternate"></link><published>2017-04-08T00:00:00+00:00</published><updated>2017-04-08T00:00:00+00:00</updated><author><name>Justyna Janczyszyn</name></author><id>tag:pyvideo.org,2017-04-08:/pycon-italia-2017/infrastructure-as-code-with-terraform.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;what is infrastructure as code&lt;/li&gt;
&lt;li&gt;best practices&lt;/li&gt;
&lt;li&gt;benefits&lt;/li&gt;
&lt;li&gt;introduction to terraform&lt;/li&gt;
&lt;li&gt;practical demo for a sample flask application&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/tramwaj29/iac-with-terraform"&gt;https://github.com/tramwaj29/iac-with-terraform&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon Italia 2017"></category><category term="infrastructure"></category><category term="devops"></category><category term="provisioning"></category><category term="terraform"></category><category term="deployment"></category><category term="infrastructure-as-code"></category></entry><entry><title>Continuous Delivery starts at your Development Environment</title><link href="https://pyvideo.org/pycon-italia-2018/continuous-delivery-starts-at-your-development-environment.html" rel="alternate"></link><published>2018-04-22T00:00:00+00:00</published><updated>2018-04-22T00:00:00+00:00</updated><author><name>Peter Bittner</name></author><id>tag:pyvideo.org,2018-04-22:/pycon-italia-2018/continuous-delivery-starts-at-your-development-environment.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Continuous Delivery is the automation of our deployment and QA, isn’t
it? The industrialized software production chain that solves all our
problems. Well, kind of. It’s more than that. Because: You still have
pain when …&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;You bring new developers on board (e.g. for installing Docker and …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Continuous Delivery is the automation of our deployment and QA, isn’t
it? The industrialized software production chain that solves all our
problems. Well, kind of. It’s more than that. Because: You still have
pain when …&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;You bring new developers on board (e.g. for installing Docker and
friends, IDEs and command line tools).&lt;/li&gt;
&lt;li&gt;Your development machine behaves weird or needs an upgrade. (Can you
simply re-install it and continue working without losing a day or a
week of productivity?)&lt;/li&gt;
&lt;li&gt;You need to explain to your colleagues, or even convince them, how to
configure which of their tools?&lt;/li&gt;
&lt;li&gt;Everyone turns to you (or your admin wizard) when there are troubles
to get some tools or projects running.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This talks explains how to bootstrap a super-efficient startup, SMB, or
just any development team that needs a development infrastructure.
You’ll see a demo of The Foreman, Puppet, Ansible and optionally FreeIPA
and virtualization technology, which gets you from zero to 100 in just a
few hours. This talk explains how you do infrastructure like software
development. Everything under control. Everything under version control.&lt;/p&gt;
&lt;p&gt;Come join the show, and take home the recipe that tells you how to step
up the next level of Continuous Delivery! - If you’re a beginner fear
not: this talk starts with an introduction to CI/CD and brings you up to
speed quickly.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Sunday 22 April&lt;/strong&gt; at 15:30 &lt;a class="reference external" href="/en/sprints/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon Italia 2018"></category><category term="infrastructure"></category><category term="continuous-delivery"></category><category term="agile"></category><category term="ansible"></category><category term="puppet"></category><category term="free-software"></category><category term="FreeIPA"></category><category term="infrastructure-as-code"></category></entry><entry><title>Da crontab a Celery senza rimpianti</title><link href="https://pyvideo.org/pycon-italia-2022/da-crontab-a-celery-senza-rimpianti.html" rel="alternate"></link><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><author><name>Marco Pavanelli</name></author><id>tag:pyvideo.org,2022-06-03:/pycon-italia-2022/da-crontab-a-celery-senza-rimpianti.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Da crontab a Celery senza rimpianti - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Eseguire i backup, controllare le email, cancellare i log .. ci sono un
sacco di cose utili che si possono automatizzare con i cron jobs ma dopo
un po’ di tempo i vostri file di crontab diventano ingestibili. In
questo caso …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Da crontab a Celery senza rimpianti - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Eseguire i backup, controllare le email, cancellare i log .. ci sono un
sacco di cose utili che si possono automatizzare con i cron jobs ma dopo
un po’ di tempo i vostri file di crontab diventano ingestibili. In
questo caso Celery può diventare il vostro migliore amico. I cont jobs
sono fantastici, possono salvarti la vita e il matrimonio, lavorano di
giorno e di notte con qualsiasi tempo e non sbagliano mai orario. Ma
quando diventano troppi diventa davvero difficile gestirli e poi avete
mai provato ad eseguire un cron job in un container?&lt;/p&gt;
&lt;p&gt;la prima volta che ho provato Celery è stato abbastanza frustrante, il
setup iniziale non è semplice ma se non ci si arrende il risultato è
sorprendente e il potere che ti può dare Celery una volta installato è
gigantesco.&lt;/p&gt;
&lt;p&gt;Speaker: Marco Pavanelli&lt;/p&gt;
</content><category term="PyCon Italia 2022"></category><category term="django"></category><category term="docker"></category><category term="infrastructure"></category></entry><entry><title>Efficient ML pipelines using Parquet and PyArrow</title><link href="https://pyvideo.org/pycon-italia-2022/efficient-ml-pipelines-using-parquet-and-pyarrow.html" rel="alternate"></link><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><author><name>Ingargiola</name></author><id>tag:pyvideo.org,2022-06-03:/pycon-italia-2022/efficient-ml-pipelines-using-parquet-and-pyarrow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Efficient ML pipelines using Parquet and PyArrow - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Parquet is an high-performance columnar data format that has become the
de facto standard in the ML world. By leveraging the powerful PyArrow
API, I’ll show how to manage parquet datasets, ranging from a single
local file to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Efficient ML pipelines using Parquet and PyArrow - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;Parquet is an high-performance columnar data format that has become the
de facto standard in the ML world. By leveraging the powerful PyArrow
API, I’ll show how to manage parquet datasets, ranging from a single
local file to a partitioned cloud-based dataset updated in real time.
Advanced analytics and Machine Learning (ML) are increasingly used to
drive business decisions or provide real-time services for end-users in
virtually every industry. Tabular data is the most ubiquitous type of
data. Therefore, efficient processing of handle tabular datasets is a
critical requirement to deliver performant products or services.&lt;/p&gt;
&lt;p&gt;In a proto-typical production ML workflow, an “ingestion pipeline” needs
to store large datasets on the cloud and continuously update them as new
data becomes available. An “analytics pipeline” usually needs to process
the entire dataset by reading it in batches, because the full dataset
would be too large to fit in RAM. An “inference pipeline” provides
real-time results (i.e.&amp;nbsp;model predictions or other online statistics)
and needs to process small batches of data in quasi-realtime. Finally,
the presentation of analytics results requires not only to show the
output from the models but also to provide context through “historical
data” for an arbitrary set of features. Therefore, low-latency access to
a small group of columns from a large dataset represents an additional
requirement.&lt;/p&gt;
&lt;p&gt;In the Python ecosystem, we can leverage tools such as Parquet and
PyArrow to address such complex workflow.&lt;/p&gt;
&lt;p&gt;Apache Parquet is a columnar storage format initially created to address
similar storage challenges in the Hadoop ecosystem. It has since become
a standard for efficient storage of large datasets in all the major
languages, including Python.&lt;/p&gt;
&lt;p&gt;The Apache Arrow project provides a cross-language in-memory
representation and query engine for tabular datasets and has a
performant IO interface for Parquet datasets. Its Python interface,
PyArrow, allows to query and process large partitioned datasets
distributed across multiple files and folders on local and cloud
storage.&lt;/p&gt;
&lt;p&gt;In this talk, combining PyArrow and Parquet datasets, we will explore
several techniques to address the use-cases of the typical production ML
workflows delineated above.&lt;/p&gt;
&lt;p&gt;Speaker: Ingargiola&lt;/p&gt;
</content><category term="PyCon Italia 2022"></category><category term="aws"></category><category term="best practice"></category><category term="infrastructure"></category><category term="machine learning"></category><category term="pandas"></category><category term="performance"></category><category term="scaling"></category></entry><entry><title>Managing large-scale ML pipelines with MLflow and serverless computing.</title><link href="https://pyvideo.org/pycon-italia-2022/managing-large-scale-ml-pipelines-with-mlflow-and-serverless-computing.html" rel="alternate"></link><published>2022-06-03T00:00:00+00:00</published><updated>2022-06-03T00:00:00+00:00</updated><author><name>ilyas chaoua</name></author><id>tag:pyvideo.org,2022-06-03:/pycon-italia-2022/managing-large-scale-ml-pipelines-with-mlflow-and-serverless-computing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Managing large-scale Machine Learning pipelines with MLflow and
serverless computing. - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;MLOps aims to manage the machine learning (ML) lifecycle including
experimentation, reproducibility, deployment, and model registry. Come
to discover how in Vedrai - one of the top AI startups in Europe - we
enhance and maintain ML pipelines …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Managing large-scale Machine Learning pipelines with MLflow and
serverless computing. - PyCon Italia 2022&lt;/p&gt;
&lt;p&gt;MLOps aims to manage the machine learning (ML) lifecycle including
experimentation, reproducibility, deployment, and model registry. Come
to discover how in Vedrai - one of the top AI startups in Europe - we
enhance and maintain ML pipelines models in production reliably and
efficiently using MLOps. Problem:&lt;/p&gt;
&lt;p&gt;One difficulty of employing Machine Learning (ML) within organizations
is managing the model’s lifecycle. Moving from experimenting to
deployment in production environments is operated by different steps:
Preparing and Analysing Data, Training, Deployment, Monitoring, and
Governance of ML models. So, it is crucial to possess a platform to
manage and organize the ML lifecycle.&lt;/p&gt;
&lt;p&gt;Solution:&lt;/p&gt;
&lt;p&gt;In Vedrai, we combined the strength of the MLflow framework and the
resilience of AWS serverless services to manage, deploy, and scale our
ML models in production. MLflow is an open-source framework for tracking
the entire ML lifecycle from training to deployment. Among the
functions, it offers model tracking, packaging, and serving. Whereas,
deploying ML applications is an infrastructure affair that needs to be
scalable with minimum server management, which makes AWS serverless
services a great choice.&lt;/p&gt;
&lt;p&gt;Value:&lt;/p&gt;
&lt;p&gt;MLflow enforces the model’s reproducibility and robustness at the same
time allowing more centralized experimentation. AWS serverless services
allow training and inferencing pipelines to run without provisioning or
managing servers while only paying for the time it takes to run.&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;State of the art of MLOps.&lt;/li&gt;
&lt;li&gt;Record and query experiments with MLflow Tracking.&lt;/li&gt;
&lt;li&gt;Package data science code with MLflow Projects.&lt;/li&gt;
&lt;li&gt;Store ML models with MLflow Models Registry.&lt;/li&gt;
&lt;li&gt;Deploy ML models in the AWS environment.&lt;/li&gt;
&lt;li&gt;Future MLOps challenges.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speaker: ilyas chaoua&lt;/p&gt;
</content><category term="PyCon Italia 2022"></category><category term="architecture"></category><category term="aws"></category><category term="best practice"></category><category term="deep learning"></category><category term="devops"></category><category term="docker"></category><category term="infrastructure"></category><category term="machine learning"></category><category term="open source"></category><category term="operations"></category><category term="packaging"></category><category term="performance"></category><category term="scaling"></category></entry><entry><title>How Python is guiding infrastructure construction in Africa (#84)</title><link href="https://pyvideo.org/pycon-us-2010/how-python-is-guiding-infrastructure-construction.html" rel="alternate"></link><published>2010-02-19T00:00:00+00:00</published><updated>2010-02-19T00:00:00+00:00</updated><author><name>Roy Hyunjin Han</name></author><id>tag:pyvideo.org,2010-02-19:/pycon-us-2010/how-python-is-guiding-infrastructure-construction.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How Python is guiding infrastructure construction in Africa&lt;/p&gt;
&lt;p&gt;Presented by Roy Hyunjin Han (InvisibleRoads)&lt;/p&gt;
&lt;p&gt;A whirlwind tour of the roles of different Python modules in the
architecture of a geospatial infrastructure planning system.&lt;/p&gt;
&lt;p&gt;We used Python to transform an elaborate, multistep process for finding
and connecting households in villages …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How Python is guiding infrastructure construction in Africa&lt;/p&gt;
&lt;p&gt;Presented by Roy Hyunjin Han (InvisibleRoads)&lt;/p&gt;
&lt;p&gt;A whirlwind tour of the roles of different Python modules in the
architecture of a geospatial infrastructure planning system.&lt;/p&gt;
&lt;p&gt;We used Python to transform an elaborate, multistep process for finding
and connecting households in villages into a single, streamlined
planning experience. I'll explain how Python's freely available
libraries empowered a small team of developers under a minimal budget
and timeframe. Now just imagine if we had to do the same thing with
Java.&lt;/p&gt;
</content><category term="PyCon US 2010"></category><category term="africa"></category><category term="casestudy"></category><category term="infrastructure"></category><category term="pycon"></category><category term="pycon2010"></category></entry><entry><title>Actors: What, Why, and How (#161)</title><link href="https://pyvideo.org/pycon-us-2010/pycon-2010--actors--what--why--and-how---161.html" rel="alternate"></link><published>2010-02-19T00:00:00+00:00</published><updated>2010-02-19T00:00:00+00:00</updated><author><name>Donovan Preston</name></author><id>tag:pyvideo.org,2010-02-19:/pycon-us-2010/pycon-2010--actors--what--why--and-how---161.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Actors: What, Why and How&lt;/p&gt;
&lt;p&gt;Presented by Donovan Preston&lt;/p&gt;
&lt;p&gt;Since the dawn of concurrency research, there have been two camps:
shared everything, and shared nothing. Most modern applications use
threads for concurrency, a shared everything architecture.&lt;/p&gt;
&lt;p&gt;Actors, however, use a shared nothing architecture where lightweight
processes communicate with each …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Actors: What, Why and How&lt;/p&gt;
&lt;p&gt;Presented by Donovan Preston&lt;/p&gt;
&lt;p&gt;Since the dawn of concurrency research, there have been two camps:
shared everything, and shared nothing. Most modern applications use
threads for concurrency, a shared everything architecture.&lt;/p&gt;
&lt;p&gt;Actors, however, use a shared nothing architecture where lightweight
processes communicate with each other using message passing. Actors can
change their state, create a new Actor, send a message to any Actor it
has the Address of, and wait for a specific kind of message to arrive in
it's mailbox.&lt;/p&gt;
&lt;p&gt;We will discuss the benefits of using the Actor architecture and
strategies for implementing an Actor system in Python.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://bitbucket.org/fzzzy/python-%20actors/"&gt;http://bitbucket.org/fzzzy/python-actors/&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon US 2010"></category><category term="concurrency"></category><category term="eventlet"></category><category term="infrastructure"></category><category term="pycon"></category><category term="pycon2010"></category><category term="rest"></category><category term="scaling"></category><category term="wsgi"></category></entry></feed>