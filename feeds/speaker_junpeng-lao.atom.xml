<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_junpeng-lao.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-09-27T00:00:00+00:00</updated><entry><title>A Hitchhiker's Guide to designing a Bayesian library in Python</title><link href="https://pyvideo.org/pydata-cordoba-2019/a-hitchhikers-guide-to-designing-a-bayesian-library-in-python.html" rel="alternate"></link><published>2019-09-27T00:00:00+00:00</published><updated>2019-09-27T00:00:00+00:00</updated><author><name>Junpeng Lao</name></author><id>tag:pyvideo.org,2019-09-27:pydata-cordoba-2019/a-hitchhikers-guide-to-designing-a-bayesian-library-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;From the perspective of a PyMC and TFP developer, I will give a developer introduction of PyMC3, and talk about some of our current design pitfall and future direction(s). I hope that with these insights and consideration of how a modern (i.e., depending on some autograd system) probabilistic libraries is designed, it could help user and practitioner to write better probabilistic programs.&lt;/p&gt;
</summary><category term="PyMC3"></category><category term="bayesian"></category></entry><entry><title>Writing effective bayesian programs using TensorFlow and TFP</title><link href="https://pyvideo.org/pydata-cordoba-2019/writing-effective-bayesian-programs-using-tensorflow-and-tfp.html" rel="alternate"></link><published>2019-09-27T00:00:00+00:00</published><updated>2019-09-27T00:00:00+00:00</updated><author><name>Junpeng Lao</name></author><id>tag:pyvideo.org,2019-09-27:pydata-cordoba-2019/writing-effective-bayesian-programs-using-tensorflow-and-tfp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial aims to provide some examples of how to write effective Bayesian programs using TensorFlow and Tensorflow Probability. In TFP land, effectiveness usually comes from writing model that could generate batch-able functions, and utilizing modern hardware (GPU, TPU) with compiler accelerator (i.e., XLA). I will give a walkthrough on how to do so and highlight some gotchas.&lt;/p&gt;
</summary><category term="bayesian"></category><category term="tensorflow"></category><category term="probability"></category></entry><entry><title>All that likelihood with PyMC3</title><link href="https://pyvideo.org/pydata-berlin-2018/all-that-likelihood-with-pymc3.html" rel="alternate"></link><published>2018-07-08T00:00:00+00:00</published><updated>2018-07-08T00:00:00+00:00</updated><author><name>Junpeng Lao</name></author><id>tag:pyvideo.org,2018-07-08:pydata-berlin-2018/all-that-likelihood-with-pymc3.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The likelihood is a central concept in Bayesian computation. In this
tutorial, we will learn about what is the likelihood function and how do
we use it for inference. Using PyMC3, I will demonstrate how to get the
likelihood from a model, how does it connect to inference using NUTS or
Variational approximation, and some practical usage of the model
likelihood to perform model comparisons.&lt;/p&gt;
</summary></entry></feed>