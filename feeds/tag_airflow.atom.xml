<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_airflow.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-02-23T00:00:00+00:00</updated><entry><title>Building Analytics Workflow using Airflow and Spark</title><link href="https://pyvideo.org/pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html" rel="alternate"></link><published>2019-02-23T00:00:00+00:00</published><updated>2019-02-23T00:00:00+00:00</updated><author><name>Yohei Onishi</name></author><id>tag:pyvideo.org,2019-02-23:pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Yohei had built and operates a data analytics system for global retail logistics operations using Airflow and Spark since the end of last year. In this session, He will talk about how you can build a scalable analytics workflow system based on Airflow (Python) and write extensible job using Python. GCP has provided fully managed Airflow service called Cloud Composer. So he will explain how you can easily build Airflow cluster compared to building your own Airflow cluster on the on-premise server or AWS EC2.&lt;/p&gt;
</summary><category term="airflow"></category><category term="spark"></category><category term="analytics"></category></entry><entry><title>Workflow Engines Up and Running</title><link href="https://pyvideo.org/pycon-us-2018/workflow-engines-up-and-running.html" rel="alternate"></link><published>2018-05-09T00:00:00+00:00</published><updated>2018-05-09T00:00:00+00:00</updated><author><name>Ian Zelikman</name></author><id>tag:pyvideo.org,2018-05-09:pycon-us-2018/workflow-engines-up-and-running.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Join us for an introduction hands on tutorial of Python based workflow engines.&lt;/p&gt;
&lt;p&gt;You will get to create, run and monitor a real time workflow job with two Python based popular workflow engines: Airflow and Luigi.
Developers may have some long running batch jobs and may be using one of the tools we will cover or might be using a different engine, and would like a more in depth hands-on experience learning these tools.&lt;/p&gt;
</summary><category term="airflow"></category><category term="luigi"></category></entry><entry><title>Two approaches to scale your processing, Task Queues and Workflows</title><link href="https://pyvideo.org/pycon-ireland-2017/two-approaches-to-scale-your-processing-task-queues-and-workflows.html" rel="alternate"></link><published>2017-10-21T00:00:00+00:00</published><updated>2017-10-21T00:00:00+00:00</updated><author><name>Eoin Brazil</name></author><id>tag:pyvideo.org,2017-10-21:pycon-ireland-2017/two-approaches-to-scale-your-processing-task-queues-and-workflows.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk covers two useful libraries/approaches to scaling your processing. Task Queues will be introduced, Celery will be covered and a specific example of a refactoring of a sequential crawler to highlight the ease and utility of this library. Workflows will then be covered, Airflow will be introduced, in the context of more complex tasks with requirements plus dependencies, building on how tasks as the atomic unit can be woven together to support complex business / app requirements as Workflows. The theme linking both aspects is the need to provide a reliable async fabric (Celery + Rabbitmq) only provides part of a solution for more complex problems, hence the need to use Airflow (with Celery executors) to provide the additional smarts / logic necessary to solve them.&lt;/p&gt;
</summary><category term="airflow"></category><category term="celery"></category></entry><entry><title>How I learned to time travel, or, data pipelining and scheduling with Airflow</title><link href="https://pyvideo.org/pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Laura Lorenz</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418"&gt;http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data warehousing and analytics projects can, like ours, start out small - and fragile. With an organically growing mess of scripts glued together and triggered by cron jobs hiding on different servers, we needed better plumbing. After perusing the data pipelining landscape, we landed on Airflow, an Apache incubating batch processing pipelining and scheduler tool from Airbnb.&lt;/p&gt;
</summary><category term="airflow"></category><category term="Data"></category></entry><entry><title>A Pratctical Introduction to Airflow</title><link href="https://pyvideo.org/pydata-san-francisco-2016/a-pratctical-introduction-to-airflow.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Matt Davis</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/a-pratctical-introduction-to-airflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Airflow is a pipeline orchestration tool for Python that allows users to configure multi-system workflows that are executed in parallel across workers. I’ll cover the basics of Airflow so you can start your Airflow journey on the right foot. This talk aims to answer questions such as: What is Airflow useful for? How do I get started? What do I need to know that’s not in the docs?&lt;/p&gt;
&lt;p&gt;Airflow is a popular pipeline orchestration tool for Python that allows users to configure complex (or simple!) multi-system workflows that are executed in parallel across any number of workers. A single pipeline might contain bash, Python, and SQL operations. With dependencies specified between tasks, Airflow knows which ones it can run in parallel and which ones must run after others. Airflow is written in Python and users can add their own operators with custom functionality, doing anything Python can do.&lt;/p&gt;
&lt;p&gt;Moving data through transformations and from one place to another is a big part of data science/engineering, but there are only two widely-used orchestration systems for doing so that are written in Python: Luigi and Airflow. We’ve been using Airflow (&lt;a class="reference external" href="http://pythonhosted.org/airflow/"&gt;http://pythonhosted.org/airflow/&lt;/a&gt;) for several months at Clover Health and have learned a lot about its strengths and weaknesses. We use it to run several pipelines multiple times per day. One includes over 450 heavily linked tasks!&lt;/p&gt;
</summary><category term="airflow"></category></entry></feed>