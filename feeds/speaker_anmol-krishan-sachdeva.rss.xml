<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 25 Jul 2018 00:00:00 +0000</lastBuildDate><item><title>Understanding and Implementing Recurrent Neural Networks using Python</title><link>https://pyvideo.org/europython-2018/understanding-and-implementing-recurrent-neural-networks-using-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recurrent Neural Networks (RNNs) have become famous over time due to
their property of retaining internal memory. These neural nets are
widely used in recognizing patterns in sequences of data, like numerical
timer series data, images, handwritten text, spoken words, genome
sequences, and much more. Since these nets possess memory, there is a
certain analogy that we can make to the human brain in order to learn
how RNNs work. RNNs can be thought of as a network of neurons with
feedback connections, unlike feedforward connections which exist in
other types of Artificial Neural Networks.&lt;/p&gt;
&lt;p&gt;The flow of talk will be as follows: - Self Introduction - Introduction
to Deep Learning - Artificial Neural Networks (ANNs) - Diving DEEP into
Recurrent Neural Networks (RNNs) - Comparing Feedforward Networks with
Feedback Networks - Quick walkthrough: Implementing RNNs using Python
(Keras) - Understanding Backpropagation Through Time (BPTT) and
Vanishing Gradient Problem - Towards more sophisticated RNNs: Gated
Recurrent Units (GRUs)/Long Short-Term Memory (LSTMs) - End of talk -
Questions and Answers Session&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anmol Krishan Sachdeva</dc:creator><pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-07-25:europython-2018/understanding-and-implementing-recurrent-neural-networks-using-python.html</guid></item></channel></rss>