<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_holden-karau.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-07-08T00:00:00+00:00</updated><entry><title>Beam and Spark with Holden Karau: GCPPodcast 126</title><link href="https://pyvideo.org/google-cloud-platform/beam-and-spark-with-holden-karau-gcppodcast-126.html" rel="alternate"></link><published>2018-05-09T00:00:00+00:00</published><updated>2018-05-09T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2018-05-09:google-cloud-platform/beam-and-spark-with-holden-karau-gcppodcast-126.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Original post: &lt;a class="reference external" href="https://www.gcppodcast.com/post/episode-126-beam-and-spark-with-holden-karau/"&gt;https://www.gcppodcast.com/post/episode-126-beam-and-spark-with-holden-karau/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Holden Karau is on the podcast this week to talk all about Spark and Beam, two open source tools that helps process data at scale, with Mark and Melanie.&lt;/p&gt;
</summary></entry><entry><title>Keynote: Making the Big Data ecosystem work together with Python: Apache Arrow, Spark, Beam, and Dask</title><link href="https://pyvideo.org/pydata-london-2018/keynote-making-the-big-data-ecosystem-work-together-with-python-apache-arrow-spark-beam-and-dask.html" rel="alternate"></link><published>2018-04-28T00:00:00+00:00</published><updated>2018-04-28T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2018-04-28:pydata-london-2018/keynote-making-the-big-data-ecosystem-work-together-with-python-apache-arrow-spark-beam-and-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;TBD&lt;/p&gt;
</summary></entry><entry><title>Simplifying Training Deep &amp; Serving Learning Models with Big Data in Python using Tensorflow</title><link href="https://pyvideo.org/pydata-berlin-2018/simplifying-training-deep-serving-learning-models-with-big-data-in-python-using-tensorflow.html" rel="alternate"></link><published>2018-07-08T00:00:00+00:00</published><updated>2018-07-08T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2018-07-08:pydata-berlin-2018/simplifying-training-deep-serving-learning-models-with-big-data-in-python-using-tensorflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Deep Learning, in addition to being a world class tool for detecting the
presence of cats, requires large amounts of data for training. As much
vendors may say &amp;quot;no data prep required&amp;quot;, they are all excessively
optimistic*. This talk will look tools to build a deep learning
pipeline with feature prep on top of existing big data technologies
without rewriting your code for on-line serving.&lt;/p&gt;
</summary></entry><entry><title>Debugging PySpark -- Or trying to make sense of a JVM stack trace when you were minding your own bus</title><link href="https://pyvideo.org/pycon-us-2018/debugging-pyspark-or-trying-to-make-sense-of-a-jvm-stack-trace-when-you-were-minding-your-own-bus.html" rel="alternate"></link><published>2018-05-11T00:00:00+00:00</published><updated>2018-05-11T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2018-05-11:pycon-us-2018/debugging-pyspark-or-trying-to-make-sense-of-a-jvm-stack-trace-when-you-were-minding-your-own-bus.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common errors and how to detect them.&lt;/p&gt;
&lt;p&gt;Spark’s own internal logging can often be quite verbose, and this talk will examine how to effectively search logs from Apache Spark to spot common problems. In addition to the internal logging, this talk will look at options for logging from within our program itself.&lt;/p&gt;
&lt;p&gt;Spark’s accumulators have gotten a bad rap because of how they interact in the event of cache misses or partial recomputes, but this talk will look at how to effectively use Spark’s current accumulators for debugging as well as a look to future for data property type accumulators which may be coming to Spark in future version.&lt;/p&gt;
&lt;p&gt;In addition to reading logs, and instrumenting our program with accumulators, Spark’s UI can be of great help for quickly detecting certain types of problems.&lt;/p&gt;
&lt;p&gt;Debuggers are a wonderful tool, however when you have 100 computers the “wonder” can be a bit more like “pain”. This talk will look at how to connect remote debuggers, but also remind you that it’s probably not the easiest path forward.&lt;/p&gt;
</summary><category term="debug"></category><category term="pySpark"></category></entry><entry><title>Improving PySpark Performance: Spark performance beyond the JVM</title><link href="https://pyvideo.org/pycon-au-2017/improving-pyspark-performance-spark-performance-beyond-the-jvm.html" rel="alternate"></link><published>2017-08-04T00:00:00+00:00</published><updated>2017-08-04T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2017-08-04:pycon-au-2017/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames/Datasets and traditional RDDs with Python. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</summary></entry><entry><title>The Magic Behind PySpark, how it impacts perf &amp; the "future"</title><link href="https://pyvideo.org/pydata-barcelona-2017/the-magic-behind-pyspark-how-it-impacts-perf-the-future.html" rel="alternate"></link><published>2017-05-20T10:30:00+02:00</published><updated>2017-05-20T10:30:00+02:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/the-magic-behind-pyspark-how-it-impacts-perf-the-future.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A look at how PySpark &amp;quot;works&amp;quot; today and how we can make it better in the future + insert engine noises of a fast car +&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk will introduce PySpark along with the magic done to make it work and be friends with the JVM. We will discuss why lazy evaluation makes a huge difference in PySpark, both in terms of general optimizations it opens up as well as Python specific considerations. From there we will explore much of the future of Spark, DataFrames &amp;amp; Datasets and what this means for PySpark. Most Spark DataFrame examples limit them selves to things written in the relational style query language, but we will explore how to add more functionality through UDFS.&lt;/p&gt;
&lt;p&gt;We will wrap up with looking at the different pieces of work being done to make PySpark faster, from using better interchange formats like Apache Arrow to crazy hair brained schemes inspired by (but not the fault of) the Javascript on Spark project.&lt;/p&gt;
&lt;p&gt;Hopefully no one is scared away from using Spark once they see the 300 small gnome like creatures behind the curtain, but parental guidance is encouraged for those who still believe in magic, reliable distributed systems, and vendor marketing brochures.&lt;/p&gt;
</summary><category term="keynote"></category><category term="pyspark"></category></entry><entry><title>Debugging PySpark - Pretending to make sense of JVM stack traces</title><link href="https://pyvideo.org/pydata-amsterdam-2017/debugging-pyspark-pretending-to-make-sense-of-jvm-stack-traces.html" rel="alternate"></link><published>2017-04-09T00:00:00+00:00</published><updated>2017-04-09T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/debugging-pyspark-pretending-to-make-sense-of-jvm-stack-traces.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common er&lt;/p&gt;
&lt;p&gt;Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common errors and how to detect them.&lt;/p&gt;
&lt;p&gt;Spark’s own internal logging can often be quite verbose, and this talk will examine how to effectively search logs from Apache Spark to spot common problems. In addition to the internal logging, this talk will look at options for logging from within our program itself.&lt;/p&gt;
&lt;p&gt;Spark’s accumulators have gotten a bad rap because of how they interact in the event of cache misses or partial recomputes, but this talk will look at how to effectively use Spark’s current accumulators for debugging as well as a look to future for data property type accumulators which may be coming to Spark in future version.&lt;/p&gt;
&lt;p&gt;In addition to reading logs, and instrumenting our program with accumulators, Spark’s UI can be of great help for quickly detecting certain types of problems.&lt;/p&gt;
</summary></entry><entry><title>Sparkling Pandas- Letting Pandas Roam on Spark DataFrames</title><link href="https://pyvideo.org/pydata-seattle-2015/sparkling-pandas-letting-pandas-roam-on-spark-dataframes.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/sparkling-pandas-letting-pandas-roam-on-spark-dataframes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a fast and expressive library for data analysis that doesn’t naturally scale to more data than can fit in memory. PySpark is the Python API for Apache Spark that is designed to scale to huge amounts of data but lacks the natural expressiveness of Pandas. This talk introduces Sparkling Pandas, a library that brings together the best features of Pandas and PySpark.&lt;/p&gt;
&lt;p&gt;Pandas is a fast and expressive library for data analysis that doesn’t naturally scale to more data than can fit in memory. PySpark is the Python API for Apache Spark that is designed to scale to huge amounts of data but lacks the natural expressiveness of Pandas. This talk introduces Sparkling Pandas, a library that brings together the best features of Pandas and PySpark; Expressiveness, speed, and scalability.&lt;/p&gt;
&lt;p&gt;While both Spark 1.3 and Pandas have classes named ‘DataFrame’ the Pandas DataFrame API is broader and not fully covered by the ‘DataFrame’ class in Spark. This talk will explore some of the differences between Spark’s DataFrames and Panda’s DataFrames and then examine some of the work done to implement Panda’s like DataFrames on top of Spark. In some cases, providing Pandas like functionality is computationally expensive in a distributed environment, and we will explore some techniques to minimize this cost.&lt;/p&gt;
&lt;p&gt;At the end of this talk you should have a better understanding of both Sparkling Pandas and Spark’s own DataFrames. Whether you end up using Sparkling Pandas or Spark directly, you will have a greater understanding of how to work with structured data in a distributed context using Apache Spark and familiar DataFrame APIs.&lt;/p&gt;
&lt;p&gt;Materials available here:
Slides: &lt;a class="reference external" href="http://www.slideshare.net/hkarau/sparkling-pandas-electric-bugaloo-py-data-seattle-2015"&gt;http://www.slideshare.net/hkarau/sparkling-pandas-electric-bugaloo-py-data-seattle-2015&lt;/a&gt;
Project github: &lt;a class="reference external" href="https://github.com/sparklingpandas/sparklingpandas"&gt;https://github.com/sparklingpandas/sparklingpandas&lt;/a&gt;
Project website: &lt;a class="reference external" href="http://sparklingpandas.com/"&gt;http://sparklingpandas.com/&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>A brief introduction to Distributed Computing with PySpark</title><link href="https://pyvideo.org/pydata-seattle-2015/a-brief-introduction-to-distributed-computing-with-pyspark.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/a-brief-introduction-to-distributed-computing-with-pyspark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Spark is a fast and general engine for distributed computing &amp;amp; big data processing with APIs in Scala, Java, Python, and R. This tutorial will briefly introduce PySpark (the Python API for Spark) with some hands-on-exercises combined with a quick introduction to Spark's core concepts. We will cover the obligatory wordcount example which comes in with every big-data tutorial, as well as discuss Spark's unique methods for handling node failure and other relevant internals. Then we will briefly look at how to access some of Spark's libraries (like Spark SQL &amp;amp; Spark ML) from Python. While Spark is available in a variety of languages this workshop will be focused on using Spark and Python together.&lt;/p&gt;
&lt;p&gt;This tutorial is intended for people new to Spark/PySpark, please install Spark (1.3.1 or later) from &lt;a class="reference external" href="http://spark.apache.org/downloads.html"&gt;http://spark.apache.org/downloads.html&lt;/a&gt; before class (we are working to have cluster resources available but having a local install is sufficient for the workshop and a good backup in case the WiFi isn't cooperating).&lt;/p&gt;
&lt;p&gt;Materials available here:
Slides: &lt;a class="reference external" href="http://www.slideshare.net/hkarau/a-really-really-fast-introduction-to-py-spark-lightning-fast-cluster-computing-with-python-1"&gt;http://www.slideshare.net/hkarau/a-really-really-fast-introduction-to-py-spark-lightning-fast-cluster-computing-with-python-1&lt;/a&gt;
Notebook:  &lt;a class="reference external" href="https://github.com/holdenk/intro-to-pyspark-demos/blob/master/ipython/Super-Fast-PySpark-Intro-PyData-Seattle-2015.ipynb"&gt;https://github.com/holdenk/intro-to-pyspark-demos/blob/master/ipython/Super-Fast-PySpark-Intro-PyData-Seattle-2015.ipynb&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Improving PySpark Performance Spark performance beyond the JVM</title><link href="https://pyvideo.org/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</summary><category term="jvm"></category><category term="performance"></category><category term="pyspark"></category><category term="spark"></category></entry><entry><title>Improving PySpark Performance: Spark performance beyond the JVM</title><link href="https://pyvideo.org/pydata-amsterdam-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames/Datasets and traditional RDDs with Python. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</summary></entry><entry><title>Sparkling Pandas - using Apache Spark to scale Pandas</title><link href="https://pyvideo.org/pygotham-2014/sparkling-pandas-using-apache-spark-to-scale-pa.html" rel="alternate"></link><published>2014-09-17T00:00:00+00:00</published><updated>2014-09-17T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2014-09-17:pygotham-2014/sparkling-pandas-using-apache-spark-to-scale-pa.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a fast and expressive library for data analysis that doesn’t
naturally scale to more data than can fit in memory. PySpark is the
Python API for Apache Spark that is designed to scale to huge amounts of
data but lacks the natural expressiveness of Pandas. We will introduce
Sparkling Pandas, a new library that brings together the best features
of Pandas and PySpark; Expressiveness, speed, and scalability.&lt;/p&gt;
</summary></entry></feed>