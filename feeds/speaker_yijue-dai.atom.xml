<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Yijue Dai</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_yijue-dai.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>An Interpretable and Sample Efficient Deep Kernel for Gaussian Process</title><link href="https://pyvideo.org/uai-2020/an-interpretable-and-sample-efficient-deep-kernel-for-gaussian-process.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Yijue Dai</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/an-interpretable-and-sample-efficient-deep-kernel-for-gaussian-process.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;An Interpretable and Sample Efficient Deep Kernel for Gaussian Process&lt;/p&gt;
&lt;p&gt;Yijue Dai (The Chinese University of Hong Kong, Shenzhen)*; Tianjian Zhang (The Chinese University of Hong Kong, Shenzhen); Zhidi Lin (The Chinese University of Hong Kong, Shenzhen); Feng Yin (The Chinese University of Hong Kong, Shenzhen); Sergios Theodoridis (National â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;An Interpretable and Sample Efficient Deep Kernel for Gaussian Process&lt;/p&gt;
&lt;p&gt;Yijue Dai (The Chinese University of Hong Kong, Shenzhen)*; Tianjian Zhang (The Chinese University of Hong Kong, Shenzhen); Zhidi Lin (The Chinese University of Hong Kong, Shenzhen); Feng Yin (The Chinese University of Hong Kong, Shenzhen); Sergios Theodoridis (National and Kapodistrian University of Athens); Shuguang Cui (The Chinese University of Hong Kong, Shenzhen )&lt;/p&gt;
&lt;p&gt;We propose a novel Gaussian process kernel that takes advantage of a deep neural network (DNN) structure but retains good interpretability. The resulting kernel is capable of addressing four major issues of the previous works of similar art, i.e., the optimality, explainability, model complexity, and sample efficiency. Our kernel design procedure comprises three steps: (1) Derivation of an optimal kernel with a non-stationary dot product structure that minimizes the prediction/test mean-squared-error (MSE); (2) Decomposition of this optimal kernel as a linear combination of shallow DNN subnetworks with the aid of multi-way feature interaction detection; (3) Updating the hyper-parameters of the subnetworks via an alternating rationale until convergence. The designed kernel does not sacrifice interpretability for optimality. On the contrary, each subnetwork explicitly demonstrates the interaction of a set of features in a transformation function, leading to a solid path toward explainable kernel learning. We test the proposed kernel with both synthesized and real-world data sets, and the proposed kernel is superior to its competitors in terms of prediction performance in most cases. Moreover, it tends to maintain the prediction performance and be robust to data over-fitting issue, when reducing the number of samples. &amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>