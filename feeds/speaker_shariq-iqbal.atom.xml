<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_shariq-iqbal.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-09-16T00:00:00+00:00</updated><entry><title>Dynamic Object-Gaze Tracking with OpenCV</title><link href="https://pyvideo.org/pydata-carolinas-2016/dynamic-object-gaze-tracking-with-opencv.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Shariq Iqbal</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/dynamic-object-gaze-tracking-with-opencv.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Using computer vision techniques, we extended eye tracking technology
to allow for data normalization across dynamic environments. We
applied these techniques to subjects viewing artwork at Duke’s Nasher
Museum of Art.&lt;/p&gt;
&lt;p&gt;Most eye tracking solutions track gaze with respect to a static object
like a computer screen, making it easy to know exactly where a person
is looking with respect to an image on the screen. This makes analysis
easier, but doesn’t accurately reflect real-life. What happens when we
move eye tracking into a more realistic, dynamic setting, using eye
tracking glasses that allow people to move around? People can interact
with objects in a much more natural manner, but a new challenge is
introduced: We only have gaze data with respect to the glasses frame
of reference. In order to apply conventional analysis methods to these
data, we need to map dynamic gaze back onto a static reference image,
compensating for distance, head movement, and perspective.&lt;/p&gt;
&lt;p&gt;Using the OpenCV package and its efficient implementations of common
computer vision algorithms, we developed a method to find objects of
interest in video from eye tracking glasses and return gaze
coordinates over those objects, enabling experimenters to apply
conventional data analysis methods to eye tracking behavior obtained
in dynamic, real-world situations.&lt;/p&gt;
</summary></entry></feed>