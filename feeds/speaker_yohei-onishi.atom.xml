<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_yohei-onishi.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-02-23T00:00:00+00:00</updated><entry><title>Building Analytics Workflow using Airflow and Spark</title><link href="https://pyvideo.org/pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html" rel="alternate"></link><published>2019-02-23T00:00:00+00:00</published><updated>2019-02-23T00:00:00+00:00</updated><author><name>Yohei Onishi</name></author><id>tag:pyvideo.org,2019-02-23:pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Yohei had built and operates a data analytics system for global retail logistics operations using Airflow and Spark since the end of last year. In this session, He will talk about how you can build a scalable analytics workflow system based on Airflow (Python) and write extensible job using Python. GCP has provided fully managed Airflow service called Cloud Composer. So he will explain how you can easily build Airflow cluster compared to building your own Airflow cluster on the on-premise server or AWS EC2.&lt;/p&gt;
</summary><category term="airflow"></category><category term="spark"></category><category term="analytics"></category></entry></feed>