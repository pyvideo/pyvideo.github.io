<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_miroslav-batchkarov.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-06-30T00:00:00+00:00</updated><entry><title>Gold standard data: lessons from the trenches</title><link href="https://pyvideo.org/pydata-berlin-2017/gold-standard-data-lessons-from-the-trenches.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Miroslav Batchkarov</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/gold-standard-data-lessons-from-the-trenches.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The first stage in a data science project is often to collect training data. However, getting a good data set is surprisingly tricky and takes longer than one expects. This talk describes our experiences in labelling gold-standard data and the lessons we learnt the hard way. We will present three case studies from natural language processing and discuss the challenges we encountered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is often said that rather than spending a month figuring out how to apply unsupervised learning to a problem domain, a data scientist should spend a week labelling data. However, the difficulty of annotating data is often underestimated. Gathering a sufficiently large collection of good-quality labelled data requires careful problem definition and multiple iterations. In this talk, I will describe three case studies and lessons learnt from them. Each case shows several aspect of the process that should be considered in advance to ensure the project is successful.&lt;/p&gt;
</summary></entry></feed>