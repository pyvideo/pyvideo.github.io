<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_artur-bujak.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-12-13T00:00:00+00:00</updated><entry><title>How we personalized onet.pl with multi-armed bandits</title><link href="https://pyvideo.org/pydata-warsaw-2019/how-we-personalized-onetpl-with-multi-armed-bandits.html" rel="alternate"></link><published>2019-12-13T00:00:00+00:00</published><updated>2019-12-13T00:00:00+00:00</updated><author><name>Artur Bujak</name></author><id>tag:pyvideo.org,2019-12-13:pydata-warsaw-2019/how-we-personalized-onetpl-with-multi-armed-bandits.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Imagine you need to choose ten articles out of hundreds in a way that
maximizes your profit. It's not as easy as it seems. In this talk, we
will explain how we prepare recommendations on the onet.pl home page for
millions of users with the use of a multi-armed bandit algorithm.&lt;/p&gt;
&lt;p&gt;Multi-armed bandits are a powerful solution for a diversity of
optimization problems that demand a balance between using existing
knowledge about item performance and acquiring new one. That's why we
would like to focus on the intuition behind the multi-armed bandit
approach and its application in recommender systems on the example of
onet.pl home page. Also, we will introduce E-greedy, UCB and Thompson
Sampling bandits, discuss their pros and cons and show how to tune them
in a simulated environment.&lt;/p&gt;
</summary></entry></feed>