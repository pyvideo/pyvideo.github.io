<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_mike-williams.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-07-16T00:00:00+00:00</updated><entry><title>Summarizing documents</title><link href="https://pyvideo.org/pygotham-2016/summarizing-documents.html" rel="alternate"></link><published>2016-07-16T00:00:00+00:00</published><updated>2016-07-16T00:00:00+00:00</updated><author><name>Mike Williams</name></author><id>tag:pyvideo.org,2016-07-16:pygotham-2016/summarizing-documents.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Extractive summarization — finding the salient points in a document or corpus — is one of the most fundamental tasks in natural language processing. I’ll show you three ways to do it. One dates back to an IBM Journal article from 1958. One uses topic modeling, a technology from the 2000s. And one uses neural network-derived language embeddings and long short term memory networks — techniques that are only a couple of years old. I’ll explain the algorithms, show code and demos for all three, and I’ll discuss the engineering trade-offs.&lt;/p&gt;
</summary></entry></feed>