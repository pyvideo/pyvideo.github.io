<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Mathis Lucka</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_mathis-lucka.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-04-17T00:00:00+00:00</updated><subtitle></subtitle><entry><title>How to Feed Facts to Large Language Models and Reduce Hallucination</title><link href="https://pyvideo.org/pydata-berlin-2023/how-to-feed-facts-to-large-language-models-and-reduce-hallucination.html" rel="alternate"></link><published>2023-04-17T00:00:00+00:00</published><updated>2023-04-17T00:00:00+00:00</updated><author><name>Mathis Lucka</name></author><id>tag:pyvideo.org,2023-04-17:/pydata-berlin-2023/how-to-feed-facts-to-large-language-models-and-reduce-hallucination.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large Language Models (LLM), like ChatGPT, have shown miraculous performances on various tasks. But there are still unsolved issues with these models: they can be confidently wrong and their knowledge becomes outdated. GPT also does not have any of the information that you have stored in your own data â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large Language Models (LLM), like ChatGPT, have shown miraculous performances on various tasks. But there are still unsolved issues with these models: they can be confidently wrong and their knowledge becomes outdated. GPT also does not have any of the information that you have stored in your own data. In this talk, you'll learn how to use Haystack, an open source framework, to chain LLMs with other models and components to overcome these issues. We will build a practical application using these techniques. And you will walk away with a deeper understanding of how to use LLMs to build NLP products that work.&lt;/p&gt;
</content><category term="PyData Berlin 2023"></category></entry></feed>