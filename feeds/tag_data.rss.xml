<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Fri, 12 Jul 2019 00:00:00 +0000</lastBuildDate><item><title>Astro Pi: Python on the International Space Station</title><link>https://pyvideo.org/europython-2019/astro-pi-python-on-the-international-space-station.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A collaboration between the Raspberry Pi Foundation and the European
Space Agency put two Raspberry Pi computers augmented with sensor boards
and camera modules on the International Space Station in 2015. Every
year we run a series of competitions for kids in schools around Europe
to design science experiments using the available sensors.&lt;/p&gt;
&lt;p&gt;Mission Zero is a low-barrier challenge where students can run a 1
minute Python program in space to display a message to the astronauts.
They have access to the sensors for conditional logic but cannot record
data or take photos.&lt;/p&gt;
&lt;p&gt;Mission Space Lab is a more involved challenge, including planning an
experiment, writing and testing code which will run for 3 hours in
space, either studying life in space or life on earth (which includes
taking photos of Earth out of the ISS window). MSL teams get to keep the
data and photo they record in their experiment and are to write a report
analysing their findings.&lt;/p&gt;
&lt;p&gt;A small tech team at the Raspberry Pi Foundation maintain the operating
system used in flight and work in collaboration with ESA and partners to
keep the operation of the Pis running smoothly on the ISS LAN.&lt;/p&gt;
&lt;p&gt;As well as sharing details of the OS maintenance and devops for the
Astro Pis, I'll share photos taken from space and show findings from
student experiments using opencv, tensorflow, scikit-learn, ephem and
more.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ben Nuttall</dc:creator><pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-12:europython-2019/astro-pi-python-on-the-international-space-station.html</guid><category>Data</category><category>Education</category><category>Linux</category><category>OpenCV</category><category>Raspberry PI</category></item><item><title>Building Data Workflows with Luigi and Kubernetes</title><link>https://pyvideo.org/europython-2019/building-data-workflows-with-luigi-and-kubernetes.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will focus on how one can build complex data pipelines in
Python. I will introduce Luigi and show how it solves problems while
running multiple chain of batch jobs like dependency resolution,
workflow management, visualisation, failure handling etc.&lt;/p&gt;
&lt;p&gt;After that, I will present how to package Luigi pipelines as Docker
image for easier testing and deployment. Finally, I will go through way
to deploy them on Kubernetes cluster, thus making it possible to scale
Big Data pipelines on- demand and reduce infrastructure costs. I will
also give tips and tricks to make Luigi Scheduler play well with
Kubernetes batch execution feature.&lt;/p&gt;
&lt;p&gt;This talk will be accompanied by demo project. It will be very
beneficial for audience who have some experience in running batch jobs
(not necessarily in Python), typically people who work in Big Data
sphere like data scientists, data engineers, BI devs and software
developers. Familiarity with Python is helpful but not needed.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nar Kumar Chhantyal</dc:creator><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-11:europython-2019/building-data-workflows-with-luigi-and-kubernetes.html</guid><category>Architecture</category><category>Big Data</category><category>Data</category><category>Distributed Systems</category><category>Scaling</category></item><item><title>Machine learning on non curated data</title><link>https://pyvideo.org/europython-2019/machine-learning-on-non-curated-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;According to industry surveys [1], the number one hassle of data
scientists is cleaning the data to analyze it. Textbook statistical
modeling is sufficient for noisy signals, but errors of a discrete
nature break standard tools of machine learning. I will discuss how to
easily run machine learning on data tables with two common dirty-data
problems: missing values and non-normalized entries. On both problems, I
will show how to run standard machine-learning tools such as
scikit-learn in the presence of such errors. The talk will be didactic
and will discuss simple software solutions. It will build on the latest
improvements to scikit-learn for missing values and the DirtyCat package
[2] for non normalized entries. I will also summarize theoretical
analyses in recent machine learning publications.&lt;/p&gt;
&lt;p&gt;This talk targets data practitioners. Its goal are to help data
scientists to be more efficient analysing data with such errors and
understanding their impacts.&lt;/p&gt;
&lt;p&gt;With missing values, I will use simple arguments and examples to outline
how to obtain asymptotically good predictions [3]. Two components are
key: imputation and adding an indicator of missingness. I will explain
theoretical guidelines for these, and I will show how to implement these
ideas in practice, with scikit-learn as a learner, or as a preprocesser.&lt;/p&gt;
&lt;p&gt;For non-normalized categories, I will show that using their string
representations to “vectorize” them, creating vectorial representations
gives a simple but powerful solution that can be plugged in standard
statistical analysis tools [4].&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;[1] Kaggle, the state of ML and data science 2017
&lt;a class="reference external" href="https://www.kaggle.com/surveys/2017"&gt;https://www.kaggle.com/surveys/2017&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[2] &lt;a class="reference external" href="https://dirty-cat.github.io/stable/"&gt;https://dirty-cat.github.io/stable/&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[3] Josse Julie, Prost Nicolas, Scornet Erwan, and Varoquaux Gaël
(2019). “On the consistency of supervised learning with missing
values”. &lt;a class="reference external" href="https://arxiv.org/abs/1902.06931"&gt;https://arxiv.org/abs/1902.06931&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;[4] Cerda Patricio, Varoquaux Gaël, and Kégl Balázs. &amp;quot;Similarity
encoding for learning with dirty categorical variables.&amp;quot; Machine
Learning 107.8-10 (2018): 1477 &lt;a class="reference external" href="https://arxiv.org/abs/1806.00979"&gt;https://arxiv.org/abs/1806.00979&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gael Varoquaux</dc:creator><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-11:europython-2019/machine-learning-on-non-curated-data.html</guid><category>Big Data</category><category>Data</category><category>Data Science</category><category>Machine-Learning</category><category>Scientific Libraries (Numpy/Pandas/SciKit/...)</category></item><item><title>The state of Machine Learning Operations in 2019</title><link>https://pyvideo.org/europython-2019/the-state-of-machine-learning-operations-in-2019.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will provide an overview of the key challenges and trends in
the productization of machine learning systems, including concepts such
as reproducibility, explainability and orchestration. The talk will also
provide a high level overview of several key open source tools and
frameworks available to tackle these issues, which have been identifyed
putting together the Awesome Machine Learning Operations list
(&lt;a class="reference external" href="https://github.com/EthicalML/awesome-machine-learning-operations"&gt;https://github.com/EthicalML/awesome-machine-learning-operations&lt;/a&gt;).&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;The key concepts that will be covered are:&lt;/div&gt;
&lt;div class="line"&gt;* Reproducibility&lt;/div&gt;
&lt;div class="line"&gt;* Explainability&lt;/div&gt;
&lt;div class="line"&gt;* Orchestration of models&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The reproducibility piece will cover key motivations as well as
practical requirements for model versioning, together with tools that
allow data scientists to achieve version control of model+config+data to
ensure full model lineage.&lt;/p&gt;
&lt;p&gt;The explainability piece will contain a high level overview of why this
has become an important topic in machine learning, including the high
profile incidents that tech companies have experienced where undesired
biases have slipped into data. This will also include a high level
overview of some of the tools available.&lt;/p&gt;
&lt;p&gt;Finally, the orchestration piece will cover some of the fundamental
challenges with large scale serving of models, together with some of the
key tools that are available to ensure this challenge can be tackled.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alejandro Saucedo</dc:creator><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-11:europython-2019/the-state-of-machine-learning-operations-in-2019.html</guid><category>Architecture</category><category>Data</category><category>Data Science</category><category>Deep Learning</category><category>Machine-Learning</category></item><item><title>Building a Powerful Pet Detector in Notebooks</title><link>https://pyvideo.org/europython-2019/building-a-powerful-pet-detector-in-notebooks.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Ever wondered what breed that dog or cat is? Let’s build a pet detector
service to recognize them in pictures! In this talk, we will walk
through the training, optimizing, and deploying of a deep learning model
using Azure Notebooks. We will use transfer learning to retrain a
MobileNet model using TensorFlow to recognize dog and cat breeds using
the Oxford IIIT Pet Dataset. Next, we’ll optimize the model and tune our
hyperparameters to improve the model accuracy. Finally, we will deploy
the model as a web service in. Come to learn how you can quickly create
accurate image recognition models with a few simple techniques!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Katherine Kampf</dc:creator><pubDate>Wed, 10 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-10:europython-2019/building-a-powerful-pet-detector-in-notebooks.html</guid><category>Data</category><category>Data Science</category><category>Deep Learning</category><category>Jupyter</category><category>Machine-Learning</category></item><item><title>A Worked Intro to Scikit-learn</title><link>https://pyvideo.org/pycon-italia-2019/a-worked-intro-to-scikit-learn.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A talk introducing the audience to Scikit-learn as a library aimed at
people who know Python at a beginner/intermediate level but are new to
machine learning concepts. The goal of the talk is for the audience to
leave with an understanding of the foundations of machine learning while
respecting how easy it is to make a wrong choice that invalidates your
model.&lt;/p&gt;
&lt;p&gt;It will be a short background on scikit-learn followed by a livecoding
demo where I demonstrate how scikit-learn works and detail common
pitfalls.&lt;/p&gt;
&lt;p&gt;I will demonstrate ways of coping with problems such as data leakage,
the importance of train-test splits, choosing metrics wisely, and
explain how cross-validation works and why we use it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1596"&gt;https://python.it/feedback-1596&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Saturday 4 May&lt;/strong&gt; at 10:45 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anders Bogsnes</dc:creator><pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-04:pycon-italia-2019/a-worked-intro-to-scikit-learn.html</guid><category>pydata</category><category>Python</category><category>scikit-learn</category><category>Machine Learning</category><category>analytics</category><category>data</category><category>sklearn</category></item><item><title>Pandas ecosystem 2019</title><link>https://pyvideo.org/pycon-italia-2019/pandas-ecosystem-2019.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas is more than 10 years old now. In this time, it became almost a
standard for building data pipelines and perform data analysis in
Python. As the popularity of the project grows, it also grows the number
of projects that depend or interact with pandas.&lt;/p&gt;
&lt;p&gt;This talk will cover this ecosystem of projects around pandas, mainly in
the prespective of scalability and performance. Discussing for example
how projects like Arrow are key for the future of pandas, or how Dask is
overcoming pandas limitations.&lt;/p&gt;
&lt;p&gt;In a first part, the talk will focus on pandas itself, its components,
and its architecture. This will give the required context for a second
part, that will explain related projects, how they interact with pandas,
and what the whole ecosystem can offer to users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1613"&gt;https://python.it/feedback-1613&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 12:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marc Garcia</dc:creator><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-03:pycon-italia-2019/pandas-ecosystem-2019.html</guid><category>pydata</category><category>analytics</category><category>data-analysis</category><category>Data Mining</category><category>data</category><category>pandas</category></item><item><title>Open Sourcing at Work</title><link>https://pyvideo.org/pycon-ca-2018/open-sourcing-at-work.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We just open sourced 2 projects (datacompy, and locopy) with roots in Data Science and Engineering which we will showcase. While is it exciting and rewarding to share your ideas with the world it isn't always easy. Thinking about licenses, copyrights, and protecting confidential information is a must!&lt;/p&gt;
&lt;p&gt;Working in a large organization which is embracing the mantra 'open source first' is really exciting. Part of this journey is to make sure we give back to the open source community when we can. Two of our projects had gained traction internally: datacompy, and locopy. As part of our commitment we wanted to make sure we could open source these projects for others to use and contribute back to. DataComPy is a package to compare two Pandas DataFrames. Originally started to be something of a replacement for SAS's PROC COMPARE for Pandas DataFrames with some more functionality than just Pandas.DataFrame.equals(Pandas.DataFrame) (in that it prints out some stats, and lets you tweak how accurate matches have to be). Then extended to carry that functionality over to Spark Dataframes. Locopy helps load flat files to S3 and then to Amazon Redshift, and assist with ETL processing. It is DB Driver (Adapter) agnostic, provides basic functionality to move data to S3 buckets, execute COPY commands to load data to S3, and into Redshift, and UNLOAD commands to unload data from Redshift into S3. While building these products was exciting and fun, some of the legal considerations were as interesting, complex, and required collaboration between many teams, from security, licensing, brand, and IP/copyright. We'll explore the projects, and some of these other considerations which can make or break if you decide to release a project into the wild, along with the road blocks we faced with in these areas.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Faisal Dosani</dc:creator><pubDate>Sun, 11 Nov 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-11-11:pycon-ca-2018/open-sourcing-at-work.html</guid><category>open source</category><category>licensing</category><category>copyright</category><category>data</category><category>security</category><category>testing</category><category>best practices</category><category>data science</category></item><item><title>Hacking Your Way Into Machine Learning</title><link>https://pyvideo.org/pycon-italia-2018/hacking-your-way-into-machine-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You might have heard of Machine Learning from your co-worker or in a local meetup and are enticed to get started but not sure how to take that first step. Confused between different sources, where to start from or how to proceed given a particular problem statement or dataset, then this talk is for you. It is aimed at complete beginners ( maybe you? ) who are just starting in machine learning and are ready to commit.
The talk will go something like this - each of the following items will be explained how it’s useful and why we should use it. Then alongside showcase, that same step applied to the real example(dataset) of that particular item so that the audience will be able to grasp the idea. It will add to around 35 minutes leaving us with 10 minutes for Q&amp;amp;A.
1) Context ( 5 mins ):
Discuss why we need Machine Learning and how we can use Machine Learning in different domains.
2) Resources ( 3 mins):
Talks about the dataset availability, online competitions, and Open Source libraries such as Scikit-learn, Matplotlib, Keras.
3) Jupyter Notebook (25 mins):
This Jupyter notebook will be a great starting point for most Supervised Machine Learning projects that involve common tasks: a) Imports and data loading (2 mins )
b) Data Exploration (5 mins)
c) Data Cleaning (3 mins)
d) Feature Engineering (4 mins)
e) Model Exploration (6 mins)
f) Final Model Building and Prediction ( 5 mins)
4) Wrap up ( 2 mins ):
Finalizing my talk, sharing some tips etc.
5) Q&amp;amp;A ( 10 mins ):
Question and Answering with the Audience.
Hope to inspire the audience to get started with machine learning, explore different domains, to learn, to create and engage with the Machine Learning Community.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Laksh Arora</dc:creator><pubDate>Sat, 21 Apr 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-04-21:pycon-italia-2018/hacking-your-way-into-machine-learning.html</guid><category>data-analysis</category><category>data-visualization</category><category>Python</category><category>scikit-learn</category><category>matplotlib</category><category>analytics</category><category>scipy</category><category>machine-learning</category><category>data</category><category>Statistical Learning</category></item><item><title>Predicting future states using High Order Markov Chains</title><link>https://pyvideo.org/pycon-italia-2018/predicting-future-states-using-high-order-markov-chains.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In modern automated systems (Interactive Voice Response, help chatbots,
routing systems, etc.) it is very often important to be able to predict
what is the most likely next step for the current user. One way of
addressing this issue is using sequence algorithms such as Markov
Chains.&lt;/p&gt;
&lt;p&gt;After a quick introduction to the concept of Markov chains and Markov
processes, we will explore the basics and the implementation of a simple
High Order Markov chain to predict what the most likely next state in a
sequence, based on previous states. We will be using anonymized
real-life data of an automated system and we will try to come up with a
model that can give us the most probable next state using Markov chains
of different orders.&lt;/p&gt;
&lt;p&gt;Things we will see in detail: - Mathematics and rationale behind Markov
Chains; - Basic implementation of First Order Markov Chains; -
Implementation of High Order Markov Chains; - Real life application of
the developed model.&lt;/p&gt;
&lt;p&gt;An undergraduate level of understanding of Linear Algebra and basic
Python skills will be useful to follow the talk.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 15:30 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Pietro Mascolo</dc:creator><pubDate>Sat, 21 Apr 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-04-21:pycon-italia-2018/predicting-future-states-using-high-order-markov-chains.html</guid><category>pydata</category><category>Pyston</category><category>algebra</category><category>Machine Learning</category><category>scipy</category><category>data-analysis</category><category>mathematics</category><category>data</category><category>python3</category></item><item><title>Using Python to bring democracy to the A.I. age</title><link>https://pyvideo.org/pycon-italia-2018/using-python-to-bring-democracy-to-the-ai-age.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;TL;DR&lt;/p&gt;
&lt;div class="section" id="when-you-go-full-big-data-at-public-data-and-become-a-citzen"&gt;
&lt;h4&gt;When you go full Big Data at public data and become a citzen.&lt;/h4&gt;
&lt;p&gt;Audience type: developers, data scientists of any level of expertise.&lt;/p&gt;
&lt;p&gt;After a political coup Brazil drowned in scandals and political
disbelief. That was the final straw for us.&lt;/p&gt;
&lt;p&gt;We created a bot persona who uses Machine Learning to analyze public
spending, launching our own data journalism investigations. As expected
we use the internet publicize our findings and icing on it was to use
Twitter to directly engage the public and politicians under the topic of
suspicious expenses.&lt;/p&gt;
&lt;p&gt;Come with me and I’ll show some figures from Brazilian corruption, share
some code and cherry-pick the best of our toolbox to deal with public
data and machine learning. I’ll introduce our public dashboard that
makes visualization and browsing government data easy peasy. And surely
we can take a look in some tweets from Rosie, the robot, and how some
politicians are now vociferating with a ROBOT on social media.&lt;/p&gt;
&lt;p&gt;And you guessed it right: everything is open-source and our mission is
to create a global community to bring democracy to the A.I. age.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 18:30 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Felipe Cabral</dc:creator><pubDate>Sat, 21 Apr 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-04-21:pycon-italia-2018/using-python-to-bring-democracy-to-the-ai-age.html</guid><category>machine-learning</category><category>Python</category><category>agile</category><category>Data Mining</category><category>bigdata</category><category>data-visualization</category><category>OpenSource</category><category>data-analysis</category><category>e-gov</category><category>data</category></item><item><title>Come vanno gli affari? Visualizziamolo con Superset</title><link>https://pyvideo.org/pycon-italia-2018/come-vanno-gli-affari-visualizziamolo-con-superset.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Superset è una piattaforma Open Source di Business Intelligence incubata
nel progetto Apache. Superset permette, senza scrivere una riga di
codice, di creare, esplorare, visualizzare e condividere piccole o
grandi quantità di dati. Nel talk verrà fatta una introduzione a
Superset, mostrandone le funzionalità. Quindi vedremo come usare questo
strumento per analizzare un database di una azienda di vendita online.
Partendo da un database creeremo delle visualizzazioni e quindi delle
dashboard. Senza scrivere una riga di Python :)&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;venerdì 20 aprile&lt;/strong&gt; at 17:30 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Riccardo Magliocchetti</dc:creator><pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-04-20:pycon-italia-2018/come-vanno-gli-affari-visualizziamolo-con-superset.html</guid><category>analytics</category><category>superset</category><category>#data</category><category>business</category></item><item><title>Unveiling the potential of graph databases with Python and Neo4j</title><link>https://pyvideo.org/pycon-italia-2018/unveiling-the-potential-of-graph-databases-with-python-and-neo4j.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Every time we are dealing with data coming from the real world, big and
not so big, you know that usually 80% of the time is needed to clean,
prepare and arrange them. We can then spend the other 20% of the time
enjoying our beloved data analysis.&lt;/p&gt;
&lt;p&gt;The thing that you may know less is that in the last years, the Neo4j
graph database went into the light of being the “right” place to store
data, thanks to its capacity of direct modelling relations among data,
its high availability and its easy, fast and clean query language
Cypher.&lt;/p&gt;
&lt;p&gt;In this talk I’m going to show you some tips to set up in the right way
your data using Pandas, in order to proper model and import them into
Neo4j. A Neo4j Python driver is available to easily import Cypher
queries embedded in Python code. Still, the py2neo package allows
building and querying your database right within your favourite snake
command line.&lt;/p&gt;
&lt;p&gt;Forget about “tall as teen” SQL queries here; thanks to Pandas, Python
and Cypher modelling, loading and query your database is going to be
really straightforward. After this talk, you’ll can’t wait to give Neo4j
a try!&lt;/p&gt;
&lt;p&gt;Prerequisite: a little knowledge of Pandas.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;venerdì 20 aprile&lt;/strong&gt; at 11:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fabio Lamanna</dc:creator><pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-04-20:pycon-italia-2018/unveiling-the-potential-of-graph-databases-with-python-and-neo4j.html</guid><category>database</category><category>graph</category><category>storage</category><category>neo4j</category><category>data</category><category>pandas</category></item><item><title>Modern ETL-ing with Python and Airflow (and Spark)</title><link>https://pyvideo.org/pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Tamara Mendt&lt;/strong&gt; (&amp;#64;TamaraMendt)&lt;/p&gt;
&lt;p&gt;Tamara Mendt is a Data Engineer at HelloFresh, a meal kit delivery service headquartered in Berlin, and one of the top 3 tech startups to come out of Europe over the past 4 years. She devotes her time to building data pipelines and designing and maintaining the company's data infrastructure. Tamara has a computer engineering degree from her native country Venezuela, and an Erasmus Mundus Masters degree in IT for Business Intelligence. She wrote her Master thesis at the TU Berlin with the research group where Apache Flink was born. At HelloFresh she is continuing to work with distributed technologies such has Apache Hadoop, Apache Kafka and Apache Spark to cope with the scalability that the fast growing company requires for dealing with their data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The challenge of data integration is real. The sheer amount of tools that exist to address this problem is proof that organizations struggle with it. This talk will discuss the inherent challenges of data integration, and show how it can be tackled using Python and Apache Airflow and Apache Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The way organizations analyze their data has evolved very quickly since the beginning of the millennium. The development of Hadoop, and the explosion in the variety of data that companies are dealing with nowadays, has fostered the appearance of the concept of data lake, and the shift of traditional ETL (extract, transform, load), to ELT (extract, load, transform). Yet, the challenge of integrating data to obtain valuable insights still remains, and despite the hype and attention being focused on data, very few organizations have actually managed to become data driven. In this talk I will present insights into how we are currently building data pipelines using Python (as a replacement to high level ETL software), Apache Airflow as a scheduler to our coded transformations, and Apache Spark for achieve scalability. Though building data pipelines is not the only element required to become data driven, it is a crucial one, and I hope to encourage the audience to use these open source technologies in their own ETL-ing (or ELT-ing) efforts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tamara Mendt</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html</guid><category>data</category><category>data-science</category><category>pipeline</category></item><item><title>Data Science &amp; Data Visualization in Python. How to harness power of Python for social good?</title><link>https://pyvideo.org/pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python as an Open Data Science tool offers many libraries for data visualization and I will show you how to use and combine the best. I strongly believe that power of data is not only in the information &amp;amp; insight that data can provide us, Data is and can be really beautiful and can not only transform our perception but also the world that we all live in.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my talk I will primarily focus on answering/offer the answer to these questions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Why we need data science and why more and more people should be really interested in analyzing data and data visualization? (motivation)&lt;/li&gt;
&lt;li&gt;What is data science and how to start doing it in Python? (introduction of procedures, tools, most popular IDE-s for Python, etc.)&lt;/li&gt;
&lt;li&gt;What tools for data analysis and data visualization Python offers? (in each stage of analysis the best libraries will be shown for the specific purpose; as for data visualization we will focus particularly on Bokeh, Seaborn, Plotly and use of Jupyter Notebook and Plotly)&lt;/li&gt;
&lt;li&gt;How to 'unlock' the insight hidden in data through Python and how to use it to transform not only public administration or business, but ultimately the transformation of the whole society and economy towards the insight &amp;amp; knowledge based? (potential of data science)&lt;/li&gt;
&lt;li&gt;Open Data, Open Government Partnership, Open Public Administration &amp;amp; all the advantages of Open Data Science &amp;amp; Python. Data-Driven Approach. Everywhere. Now. (the end of talk +vision)&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Radovan Kavicky</dc:creator><pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-06-30:pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html</guid><category>python</category><category>data-science</category><category>data-visualization</category><category>analytics</category><category>PyData</category><category>PyDataBLN</category><category>PyDataBerlin</category><category>PyDataBA</category><category>PyDataBratislava</category><category>talk</category><category>Data</category><category>Bokeh</category><category>Social Good</category><category>datascience</category><category>jupyter</category><category>open science</category><category>open data science</category><category>DataVisualization</category><category>data-analysis</category><category>analysis</category><category>matplotlib</category><category>numpy</category><category>data wrangling</category><category>jupyter notebook</category><category>pandas</category><category>machine learning</category><category>deep learning</category><category>Open Data</category><category>Citizen Data Science</category></item><item><title>Becoming a Data Scientist Advice From My Podcast Guests</title><link>https://pyvideo.org/pydata-dc-2016/becoming-a-data-scientist-advice-from-my-podcast-guests.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Overwhelmed by the vast resources (of varying quality) available online for learning data science? In this talk, I compile resources from data scientists on twitter, advice from guests of my podcast, and some of my own experience to help get you started on the path to Becoming a Data Scientist.&lt;/p&gt;
&lt;p&gt;The options for learning data science online are vast and overwhelming, but it is possible to find great resources that work well for you and learn data science without going back to school if you know how to approach it.&lt;/p&gt;
&lt;p&gt;On my &amp;quot;Becoming a Data Scientist&amp;quot; podcast, I have interviewed 17 data scientists (or those on the way to becoming data scientists) about their career paths and how they learned data science. I also interact with hundreds of data scientists regularly on Twitter. In this talk, I compile the frequent advice and the best resources, and give my answers to some common questions about how to become a data scientist.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Renee Teate</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/becoming-a-data-scientist-advice-from-my-podcast-guests.html</guid><category>Data</category></item><item><title>Data Transformation: A Framework for Exploratory Data Analysis</title><link>https://pyvideo.org/pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Exploratory data analysis plays a critical role in the job of every data scientist, but very few have a structured process or framework for exploring data quickly and efficiently. This talk will introduce the exploratory framework I use in my day-to-day work and will walk attendees through a practical example of how to use the framework to unlock hidden insights with the help of Python libraries.&lt;/p&gt;
&lt;p&gt;At the heart of data analysis, there lies a need to understand the real world entities being represented in the data. Every data set we encounter is an attempt to capture a slice of our complex world and communicate some information about it in a way that has potential to be informative to humans, machines, or both. Moving from basic analyses to advanced analytics requires the ability to imagine multiple ways of conceptualizing the composition of entities and the relationships present in our data. It also requires the realization that different levels of aggregation, disaggregation, and transformation can open up new pathways to understanding our data and identifying the valuable insights it contains.&lt;/p&gt;
&lt;p&gt;In this talk, we’ll discuss several ways to think about the composition and representation of our data. We’ll also demonstrate a series of methods that leverage tools like networks, hierarchical aggregations, and unsupervised clustering to visually explore our data, transform it to discover new insights, help frame analytical problems and questions, and even improve machine learning model performance. In exploring these approaches, and with the help of Python libraries such as Pandas, Scikit-Learn, Seaborn, and NetworkX, we will provide a practical framework for thinking creatively and visually about your data and unlocking latent value and insights hidden deep beneath its surface.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tony Ojeda</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html</guid><category>analysis</category><category>Data</category><category>Data Analysis</category><category>framework</category></item><item><title>Dev Ops meets Data Science Taking models from prototype to production with Docker</title><link>https://pyvideo.org/pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We present the evolution of a model to a production API that can scale to large e-commerce needs. On the journey we discuss metrics of success and how to use the Kubernetes cluster manager and associated tools for deploy. In addition to the use of these tools we highlight how to make use of the cluster management system for further testing and experimentation with your models.&lt;/p&gt;
&lt;p&gt;The chasm between data science and dev ops is often wide and impenetrable, but the two fields have more in common than meets the eye. Every data scientist will be able to lean in and help their career by investing in a basic understanding the basic principles of dev ops. In this talk I present the notions of service level indicators, objectives, and agreements. I cover the rigorous monitoring and testing of services. Finally we demonstrate how to build a basic data science workflow and push to production level APIs with Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes is an opinionated container cluster manager with an easy to use, robust interface. It can be use on very small and very large clusters. Docker is a container system that allows one to build code in an isolated environment. Paired with a container manager such as Kubernetes we are able to manage millions of instances as needed for a production deployment. These tools are two of many different options but are considered among the best open source solutions available.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andy Terrel</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</guid><category>Data</category><category>data science</category><category>docker</category><category>models</category><category>science</category></item><item><title>Keynote: Extending from Open to Usable: A Commerce Data Conundrum</title><link>https://pyvideo.org/pydata-dc-2016/keynote-extending-from-open-to-usable-a-commerce-data-conundrum.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Keynote: Extending from Open to Usable: A Commerce Data Conundrum&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Star Ying</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/keynote-extending-from-open-to-usable-a-commerce-data-conundrum.html</guid><category>Data</category></item><item><title>Keynote: The Culture of Data Transformation</title><link>https://pyvideo.org/pydata-dc-2016/keynote-the-culture-of-data-transformation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">SriSatish Ambati</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/keynote-the-culture-of-data-transformation.html</guid><category>Culture</category><category>Data</category></item><item><title>Open Data Dashboards &amp; Python Web Scraping</title><link>https://pyvideo.org/pydata-dc-2016/open-data-dashboards-python-web-scraping.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Distilling a world of data down to a few key indicators can be an effective way of keeping an audience informed, and this concept is at the heart of a good dashboard. This talk will cover a few methods of scraping and reshaping open data for dashboard visualization, to automate the boring stuff so you have more time and energy to focus on the analysis and content.&lt;/p&gt;
&lt;p&gt;This talk will cover a basic scenario of curating open data into visualizations for an audience. The main goal is to automate data scraping/downloading and reshaping. I use python to automate data gathering, and Tableau and D3 as visualization tools -- but the process can be applied to numerous analytical/visualization suites.&lt;/p&gt;
&lt;p&gt;I'll discuss situations where a dashboard makes sense (and when one doesn't). I will make a case also that automation makes for a more seamless data gathering and updating process, but not always for smarter data analysis.&lt;/p&gt;
&lt;p&gt;Some python packages I'll cover for web scraping and downloading/reshaping open data include: openpyxl, pandas, xlsxwriter, and BeautifulSoup. I'll also touch on APIs.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marie Whittaker</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/open-data-dashboards-python-web-scraping.html</guid><category>Data</category><category>scraping</category><category>web</category></item><item><title>Triaging Feedback Form Data</title><link>https://pyvideo.org/pydata-dc-2016/triaging-feedback-form-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk will cover how to use predictive modeling on unstructured text data including feedback form, social media or chat message data to triage issues in order to prevent future problems with a service, platform or user interface using NLP techniques in Python and R.&lt;/p&gt;
&lt;p&gt;Companies gain useful insights about their users from feedback form and other unstructured text data including live chat messages. Even though they are read and responded to, often such data is ignored when thinking about larger scale trend analysis and this can result in missed insight about how users react to a product or service. Sometimes analysis is being done by looking at changes in user sentiment or other heuristics, however it could be taken a step further by applying predictive modeling in attempt to recognize areas that need more attention and support. While you can use predictive modeling on network and log data, that is looking at how the hardware is handling your users requests, not how it's being perceived by users. By predicting areas where users are having difficulty whether it's with the UI or with the platform's response time you can triage these areas of concern to prevent future cases of negative perception. This talk will cover how to utilize common NLP tools used to gather and process the features in Python then will use R to perform trend analysis and predictive modeling then use the results to triage what areas should be focused on in the future.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Stephanie Kim</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/triaging-feedback-form-data.html</guid><category>Data</category></item><item><title>You got your engineering in my Data Science: Addressing the reproducibility crisis</title><link>https://pyvideo.org/pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering"&gt;http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. Unfortunately, multiple recent studies have been found to be unreliable and non-reproducible. Adopting techniques from software engineering might help mitigate some of these problems.&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. It makes sense of everything from cancer trials to package delivery logistics. But all is not well with data science. Over the past decade, multiple studies have been found to be unreliable and non-reproducible when other scientists tried to recreate their results. This is due to a variety of factors, including fraud, pressure to publish, improper data handling practices, and bugs in analytic tools.&lt;/p&gt;
&lt;p&gt;The problems faced by data science mirror problems that software engineering has been trying to solve. While there are no silver bullets to guarantee quality software, techniques have been developed over time that have improved quality and reliability. Some of these techniques, including open source, version control, automation, and fuzzing could be adapted to the data science domain to improve reliability and help address the reproducibility crisis.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jon Bodner</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</guid><category>Data</category><category>data science</category><category>engineering</category><category>reproducibility</category></item><item><title>Creating Python Data Pipelines in the Cloud</title><link>https://pyvideo.org/pydata-dc-2016/creating-python-data-pipelines-in-the-cloud.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;My talk will be an analysis of the various approaches to creating data pipelines the public cloud using Python.I will compare and contrast using various Python libraries such as Luigi, Airflow and native cloud frameworks such as Cloud Dataflow (Google), AWS Data Pipeline to create a real world data pipeline in Amazon AWS and Google Compute Engine.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Femi Anthony</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/creating-python-data-pipelines-in-the-cloud.html</guid><category>Cloud</category><category>Data</category></item><item><title>Data Sciencing While Female</title><link>https://pyvideo.org/pydata-dc-2016/data-sciencing-while-female.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;How can we increase the number of female data scientists in the workplace? By building a community. Dr. Amanda Traud was the only woman on her data science team when she started the group Women Data Scientists DC. In one year, the group grew to over 1,000 members. Dr. Traud will discuss the ups and downs of being a woman in data science and how to encourage and include more women in the field.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mandi Traud</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/data-sciencing-while-female.html</guid><category>Data</category></item><item><title>Eat Your Vegetables Data Security for Data Scientists</title><link>https://pyvideo.org/pydata-dc-2016/eat-your-vegetables-data-security-for-data-scientists.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/WilliamVoorhees1/eat-your-vegetables-data-security-for-data-scientists"&gt;http://www.slideshare.net/WilliamVoorhees1/eat-your-vegetables-data-security-for-data-scientists&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You've got data: lots of it. People want to get their hands on that data. You don't want that, so let's go over a few things you can do to dissuade attackers from getting their grubby mitts on your hard processed datastore. We'll cover the obvious things (spoiler alert: encryption) and then some advanced techniques for keeping data secure while still keeping it usable (that is to say, analyzable).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Will Voorhees</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/eat-your-vegetables-data-security-for-data-scientists.html</guid><category>Data</category><category>Security</category></item><item><title>Forecasting critical food violations at restaurants using open data</title><link>https://pyvideo.org/pydata-dc-2016/forecasting-critical-food-violations-at-restaurants-using-open-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Dc 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/NicoleDonnelly6/pydatadc-forecasting-critical-food-violations-at-restaurants-using-open-data"&gt;http://www.slideshare.net/NicoleDonnelly6/pydatadc-forecasting-critical-food-violations-at-restaurants-using-open-data&lt;/a&gt;
Github: &lt;a class="reference external" href="https://github.com/nd1/DC_RestaurantViolationForecasting"&gt;https://github.com/nd1/DC_RestaurantViolationForecasting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As many as 105 million Americans suffer from foodborne illness annually. In 2014, the City of Chicago began forecasting these outbreaks targeting limited health inspection resources toward likely sites, showing a 7 day improvement in locating critical violations at food establishments. This talk provides an end-to-end walkthrough of predicting critical violations in Washington, DC using Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicole Donnelly</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/forecasting-critical-food-violations-at-restaurants-using-open-data.html</guid><category>Data</category></item><item><title>How I learned to time travel, or, data pipelining and scheduling with Airflow</title><link>https://pyvideo.org/pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418"&gt;http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data warehousing and analytics projects can, like ours, start out small - and fragile. With an organically growing mess of scripts glued together and triggered by cron jobs hiding on different servers, we needed better plumbing. After perusing the data pipelining landscape, we landed on Airflow, an Apache incubating batch processing pipelining and scheduler tool from Airbnb.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Laura Lorenz</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html</guid><category>airflow</category><category>Data</category></item><item><title>Keynote: Become a Data Superhero How Data Can Change the World</title><link>https://pyvideo.org/pydata-dc-2016/keynote-become-a-data-superhero-how-data-can-change-the-world.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;The capacity to gather and interpret data can be low for many nonprofits. Working together, data scientists and organizations can be a world-changing combination. Byte Back has found a way to use data analysis for good and will help you learn how to tap into your own superpowers.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Elizabeth Lindsey</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-become-a-data-superhero-how-data-can-change-the-world.html</guid><category>Data</category></item><item><title>Keynote: Building a Data Driven Dialogue From Filling Potholes to Disrupting the Cycle</title><link>https://pyvideo.org/pydata-dc-2016/keynote-building-a-data-driven-dialogue-from-filling-potholes-to-disrupting-the-cycle.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kelly Jin</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-building-a-data-driven-dialogue-from-filling-potholes-to-disrupting-the-cycle.html</guid><category>Data</category></item><item><title>Keynote: How Open Data Science Opens the World of Innovation</title><link>https://pyvideo.org/pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Innovation today appears to be instantaneous in large part due to open source technology. Open Data Science is no exception. Python, a pillar in the Open Data Science bedrock, is well positioned to harvest innovation in software and with Anaconda, it’s also well positioned to capitalize on the latest hardware innovations. Anaconda and Intel are blazing a path for the Python community to take advantage of cognitive computing, including machine learning and deep learning.&lt;/p&gt;
&lt;p&gt;In this keynote, Peter and Robert will talk about how Open Data Science––a connected ecosystem of data, analytics and compute––streamlines the path to high performance and innovation to achieve breakthrough results.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Robert Cohn</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</guid><category>Data</category><category>data science</category><category>science</category></item><item><title>Promoting a data driven culture in a world of microservices</title><link>https://pyvideo.org/pydata-dc-2016/promoting-a-data-driven-culture-in-a-world-of-microservices.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/promoting-a-data-driven-culture-in-a-microservices-environment"&gt;http://www.slideshare.net/PyData/promoting-a-data-driven-culture-in-a-microservices-environment&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At Hudl, we give every employee full access to our data warehouse, and over 50% of our employees have personally written a query against it. In this talk, I discuss our journey to democratize our data. I touch on technical and non-technical challenges, including the tools we use and the structure of our teams.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex DeBrie</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/promoting-a-data-driven-culture-in-a-world-of-microservices.html</guid><category>Culture</category><category>Data</category></item><item><title>Scaling up to Big Data Devops for Data Science</title><link>https://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marck Vaisman</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</guid><category>big data</category><category>Data</category><category>data science</category><category>devops</category><category>scaling</category><category>science</category></item><item><title>Building Your First Data Pipelines</title><link>https://pyvideo.org/pydata-dc-2016/building-your-first-data-pipelines.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;You need a data pipeline. This talk will discuss the lifecycle of projects using Jupyter notebooks &amp;amp; Luigi as a data pipeline management tool for a variety of projects, from greenfield to retrofitting complex systems. It will included a hands on demo.&lt;/p&gt;
&lt;p&gt;Data pipelines are hard. Too often we resort to retrofitting janky scripts, relying on keeping a readme up to data, etc.&lt;/p&gt;
&lt;p&gt;First, this proposal lays out the variety of tools that are available to build data pipelines. This talk will discuss why you should be using Luigi and how to use it in a variety of common use cases.&lt;/p&gt;
&lt;p&gt;Next, we will build a basic exploratory analysis using DC open data and Luigi to demonstrate the power of this concept and how it works with Jupyter.&lt;/p&gt;
&lt;p&gt;Finally, we'll retrofit a larger, more complex project to use Luigi to show how you can use it in bigger organizations.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hunter Owens</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/building-your-first-data-pipelines.html</guid><category>Data</category></item><item><title>Parallel Python Analyzing Large Data Sets</title><link>https://pyvideo.org/pydata-dc-2016/parallel-python-analyzing-large-data-sets.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Students will walk away with a high-level understanding of both parallel problems and how to reason about parallel computing frameworks. They will also walk away with hands-on experience using a variety of frameworks easily accessible from Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aron Ahmadia</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/parallel-python-analyzing-large-data-sets.html</guid><category>Data</category><category>parallel</category><category>sets</category></item><item><title>Finding Driving Style Patterns in Caterpillar Machine Data</title><link>https://pyvideo.org/pydata-chicago-2016/finding-driving-style-patterns-in-caterpillar-machine-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://cat.app.box.com/s/1c4mvt8eayb5o7g2wp8nsdwbguhgersm"&gt;https://cat.app.box.com/s/1c4mvt8eayb5o7g2wp8nsdwbguhgersm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Identifying predominant driving-style patterns in logged time series data of Caterpillar machines is daunting due to the nature and size of the data. However, insight gained from field data can deliver optimized powertrain control software and better machine performance. A solution for finding patterns was built using engineered features, dimensionality reduction, and unsupervised learning.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Benjamin Hodel</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/finding-driving-style-patterns-in-caterpillar-machine-data.html</guid><category>Data</category><category>patterns</category><category>style</category></item><item><title>Machine learning techniques for data cleaning</title><link>https://pyvideo.org/pydata-chicago-2016/machine-learning-techniques-for-data-cleaning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1k42esoWoc_WezfPfQ5vxbHTsuFOvAshEusD-GFCElTQ/edit#slide=id.g166bf446d8_1_12"&gt;https://docs.google.com/presentation/d/1k42esoWoc_WezfPfQ5vxbHTsuFOvAshEusD-GFCElTQ/edit#slide=id.g166bf446d8_1_12&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Often, the most interesting datasets - data about people and organizations - are the messiest and most difficult to analyze. When data comes from multiple sources, or when data is entered manually, variation &amp;amp; ambiguity are inevitable. Learn about ways to infer structure and relationships in messy data, using open source Python libraries.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Cathy Deng</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/machine-learning-techniques-for-data-cleaning.html</guid><category>Data</category><category>learning</category><category>machine learning</category></item><item><title>Using Exploratory Data Analysis to Discover Patterns in Image and Document Collections</title><link>https://pyvideo.org/pydata-chicago-2016/using-exploratory-data-analysis-to-discover-patterns-in-image-and-document-collections.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1StRN0-_0x4BPkFFQ79GusgQAqXgwvJ1Fc8Tlu68YO4E/edit"&gt;https://docs.google.com/presentation/d/1StRN0-_0x4BPkFFQ79GusgQAqXgwvJ1Fc8Tlu68YO4E/edit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Exploratory Data Analysis (EDA) is one of the key sets of procedures for summarizing a dataset. In this talk we will develop an EDA procedure for large collections of documents and images (such as photo albums, emails, articles, etc). We will show features used from NLP and Deep Neural Nets and also introduce novel visualization techniques for large image collections using PyImagePlot.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mehrdad Yazdani</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/using-exploratory-data-analysis-to-discover-patterns-in-image-and-document-collections.html</guid><category>analysis</category><category>Data</category><category>data analysis</category><category>patterns</category></item><item><title>When Worlds Collide: Productionalizing a Data Science Model</title><link>https://pyvideo.org/pydata-chicago-2016/when-worlds-collide-productionalizing-a-data-science-model.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;On our first data science project at Shiftgig, the data science and engineering teams had to build software that was production-ready while maintaining the flexibility of a data science sandbox. Although these seem like irreconcilable goals, they forced us to improve inter-team communication and ultimately helped create a great product. We’ll walk through our process and the lessons we learned.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tudor Radoaca</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/when-worlds-collide-productionalizing-a-data-science-model.html</guid><category>Data</category><category>data science</category><category>model</category><category>science</category></item><item><title>Data Engineering Architecture at Simple</title><link>https://pyvideo.org/pydata-chicago-2016/data-engineering-architecture-at-simple.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;A walk through Simple's Data Engineering stack, including lessons learned and why we chose certain tools and languages for different parts of our infrastructure.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rob Story</dc:creator><pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-27:pydata-chicago-2016/data-engineering-architecture-at-simple.html</guid><category>architecture</category><category>Data</category><category>engineering</category></item><item><title>Keynote: Using Data Science for Social Good: Examples, Opportunities, and Challenges</title><link>https://pyvideo.org/pydata-chicago-2016/keynote-using-data-science-for-social-good-examples-opportunities-and-challenges.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rayid Ghani</dc:creator><pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-27:pydata-chicago-2016/keynote-using-data-science-for-social-good-examples-opportunities-and-challenges.html</guid><category>Data</category><category>data science</category><category>science</category></item><item><title>What Data Analysts Wish Application Developers Knew</title><link>https://pyvideo.org/pydata-chicago-2016/what-data-analysts-wish-application-developers-knew.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://speakerdeck.com/alison985/what-data-analysts-wish-application-developers-knew"&gt;https://speakerdeck.com/alison985/what-data-analysts-wish-application-developers-knew&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data analysts frequently do not get to participate in the app development process despite being some of its biggest stakeholders. This talk focuses on general guidelines and best practices for application developers on what they can do to optimize data content and quality available for analysis.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alison Stanton</dc:creator><pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-27:pydata-chicago-2016/what-data-analysts-wish-application-developers-knew.html</guid><category>Data</category></item><item><title>Luigi &amp; Data Pipelines</title><link>https://pyvideo.org/pydata-chicago-2016/luigi-data-pipelines.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/hunterowens/data-pipelines"&gt;https://github.com/hunterowens/data-pipelines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You need a data pipeline. This talk will discuss the lifecycle of projects using Jupyter notebooks &amp;amp; Luigi as a data pipeline management tool for a variety of projects, from greenfield to retrofitting complex systems. It will included a hands on demo.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hunter Owens</dc:creator><pubDate>Fri, 26 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-26:pydata-chicago-2016/luigi-data-pipelines.html</guid><category>Data</category></item><item><title>Working with real-time data streams in Python</title><link>https://pyvideo.org/pycon-au-2016/working-with-real-time-data-streams-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;An increasing number of devices and applications are producing vast amounts of data in real time. This can include measurements, sensor readings, and performance data. Making this data useful often requires that we analyse and use the data in real time but this requires techniques to aggregate, filter, and smooth the data. Drawing on simple and well-tested techniques from mathematics and engineering allows us to solve these problems quickly and efficiently. This talk will describe how Python can be used to develop powerful capabilities for working with real-time data streams and provide simple examples you can start using yourself.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lachlan Blackhall</dc:creator><pubDate>Fri, 12 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-12:pycon-au-2016/working-with-real-time-data-streams-in-python.html</guid><category>Internet-of-Things</category><category>Data</category><category>Real-time</category><category>Kalman Filter</category></item><item><title>PyGotham 2011: Intro to Data Visualization</title><link>https://pyvideo.org/pygotham-2011/pygotham-2011--intro-to-data-visualization.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Have lots of data? Want to turn it into pictures to help you better
understand it or explain it to others? This session will address best
practices for encoding information through design, and will look at a
few ways of doing this in Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Julie Steele</dc:creator><pubDate>Fri, 16 Sep 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-09-16:pygotham-2011/pygotham-2011--intro-to-data-visualization.html</guid><category>data</category><category>pygotham</category><category>pygotham2011</category><category>visualization</category></item><item><title>Data mining and integration with Python</title><link>https://pyvideo.org/pytexas-2015/data-mining-and-integration-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is an abundance of data in social media sites (Wikipedia,
Facebook, Instagram, etc.) which can be accessed through web APIs. But
how do we know that the data from the Wikipedia article on &amp;quot;Golden Gate
Bridge&amp;quot; goes along with the data from &amp;quot;Golden Gate Bridge&amp;quot; Facebook
page? This represents an important question about integrating data from
various sources.&lt;/p&gt;
&lt;p&gt;In this talk, I'll outline important aspects of structured data mining,
integration and entity resolution methods in a scalable system.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Isaac Vidas</dc:creator><pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-10-09:pytexas-2015/data-mining-and-integration-with-python.html</guid><category>Data</category><category>Data Analysis</category><category>data mining</category></item><item><title>Real-Time Django</title><link>https://pyvideo.org/djangocon-us-2011/djangocon-2011--real-time-django.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Real-Time Django&lt;/p&gt;
&lt;p&gt;Presented by Ben Slavin, Adam Miskiewicz&lt;/p&gt;
&lt;p&gt;The web is live. APIs give us access to continuously changing data. We
discuss ways to get real-time data into your app, how to handle data
processing and what to do when you get thousands of updates per second.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Adam Miskiewicz</dc:creator><pubDate>Mon, 05 Sep 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-09-05:djangocon-us-2011/djangocon-2011--real-time-django.html</guid><category>data</category><category>djangocon</category><category>djangocon2011</category><category>realtime</category></item><item><title>Guy Kloss - Python Data Plotting and Visualisation Extravaganza</title><link>https://pyvideo.org/kiwi-pycon-2009/guy-kloss---python-data-plotting-and-visualisatio.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python Data Plotting and Visualization Extravaganza&lt;/p&gt;
&lt;p&gt;Presented by Guy Kloss&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In various fields data is accumulated or produced. This can be
observation data, statistical data, simulation data, ... Information
like that can in many cases be much more easily analysed through the
user's eyes employing data visualisation. This talk is trying to dive
briefly into various means and tools to visually analyse data of
different qualities: time series, simple 2D plots, surface plots, volume
plots, quiver plots, etc.&lt;/p&gt;
&lt;p&gt;Outline&lt;/p&gt;
&lt;p&gt;I am planning on doing a &amp;quot;fly by&amp;quot; through the world of data
visualisation for different types of data using different tools. Types
of data:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1D data and simple functions&lt;/li&gt;
&lt;li&gt;2D data for surface plots&lt;/li&gt;
&lt;li&gt;3D data through quiver plots, iso surfaces, and cutting planes&lt;/li&gt;
&lt;li&gt;n-D data through different means&lt;/li&gt;
&lt;li&gt;continuous and non-continuously structured data&lt;/li&gt;
&lt;li&gt;time series&lt;/li&gt;
&lt;li&gt;real time data visualisation/analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The tools that will probably appear in the demos and discussions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;GNUplot&lt;/li&gt;
&lt;li&gt;matplotlib&lt;/li&gt;
&lt;li&gt;Mayavi2&lt;/li&gt;
&lt;li&gt;Visual Python&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[VIDEO HAS ISSUES: Sound and video are poor. Slides are hard to read.]&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guy Kloss</dc:creator><pubDate>Sat, 07 Nov 2009 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2009-11-07:kiwi-pycon-2009/guy-kloss---python-data-plotting-and-visualisatio.html</guid><category>data</category><category>gnuplot</category><category>kiwipycon</category><category>kiwipycon2009</category><category>matplotlib</category><category>mayavi2</category><category>plotting</category><category>visualpython</category></item></channel></rss>