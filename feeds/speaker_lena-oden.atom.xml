<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_lena-oden.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-09-04T00:00:00+00:00</updated><entry><title>Lessons learned from comparing Numba-CUDA and C-CUDA</title><link href="https://pyvideo.org/euroscipy-2019/lessons-learned-from-comparing-numba-cuda-and-c-cuda.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Lena Oden</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/lessons-learned-from-comparing-numba-cuda-and-c-cuda.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Numba allows the development of GPU code in Python style. When a Python
script using Numba is executed, the code is compiled just-in-time (JIT)
using the LLVM framework. Using Python for GPU programming can mean a
considerable simplification in the development of parallel applications
compared to C and C-CUDA.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Python, however, has to live with the prejudice of low performance,
especially in HighPerformance Computing.&lt;/div&gt;
&lt;div class="line"&gt;We wanted to get to the bottom of whether this is really true and
where these differences come from. For this reason, we first analyzed
the performance of typical micro benchmarks used in HPC. By analyzing
the assembly codes, we learned a lot about the difference between
codes produced by C-CUDA and NUMBA- CUDA. Some of these insights have
helped us to improve the performance of our application - and also of
Numba-CUDA. With a few tricks it is possible to achieve very good
performance with our Numba-Codes, which are very close - or sometimes
even better than the C-CUDA versions.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We compared the performance of GPU-Applications written in C-CUDA and
Numba- CUDA. By analyzing the GPU assembly code, we learned about the
reasons for the differences. This helped us to optimize our codes
written in NUMBA-CUDA and NUMBA itself.&lt;/p&gt;
</summary></entry></feed>