<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_shap.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-10-31T00:00:00+00:00</updated><entry><title>Making sense of ML Black Box: Interpreting ML Models Using SHAP</title><link href="https://pyvideo.org/pycon-se-2019/making-sense-of-ml-black-box-interpreting-ml-models-using-shap.html" rel="alternate"></link><published>2019-10-31T00:00:00+00:00</published><updated>2019-10-31T00:00:00+00:00</updated><author><name>Ravi Singh</name></author><id>tag:pyvideo.org,2019-10-31:pycon-se-2019/making-sense-of-ml-black-box-interpreting-ml-models-using-shap.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Extracting insights from a complex machine learning model is not easy hence for many people machine learning models are in a sense black box. This is a problem especially in high stake sectors like banking and healthcare. In this talk we will discuss how we can increase transparency, auditability, and stability of the model using valuable insights we can get from SHAP and explain reasoning behind individual predictions and how this can be aggregated into powerful model-level insights. We will also see the code to calculate SHAP values.&lt;/p&gt;
</summary><category term="machine learning"></category><category term="SHAP"></category></entry></feed>