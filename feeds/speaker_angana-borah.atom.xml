<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Angana Borah</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_angana-borah.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-04-22T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Approaches to Fairness and Bias Mitigation in Natural Language Processing</title><link href="https://pyvideo.org/pycon-us-2023/approaches-to-fairness-and-bias-mitigation-in-natural-language-processing.html" rel="alternate"></link><published>2023-04-22T00:00:00+00:00</published><updated>2023-04-22T00:00:00+00:00</updated><author><name>Angana Borah</name></author><id>tag:pyvideo.org,2023-04-22:/pycon-us-2023/approaches-to-fairness-and-bias-mitigation-in-natural-language-processing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With the advent of large pre-trained language models like GPT, BERT,
etc., and their usage in almost all natural language understanding and
generation applications, it is important that we evaluate the fairness
and mitigate biases of these models. Since these models are fed with
human-generated data (mostly from the â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With the advent of large pre-trained language models like GPT, BERT,
etc., and their usage in almost all natural language understanding and
generation applications, it is important that we evaluate the fairness
and mitigate biases of these models. Since these models are fed with
human-generated data (mostly from the web), they are exposed to human
biases. Hence, they carry forward and also amplify these biases in their
results. In this talk, we will discuss the motivation for fairness and
bias research in NLP and discuss different approaches used to detect and
mitigate biases. We will also explore some available tools to include in
your models to ensure fairness.&lt;/p&gt;
</content><category term="PyCon US 2023"></category></entry></feed>