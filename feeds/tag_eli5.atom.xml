<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_eli5.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-02-23T00:00:00+00:00</updated><entry><title>How did you know? Explaining Black Box Model Predictions in Python</title><link href="https://pyvideo.org/pycon-philippines-2019/how-did-you-know-explaining-black-box-model-predictions-in-python.html" rel="alternate"></link><published>2019-02-23T00:00:00+00:00</published><updated>2019-02-23T00:00:00+00:00</updated><author><name>Suzy Lee</name></author><id>tag:pyvideo.org,2019-02-23:pycon-philippines-2019/how-did-you-know-explaining-black-box-model-predictions-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As algorithms get more and more complex (i.e. Ensemble models - XGBoost, Random Forest, Neural Networks), it becomes harder to explain the predictions they make. These 'Black Box' models may produce more accurate results but may in fact hard to operationalize in the real world as it gets harder and harder to explain to business decision makers how a model came up with the prediction. In certain cases such as in credit scoring model interpretability is crucial particularly for regulatory compliance. This talk will highlight certain Python tools and libraries such as LIME, ELI5 and Skater, that would allow data scientists to finally be able to explain how their models came up with its predictions.&lt;/p&gt;
</summary><category term="machine learning"></category><category term="eli5"></category><category term="lime"></category><category term="skater"></category></entry><entry><title>“Why Should I Trust You?” - Debugging black-box text classifiers</title><link href="https://pyvideo.org/pydata-amsterdam-2018/why-should-i-trust-you-debugging-black-box-text-classifiers.html" rel="alternate"></link><published>2018-05-26T00:00:00+00:00</published><updated>2018-05-26T00:00:00+00:00</updated><author><name>Tobias Sterbak</name></author><id>tag:pyvideo.org,2018-05-26:pydata-amsterdam-2018/why-should-i-trust-you-debugging-black-box-text-classifiers.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Classifying text is a common use case for machine learning algorithms. But despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction. We will use eli5 and the LIME algorithm to explain text classifiers.&lt;/p&gt;
</summary><category term="eli5"></category><category term="lime"></category></entry></feed>