<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 26 May 2018 00:00:00 +0000</lastBuildDate><item><title>Bayesian Deep learning with 10% of the weights</title><link>https://pyvideo.org/pydata-amsterdam-2018/bayesian-deep-learning-with-10-of-the-weights.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Deep learning grows in popularity and use, but it has two problems. Neural networks have millions of parameters and provide no uncertainty. In this talk, we solve both problems with one simple trick: Bayesian deep learning. We show how to prune 90% of the parameters while maintaining performance. As a bonus, we get the uncertainty over our predictions, which is useful for critical applications.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rob Romijnders</dc:creator><pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-26:pydata-amsterdam-2018/bayesian-deep-learning-with-10-of-the-weights.html</guid></item><item><title>Using deep learning in natural language processing: explaining Google's Neural Machine Translation</title><link>https://pyvideo.org/pydata-amsterdam-2017/using-deep-learning-in-natural-language-processing-explaining-googles-neural-machine-translation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Using deep learning in natural language processing: explaining Google's Neural Machine Translation&lt;/p&gt;
&lt;p&gt;Recent advancements in Natural Language Processing (NLP) use deep learning to improve performance. In September 2016, Google announced that Google Translate will shift from phrase-based to neural machine translation. Other fields of NLP are making a similar shift. This talk motivates and explains these algorithms and discusses implementations.&lt;/p&gt;
&lt;p&gt;Recent advancements in Natural Language Processing (NLP) use deep learning algorithms to improve performance. Google Translate shifts to neural machine translation, Baidu speech genetarion uses neural nets and question answering too. These neural networks share common architectures. They exploit recurrent computation to traverse the input and output. This talk will motivate the recurrent neural networks and discuss architectures. In the second half we discuss extensions such as attention mechanisms. Key words: RNN, seq2seq, attention, word vectors, data/model parallelism, low precision inference, TPU (explained in this order)&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rob Romijnders</dc:creator><pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/using-deep-learning-in-natural-language-processing-explaining-googles-neural-machine-translation.html</guid></item></channel></rss>