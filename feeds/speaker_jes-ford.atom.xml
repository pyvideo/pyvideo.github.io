<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Jes Ford</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_jes-ford.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2022-04-27T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Getting Started Testing in Data Science</title><link href="https://pyvideo.org/pycon-us-2019/getting-started-testing-in-data-science.html" rel="alternate"></link><published>2019-05-05T13:50:00+00:00</published><updated>2019-05-05T13:50:00+00:00</updated><author><name>Jes Ford</name></author><id>tag:pyvideo.org,2019-05-05:/pycon-us-2019/getting-started-testing-in-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;em&gt;How do you know if your data science results are correct?&lt;/em&gt; Robust
software usually has tests asserting that certain conditions hold, but
as a data scientist it’s often not straightforward or obvious how to
integrate these best practices. Our workflow includes exploration,
statistical models, and one-off analysis. This …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;em&gt;How do you know if your data science results are correct?&lt;/em&gt; Robust
software usually has tests asserting that certain conditions hold, but
as a data scientist it’s often not straightforward or obvious how to
integrate these best practices. Our workflow includes exploration,
statistical models, and one-off analysis. This talk will give concrete
examples of when and how testing should play a role, and provide you
with enough introduction to get started writing your first data science
tests using &lt;tt class="docutils literal"&gt;pytest&lt;/tt&gt; &amp;amp; &lt;tt class="docutils literal"&gt;hypothesis&lt;/tt&gt;.&lt;/p&gt;
</content><category term="PyCon US 2019"></category><category term="talk"></category></entry><entry><title>The Model Review: improving transparency, reproducibility, &amp; knowledge sharing using MLflow</title><link href="https://pyvideo.org/pycon-us-2022/the-model-review-improving-transparency-reproducibility-knowledge-sharing-using-mlflow.html" rel="alternate"></link><published>2022-04-27T00:00:00+00:00</published><updated>2022-04-27T00:00:00+00:00</updated><author><name>Jes Ford</name></author><id>tag:pyvideo.org,2022-04-27:/pycon-us-2022/the-model-review-improving-transparency-reproducibility-knowledge-sharing-using-mlflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Code Review is an integral part of software development, but many teams don’t have similar processes in place for the development and deployment of Machine Learning (ML) models. I will motivate the decision to create a Model Review process, starting from the principles of transparency, reproducibility, and knowledge …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Code Review is an integral part of software development, but many teams don’t have similar processes in place for the development and deployment of Machine Learning (ML) models. I will motivate the decision to create a Model Review process, starting from the principles of transparency, reproducibility, and knowledge sharing. MLflow is a useful Python package to help simplify and automate much of the tracking necessary to create detailed records of machine learning experiments. Much of this talk will be spent introducing this tool, and demonstrating the core MLflow Tracking functionality. I’ll discuss how my team is currently running a Model Review process for any ML models that we push to production, and how we use MLflow to streamline this work and learn from each other.&lt;/p&gt;
</content><category term="PyCon US 2022"></category></entry></feed>