<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Fri, 03 Jun 2016 00:00:00 +0000</lastBuildDate><item><title>Assurance Scoring Using Machine Learning and Analytics to Reduce Risk in the Public Sector</title><link>https://pyvideo.org/pydata-london-2016/assurance-scoring-using-machine-learning-and-analytics-to-reduce-risk-in-the-public-sector.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData London&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;Matt Thomas &amp;amp; Natalia Angarita-Jaimes&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;We will talk about a framework we have developed to use Machine Learning and other advanced analytical methods to reduce risk in the Public Sector. This python-based assurance scoring framework, developed with Pandas &amp;amp; Scikit-Learn, changes the emphasis of traditional risk-scoring frameworks to identifying compliant behaviour; we discuss some of the challenges faced and present a case study.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Traditionally, risk scoring frameworks are built around the customer journey to identify non-compliant or fraudulent behaviour. These frameworks combine data from different sources and historical known fraud to identify high risk transactions or applications. In the public sector, however, the emphasis is often on identifying low-risk customers. In this talk we will discuss an Assurance Scoring framework which applies these traditional machine learning and analytics techniques but changes this emphasis and identifies those customers posing minimum risk. The advantage of this approach is that low risk transactions can be automated (which account for the majority of customers) and resources can be focused more effectively to handle those exceptional high risk cases. This framework has been developed in Python, in particular with Pandas and Scikit-Learn. But we also go beyond Machine Learning to incorporate other techniques such as rules based linking, anomaly detection and graph based analysis, and show how these can be used to boost the confidence of the low-risk group. In particular, we will showcase how different python packages have been integrated to address the data pre-processing, feature engineering, model building &amp;amp; validation problems and how we have solved the challenges faced during the integration process by developing a range of testing procedures.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;p&gt;PyData London 2016
PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matt Thomas</dc:creator><pubDate>Fri, 03 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-03:pydata-london-2016/assurance-scoring-using-machine-learning-and-analytics-to-reduce-risk-in-the-public-sector.html</guid></item><item><title>Statistically Speculating on the Source of Sneezes and Sniffles</title><link>https://pyvideo.org/pydata-london-2016/ian-ozsvald-statistically-speculating-on-the-source-of-sneezes-and-sniffles.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Since April 2015 our group has studied the Allergic Rhinitis of a subject with the goal of building a machine learned model that predicts the need for antihistamines. Approximately 30% of the world's population suffers from allergies, we aim to provide a methodology for others to identify the drivers of their own symptoms. This is a &amp;quot;citizen science&amp;quot; project, currently focused on one individual and a year's worth of self-reported antihistamine usage, sneezing data and geolocated points. We'll discuss the available external data (including the London Air project's pollution readings, weather, diet, exercise and commute data), exploratory data analysis, our approach to feature engineering from time-series and text sources and our modeling progress. The data logging iPhone app and data preparation tools are all open sourced. Python tools discussed include scikit-learn, seaborn, statsmodels and textract. We'll also review our distributed working practices.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://speakerdeck.com/ianozsvald/statistically-solving-sniffles-step-by-step-a-work-in-progress"&gt;https://speakerdeck.com/ianozsvald/statistically-solving-sniffles-step-by-step-a-work-in-progress&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More information: &lt;a class="reference external" href="http://ianozsvald.com/2016/05/07/statistically-solving-sneezes-and-sniffles-a-work-in-progress-report-at-pydatalondon-2016/"&gt;http://ianozsvald.com/2016/05/07/statistically-solving-sneezes-and-sniffles-a-work-in-progress-report-at-pydatalondon-2016/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ian Ozsvald</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-london-2016/ian-ozsvald-statistically-speculating-on-the-source-of-sneezes-and-sniffles.html</guid></item><item><title>The IceCube data pipeline from the South Pole to publication</title><link>https://pyvideo.org/pydata-london-2016/jakob-van-santen-the-icecube-data-pipeline-from-the-south-pole-to-publication.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Transforming data produced at the IceCube Neutrino Observatory into scientific results presents several interesting challenges. My talk will give an overview of our software stack, written in a colorful mix of Python, C++, Java, and FORTRAN, present some of our custom solutions, and highlight areas where we use standard libraries and design patterns.&lt;/p&gt;
&lt;p&gt;The IceCube Neutrino Observatory (&lt;a class="reference external" href="http://icecube.wisc.edu/"&gt;http://icecube.wisc.edu/&lt;/a&gt;) uses a grid of more than 5000 light sensors embedded in a cubic kilometer of glacial ice at the geographic South Pole to detect neutrinos, some of the most elusive particles in the universe. These neutrinos are interesting because the most energetic of them likely hold the key to one of the longest-standing problems in astrophysics, the origin of ultra-high-energy cosmic rays.&lt;/p&gt;
&lt;p&gt;Cosmic rays were discovered over 100 years ago, and in the mean time we know that they are electrically charged particles (mostly protons) from outer space. The most energetic of them pack the kinetic energy of a rifle bullet into a single atomic nucleus, well beyond the reach of any particle accelerator that we could ever build on Earth. What we don't know, however, is where or how they are accellerated to such energies, because their directions are scrambled by intervening magnetic fields. This is where neutrinos enter the picture. They are produced in large numbers whenever particles like protons are smashed into each other at high energies, something that is bound to happen in the cosmic accelerators that produce the cosmic rays we observe at Earth. Neutrinos are, however, electrically neutral and only interact via the weak force, which makes them extremely difficult to stop. That makes high-energy neutrinos a very interesting way to do astronomy: unlike photons, they can't be absorbed in dust clouds and radiation fields, and unlike cosmic rays, they can't be deflected by magnetic fields.&lt;/p&gt;
&lt;p&gt;The elusiveness of neutrinos is also what makes IceCube data analysis interesting and challenging. The first challenge on the way from raw data to scientific result is data preparation: one needs to actually find neutrino events in the data. For every neutrino event, there are millions of events caused by cosmic-ray air showers that penetrate the 1.5 km of ice over the detector. We separate the neutrinos from uninteresting background events using the features of each event. The optimal set of features and the separation strategy depend on the kind of measurement one wants to do, and there are many different directions of interest within the 300-strong IceCube Collaboration. This means that the event processing chain and data format need to be extremely flexible, allowing physicists with limited software skills to add new features to the data. Our data processing framework, called IceTray, represents each event as a frame, essentially a dictionary of string-key pairs. Each frame is passed through a series of modules that can add new features of arbitrary type based on the data already in the frame, or drop frames from further processing. While the core of IceTray and many of its modules are written in C++, all the configuration is done in Python, and modules themselves can be written in Python. This allows for quite rapid and accessible development. The serialized form of these frames, called an &amp;quot;I3&amp;quot; file, also serves as our long-term archive format.&lt;/p&gt;
&lt;p&gt;Once a data sample has been prepared, it's time for visualization and analysis. Here we move out of our custom world and start using standard tools and data formats. We support two analysis universes: one based on HDF5 (&lt;a class="reference external" href="https://www.hdfgroup.org/HDF5/"&gt;https://www.hdfgroup.org/HDF5/&lt;/a&gt;), which is convenient to read with PyTables (&lt;a class="reference external" href="http://www.pytables.org/"&gt;http://www.pytables.org/&lt;/a&gt;), and another based on ROOT (&lt;a class="reference external" href="https://root.cern.ch/"&gt;https://root.cern.ch/&lt;/a&gt;), an analysis framework and associated data format specific to high-energy physics. We write to such &amp;quot;tabular&amp;quot; formats using an internal framework called tableio that hammers each object in a frame into a series of table rows. Backends for HDF5 and ROOT then emit these rows in a specific format, along with a description and unit string for each column.&lt;/p&gt;
&lt;p&gt;I will illustrate a typical Python-based analysis using the discovery of the first high-energy astrophysical neutrinos as an example (&lt;a class="reference external" href="http://science.sciencemag.org/content/342/6161/1242856"&gt;http://science.sciencemag.org/content/342/6161/1242856&lt;/a&gt;).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jakob van Santen</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-london-2016/jakob-van-santen-the-icecube-data-pipeline-from-the-south-pole-to-publication.html</guid></item><item><title>Holy D@t*! How to Deal with Imperfect, Unclean Datasets</title><link>https://pyvideo.org/pydata-london-2016/katharine-jarmul-holy-dt-how-to-deal-with-imperfect-unclean-datasets.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Ever wondered what sort of sick person created the datasets you work with? Sadly, we can't answer that question directly, but we can aim to handle messy data problems. From the non-significant or null datasets, to unclean and unclear string data, to difficult formats like PDFs, we'll take a closer look at how to best work with imperfect data and what questions you can answer given your datasets.&lt;/p&gt;
&lt;p&gt;This talk will cover how to handle and manage working with unclean and imperfect datasets. We'll cover several issues and suggestions as well as some code examples for managing that messy data.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The Noble Quest against Messy Data&lt;/li&gt;
&lt;li&gt;Working with null data&lt;/li&gt;
&lt;li&gt;Insignificant data&lt;/li&gt;
&lt;li&gt;Messy strings Regex Fuzzy Match&lt;/li&gt;
&lt;li&gt;XML / HTML Data&lt;/li&gt;
&lt;li&gt;PDF Data&lt;/li&gt;
&lt;li&gt;Where to go from here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1G-lgHKTdrqeeJhcvVmd7C9gOIfTRe429zhBN6lmKKzA/"&gt;https://docs.google.com/presentation/d/1G-lgHKTdrqeeJhcvVmd7C9gOIfTRe429zhBN6lmKKzA/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Katharine Jarmul</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-london-2016/katharine-jarmul-holy-dt-how-to-deal-with-imperfect-unclean-datasets.html</guid></item><item><title>Deep learning tutorial - advanced techniques</title><link>https://pyvideo.org/pydata-london-2016/geoffrey-french-deep-learning-tutorial-advanced-techniques.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Some of the more advanced deep learning to help you get the best out of it in a practical setting. The main focus is on computer vision and image processing.&lt;/p&gt;
&lt;p&gt;In the last few years, deep neural networks have been used to generate state-of-the-art results in image classification, segmentation and object detection. They have also successfully been used for speech recognition.&lt;/p&gt;
&lt;p&gt;In this tutorial we build on the basics, demonstrating some useful techniques that are useful in a practical setting. Along with tips and tricks found to be useful, we will discuss the following:&lt;/p&gt;
&lt;p&gt;active learning; train a neural network using less training data
using pre-trained networks; using the body of a pre-trained network (e.g. an ImageNet network) and re-using it for localisation or locating objects not within the original training set
having fun with neural networks; some of the fun techniques that have been demonstrated in the last couple of years
Prior knowledge of deep learning, machine learning, linear algebra, a bit of calculus and the NumPy/SciPy stack would be helpful for participation. We will mainly focus on using the Theano toolkit along with the Lasagne neural network library.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/britefury/deep-learning-tutorial-advanced-techniques-pydata-london-2016"&gt;https://speakerdeck.com/britefury/deep-learning-tutorial-advanced-techniques-pydata-london-2016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/Britefury/deep-learning-tutorial-pydata2016"&gt;https://github.com/Britefury/deep-learning-tutorial-pydata2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Geoffrey French</dc:creator><pubDate>Thu, 26 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-26:pydata-london-2016/geoffrey-french-deep-learning-tutorial-advanced-techniques.html</guid></item><item><title>Probablistic Programming Data Science with PyMC3</title><link>https://pyvideo.org/pydata-london-2016/thomas-wiecki-probablistic-programming-data-science-with-pymc3-1.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Probabilistic programming is a new paradigm that greatly increases the number of people who can successfully build statistical models and machine learning algorithms, and makes experts radically more effective. This talk will provide an overview of PyMC3, a new probabilistic programming package for Python featuring intuitive syntax and next-generation sampling algorithms.&lt;/p&gt;
&lt;p&gt;Machine learning is the driving force behind many recent revolutions in data science. Comprehensive libraries provide the data scientist with many turnkey algorithms that have very weak assumptions on the actual distribution of the data being modeled. While this blackbox property makes machine learning algorithms applicable to a wide range of problems, it also limits the amount of insight that can be gained by applying them.&lt;/p&gt;
&lt;p&gt;The field of statistics on the other hand often approaches problems individually and hand-tailors statistical models to specific problems. To perform inference on these models, however, is often mathematically very challenging, and thus requires time-deriving equations as well as simplifying assumptions (like the normality assumption) to make inference mathematically tractable.&lt;/p&gt;
&lt;p&gt;Probabilistic programming is a new programming paradigm that provides the best of both worlds and revolutionizes the field of machine learning. Recent methodological advances in sampling algorithms like Markov Chain Monte Carlo (MCMC), as well as huge increases in processing power, allow for almost complete automation of the inference process. Probabilistic programming thus greatly increases the number of people who can successfully build statistical models and machine learning algorithms, and makes experts radically more effective. Data scientists can create complex generative Bayesian models tailored to the structure of the data and specific problem at hand, but without the burden of mathematical tractability or limitations due to mathematical simplifications.&lt;/p&gt;
&lt;p&gt;This talk will provide an overview of PyMC3, a new probabilistic programming package for Python featuring intuitive syntax and next-generation sampling algorithms.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;Notebook: &lt;a class="reference external" href="https://gist.github.com/anonymous/9287a213fe188a79d7d7774eef79ad4d"&gt;https://gist.github.com/anonymous/9287a213fe188a79d7d7774eef79ad4d&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1QNxSjDHJbFL7vFwQHHheeGmBHEJAo39j28xdObFY6Eo/edit"&gt;https://docs.google.com/presentation/d/1QNxSjDHJbFL7vFwQHHheeGmBHEJAo39j28xdObFY6Eo/edit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Twitter: &lt;a class="reference external" href="https://twitter.com/twiecki"&gt;https://twitter.com/twiecki&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Wiecki</dc:creator><pubDate>Thu, 26 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-26:pydata-london-2016/thomas-wiecki-probablistic-programming-data-science-with-pymc3-1.html</guid></item><item><title>Detecting novel anomalies in Twitter</title><link>https://pyvideo.org/pydata-london-2016/delia-rusu-mattia-boni-sforza-detecting-novel-anomalies-in-twitter.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In recent years Twitter has gained importance as a datasource for finance professionals alongside news. However, several challenges appear when identifying relevant information and finding the latest unexpected developments. In this talk we are going to present our experience developing an anomaly detection and alerting system and analyse a particular anomalous scenario.&lt;/p&gt;
&lt;p&gt;We are overwhelmed by the amount of information available via news, blogs, social media, etc. and there is growing demand for automatic or semi-automatic systems that filter information and surface the most relevant, interesting, novel pieces to the users. Finance professionals face multiple challenges as the volume of data they are analysing increases and at the same time the data sources get more diverse. In order to address the information overload problem both established financial news providers and social media services present condensed information: in the form of news headlines (e.g. Bloomberg and Reuters) or by imposing limits on the text length (a Twitter message is formed of maximum 140 characters). On the other hand, in recent years, an increasing number of content filtering and alerting systems have been developed.&lt;/p&gt;
&lt;p&gt;Knowsis is a fintech startup focusing on social media analytics. One of the systems that we developed - Anomaly Detection and Alerting - is aimed at informing the user whenever it identifies an unexpected and novel tweet. Anomaly detection is driven by social volume, i.e. not all tweets which deviate from what is expected are anomalies, but a critical number of tweets is required. In addition, we do not want to alert the user about old information.&lt;/p&gt;
&lt;p&gt;The talk will present the approach that we took in building the system, highlighting issues that we encountered along the way and how we tackled them. In the second part of the talk we are going to show how one can build a similar anomaly detection system from scratch and use it to detect a particular anomalous scenario. We are going to use Poisson models to identify anomalies and Locality-Sensitive Hashing for novelty detection. The code will be written in Python using the scikit-learn and statsmodels libraries and made available to the audience.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://nbviewer.jupyter.org/format/slides/github/knowsis/novel-twitter-anomalies-pydatalondon2016/blob/master/pydata_presentation.ipynb#/"&gt;http://nbviewer.jupyter.org/format/slides/github/knowsis/novel-twitter-anomalies-pydatalondon2016/blob/master/pydata_presentation.ipynb#/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/knowsis/novel-twitter-anomalies-pydatalondon2016"&gt;https://github.com/knowsis/novel-twitter-anomalies-pydatalondon2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Delia Rusu</dc:creator><pubDate>Mon, 16 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-16:pydata-london-2016/delia-rusu-mattia-boni-sforza-detecting-novel-anomalies-in-twitter.html</guid></item><item><title>JupyterHub: Deploying Jupyter Notebooks for students and researchers</title><link>https://pyvideo.org/pydata-london-2016/jupyterhub-deploying-jupyter-notebooks-for-students-and-researchers.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016
Min Ragan Kelley &amp;amp; Thomas Kluyver&lt;/p&gt;
&lt;p&gt;Learn to deploy JupyterHub! JupyterHub is a simple, highly extensible, multi-user system for managing per-user Jupyter Notebook servers, designed for research groups or classes. We will cover deploying JupyterHub on a single server, as well as deploying with Docker using GitHub for authentication.&lt;/p&gt;
&lt;p&gt;The Jupyter Notebook is an interactive web-based tool for interactive programming and writing code-centric documents. Being a web-based environment, the notebook server can be run remotely, not just on your local machine. JupyterHub is a multi-user server, aimed at helping research groups and instructors host notebook servers for their users or students. By default, JupyterHub uses the local system users and PAM authentication, but it can be customized to use any authentication system, including GitHub, CILogon, Shibboleth, and more. The way single-user servers are spawned can also be customized to use services such as Docker, Kubernetes, or HPC cluster queuing systems. The tutorial will cover a basic deployment of JupyterHub on a single machine, then extending it to use docker and GitHub authentication, as well as general best practices for JupyterHub deployment.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/minrk/jupyterhub-pydata-2016/blob/master/JupyterHub.pdf"&gt;https://github.com/minrk/jupyterhub-pydata-2016/blob/master/JupyterHub.pdf&lt;/a&gt;
GitHub Repo: &lt;a class="reference external" href="https://github.com/minrk/jupyterhub-pydata-2016"&gt;https://github.com/minrk/jupyterhub-pydata-2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Min Ragan Kelley</dc:creator><pubDate>Fri, 13 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-13:pydata-london-2016/jupyterhub-deploying-jupyter-notebooks-for-students-and-researchers.html</guid></item><item><title>bandicoot: a toolbox to analyze mobile phone metadata</title><link>https://pyvideo.org/pydata-london-2016/luc-rocher-bandicoot-a-toolbox-to-analyze-mobile-phone-metadata.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Bandicoot is a free and open-source toolbox to process mobile phone metadata. It provides standardized and privacy-preserving methods to analyze such datasets, returning more than 160 behavioral indicators. Bandicoot is a complete easy-to-use environment for researchers and developers, allowing them to load their data, perform analysis, and export their results with a few lines of code.&lt;/p&gt;
&lt;p&gt;The metadata generated at large scale by cellphones and collected by literally every carrier around the world have the potential to fundamentally transform the way we fight diseases, design transportation systems, and do research. Scientists have compared the recent availability of these large-scale behavioral data sets to the invention of the microscope and new fields such as Computational Social Science have recently emerged. Mobile phone metadata have, for example, already been used to study human mobility and behavior in cities, understand the propagation of viruses such as malaria and dengue fever. They have been combined with machine learning algorithms to predict people's age, gender, personality, loan repayments, and crime.&lt;/p&gt;
&lt;p&gt;Bandicoot is a free and open-source Python toolbox to extract more than 160 features from standard mobile phone metadata. bandicoot focuses on making it easy for researchers and practitioners to load mobile phone data, analyze them, as well as compute and extract robust features from them. Emphasis is put on ease of use, consistency, and documentation. bandicoot has no dependencies and is distributed under the MIT license.&lt;/p&gt;
&lt;p&gt;bandicoot indicators: individual, spatial, and network features&lt;/p&gt;
&lt;p&gt;In this talk, we provide an introduction to bandicoot via real life case studies, showing you how to visualise and analyze large scale data sets, or directly metadata from your own phone.&lt;/p&gt;
&lt;p&gt;Materials available here: &lt;a class="reference external" href="https://github.com/cynddl/pydata-london-2016"&gt;https://github.com/cynddl/pydata-london-2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Luc Rocher</dc:creator><pubDate>Fri, 13 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-13:pydata-london-2016/luc-rocher-bandicoot-a-toolbox-to-analyze-mobile-phone-metadata.html</guid></item><item><title>Python and Julia: the best of friends?</title><link>https://pyvideo.org/pydata-london-2016/malcolm-sherrington-python-and-julia-the-best-of-friends.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;The talk will focus on both the Python and Julia languages, concentrating on cooperation between the two rather than confrontation and bringing out what each can learn from the other. There will be a demonstration of interoperability via a Jupyter notebook, but no detailed knowledge of Julia (or Python) is necessary.&lt;/p&gt;
&lt;p&gt;Python and Julia are often thought of as in competition for the same workspace. In this talk Malcolm Sherrington argues quite the reverse and in fact there is a tight connection between the two, one from which each benefits. It will focus on the cooperation between both programming languages and discuss what each has to learn from the other.&lt;/p&gt;
&lt;p&gt;The talk will demonstrate their mutual interoperability by showing how Python and Julia can call each other seemlessly and with no overhead, concentrating on the areas in which each excels and contends that it is not all about speed!!!&lt;/p&gt;
&lt;p&gt;Materials available here: &lt;a class="reference external" href="https://github.com/sherrinm/notebooks/tree/master/PYD"&gt;https://github.com/sherrinm/notebooks/tree/master/PYD&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Malcolm Sherrington</dc:creator><pubDate>Fri, 13 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-13:pydata-london-2016/malcolm-sherrington-python-and-julia-the-best-of-friends.html</guid></item><item><title>Bokeh for Data Applications and Visualization</title><link>https://pyvideo.org/pydata-london-2016/bryan-van-de-ven-bokeh-for-data-applications-and-visualization.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Bokeh is an Interactive visualization library that targets modern web browsers for presentation. It provides elegant, concise construction of basic exploratory and advanced custom graphics, but can also be used to develop sophisticated dashboards and data applications.&lt;/p&gt;
&lt;p&gt;0:00
- 0:05 Introduction and Setup
0:05
- 0:20 Review and Practice with Bokeh Basics
0:20
- 0:40 Widgets, Interactions and Streaming in the Notebook
0:40
- 1:05 Handling Larger Data Sets with Bokeh and Datashader
1:05
- 1:30 Building Bokeh Applications&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bryan Van de Ven</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:pydata-london-2016/bryan-van-de-ven-bokeh-for-data-applications-and-visualization.html</guid></item><item><title>Finding needles in haystacks with Deep Neural Networks</title><link>https://pyvideo.org/pydata-london-2016/calvin-giles-finding-needles-in-haystacks-with-deep-neural-networks.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;With an inventory of millions of products, finding duplicates is intractable by hand. Neural networks can be used to learn a representation that maps duplicates close together in a manifold.&lt;/p&gt;
&lt;p&gt;We trained a network to perform multi-task classification. Using this as a basis, we trained a manifold that maps images to a space where duplicates have a small distance and different products have a large distance.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/CalvinGiles/finding-needles-in-haystacks-with-deep-neural-networks"&gt;http://www.slideshare.net/CalvinGiles/finding-needles-in-haystacks-with-deep-neural-networks&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Calvin Giles</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:pydata-london-2016/calvin-giles-finding-needles-in-haystacks-with-deep-neural-networks.html</guid></item><item><title>Assessing the quality of a clustering</title><link>https://pyvideo.org/pydata-london-2016/christian-hennig-assessing-the-quality-of-a-clustering.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;There are many different methods for finding groups in data (cluster analysis), and on many datasets they will deliver different results. How good a clustering is for given data depends on the aim of clustering. I will present a number of methods that can be used to assess the quality of a clustering and to compare different clusterings, taking into account different aims of clustering.&lt;/p&gt;
&lt;p&gt;There are many different methods for finding groups in data (cluster analysis), and on many datasets they will deliver different results. How good a clustering is for given data depends on the aim of clustering and on the user's concept of what makes objects &amp;quot;belong together&amp;quot;. I will present some approaches to assess the quality of a clustering and to compare different clusterings. Particularly, I will present some indexes that measure various desirable aspects of a clustering such as stability, separateness of clusters etc. Different aims of clustering can be taken into account by specifying which aspects are particularly relevant in the situation at hand.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/christian-henning-assessing-the-quality-of-a-clustering"&gt;http://www.slideshare.net/PyData/christian-henning-assessing-the-quality-of-a-clustering&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Christian Hennig</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:pydata-london-2016/christian-hennig-assessing-the-quality-of-a-clustering.html</guid></item><item><title>Working with Fashion Models</title><link>https://pyvideo.org/pydata-london-2016/eddie-bell-working-with-fashion-models.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Since the dawn of time man has harnessed the power of convolutional neural networks to understand fashion. In this presentation, I carry on the trend and discuss how we've built a general purpose visual fashion representation by simultaneously training against multiple objectives with multiple images per objective. There will be lots of pictures.&lt;/p&gt;
&lt;p&gt;Fashion is a visual medium so it makes sense for our models of fashion to include visual features. In this presentation, I'll describe how we've build a general purpose visual fashion representation using CNNs. The network is multi-task (multiple labels per image), multi-image (multiple images per label) and it runs on multiple GPUs. We used the python library Chainer to fit the network.&lt;/p&gt;
&lt;p&gt;I'll visually explore what is going on inside the black box of a neural network and discover how a fashion specific model sees the world differently from generic visual models. Lastly, I'll demonstrate some applications of the representation learned by the model.&lt;/p&gt;
&lt;p&gt;The initial part of this presentation will be technical but the remainder will be accessible.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/ejlbell/working-with-fashion-models-pydatalondon-2016"&gt;http://www.slideshare.net/ejlbell/working-with-fashion-models-pydatalondon-2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eddie Bell</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:pydata-london-2016/eddie-bell-working-with-fashion-models.html</guid></item><item><title>Hierarchical Bayesian Modelling with PyMC3 and PySTAN</title><link>https://pyvideo.org/pydata-london-2016/jonathan-sedar-hierarchical-bayesian-modelling-with-pymc3-and-pystan.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Can we use Bayesian inference to determine unusual car emissions test for Volkswagen? In this worked example, I'll demonstrate hierarchical linear regression using both PyMC3 and PySTAN, and compare the flexibility and modelling strengths of each framework.&lt;/p&gt;
&lt;p&gt;Overview&lt;/p&gt;
&lt;p&gt;Bayesian inference bridges the gap between white-box model introspection and black-box predictive performance. We gain the ability to fully specify a model and fit it to observed data according to our prior knowledge. Small datasets are handled well and the overall method and results are very intuitive: lending to both statistical insight and future prediction.&lt;/p&gt;
&lt;p&gt;This talk will demonstrate the use of Bayesian inference in a real-world scenario: using a set of hierarchical models to compare exhaust emissions data from a set of vehicle manufacturers.&lt;/p&gt;
&lt;p&gt;This will be interesting to people who work in the Type A side of data science, and will demonstrate usage of the tools as well as some theory.&lt;/p&gt;
&lt;p&gt;The Frameworks&lt;/p&gt;
&lt;p&gt;PyMC3 and PySTAN are two of the leading frameworks for Bayesian inference in Python: offering concise model specification, MCMC sampling, and a growing amount of built-in conveniences for model validation, verification and prediction.&lt;/p&gt;
&lt;p&gt;PyMC3 is an iteration upon the prior PyMC2, and comprises a comprehensive package of symbolic statistical modelling syntax and very efficient gradient-based samplers using the Theano library of deep-learning fame for gradient computation. Of particular interest is that it includes the Non U-Turn Sampler NUTS developed recently by Hoffman &amp;amp; Gelman in 2014, which is only otherwise available in STAN.&lt;/p&gt;
&lt;p&gt;PySTAN is a wrapper around STAN, a major3 open-source framework for Bayesian inference developed by Gelman, Carpenter, Hoffman and many others. STAN also has HMC and NUTS samplers, and recently, Variational Inference - which is a very efficient way to approximate the joint probability distribution. Models are specified in a custom syntax and compiled to C++.&lt;/p&gt;
&lt;p&gt;The Real-World Problem &amp;amp; Dataset&lt;/p&gt;
&lt;p&gt;I'm currently quite interested in road traffic and vehicle insurance, so I've dug into the UK VCA Vehicle Type Approval to find their Car Fuel and Emissions Information for August 2015. The raw dataset is available for direct download and is small but varied enough for our use here: roughly 2500 cars and 10 features inc hierarchies of car parent-manufacturer - manufacturer - model.&lt;/p&gt;
&lt;p&gt;I will investigate the car emissions data from the point-of-view of the Volkswagen Emissions Scandal which seems to have meaningfully damaged their sales. Perhaps we can find unusual results in the emissions data for Volkswagen.&lt;/p&gt;
&lt;p&gt;GitHub repo: &lt;a class="reference external" href="https://github.com/jonsedar/pymc3_vs_pystan"&gt;https://github.com/jonsedar/pymc3_vs_pystan&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jonathan Sedar</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:pydata-london-2016/jonathan-sedar-hierarchical-bayesian-modelling-with-pymc3-and-pystan.html</guid></item><item><title>Word Embeddings for fun and profit in Gensim</title><link>https://pyvideo.org/pydata-london-2016/lev-konstantinovskiy-word-embeddings-for-fun-and-profit-in-gensim.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Python has great open source libraries to extract data from its most raw format - the human readable text. We will discuss a family of algorithms called word embeddings - Word2Vec being most famous and how they can be used in practice using Gensim package&lt;/p&gt;
&lt;p&gt;A tour of word embeddings, their Python implementations and their use in the industry.&lt;/p&gt;
&lt;p&gt;We will start with theory and academic results for word2vec, glove, swivel and Word Movers Distance. Then proceed to their Python open source implementations mainly in the Gensim package&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/tmylk/word-embeddings-for-fun-and-profit-with-gensim-pydata-london-2016"&gt;https://speakerdeck.com/tmylk/word-embeddings-for-fun-and-profit-with-gensim-pydata-london-2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lev Konstantinovskiy</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:pydata-london-2016/lev-konstantinovskiy-word-embeddings-for-fun-and-profit-in-gensim.html</guid></item><item><title>Introduction to Deep Learning &amp; Natural Language Processing</title><link>https://pyvideo.org/pydata-london-2016/raghotham-sripadraj-nischal-hp-introduction-to-deep-learning-natural-language-processing.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This workshop will provide an introduction to deep learning for natural language processing (NLP). It will cover some of the common deep learning architectures, describe advantages and concerns, and provide some hands-on experience.&lt;/p&gt;
&lt;p&gt;We would cover the following:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;What is deep learning?&lt;/li&gt;
&lt;li&gt;Motivation: Some use cases where it has produced state-of-art results&lt;/li&gt;
&lt;li&gt;Basic building blocks of Neural networks (Neuron, activation function, back propagation algorithm, gradient descent algorithm)&lt;/li&gt;
&lt;li&gt;Supervised learning (multi-layer perceptron, convolution neural networks, recurrent neural network)&lt;/li&gt;
&lt;li&gt;Introduction to word2vec&lt;/li&gt;
&lt;li&gt;Introduction to Recurrent Neural Networks&lt;/li&gt;
&lt;li&gt;Text classification using RNN&lt;/li&gt;
&lt;li&gt;Impact of GPUs (Some practical thoughts on hardware and software)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Broadly, there will be three hands-on modules&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A simple multi-layer perceptron - to understand basics of neural networks (everything will be coded from scratch)&lt;/li&gt;
&lt;li&gt;A text classification problem and a text generation problem: This will be solved using Recurrent Neural Networks. The data and software requirements are posted on the github repository.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The repository for this workshop: &lt;a class="reference external" href="https://github.com/rouseguy/intro2deeplearning/"&gt;https://github.com/rouseguy/intro2deeplearning/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The slides for this workshop are available at: &lt;a class="reference external" href="https://speakerdeck.com/bargava/introduction-to-deep-learning"&gt;https://speakerdeck.com/bargava/introduction-to-deep-learning&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Raghotham Sripadraj</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:pydata-london-2016/raghotham-sripadraj-nischal-hp-introduction-to-deep-learning-natural-language-processing.html</guid></item><item><title>Customising nbconvert. How to turn Jupyter notebooks into anything you want.</title><link>https://pyvideo.org/pydata-london-2016/thomas-kluyver-min-ragan-kelley-customising-nbconvert-how-to-turn-jupyter-notebooks-into-anythi.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;nbconvert is a set of tools to convert Jupyter notebooks into other document formats. We'll describe the different ways you can extend and customise nbconvert to modify the output and define extra output formats.&lt;/p&gt;
&lt;p&gt;Jupyter Notebooks are code-centric documents including prose, maths, code, and rich output, such as images and HTML. These are all stored in a structured JSON file, with metadata about each input and output. nbconvert is a highly extensible tool for converting those JSON notebooks to other formats, such as HTML, LaTeX, PDF, and restructuredText. nbconvert powers the web service nbviewer, which renders any notebooks on the Web as HTML, so that they can be read by anyone with a browser and internet connection. We will cover the various ways you can extend nbconvert, from defining your own export formats to customising the output with Jinja templates, enabling things like hiding input or boilerplate cells.&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/takluyver/customising-nbconvert"&gt;https://github.com/takluyver/customising-nbconvert&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Kluyver</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:pydata-london-2016/thomas-kluyver-min-ragan-kelley-customising-nbconvert-how-to-turn-jupyter-notebooks-into-anythi.html</guid></item><item><title>Using Python with Hadoop to create production ready big data applications</title><link>https://pyvideo.org/pydata-london-2016/ulrich-zink-using-python-with-hadoop-to-create-production-ready-big-data-applications.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;A practical end to end walk through on how to use Python for creating an Hadoop production pipeline for big data: Data analysis - Python APIs for Hadoop Streaming, Pyspark... Interactive visualisation and reporting dashboards - Bokeh, Matplotlib... and D3 * Python web service to run Hadoop/Spark based applications.&lt;/p&gt;
&lt;p&gt;For practical and economical reasons the open source Apache Hadoop ecosystem has become the de facto solution for Big Data and machine learning at scale.&lt;/p&gt;
&lt;p&gt;Although Hadoop is extremely powerful for analysing data, it was initially designed as an engineer's tool and allowing non technical user to interact with it can be challenging. Many business intelligence solutions coming from the RDBMS era keep failing the Hadoop test.&lt;/p&gt;
&lt;p&gt;In this talk we are proposing to walk trough a practical example, demonstrating how to combine Python and Hadoop in order to create production grade big data applications.&lt;/p&gt;
&lt;p&gt;We will start by going through some of the APIs that allow Python to be used as the glue to hold the different components of the Hadoop ecosystem together.&lt;/p&gt;
&lt;p&gt;Moving on to visualisation and dashboarding. We will demonstrate how to use some of the most popular Python libraries such as Bokeh, Matplotlib in a big data context and for more data intensive jobs how to produce json output that can be consumed by javascript.&lt;/p&gt;
&lt;p&gt;Finally we will show an example of Flask based web application running on Hadoop/Spark.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ulrich Zink</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:pydata-london-2016/ulrich-zink-using-python-with-hadoop-to-create-production-ready-big-data-applications.html</guid></item><item><title>Robot detection in IT environments</title><link>https://pyvideo.org/pydata-london-2016/eszter-windhager-pokal-robot-detection-in-it-environments.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In this talk, I will present the robot detection module of a machine learning-driven user behavior analytic tool. New methodologies were developed to distinguish between scripted accounts and human users, based on their activities. I will demonstrate a couple of statistical methods, like hypothesis tests from the SciPy library that we use to capture different aspects of robotic operation.&lt;/p&gt;
&lt;p&gt;The robot detection module that I will present is a part of the machine learning-driven, real-time user behavior analytic (UBA) tool of Balabit, called Blindspotter. Its main purpose is to detect internal and external attackers in an IT environment. Blindspotter gathers contextual information about the activities of users, and notifies the security analyst when an unusual event is found. We use the Python language both for the proof of concept experimentation of data scientists and for production as well.&lt;/p&gt;
&lt;p&gt;Humans and robots have different behavior patterns, for instance robots can be active for very long time periods, or they can have extremely high or very low diversity in some features. This implicates that in many aspects humans and robots need different monitoring techniques. Therefore, we should be able to identify the usage type of the account and handle humans and scripts separately in the UBA tool.&lt;/p&gt;
&lt;p&gt;Furthermore, it is also critical to know for security reasons when a human user's account becomes a scripted account or when a scheduled robot suddenly starts to behave like it was used by a human.&lt;/p&gt;
&lt;p&gt;For example, an impostor coming from outside might attack a more easily crackable scripted account and start some manual activities there to look around in the system. Or a malicious insider could start running scripts to collect sensitive data. With the robot detection algorithm we can notice the sudden change in the usage patterns of the accounts.&lt;/p&gt;
&lt;p&gt;In the presentation I will introduce the methodologies that we developed to detect scripted accounts (robots) in IT systems, based on their activities. Different aspects of robotic behavior has been investigated, for example rare scheduled activities surrounded by normal behavior or periodic patterns with some noise in it. I will illustrate the cases with examples found in real data.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://drive.google.com/file/d/0B8CSc3Je587HOXhUYUM2Ty1IYVE/view?usp=sharing"&gt;https://drive.google.com/file/d/0B8CSc3Je587HOXhUYUM2Ty1IYVE/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eszter Windhager Pokal</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/eszter-windhager-pokal-robot-detection-in-it-environments.html</guid></item><item><title>AlzHack Data Driven Diagnosis of Alzheimer's Disease 1</title><link>https://pyvideo.org/pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease-1.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Alzheimer's disease is a form of dementia that affects over 44 million people globally. Unfortunately the condition is very hard to detect in its early stages. It is usually diagnosed by a simple questionnaire test, an approach that can only detect Alzheimer's disease many years after its onset. The challenge set in this project was earlier detection using Python and data science.&lt;/p&gt;
&lt;p&gt;AlzHack is a collaborative citizen science project undertaken by a small but diverse group of data scientists. We will discuss the challenges encountered in discovering and acquiring suitable data, describe how we cleaned and merged multiple data sources, and how it was possible to extract meaningful features from within.&lt;/p&gt;
&lt;p&gt;We will cover textual feature extraction, examining; amongst other methods, part-of-speech tagging, readability calculations, locality sensitive hashing as well as sentiment analysis, all in Python 3.&lt;/p&gt;
&lt;p&gt;In addition we will show how a variety of machine learning techniques (including text clustering and classification) were used; with the aim of distinguishing diagnosed Alzheimer's sufferers from their healthy peers solely based on samples of their written correspondence.&lt;/p&gt;
&lt;p&gt;This will be followed by a look at changepoint and ramp detection on noisy time series data; deployed to identify subtle changes in signals obtained from correspondence of individuals over time; thus allowing a form of non-medical, 'early warning' style detection of Alzheimer's disease.&lt;/p&gt;
&lt;p&gt;Finally we will address the tough task of scaling up a small, collaborative data science project to become an extremely powerful, widely available self-diagnosis tool.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/FrankKelly3/alz-hack-ii"&gt;http://www.slideshare.net/FrankKelly3/alz-hack-ii&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Frank Kelly</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease-1.html</guid></item><item><title>Statistically Solving Sneezes and Sniffles Step by Step</title><link>https://pyvideo.org/pydata-london-2016/ian-ozsvald-giles-weaver-statistically-solving-sneezes-and-sniffles-step-by-step.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Since April 2015 our group has studied the Allergic Rhinitis of a subject with the goal of building a machine learned model that predicts the need for antihistamines. Approximately 30% of the world's population suffers from allergies, we aim to provide a methodology for others to identify the drivers of their own symptoms.&lt;/p&gt;
&lt;p&gt;This is a &amp;quot;citizen science&amp;quot; project, currently focused on one individual and a year's worth of self-reported antihistamine usage, sneezing data and geolocated points. We'll discuss the available external data (including the London Air project's pollution readings, weather, diet, exercise and commute data), exploratory data analysis, our approach to feature engineering from time-series and text sources and our modeling progress.&lt;/p&gt;
&lt;p&gt;The data logging iPhone app and data preparation tools are all open sourced. Python tools discussed include scikit-learn, statsmodels, glueviz, textract and t-sne. We'll also review our distributed working practices.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/ianozsvald/statistically-solving-sniffles-step-by-step-a-work-in-progress"&gt;https://speakerdeck.com/ianozsvald/statistically-solving-sniffles-step-by-step-a-work-in-progress&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ian Ozsvald</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/ian-ozsvald-giles-weaver-statistically-solving-sneezes-and-sniffles-step-by-step.html</guid></item><item><title>Bayesianism and Survival Analysis</title><link>https://pyvideo.org/pydata-london-2016/jake-coltman-jacob-goodwin-bayesianism-and-survival-analysis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;At first encounter Bayesianism can easily seem like a totally new and intimidating way of doing statistics. However, in this tutorial you will learn that you dont need to make a big commitment to gain real benefits from using some Bayesian techniques. We will model the effectiveness of advertising using survival analysis and you will see how Bayesian tools can complement traditional models&lt;/p&gt;
&lt;p&gt;If you come from a traditional statistical or econometric background, Bayesianism can seem like a totally new and different way of approaching statistics. It can feel like there is no middle ground, you either need to commit to it entirely or move on.&lt;/p&gt;
&lt;p&gt;But neither of these are the case. Rather than having to entirely replace our models with Bayesian ones, we can augment particular parts of the models using Bayesian techniques. By the end of the tutorial you will be able to do exactly this when looking at modelling life time value or expected life times.&lt;/p&gt;
&lt;p&gt;The tutorial will focus on modelling the effectiveness of advertising as this is something that affects a wide range of businesses, but the techniques we cover are widely applicable.&lt;/p&gt;
&lt;p&gt;We will begin by looking at how Bayesian ways of thinking can be very useful in helping flag up when we are violating the core assumptions of our models. We will then look at how Bayesian techniques can be used to essentially provide modelling short cuts, i.e. how they allow us to do some things much more easily than we would otherwise be able to do them.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jake Coltman</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/jake-coltman-jacob-goodwin-bayesianism-and-survival-analysis.html</guid></item><item><title>Python and Johnny Cash</title><link>https://pyvideo.org/pydata-london-2016/james-powell-python-and-johnny-cash.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;I was given this title by the Executive Director of NumFOCUS as punishment for being late in putting together a proposal. This talk will be a huge stretch to link Python to Johnny Cash. I suppose this means I'll have to wear all black. I'll probably show off some nifty (C)Python hacks, like adding AST-literals or resurrecting the print statement or implementing lambda-fusion.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">James Powell</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/james-powell-python-and-johnny-cash.html</guid></item><item><title>Challenges of analysing the wheat genome</title><link>https://pyvideo.org/pydata-london-2016/katie-barr-challenges-of-analysing-the-wheat-genome.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;TBC, but along the lines of: What do you do when your data, whilst formally meeting common requirements, is, to put it mildly, an edge case? And what if you don't have the time to write a bespoke tool for your analysis? I discuss this scenario, and will explain why my data is such difficult to start with.&lt;/p&gt;
&lt;p&gt;TBC, but along the lines of: What has 5 times more DNA than a human, hugely repetitive regions, is a mixture of three plants and can differ significantly between varieties. It also supplies 20% of the calories consumed by humanity, and 35% of us depend on it for survival, so there is a strong motivation to understand it. The fact that wheat is not only not a 'model organism' but has some features which means assumptions generally made by the writers of bioinformatics software don't hold makes it hard to work with. In fact, often tools which document themselves as being suitable for use with data in the formats wheat researchers use fall over in this use case. Ideally, we would have time to rewrite them exactly for our use case, but this doesn't always happen.&lt;/p&gt;
&lt;p&gt;This may either be a discussion or a talk. So far I've moved hardware twice, abandoned tools, rewritten parts of tools for my use case (I have a simple python example of this), my current task is to understand in detail why a particular piece of software is segfaulting so I will have a lot more insight very shortly.&lt;/p&gt;
&lt;p&gt;I will aim to keep the talk more about how to deal with data which differs significantly from the standards in a given field than the particulars of wheat, but there are some neat bioinformatics algorithms so I will explain one to demonstrate why we need to store so much in RAM.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Katie Barr</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/katie-barr-challenges-of-analysing-the-wheat-genome.html</guid></item><item><title>To The Web and Beyond</title><link>https://pyvideo.org/pydata-london-2016/kyran-dale-to-the-web-and-beyond.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Working from the premise that the fruits of your data explorations find their natural home on the web, this talk will discuss a few modern work-flows that deliver your data to the browser. I'll be aiming to distil a few of the things I learned in the last year while writing the book 'Data-visualisation with Python and JavaScript' and highlighting some of the key changes that have occurred in the Python and JS dataviz ecosystems in that little time.&lt;/p&gt;
&lt;p&gt;If you take dataviz seriously I think you have to engage and make friends with JavaScript. The good news for Pythonistas is that Modern JS (EcmaScript 6) has 'borrowed' more than a few of Python's clothes, which makes moving between the two languages that much easier. I'll demonstrate a few of these, showing bilingual development is getting less and less arduous.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kyran Dale</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/kyran-dale-to-the-web-and-beyond.html</guid></item><item><title>Survival Analysis in Python and R</title><link>https://pyvideo.org/pydata-london-2016/linda-uruchurtu-survival-analysis-in-python-and-r.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Survival analysis is a set of statistical techniques that has many applications in the industry. This talk will discuss key concepts behind survival analysis by means of examples implemented via Lifelines, an open source python library, and in R for comparison purposes. I will also describe how we have made use of these techniques in Lyst to try to predict when items go out of stock.&lt;/p&gt;
&lt;p&gt;Many problems involve the understanding the duration of specific events; for example, predicting when a customer will churn, when a person will default on a credit, how long a machine will work, etc. These type of questions constitute the realm of Survival analysis, a branch of statistics historically developed by professionals in the actuarial and medical fields dealing with event durations as governed by probability laws.&lt;/p&gt;
&lt;p&gt;In this talk I will cover the basics of Survival analysis via examples implemented via Lifelines, an open-source python library and in R (survival and KMsurv libraries), going from survival curves to regression models. I will discuss how survival analysis can be applied to a variety of problems and in particular, I will focus on the problem of out of stock prediction for an online retailer.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://bit.ly/1Oe1gf7"&gt;http://bit.ly/1Oe1gf7&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Linda Uruchurtu</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/linda-uruchurtu-survival-analysis-in-python-and-r.html</guid></item><item><title>Gotta catch'em all recognizing sloppy work in crowdsourcing tasks</title><link>https://pyvideo.org/pydata-london-2016/maciej-gryka-gotta-catchem-all-recognizing-sloppy-work-in-crowdsourcing-tasks.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;If you have ever used crowdsourcing, you know that dealing with sloppy workers is a major part of the effort. Come see this talk if you want to learn about how to solve this problem using machine learning and some elbow grease. As a bonus, you will also find out how to properly persist your ML models and use them to serve predictions through an HTTP API.&lt;/p&gt;
&lt;p&gt;In 2016 nobody needs convincing that crowdsourced work is a solution to many problems from data labeling, to gathering subjective opinions, to producing transcripts etc. Turns out it can also work really well for functional software testing - but it's not easy to get right.&lt;/p&gt;
&lt;p&gt;One well-known problem with crowdsourcing is sloppy work - where people perform only the absolute minimum actions allowing them to get paid, without actually fulfilling the intended tasks. In many scenarios this can be counteracted by asking multiple workers to complete the same task, but that dramatically increases cost and can still be error-prone. Detecting lazy work is another way to increase quality of gathered data and we have found a way to do this reliably for quite a large variety of tasks.&lt;/p&gt;
&lt;p&gt;In this talk I will describe how we have trained a machine learning model to discriminate between good and sloppy work. The outline is as follows:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;describe the specific problem we had 5m&lt;/li&gt;
&lt;li&gt;overview of the solution 2m&lt;/li&gt;
&lt;li&gt;ML model details 10m&lt;/li&gt;
&lt;li&gt;data capture&lt;/li&gt;
&lt;li&gt;labelling&lt;/li&gt;
&lt;li&gt;balancing the dataset&lt;/li&gt;
&lt;li&gt;feature engineering&lt;/li&gt;
&lt;li&gt;training, retraining&lt;/li&gt;
&lt;li&gt;model itself&lt;/li&gt;
&lt;li&gt;model persistence 10m&lt;/li&gt;
&lt;li&gt;productizing the result by putting it behind an HTTP API 3m&lt;/li&gt;
&lt;li&gt;limitations, trade-offs, what could we do better 3m&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1ZY-VFCOh54LGfE5LVz4cIFpvizwdrqomb4-suNTKSZo/edit?usp=sharing"&gt;https://docs.google.com/presentation/d/1ZY-VFCOh54LGfE5LVz4cIFpvizwdrqomb4-suNTKSZo/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/rainforestapp/destimator"&gt;https://github.com/rainforestapp/destimator&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maciej Gryka</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/maciej-gryka-gotta-catchem-all-recognizing-sloppy-work-in-crowdsourcing-tasks.html</guid></item><item><title>Building Data Pipelines in Python</title><link>https://pyvideo.org/pydata-london-2016/marco-bonzanini-building-data-pipelines-in-python-1.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This talk discusses the process of building data pipelines, e.g. extraction, cleaning, integration, pre-processing of data, in general all the steps that are necessary to prepare your data for your data-driven product. In particular, the focus is on data plumbing and on the practice of going from prototype to production.&lt;/p&gt;
&lt;p&gt;This talk discusses the process of building data pipelines, e.g. extraction, cleaning, integration, pre-processing of data, in general all the steps that are necessary to prepare your data for your data-driven product. In particular, the focus is on data plumbing and on the practice of going from prototype to production.&lt;/p&gt;
&lt;p&gt;Starting from some common anti-patterns, we'll highlight the need for a workflow manager for any non-trivial project.&lt;/p&gt;
&lt;p&gt;We'll discuss the case for Luigi as an interesting option to consider, and we'll consider where it fits in the bigger picture of deploying a data product.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/marcobonzanini/building-data-pipelines-in-python-pydata-london-2016"&gt;https://speakerdeck.com/marcobonzanini/building-data-pipelines-in-python-pydata-london-2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marco Bonzanini</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/marco-bonzanini-building-data-pipelines-in-python-1.html</guid></item><item><title>Mining smartphone sensor data with python</title><link>https://pyvideo.org/pydata-london-2016/neal-lathia-mining-smartphone-sensor-data-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Data from smartphone sensors can be used to learn from and analyse our daily behaviours. In this talk, I'll discuss processing and learning from sensor data with Python. I'll focus on accelerometers - a triaxial sensor that measures motion - starting with an overview pre-processing the data and ending with supervised and unsupervised learning applications and visualisations.&lt;/p&gt;
&lt;p&gt;Our smartphones are increasingly being built with sensors, that can measure everything from where we are (GPS, Wi-Fi) to how we move (accelerometers) and other aspects of our environments (e.g., temperature, humidity). Many apps are now being designed to collect and leverage this data, in order to provide interesting context-aware services and quantify our daily routines.&lt;/p&gt;
&lt;p&gt;In this talk, I'll give an overview of collecting sensor data from an Android app and processing the data with Python. I'll focus on accelerometers - a triaxial sensor that measures the device's motion - which is now being used in apps that detect what you are doing (cycling, running, riding a train); if we have enough time I'll also briefly cover a similar example with Wi-Fi/location data. Using an open-sourced Android app and iPython notebook, I'll discuss the following questions:&lt;/p&gt;
&lt;p&gt;What does the raw data look like? There are a number of trade-offs when collecting sensor data: most notably, data collection needs to be balanced against battery consumption. Plotting the raw data gives a view of how the data was sampled and how it changes across activities.
How can I pre-process and extract features from this data? Three kinds of features can be extracted from acceleromter data: statistical, time-series, and signal-based. Most of these are readily available in well-known Python libraries (scipy, numpy, statsmodels).
How can these features be used to analyse behaviours? I'll show an example of using accelerometer data to cluster users into groups, based on how active they are.
How can these features be used to detect behaviours? I'll show an example of training a supervised learning algorithm (using scikit-learn) to detect walking vs. running vs. standing.
I'll close by discussing how these techniques are being applied in novel smartphone apps for health monitoring.&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/nlathia/pydata_2016"&gt;https://github.com/nlathia/pydata_2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Neal Lathia</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/neal-lathia-mining-smartphone-sensor-data-with-python.html</guid></item><item><title>Spherical Voronoi Diagrams in Python</title><link>https://pyvideo.org/pydata-london-2016/nikolai-nowaczyk-spherical-voronoi-diagrams-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Spherical Voronoi diagrams partition the surface of a sphere into regions of closest distance to a set of generator points. As discussed by Tyler Reddy at PyData 2015, there was no ready-to-go implementation of this in Python. After a self-contained introduction, we will tell the story about how this talk led to an extension of scipy to handle this problem.&lt;/p&gt;
&lt;p&gt;Imagine you are in London and want to travel somewhere within London. Your smartphone ran out of battery and you are not familiar with the area. So you just go the closest tube station. In this manner, you could get a map of London partitioned into regions of closest distance to the various tube stations. Mathematically, this is called a Voronoi diagram. It is defined by a finite set of points in the plane, which are called generators (the tube stations in this example), and consists of a finite number of polygons representing the regions of closest distance to the generators (see Wikipedia for pictures and more details). Calculating Voronoi diagrams in the plane is a well known problem in computational geometry and can be easily handled in Python using scipy.spatial.Voronoi.&lt;/p&gt;
&lt;p&gt;Imagine you are in London and want to travel to some destination far away. Since you are close to an airport, you want to go by plane. In a similar fashion, the airports partition the surface of the Earth into regions of closest distance. Mathematically, this is called a spherical Voronoi diagram. Analogously, this is defined by a finite set of generator points on a sphere and consists of finitely many spherical polygons representing the regions of closest spherical distance to the generator points. (Play with this online sandbox if you like.)&lt;/p&gt;
&lt;p&gt;At PyData London 2015, Tyler Reddy discussed the problem of calculating spherical Voronoi diagrams in Python in this talk. While the problem had been studied and solved conceptually by various computer scientists, there was no ready-to-go implementation available in Python. This initiated a collaboration, which ultimately led to the creation of scipy.spatial.SphericalVoronoi.&lt;/p&gt;
&lt;p&gt;This talk will give a self-contained introduction to the topic of Voronoi diagrams and tell the story of the pull request mentioned above. We will also highlight some applications of spherical Voronoi diagrams in computational virology aimed at getting a better insight into the Dengue virus and Hepatitis C, which was Tylers original motivation to consider the problem.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.nullteilerfrei.de/%7Eairwalker/pydata2016/spherical_voronoi.pdf"&gt;http://www.nullteilerfrei.de/%7Eairwalker/pydata2016/spherical_voronoi.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/niknow/pydata-london-2016-voronoi"&gt;https://github.com/niknow/pydata-london-2016-voronoi&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nikolai Nowaczyk</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/nikolai-nowaczyk-spherical-voronoi-diagrams-in-python.html</guid></item><item><title>A B Testing: Harder than just a color change</title><link>https://pyvideo.org/pydata-london-2016/or-weizman-a-b-testing-harder-than-just-a-color-change.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Is your Product Manager asking you to test out different text or button colors? Not sure where to start? This talk will contain methodology and two case studies from Yelps Transaction Platform on how to properly run an experiment and get the best result. Learn about how to run a simple button color experiment, avoid pitfalls, test, and analyze the results with confidence. Statistical confidence!&lt;/p&gt;
&lt;p&gt;A/B testing is a common practice for websites...but where do you begin? This data-driven approach allows you to launch experiments and features with confidence. So how do you prepare, launch, and analyze an A/B experiment? How do you know for how long to keep it running? What about which metrics to track?&lt;/p&gt;
&lt;p&gt;This talk will present a procedure developed to run an A/B experiment, from planning the task and understanding the key metrics to analyzing the results. We will cover both simple and more complex case study, which help us understand the challenges involved in running experiments.&lt;/p&gt;
&lt;p&gt;This talk will cover a topic that will enable developers to make more data-driven decisions but has not been covered at Pycon. By providing case studies as motivation and a procedure to implement A/B testing this talk will excite the audience. Yelp runs multiple experiments on different aspects and the Transaction Platform team has gotten unique experience of needing to create experiments with limited traffic which will be discussed in the talk.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1H32OKYh6aJ31eE_Slof-WBKGlJVEBl813ZH_tmU0Dbk/edit#slide=id.p"&gt;https://docs.google.com/presentation/d/1H32OKYh6aJ31eE_Slof-WBKGlJVEBl813ZH_tmU0Dbk/edit#slide=id.p&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Or Weizman</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/or-weizman-a-b-testing-harder-than-just-a-color-change.html</guid></item><item><title>Lies damned lies and statistics in Python</title><link>https://pyvideo.org/pydata-london-2016/peadar-coyle-lies-damned-lies-and-statistics-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Join Peadar as he talks you through how to do good statistical analysis with Statsmodels, scikit-learn and PyMC3&lt;/p&gt;
&lt;p&gt;We'll compare and debug models (logistic regression and more advanced versions) in three different statistics packages in Python.&lt;/p&gt;
&lt;p&gt;Peadar will show you how Python can help you fake statistics without having to open up your graduate textbook :)&lt;/p&gt;
&lt;p&gt;Statistical inference is a fundamental tool in science and engineering, but it is often poorly understood. This tutorial will take you from first principles through how to do good statistical analysis in Python.&lt;/p&gt;
&lt;p&gt;Both frequentist and Bayesian versions will be included, and attendees will leave the talk with a good understanding of statistical inference, how to use Scikit-learn, Statsmodels and PyMC3.&lt;/p&gt;
&lt;p&gt;This will be an entire case study comparing three different libraries for solving the same regression problem.&lt;/p&gt;
&lt;p&gt;Attendees should bring a Laptop with Anaconda installed and the latest version of Scikitlearn, PyMC3 and Scikit-Learn.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://slides.com/springcoil/deck-17#/"&gt;http://slides.com/springcoil/deck-17#/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo:  &lt;a class="reference external" href="https://github.com/springcoil/PyDataLondonTutorial"&gt;https://github.com/springcoil/PyDataLondonTutorial&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Peadar Coyle</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/peadar-coyle-lies-damned-lies-and-statistics-in-python.html</guid></item><item><title>PySpark in Practice</title><link>https://pyvideo.org/pydata-london-2016/ronert-obst-dat-tran-pyspark-in-practice.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In this talk we will share our best practices of using PySpark in numerous customer facing data science engagements. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Unit testing with PySpark&lt;/li&gt;
&lt;li&gt;Integration with SQL on Hadoop engines&lt;/li&gt;
&lt;li&gt;Data pipeline management and workflows&lt;/li&gt;
&lt;li&gt;Data Structures (RDDs vs. Data Frames vs. Data Sets)&lt;/li&gt;
&lt;li&gt;When to use MLlib vs scikit-learn&lt;/li&gt;
&lt;li&gt;Operationalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At Pivotal Labs we have many data science engagements on big data. Typical problems involve real-time data from sensors collected by telecom operators to GPS data produced by vehicle tracking systems. One widespread framework to solve those inherently difficult problems is Apache Spark. In this talk, we want to share our best practices with PySpark, Sparks Python API, highlighting our experience as well as dos and don'ts. In particular, we will focus on the whole data science pipeline from data ingestion, data munging and wrangling to the actual model building.&lt;/p&gt;
&lt;p&gt;Finally, many businesses have started to realise that there is no return on investment from data science if the models do not go into production. At Pivotal Labs, one our core principle is API first. Therefore, we will also talk how we put our models into production, sharing our hands-on knowledge in this field and also how this fits into test-driven development.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://pydata2016.cfapps.io/"&gt;http://pydata2016.cfapps.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/datitran/spark-tdd-example"&gt;https://github.com/datitran/spark-tdd-example&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ronert Obst</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/ronert-obst-dat-tran-pyspark-in-practice.html</guid></item><item><title>Twinkle twinkle little star, how I wonder what you are</title><link>https://pyvideo.org/pydata-london-2016/sandra-greiss-twinkle-twinkle-little-star-how-i-wonder-what-you-are.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In my pervious life, I was an astronomer and one of the big tasks many PhD students face is manual star classification. But why ask a student to do what a machine should be able to achieve quicker? In this talk, I use the spectra (stellar flux vs wavelength) of identified stars to build a classifier which will detect the identity of the stars I am interested in.&lt;/p&gt;
&lt;p&gt;In my pervious life, I was an astronomer and one of the big tasks many PhD students face is manual star classification. But why ask a student to do what a machine should be able to do too? In this talk, I use the spectra (stellar flux vs wavelength) of identified stars to build a classifier which will detect the identity of the stars. Using a very simple non linear SVM, I achieve an 86% accuracy with my model. The next step is to use deep neural nets to achieve a better accuracy.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://drive.google.com/file/d/0B2vz1haz5096b1ZEbkRKM3BaX3c/view?usp=sharing"&gt;https://drive.google.com/file/d/0B2vz1haz5096b1ZEbkRKM3BaX3c/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sandra Greiss</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/sandra-greiss-twinkle-twinkle-little-star-how-i-wonder-what-you-are.html</guid></item><item><title>Julia for Data Analysis features, interfaces and future directions</title><link>https://pyvideo.org/pydata-london-2016/simon-byrne-julia-for-data-analysis-features-interfaces-and-future-directions.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This will showcase the data analysis features of Julia, a new high-performance, dynamic language for technical computing. We will give an overview of the various data, statistics and graphics libraries, interfaces with Python and R, as well as outline future directions for this new and exciting language.&lt;/p&gt;
&lt;p&gt;Julia has a rapidly developing ecosystem of packages for data analysis. This talk will give an introduction to some of these, such as Distributions.jl, DataFrames.jl and Gadfly.jl. We will also demonstrate how to leverage existing libraries in Python and R, via the PyCall.jl and RCall.jl packages. Finally, we will describe the future plans, funded by a Moore Foundation grant.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://gist.github.com/simonbyrne/53b3bd29d2f347fc4f6539cf7d2442e1"&gt;https://gist.github.com/simonbyrne/53b3bd29d2f347fc4f6539cf7d2442e1&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Simon Byrne</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/simon-byrne-julia-for-data-analysis-features-interfaces-and-future-directions.html</guid></item><item><title>Pandas from the inside</title><link>https://pyvideo.org/pydata-london-2016/stephen-simmons-pandas-from-the-inside.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;[THIS SESSION WILL BEGIN AT 11:00] Pandas is great way to quickly get started with data analysis in Python: intuitive DataFrames from R; fast numpy arrays under the hood; groupby like SQL. But this familiarity is deceptive and new Pandas users often get stuck on things they feel should be simple. This tutorial/talk takes a look inside Pandas to see how DataFrames actually work when indexing, grouping and joining tables.&lt;/p&gt;
&lt;p&gt;We briefly set the scene with a map of the evolving Pandas landscape for large scale analytics. This is all very exciting and brings many new options. However for many users, Pandas' sweet spot remains much smaller scale. For this talk, we focus in on the DataFrame and how it actually works.&lt;/p&gt;
&lt;p&gt;We will dig deeper and deeper into the components of a DataFrame, including Series and Categorical, DataFrame, Index, MultiIndex and GroupBy objects. For common operations like selecting rows, grouping, subtotals and joins, we will see what internal structures are created and how they fit together. From this, get a better understanding of the various API layers, and how to use them effectively.&lt;/p&gt;
&lt;p&gt;We conclude the talk with a jump back to large scale data analysis and a quick look at how a distributed storage solution like Dask solves one of these problems.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/SteveSimmons/PyData-PandasFromTheInside/blob/master/pfi.pdf"&gt;https://github.com/SteveSimmons/PyData-PandasFromTheInside/blob/master/pfi.pdf&lt;/a&gt;
GitHub Repo: &lt;a class="reference external" href="https://github.com/SteveSimmons/PyData-PandasFromTheInside"&gt;https://github.com/SteveSimmons/PyData-PandasFromTheInside&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Stephen Simmons</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/stephen-simmons-pandas-from-the-inside.html</guid></item><item><title>Interactive Visualization in Jupyter with Bqplot and Interactive Widgets</title><link>https://pyvideo.org/pydata-london-2016/sylvain-corlay-interactive-visualization-in-jupyter-with-bqplot-and-interactive-widgets.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/SylvainCorlay/tutorial"&gt;https://github.com/SylvainCorlay/tutorial&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sylvain Corlay</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/sylvain-corlay-interactive-visualization-in-jupyter-with-bqplot-and-interactive-widgets.html</guid></item><item><title>How to become a Data Scientist in 6 months a hackers approach to career planning</title><link>https://pyvideo.org/pydata-london-2016/tetiana-ivanova-how-to-become-a-data-scientist-in-6-months-a-hackers-approach-to-career-planning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This talk outlines my journey from complete novice to machine learning practitioner. It started in November 2015 when I left my job as a project manager, and by April 2016 I was hired as a Data Scientist by a startup developing bleeding edge deep learning algorithms for medical imagery processing.&lt;/p&gt;
&lt;p&gt;SHORT INTRO
Who I am, my background and short summary of my story. Here I will list the steps I personally took to achieve the goal I had.&lt;/p&gt;
&lt;p&gt;HOW DID I DO IT?
Why I chose a hacky way to enter this career path. First mover advantage, why getting a degree doesnt always improve your career prospects. Possibly a rant on the signalling function of formal education and how that is rarely aligned with a relevant practical skill set. Some stats to back it up (best career success predictors). Examples of hacking bureaucracies/social hierarchies from my experience and elsewhere.
List of things not to do and common cognitive pitfalls.
Networking for nerds - how to do it right.
Time management for chronic procrastinators - how to plan a self-guided project. Some notes on psychology of time discounting and need for external reinforcement, with autobiographical examples.
CONCLUSION
You dont need a PhD or even a masters to do machine learning. On taking calculated risks and especially calculated exits from ones comfort zone. Some notes on soul searching and how to choose a career that is also a passion. Reading list.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://slack-files.com/T0LFE6T6J-F170V6945-ad054e4f6a"&gt;https://slack-files.com/T0LFE6T6J-F170V6945-ad054e4f6a&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tetiana Ivanova</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/tetiana-ivanova-how-to-become-a-data-scientist-in-6-months-a-hackers-approach-to-career-planning.html</guid></item><item><title>Using Support Vector Machines in Scikit Learn to discover genetic aetiologies</title><link>https://pyvideo.org/pydata-london-2016/tim-vivian-griffiths-using-support-vector-machines-in-scikit-learn-to-discover-genetic-aetiologies.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This study uses machine learning methods in scikit-learn to find insights into the genetic aetiology of schizophrenia. It expands on the method of using a single genetic risk score per sample, by creating features from sub-sets of the genome - from single DNA mutations to functional gene-networks. The results have identified potential risk networks and evidence for interactions between mutations.&lt;/p&gt;
&lt;p&gt;Using Support Vector Machines to gain insights into the genetic aetiology of Schizophrenia&lt;/p&gt;
&lt;p&gt;Background&lt;/p&gt;
&lt;p&gt;Schizophrenia is a debilitating psychiatric disorder which affects approximately 1% of the general population. Results from twin studies suggest that genetic factors account for 80% of the variation of the disorder. However, to date, these have not been fully identified. Traditional methods involve performing a Genome Wide Association Study (GWAS), to find estimates of association that different mutations in DNA have with the disease.&lt;/p&gt;
&lt;p&gt;Schizophrenia is a highly polygenic disorder, meaning that it arises from a combination of genetic mutations, all contributing a small level of effect. These studies require very large sample sizes to have the statistical power to identify mutations of interest.&lt;/p&gt;
&lt;p&gt;The common approach is to combine this information into a single polygenic risk score for an individual, which can then be assessed for its predictive power to identify cases of the disease using a single feature Logistic Regression model.&lt;/p&gt;
&lt;p&gt;The machine learning approach&lt;/p&gt;
&lt;p&gt;Instead of creating this single score, different features from the individual association scores were created for use as inputs to machine learning algorithms. These features were either the information from the individual mutations themselves, or collections of mutations in genes which are known to be involved in various functional networks.&lt;/p&gt;
&lt;p&gt;The algorithm chosen was the Support Vector Machine (SVM). This was for a number of reasons:&lt;/p&gt;
&lt;p&gt;They can cope with a large number of input features, and can apply penalty procedures such as L1 regression to identify important features.
Kernel methods can be used to find evidence for interactions between the features - something that is lacking in the traditional methods.
Python libraries used&lt;/p&gt;
&lt;p&gt;This work was made possible by the use of two seminal libraries for Python: Pandas and scikit-learn. In addition the Joblib modules were used to carry out some simple parallel processing tasks on an HPC cluster if the desired information was not already provided by the modules in scikit-learn which already have these built in.&lt;/p&gt;
&lt;p&gt;Pandas made collecting information from sub-sets of the data very quick and efficient to carry out. Examples of this functionality will be given in the talk.&lt;/p&gt;
&lt;p&gt;Results so far&lt;/p&gt;
&lt;p&gt;While not improving on predictive power, the models did provide a rich insight into the association that different gene-networks have with the disorder, and identified genes which are targets of the Fragile X Mental Retardation Protein (FMRP) supporting findings from recent research in molecular biology. The information showing this were the coefficients assigned to the features seen in the boxplot provided. This was created by building multiple models, each with different train/test splits of the data to get the distributions of the coefficients. As can be seen, those assigned to the FMRP feature are consistently higher.&lt;/p&gt;
&lt;p&gt;The findings also show evidence for pair wise interactions between single point mutations. A series of procedures were carried out to simulate positive cases based on differing levels of contributions from main effects and interactions. This showed that without the contribution of the interactions, the results seen in the real world dataset by the kernel based models would not have been possible. Full details of how this was carried out will be discussed.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/timvg80/PyDataLondon2016/blob/master/pydata.md"&gt;https://github.com/timvg80/PyDataLondon2016/blob/master/pydata.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/timvg80/PyDataLondon2016"&gt;https://github.com/timvg80/PyDataLondon2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tim Vivian Griffiths</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/tim-vivian-griffiths-using-support-vector-machines-in-scikit-learn-to-discover-genetic-aetiologies.html</guid></item><item><title>The Duct Tape of Heroes Bayesian statistics</title><link>https://pyvideo.org/pydata-london-2016/vincent-d-warmerdam-the-duct-tape-of-heroes-bayesian-statistics.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In this talk I will give many examples of when Bayes rule will help you in your day to day work. I'll quickly show many examples of bayesian statistical thinking in action; the pleasure of inference, probabilistic graphs, model selection, feature generation, even operations research! I'll finish with a dataset from Heroes of the Storm and I'll show why Bayesian models can outperform randomforests.&lt;/p&gt;
&lt;p&gt;My talk is made up of the following examples;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;basic disease example: what is the value of adding an extra test to a patient&lt;/li&gt;
&lt;li&gt;give an example of an inference task that is very hard to do properly without bayesian thinking&lt;/li&gt;
&lt;li&gt;creating simple probibalistic models with pandas and showing how they are robust against missing data&lt;/li&gt;
&lt;li&gt;demo the daft, corner and pomegrenate library&lt;/li&gt;
&lt;li&gt;show how you can use bayes rule to pick models&lt;/li&gt;
&lt;li&gt;demo a bayesian probablistic approach to finding overpowered characters in the Heroes of the Storm video game.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://koaning.io/theme/notebooks/bayes.pdf"&gt;http://koaning.io/theme/notebooks/bayes.pdf&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Vincent D  Warmerdam</dc:creator><pubDate>Wed, 11 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-11:pydata-london-2016/vincent-d-warmerdam-the-duct-tape-of-heroes-bayesian-statistics.html</guid></item><item><title>Irregular time series and how to whip them</title><link>https://pyvideo.org/pydata-london-2016/aileen-nielsen-irregular-time-series-and-how-to-whip-them.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This talk will present best-practices and most commonly used methods for dealing with irregular time series. Though we'd all like data to come at regular and reliable intervals, the reality is that most time series data doesn't come this way. Fortunately, there is a long-standing theoretical framework for knowing what does and doesn't make sense for corralling this irregular data.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;p&gt;Irregular time series and how to whip them&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;p&gt;History of irregular time series&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;p&gt;Statisticians have long grappled with what to do in the case of missing data, and missing data in a time series is a special, but very common, case of the general problem of missing data. Luckily, irregular time series offer more information and more promising techniques than simple guesswork and rules of thumb.&lt;/p&gt;
&lt;p&gt;Your best options&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;p&gt;I'll discuss best-practices for irregular time series, emphasizing in particular early-stage decision making driven by data and the purpose of a particular analysis. I'll also highlight best-Python practices and state of the art frameworks that correspond to statistical best practices.&lt;/p&gt;
&lt;p&gt;In particular I'll cover the following topics:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Visualizing irregular time series&lt;/li&gt;
&lt;li&gt;Drawing inferences from patterns of missing data&lt;/li&gt;
&lt;li&gt;Correlation techniques for irregular time series&lt;/li&gt;
&lt;li&gt;Causal analysis for irregular time series&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://slack-files.com/T0LFE6T6J-F17Q90AC9-b3eabe5c42"&gt;https://slack-files.com/T0LFE6T6J-F17Q90AC9-b3eabe5c42&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aileen Nielsen</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/aileen-nielsen-irregular-time-series-and-how-to-whip-them.html</guid></item><item><title>Building a Pong playing AI in just 1 hour (plus 4 days training...)</title><link>https://pyvideo.org/pydata-london-2016/daniel-k-slater-building-a-pong-playing-ai-in-just-1-hour-plus-4-days-training.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;We will build an AI that can master the game of Pong in just 1 hour. In the course of this we will talk through some of the tools involved. Q-learning , Deep learning and Convolutional nets and how they fit into Pong. Most of the heavy lifting will be done using Google's recently released Tensorflow libraries.&lt;/p&gt;
&lt;p&gt;We will start by setting up a Python an agent in Pong that moves completely randomly. I will then talk over what Q-learning is and how it works and a bit about Convolutional nets. We will then build the actual agent using Tensorflow.&lt;/p&gt;
&lt;p&gt;Full code will be given, including links for downloading the resources required, so hopefully the audience can build this on their laptops. I would hope to give enough information that users will be able to make there own agents independently once finished.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/danielslater/building-a-pong-ai"&gt;https://speakerdeck.com/danielslater/building-a-pong-ai&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a class="reference external" href="https://github.com/DanielSlater/PyDataLondon2016"&gt;https://github.com/DanielSlater/PyDataLondon2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel K Slater</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/daniel-k-slater-building-a-pong-playing-ai-in-just-1-hour-plus-4-days-training.html</guid><category>tensorflow</category></item><item><title>Python vs Orangutan</title><link>https://pyvideo.org/pydata-london-2016/dirk-gorissen-python-vs-orangutan.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Somewhere in 2000km2 worth of Bornean Jungle there are 6 orangutans that need to be found and tracked. All you have is a short radio ping against a noisy background resulting in a unique and interesting anomaly detection problem.&lt;/p&gt;
&lt;p&gt;Borneo and Sumatra in South East Asia are home to a number of orangutan rescue and rehabilitation centres. A common problem each of these have is keeping track of the animals they release back in the wild. Traditionally this is done with ground teams using a radio antenna to search for the weak radio pings each animal sends out. A true needle in the haystack problem.&lt;/p&gt;
&lt;p&gt;This talk will discuss a drone-based tracking system that has been developed to alleviate this problem. More specifically the data analysis problem of searching and locating the signal of each animal against a noisy background.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dirk Gorissen</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/dirk-gorissen-python-vs-orangutan.html</guid></item><item><title>AlzHack Data Driven Diagnosis of Alzheimer's Disease</title><link>https://pyvideo.org/pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Alzheimer's disease is a form of dementia that affects over 44 million people globally. Unfortunately the condition is very hard to detect in its early stages. It is usually diagnosed by a simple questionnaire test, an approach that can only detect Alzheimer's disease many years after its onset. The challenge set in this project was earlier detection using Python and data science.&lt;/p&gt;
&lt;p&gt;AlzHack is a collaborative citizen science project undertaken by a small but diverse group of data scientists. We will discuss the challenges encountered in discovering and acquiring suitable data, describe how we cleaned and merged multiple data sources, and how it was possible to extract meaningful features from within.&lt;/p&gt;
&lt;p&gt;We will cover textual feature extraction, examining; amongst other methods, part-of-speech tagging, readability calculations, locality sensitive hashing as well as sentiment analysis, all in Python 3.&lt;/p&gt;
&lt;p&gt;In addition we will show how a variety of machine learning techniques (including text clustering and classification) were used; with the aim of distinguishing diagnosed Alzheimer's sufferers from their healthy peers solely based on samples of their written correspondence.&lt;/p&gt;
&lt;p&gt;This will be followed by a look at changepoint and ramp detection on noisy time series data; deployed to identify subtle changes in signals obtained from correspondence of individuals over time; thus allowing a form of non-medical, 'early warning' style detection of Alzheimer's disease.&lt;/p&gt;
&lt;p&gt;Finally we will address the tough task of scaling up a small, collaborative data science project to become an extremely powerful, widely available self-diagnosis tool.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Frank Kelly</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/frank-kelly-giles-weaver-alzhack-data-driven-diagnosis-of-alzheimers-disease.html</guid></item><item><title>DyND: Enabling complex analytics across the language barrier</title><link>https://pyvideo.org/pydata-london-2016/irwin-zaid-dynd-enabling-complex-analytics-across-the-language-barrier.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;DyND is a C++ library for dynamic, multidimensional arrays. Inspired by NumPy, it aims to be a cross-language platform for data analysis, by bringing the popularity and flexibility of the Python data science stack to other languages, such as C++, R, and Javascript.&lt;/p&gt;
&lt;p&gt;DyND is a dynamic array library for structured and semi-structured data, written with C++ as a first-class target and extended to Python with a lightweight binding. It aims to be a cross-language platform for data analysis, by bringing the popularity and flexibility of the Python data science stack to other languages, such as C++, R, and Javascript. It is inspired by NumPy, the Python array programming library at the core of the scientific Python stack, but tries to address a number of obstacles encountered by some of NumPys users. Examples of these are support for variable-sized strings, missing values, ragged array dimensions, and versatile tools for creating functions that apply generic patterns across arrays.&lt;/p&gt;
&lt;p&gt;This talk will introduce the DyND library, motivating it with simple datasets that are hard to process with existing tools. We'll discuss its architecture, features, and a roadmap for the future.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://datashape.pydata.org/"&gt;http://datashape.pydata.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://libdynd.org/"&gt;http://libdynd.org/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Irwin Zaid</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/irwin-zaid-dynd-enabling-complex-analytics-across-the-language-barrier.html</guid></item><item><title>Python flying at 40,000 feet</title><link>https://pyvideo.org/pydata-london-2016/marko-vasiljevski-raffaele-rainone-python-flying-at-40000-feet.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Have you ever seen real black-boxes' data? For safety, airlines worldwide are obliged to monitor their flights' data - the same that is stored in aircraft black boxes. Look how we do it with Python and open source technologies at hand. It's never easy with these multivariate time-series - they can be incomplete and contain varying number of flight parameters - we'll deal with it!&lt;/p&gt;
&lt;p&gt;Introduction&lt;/p&gt;
&lt;p&gt;In this talk we present data exploration and machine learning algorithms applied to aircraft black-box data. We believe that safety algorithms have to be open source so everybody could apply them to increase flight safety: it is all about using Python to fly safer.&lt;/p&gt;
&lt;p&gt;Data&lt;/p&gt;
&lt;p&gt;We shall firstly briefly explain how the black boxes data is downloaded from an aircraft upon landing. Shortly after that, a Python tool to analyse the flight data is launched (you can find it in the open source repo FlightDataAnalyzer of Flight Data Services). To be able to analyse the data properly and to give context to it, a flight has to be segmented into taxi-out, take-off roll, take-off, climb, cruise, holding, descent, landing, touchdown, decelerating and taxi-in phases. A set of sophisticated deterministic and probabilistic algorithms is applied to the data to detect key-points of a flight, like take-off and touchdown points, and values of flight parameters at those points. The detection of these points and the calculation of the associated values of parameters (like acceleration at touchdown) are difficult problems which well share and try to solve with the help of the audience.&lt;/p&gt;
&lt;p&gt;We plan to present a simplified version (due to short time available for presentations) of detecting the end-point of take-off phase as an important datum for further flight analysis.&lt;/p&gt;
&lt;p&gt;We want to stress about the importance of having a large number and good-quality data. Even with a not so sophisticated algorithm, one can get great results if he/she has a good dataset at hand. We have 4,000,000 flights in the database that grows daily and that enables us to give better predictions. We want to go farther by promoting flight data-sharing concepts between different airlines so we get better statistics on relevant parameters - everything for the sake of flight safety!&lt;/p&gt;
&lt;p&gt;Algorithms and tools&lt;/p&gt;
&lt;p&gt;The detection of the switching-phase points is very challenging since the parameters vary very much amongst aircraft types. It is even a challenge for an experienced pilot when he looks at the data! However, we can learn from the data across multiple flights first and this is where Bayesian logic gets in the play. Assuming the independence between the parameters (for the sake of simplicity), we obtain simple formulas to calculate the posterior probability for the end-of-take-off point. This assumption, though very strong, gives great initial results. Furthermore, sometimes a sensor on one engine stops working (not the engine itself! :). In this case we can just omit this signal and use what we are left with. Again, the results are more than acceptable.&lt;/p&gt;
&lt;p&gt;We will show how we use R for data exploration and Jupyter-notebooks (Python) for prototyping and how these two are complimentary. The fine-tuned algorithms are integrated in the system by using Python, exclusively. We make heavy use of third party open source libraries such as numpy, scipy, pandas, pymc, etc..&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/raffo/python-flying-at-40-000-feet-part-1"&gt;https://speakerdeck.com/raffo/python-flying-at-40-000-feet-part-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a class="reference external" href="https://github.com/raino01r/pydata_london_2016"&gt;https://github.com/raino01r/pydata_london_2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marko Vasiljevski</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/marko-vasiljevski-raffaele-rainone-python-flying-at-40000-feet.html</guid></item><item><title>Estimating Residential Land Prices in the UK</title><link>https://pyvideo.org/pydata-london-2016/philippe-bracke-estimating-residential-land-prices-in-the-uk.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;We address a gap in UK statistics by producing aggregate and local residential land price indices for England and Wales starting from house sales data from the Land Registry (LR) Price Paid Dataset. We decompose each sale price into a structure and a land component using data on building volumes and land areas derived from Ordnance Survey MasterMap and LR maps of freehold land parcels.&lt;/p&gt;
&lt;p&gt;The goal of this project is to produce aggregate and local residential land price indices for England and Wales, and their local authorities, starting from house sales data (data on actual land sales are very sparse and not readily available). We address a current gap in UK statistics: the Valuation Office Agency discontinued its land price indices in 2011. Data on land values are important: they provide builders with information on the profitability of their investments and offer the public sector an instrument to evaluate decisions on land releases.&lt;/p&gt;
&lt;p&gt;We attach to each house transaction in the England and Wales Land Registry (LR) Price Paid Dataset an estimate of the building volume and the associated land area. These additional variables are gathered from two data sources: (1) Ordnance Survey MasterMap, which includes information on the footprint and height of all buildings in Great Britain, and (2) LR maps of freehold land parcels, which can be freely downloaded online for each local authority. After putting together our dataset with QGIS and R, we can decompose each sale price in LR Price Paid into a structure and a land component using regression methods. We double check our results on structures using data on construction costs from the Office for National Statistics.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/philippe-bracke-estimating-residential-land-prices-in-the-uk"&gt;http://www.slideshare.net/PyData/philippe-bracke-estimating-residential-land-prices-in-the-uk&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Philippe Bracke</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/philippe-bracke-estimating-residential-land-prices-in-the-uk.html</guid></item><item><title>Lightning Talks and Closing Address</title><link>https://pyvideo.org/pydata-london-2016/pydata-london-2016-lightning-talks-and-closing-address.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Various speakers</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/pydata-london-2016-lightning-talks-and-closing-address.html</guid><category>lightning talks</category></item><item><title>Modelling a text corpus using Deep Boltzmann Machines in python</title><link>https://pyvideo.org/pydata-london-2016/ricardo-pio-monti-modelling-a-text-corpus-using-deep-boltzmann-machines-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Deep Boltzmann machines (DBMs) are exciting for a variety of reasons, principal among which is the fact that they are able to learn probabilistic representations of data in an entirely unsupervised manner. This allows DBMs to leverage large quantities of unlabelled data which are often available. The resulting representations can then be fine-tuned using limited labelled data or studied to obtain a more comprehensive understanding of the data at hand.&lt;/p&gt;
&lt;p&gt;This talk will begin by providing a high level description of DBMs and the training algorithms involved in learning such models. A topic modelling example will be used as a motivating example to discuss practical aspects of fitting DBMs and potential pitfalls. The entire code for this project is written in python using only standard libraries e.g., bumpy.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/piomonti/DeepTextMining/blob/master/PyData%20London%20Slides.pdf"&gt;https://github.com/piomonti/DeepTextMining/blob/master/PyData%20London%20Slides.pdf&lt;/a&gt;
GitHub: &lt;a class="reference external" href="https://github.com/piomonti/DeepTextMining"&gt;https://github.com/piomonti/DeepTextMining&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ricardo Pio Monti</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/ricardo-pio-monti-modelling-a-text-corpus-using-deep-boltzmann-machines-in-python.html</guid></item><item><title>Deep Learning for QSAR</title><link>https://pyvideo.org/pydata-london-2016/rich-lewis-deep-learning-for-qsar.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This talk details techniques used to train state of the art neural networks on chemical data, from data processing to predictive screening, all in Python!&lt;/p&gt;
&lt;p&gt;The phrase &amp;quot;Deep learning&amp;quot; has gained buzzword status in recent years. Regardless of whether the unprecedented hype surrounding technique is well placed, it has established itself as state of the art in many fields, most notably in image and speech recognition and natural language processing, where tech giants such as Google, Facebook, Baidu, Microsoft and Apple have invested billions of dollars in driving the technology forward.&lt;/p&gt;
&lt;p&gt;Less well known is that it has been shown to be state of the art for Quantitative Structure Activity Relationship (QSAR) problems, including winning the Kaggle Merck Molecular Activity Challenge, and the recent Tox21 toxicity challenge. These problems aim to predict, given only the chemical structure of a compound, how the application of that compound would affect the activity of a biological system.&lt;/p&gt;
&lt;p&gt;This talk will cover research undertaken as part of my PhD. I will cover:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;data formatting and preprocessing using sqlalchemy and pandas&lt;/li&gt;
&lt;li&gt;feature extraction from a chemical structure using the RDKit cheminformatics toolkit&lt;/li&gt;
&lt;li&gt;training of models using Theano and Keras.&lt;/li&gt;
&lt;li&gt;results and comparison to other techniques&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rich Lewis</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/rich-lewis-deep-learning-for-qsar.html</guid></item><item><title>A Gentle Introduction to Neural Networks and making your own with Python</title><link>https://pyvideo.org/pydata-london-2016/tariq-rashid-a-gentle-introduction-to-neural-networks-and-making-your-own-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Neural networks are not only a powerful data science tool, they're at the heart of recent breakthroughs in deep learning and artificial intelligence.&lt;/p&gt;
&lt;p&gt;This talk, designed for a complete beginners and anyone non-technical, will introduce the history and ideas behind neural networks. You won't need anything more than basic school maths. We'll gently build our own neural network in Python too.&lt;/p&gt;
&lt;p&gt;Ideas: - search for intelligence machines, what's easy for us not easy for computers.&lt;/p&gt;
&lt;p&gt;DIY: - MINST dataset - simple network 3 layer - matrix multiplication to aid calculations - preprocessing, priming weights - 95% accuracy with very simple code! - improvements lead to 98% accuracy!&lt;/p&gt;
&lt;p&gt;source code will be online Python 3 notebook on makeyourownneuralnetwork.blogspot.com and &amp;#64;myoneuralnet&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1Mai96er3d9Tca6mwVaO_bI7nZ09L6QQbww233yAnaGg/edit"&gt;https://docs.google.com/presentation/d/1Mai96er3d9Tca6mwVaO_bI7nZ09L6QQbww233yAnaGg/edit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a class="reference external" href="https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork"&gt;https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tariq Rashid</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/tariq-rashid-a-gentle-introduction-to-neural-networks-and-making-your-own-with-python.html</guid></item><item><title>Scaling Out PyData</title><link>https://pyvideo.org/pydata-london-2016/travis-oliphant-keynote-scaling-out-pydata.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/teoliphant/scaling-pydata-up-and-out"&gt;http://www.slideshare.net/teoliphant/scaling-pydata-up-and-out&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Travis Oliphant</dc:creator><pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-09:pydata-london-2016/travis-oliphant-keynote-scaling-out-pydata.html</guid><category>keynote</category></item><item><title>10 things I learned about writing data pipelines in Python and Spark.</title><link>https://pyvideo.org/pydata-london-2016/ali-zaidi-10-things-i-learned-about-writing-data-pipelines-in-python-and-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Starting in the Q4, 2015, I wrote the financials data pipeline that collates ~200 data points and calculates ~300 metrics for ~80M account filings from ~11M private companies.&lt;/p&gt;
&lt;p&gt;As I write, this is in production: &lt;a class="reference external" href="http://bit.ly/1T3CzDG"&gt;http://bit.ly/1T3CzDG&lt;/a&gt;, &lt;a class="reference external" href="http://bit.ly/1Q8iBBq"&gt;http://bit.ly/1Q8iBBq&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I used Python, Spark and loads of good fortune to make this. I would like to share my journey with the PyData community - purely to give something back, as I have learned so much out of the meetups.&lt;/p&gt;
&lt;p&gt;My talk would include takeaways, patterns, anti-patterns, mistakes and big mistakes that I made and learned from. I think this will be very useful for beginner-intermediate data wranglers.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/alixedi/PyData2016/blob/master/Enhanced%20Financials.pdf"&gt;https://github.com/alixedi/PyData2016/blob/master/Enhanced%20Financials.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a class="reference external" href="https://github.com/alixedi/PyData2016"&gt;https://github.com/alixedi/PyData2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ali Zaidi</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/ali-zaidi-10-things-i-learned-about-writing-data-pipelines-in-python-and-spark.html</guid></item><item><title>KEYNOTE: Laser ranging in a new dimension</title><link>https://pyvideo.org/pydata-london-2016/andreas-freise-keynote-laser-ranging-in-a-new-dimension.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Recently the LIGO project announced the first detection of a gravitational wave. This achievements is a triumph for experimental physics, after decades of effort on developing an instrument sensitive enough to react to the tiny push and pull from a gravitational wave. I will present examples from our work on modelling and understanding the complex laser interferometers of LIGO.&lt;/p&gt;
&lt;p&gt;Gravitational waves are a prediction from Einstein's theory of gravity: when very compact and massive objects such as black holes collide, they produce strong gravitational waves, but until recently we did not have an instrument sensitive enough to measure them. In September 2015 the LIGO detectors achieved the first detection of a gravitational wave and could estimate the parameters of the black holes that had produced the signal, a billion years ago in a galaxy far away. LIGO and other detectors will now be improved further to detect more signals from black holes and other elusive objects, kickstarting a new type of astronomy.&lt;/p&gt;
&lt;p&gt;At the core of the LIGO observatories are very large laser interferometers. The concept for these interferometers is well known but has been enhanced with new technology. It typically takes several years of work after the interferometers have been first turned on, until they reach their full sensitivity. Numerical simulations play an important role in the process. In recent years we have developed new Python-based tools to model the optical systems. Such tools must remain accessible to the experimentalists in charge of the instruments, to be used effectively in large collaborations.&lt;/p&gt;
&lt;p&gt;I will give a short introduction to the LIGO project and present examples from our work to support the LIGO detectors with numerical modelling tools. I will mention how the evolution of our work over the last ten years led to Python as a language of choice.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.gwoptics.org/talks/2016/pydata/"&gt;http://www.gwoptics.org/talks/2016/pydata/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andreas Freise</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/andreas-freise-keynote-laser-ranging-in-a-new-dimension.html</guid></item><item><title>Classifying train and car journeys using telematics data</title><link>https://pyvideo.org/pydata-london-2016/annabelle-rolland-classifying-train-and-car-journeys-using-telematics-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In this talk we will review a methodology to classify train and car journeys using telematics data and go through the challenges often encountered when working on this type of projects.&lt;/p&gt;
&lt;p&gt;As insurers are moving away from traditional demographics factors to price car insurance products, telematics is becoming an increasingly popular solution to assess individual driver risk, optimise motor insurance premiums and encourage safer driving habits. As MyDrive Solutions is using a mobile application as an option to capture data, it is crucial for the company to be able to distinguish between train and car journeys to ensure that policy holders' driving behaviours are assessed fairly.&lt;/p&gt;
&lt;p&gt;In this talk we will go through a methodology to classify train and car journeys, reviewing the various tools used to accomplish each task and highlighting the challenges encountered.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/annabeller/classifying-train-and-car-journeys-using-telematics-data"&gt;https://speakerdeck.com/annabeller/classifying-train-and-car-journeys-using-telematics-data&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Annabelle Rolland</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/annabelle-rolland-classifying-train-and-car-journeys-using-telematics-data.html</guid></item><item><title>Real time association mining in large social networks</title><link>https://pyvideo.org/pydata-london-2016/ben-chamberlain-real-time-association-mining-in-large-social-networks.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Social media can be used to perceive the relationships between individuals, companies and brands. Understanding the relationships between key entities is of vital importance for decision support in a swathe of industries. We present a real-time method to query and visualise regions of networks that could represent an industries, sports or political parties etc.&lt;/p&gt;
&lt;p&gt;There is a growing realisation that to combat the waning effectiveness of traditional marketing, social media platform owners need to find new ways to monetise their data. Social media data contains rich information describing how real world entities relate to each other. Understanding the allegiances, communities and structure of key entities is of vital importance for decision support in a swathe of industries that have hitherto relied on expensive, small scale survey data. We present a real-time method to query and visualise regions of networks that are closely related to a set of input vertices. The input vertices can define an industry, political party, sport etc. The key idea is that in large digital social networks measuring similarity via direct connections between nodes is not robust, but that robust similarities between nodes can be attained through the similarity of their neighbourhood graphs. We are able to achieve real-time performance by compressing the neighbourhood graphs using minhash signatures and facilitate rapid queries through Locality Sensitive Hashing. These techniques reduce query times from hours using industrial desktop machines to milliseconds on standard laptops. Our method allows analysts to interactively explore strongly associated regions of large networks in real time. Our work has been deployed in Python based software and uses the scipy stack (specifically numpy, pandas, scikit-learn and matplotlib) as well as the python igraph implementation.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1-NkcPM3XYn-7jk6233MvvFJiC5Abi3e2nGkF_NSFuFA/edit?usp=sharing"&gt;https://docs.google.com/presentation/d/1-NkcPM3XYn-7jk6233MvvFJiC5Abi3e2nGkF_NSFuFA/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Additional information: &lt;a class="reference external" href="http://krondo.com/in-which-we-begin-at-the-beginning/"&gt;http://krondo.com/in-which-we-begin-at-the-beginning/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ben Chamberlain</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/ben-chamberlain-real-time-association-mining-in-large-social-networks.html</guid></item><item><title>Estimating stock price correlations using Wikipedia</title><link>https://pyvideo.org/pydata-london-2016/delia-rusu-estimating-stock-price-correlations-using-wikipedia.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;Building an equities portfolio is a challenging task for a finance professional as it requires, among others, future correlations between stock prices. As this data is not always available, in this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures based on Wikipedia data.&lt;/p&gt;
&lt;p&gt;According to Modern Portfolio Theory, assembling a portfolio involves forming expectations about the individual stock's future risk and return as well as future correlations between stock prices. These future correlations are typically estimated using historical stock price data. However, there are situations where this type of data is not available, such as the time preceding an IPO.&lt;/p&gt;
&lt;p&gt;In this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures in order to estimate the correlation between stock prices.&lt;/p&gt;
&lt;p&gt;The focus of the analysis will be on companies listed on the London Stock Exchange which form the FTSE 100 Index. I am going to use Wikipedia articles in order to derive the textual description for each company. Additionally, I will use the Wikipedia category structure to derive a graph describing relations between companies.&lt;/p&gt;
&lt;p&gt;The analysis will be performed using the scikit-learn and networkX libraries and example code will be available to the audience.&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a class="reference external" href="https://github.com/deliarusu/wikipedia-correlation"&gt;https://github.com/deliarusu/wikipedia-correlation&lt;/a&gt;
&lt;a class="reference external" href="https://github.com/idio/wiki2vec"&gt;https://github.com/idio/wiki2vec&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Delia Rusu</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/delia-rusu-estimating-stock-price-correlations-using-wikipedia.html</guid></item><item><title>Cat modelling with python</title><link>https://pyvideo.org/pydata-london-2016/john-gill-cat-modelling-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;A brief history of use of python within the Bermuda based reinsurance industry.&lt;/p&gt;
&lt;p&gt;A tale of matplotlib, reportlab, numpy, ipython, jupyter and more.&lt;/p&gt;
&lt;p&gt;Current challenges and opportunities and an oasis on the horizon.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/openbermuda/80days/blob/blog/talks/pydata/talk.pdf"&gt;https://github.com/openbermuda/80days/blob/blog/talks/pydata/talk.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a class="reference external" href="https://github.com/openbermuda/80days/tree/blog/talks/pydata"&gt;https://github.com/openbermuda/80days/tree/blog/talks/pydata&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">John Gill</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/john-gill-cat-modelling-with-python.html</guid></item><item><title>Building Data Pipelines in Python</title><link>https://pyvideo.org/pydata-london-2016/marco-bonzanini-building-data-pipelines-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This talk discusses the process of building data pipelines, e.g. extraction, cleaning, integration, pre-processing of data, in general all the steps that are necessary to prepare your data for your data-driven product. In particular, the focus is on data plumbing and on the practice of going from prototype to production.&lt;/p&gt;
&lt;p&gt;Starting from some common anti-patterns, we'll highlight the need for a workflow manager for any non-trivial project.&lt;/p&gt;
&lt;p&gt;We'll discuss the case for Luigi as an interesting option to consider, and we'll consider where it fits in the bigger picture of deploying a data product.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/marcobonzanini/building-data-pipelines-in-python-pydata-london-2016"&gt;https://speakerdeck.com/marcobonzanini/building-data-pipelines-in-python-pydata-london-2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marco Bonzanini</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/marco-bonzanini-building-data-pipelines-in-python.html</guid></item><item><title>The NetworkL python package</title><link>https://pyvideo.org/pydata-london-2016/moreno-bonaventura-the-networkl-python-package.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;NetworkL.github.io is an experimental python package which supports the manipulation and efficient (L)ongitudinal analysis of large-scale time-varying graphs. NetworkL reduces the memory load of the Distance Matrix up to 50% and performs re-computation of shortest paths in centiseconds after edges updates. The package opens the possibility to perform real-time network analysis on streaming data.&lt;/p&gt;
&lt;p&gt;Graphs are the most popular way to represent and analyse a variety of real-world system and data sets from different domains. Today data grow fast and change rapidly over time.&lt;/p&gt;
&lt;p&gt;NetworkL (&lt;a class="reference external" href="http://networkl.github.io/"&gt;http://networkl.github.io/&lt;/a&gt;) is an experimental python package I wrote to supports the manipulation and efficient (L)ongitudinal analysis of large-scale time-varying graphs. NetworkL includes a set of optimized algorithms and data structures which create the basics to carry out network analysis of large time-varying networks even on commodity workstations.&lt;/p&gt;
&lt;p&gt;In particular it implements a smart way to representation the full distance matrix of the network as a sparse matrix. This reduces the memory load up to 50%. Moreover, NetworkL include an implementation of the Ramalingam&amp;amp;Reps algorithm to recompute all the shortest paths length. Re-computations are performed in centiseconds regardless of the graph size. This performances makes NetworkL particularly suitable for the analysis of (L)ongitudinal network data-sets.&lt;/p&gt;
&lt;p&gt;Biblio: Ramalingam, G., &amp;amp; Reps, T. (1996). On the computational complexity of dynamic graph problems. Theoretical Computer Science, 158(1), 233-277.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Moreno Bonaventura</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/moreno-bonaventura-the-networkl-python-package.html</guid></item><item><title>Lessons from 6 months of using Luigi in production</title><link>https://pyvideo.org/pydata-london-2016/peter-owlett-lessons-from-6-months-of-using-luigi-in-production.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;At Deliveroo we've built our data plumbing from the ground up using Luigi to manage our data workflows. In this talk I'll be walking through our experiences using Luigi scaling from a few simple jobs to a complex, production grade system. This talk is mostly about building robust data pipelines, but is also a little bit about why it's better to be woken up by your cat than by the server alarm.&lt;/p&gt;
&lt;p&gt;In the beginning, there was Cron. We had one job, it ran at 1AM, and it was good. Then we added another job, and to make them run one after the other, we used Luigi, which says &amp;quot;This can only run when this is finished&amp;quot;. Then we added another ~500 jobs, long running scikitlearn computes, external API dependencies, a business reporting systems with 2000+ reports and 400+ users and a scheduling system with 5000+ users. This is when things got interesting.&lt;/p&gt;
&lt;p&gt;This is the story of building the data systems at Deliveroo. This is not a talk about Big Data, cutting edge algorithms or new open source technology. Rather, this is a talk about coping with complexity in a rapidly changing landscape. I'll start from the beginning, giving a brief overview of what Luigi is and why we decided to roll with it. The body of the talk will be about the challenges we faced as our company grew in size and complexity, the solutions that worked (and those that didn't), and what we know now that we didn't know then. I'll cover a bit of the luigi syntax itself, but mostly I'll focus on the things we did around luigi that made it work for us; how (not) to design pipelines, how to test them, how to manage issues gracefully and how to detect problems in advance.&lt;/p&gt;
&lt;p&gt;By attending this session you'll learn:&lt;/p&gt;
&lt;p&gt;Why DAG based ETL systems are fundamentally useful
What to think about when designing your DAG
What to implement early to save you pain later on&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/peteowlett/lessons-from-6-months-of-using-luigi"&gt;https://speakerdeck.com/peteowlett/lessons-from-6-months-of-using-luigi&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Peter Owlett</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/peter-owlett-lessons-from-6-months-of-using-luigi-in-production.html</guid></item><item><title>Making Recommendations without Data</title><link>https://pyvideo.org/pydata-london-2016/ruby-childs-nick-sorros-making-recommendations-without-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;As Data Scientists, we often assume we have data! Its crazy not to. What should you recommend to a new user when you know nothing about them? In this talk we will discuss the challenges we faced, the assumption we took and the solutions we came up with while building a recommendations system for an interest based social network with limited data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ruby Childs</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/ruby-childs-nick-sorros-making-recommendations-without-data.html</guid></item><item><title>The CV: A Data Scientist's View</title><link>https://pyvideo.org/pydata-london-2016/rui-miguel-forte-the-cv-a-data-scientists-view.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This talk will focus on two main goals. The first of these is to present the CV, and a job seeker's career trajectory more generally, as a highly interesting and fertile application domain for data science. Secondly, the talk will highlight a number of practical lessons and tips gleaned though years of hands-on experience in this area.&lt;/p&gt;
&lt;p&gt;Despite the proliferation of professional networks such as LinkedIn, and the widespread use of online forms in which job seekers can fill in their details when applying for a job, the CV still remains relevant today. From online job boards to recruitment firms, personal websites to companies who remain old-fashioned in their hiring, the CV finds its way in every facet of job hunting. For the HR professional however, the CV is cumbersome to work with. For this reason, it has consumed countless man-hours of work usually spent on mundane tasks. For example, candidate details from CVs are often manually copied into forms to create a structured profile in a database.&lt;/p&gt;
&lt;p&gt;From a data scientist's perspective, the CV provides us with a wealth of interesting applications and opportunities for investigation. In this talk, the author will present learnings from over three years of working in this domain as well as a range of applications that can arise.&lt;/p&gt;
&lt;p&gt;Concretely, the talk will cover:&lt;/p&gt;
&lt;p&gt;Introducing the CV Parsing task and its components
Automatically extracting text from documents with Apache Tika
Understanding and mitigating text extraction errors
Analyzing CV text with an NLP pipeline
Using open source tools for core NLP tasks
High-level CV parsing with section identification and sentence classification
Identifying important entities with Named Entity Recognition (NER)
Understanding the limitations of existing NER systems
Building your own NER model
Reproducibility in experiments
Building a corpus of annotated data
Recognizing entity relationships
Entity normalization
Discovering and extracting skills from CV's
Clustering CV's by word vectors
Identifying duplicate profiles
Though each of these topics could be an entire talk on their own, the objective here is to present the main idea of each and the role that they play within the broader context of CV parsing and interpreting job seeker data. The talk will also feature code snippets in Python and Apache Spark to give a practical foundation for some of the concepts discussed.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/ruimiguelforte/the-cv-a-data-scientists-view"&gt;https://speakerdeck.com/ruimiguelforte/the-cv-a-data-scientists-view&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rui Miguel Forte</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/rui-miguel-forte-the-cv-a-data-scientists-view.html</guid></item><item><title>Iterables and Iterators: Going Loopy With Python</title><link>https://pyvideo.org/pydata-london-2016/steve-holden-iterables-and-iterators-going-loopy-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;This talk describes how the interpreter iterates over containers when any construct involving the for keyword is used. It explains both the &amp;quot;old&amp;quot; and the &amp;quot;new&amp;quot; iteration protocols, demonstrates the difference between iterables and iterators, suggests implementation techniques to allow you to define your own iterables and iterators and points out some advantages of generators.&lt;/p&gt;
&lt;p&gt;The talk's outline is given below. It will take approximately 25 minutes to deliver the material, the remainder being reserved for Q&amp;amp;A.&lt;/p&gt;
&lt;p&gt;Iteration History (4 minutes)&lt;/p&gt;
&lt;p&gt;Enter the Iterable (10 minutes) Recognizing Iterators Iterating over Iterators Why we Need Iterables&lt;/p&gt;
&lt;p&gt;Writing Your Own Iterators and Iterables (10 minutes) The Basic Iterator Pattern The Advantages of Generators The Basic Iterable Pattern Python Native Iterables&lt;/p&gt;
&lt;p&gt;Summary (1 minute)&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a class="reference external" href="https://github.com/steveholden/iteration"&gt;https://github.com/steveholden/iteration&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Steve Holden</dc:creator><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-08:pydata-london-2016/steve-holden-iterables-and-iterators-going-loopy-with-python.html</guid></item><item><title>Building a recommendation engine with Python and Neo4j</title><link>https://pyvideo.org/pydata-london-2016/mark-needham-building-a-recommendation-engine-with-python-and-neo4j.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2016&lt;/p&gt;
&lt;p&gt;In this session Mark will show how to build a meetup.com recommendation engine using Neo4j and Python.&lt;/p&gt;
&lt;p&gt;Our solution will be a hybrid which makes uses of both content based and collaborative filtering to come up with multi layered recommendations that take different datasets into account e.g. we'll combine data from the meetup.com and twitter APIs.&lt;/p&gt;
&lt;p&gt;We'll evolve the solution from scratch and look at the decisions we make along the way in terms of modelling and coming up with factors that might lead to better recommendations for the end user.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mark Needham</dc:creator><pubDate>Sat, 07 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-07:pydata-london-2016/mark-needham-building-a-recommendation-engine-with-python-and-neo4j.html</guid></item></channel></rss>