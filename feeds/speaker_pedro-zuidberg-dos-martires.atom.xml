<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Pedro Zuidberg Dos Martires</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_pedro-zuidberg-dos-martires.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-07-31T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Ordering Variables for Weighted Model Integration</title><link href="https://pyvideo.org/uai-2020/ordering-variables-for-weighted-model-integration.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Vincent Derkinderen</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/ordering-variables-for-weighted-model-integration.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Ordering Variables for Weighted Model Integration&lt;/p&gt;
&lt;p&gt;Vincent Derkinderen (KU Leuven)*; Evert Heylen (KU Leuven); Pedro Zuidberg Dos Martires (KU Leuven); Samuel Kolb (KU Leuven); Luc de Raedt (KU Leuven university)&lt;/p&gt;
&lt;p&gt;State-of-the-art probabilistic inference algorithms, such as variable elimination and search-based  approaches, rely heavily on  the order in which variables …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Ordering Variables for Weighted Model Integration&lt;/p&gt;
&lt;p&gt;Vincent Derkinderen (KU Leuven)*; Evert Heylen (KU Leuven); Pedro Zuidberg Dos Martires (KU Leuven); Samuel Kolb (KU Leuven); Luc de Raedt (KU Leuven university)&lt;/p&gt;
&lt;p&gt;State-of-the-art probabilistic inference algorithms, such as variable elimination and search-based  approaches, rely heavily on  the order in which variables are marginalized. Finding the optimal ordering is an NP-complete problem. This computational hardness has led to heuristics to find adequate variable orderings. However, these heuristics have mostly been targeting discrete random variables. We show how variable ordering heuristics from the discrete domain can be ported to the discrete-continuous domain. We equip the state-of-the-art F-XSDD(BR) solver for  discrete-continuous  problems  with  such heuristics. Additionally, we propose a novel heuristic called bottom-up min-fill (BU-MiF), yielding a solver capable of determining good variable orderings without having to rely on the user to provide such an ordering. We empirically demonstrate its performance on a set of benchmark problems.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry><entry><title>Neural Probabilistic Logic Programming in Discrete Continuous Domains</title><link href="https://pyvideo.org/uai-2023/neural-probabilistic-logic-programming-in-discrete-continuous-domains.html" rel="alternate"></link><published>2023-07-31T00:00:00+00:00</published><updated>2023-07-31T00:00:00+00:00</updated><author><name>Lennert De Smet</name></author><id>tag:pyvideo.org,2023-07-31:/uai-2023/neural-probabilistic-logic-programming-in-discrete-continuous-domains.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Neural Probabilistic Logic Programming in Discrete-Continuous Domains&amp;quot;   Lennert De Smet, Pedro Zuidberg Dos Martires, Robin Manhaeve, Giuseppe Marra, Angelika Kimmig, Luc De Raedt
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/de-smet23a.html"&gt;https://proceedings.mlr.press/v216/de-smet23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic background knowledge in the form of logic. It has …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Neural Probabilistic Logic Programming in Discrete-Continuous Domains&amp;quot;   Lennert De Smet, Pedro Zuidberg Dos Martires, Robin Manhaeve, Giuseppe Marra, Angelika Kimmig, Luc De Raedt
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/de-smet23a.html"&gt;https://proceedings.mlr.press/v216/de-smet23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic background knowledge in the form of logic. It has been shown to aid learning in the limited data regime and to facilitate inference on out-of-distribution data. Probabilistic NeSy focuses on integrating neural networks with both logic and probability theory, which additionally allows learning under uncertainty. A major limitation of current probabilistic NeSy systems, such as DeepProbLog, is their restriction to finite probability distributions, i.e., discrete random variables. In contrast, deep probabilistic programming (DPP) excels in modelling and optimising continuous probability distributions. Hence, we introduce DeepSeaProbLog, a neural probabilistic logic programming language that incorporates DPP techniques into NeSy. Doing so results in the support of inference and learning of both discrete and continuous probability distributions under logical constraints. Our main contributions are 1) the semantics of DeepSeaProbLog and its corresponding inference algorithm, 2) a proven asymptotically unbiased learning algorithm, and 3) a series of experiments that illustrate the versatility of our approach.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://www.auai.org/uai2023/oral_slides/233-oral-slides.pdf"&gt;https://www.auai.org/uai2023/oral_slides/233-oral-slides.pdf&lt;/a&gt;&lt;/p&gt;
</content><category term="UAI 2023"></category></entry></feed>