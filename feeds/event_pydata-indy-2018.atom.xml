<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-indy-2018.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-10-12T00:00:00+00:00</updated><entry><title>Brief Intro to Natural Language Processing (NLP)</title><link href="https://pyvideo.org/pydata-indy-2018/brief-intro-to-natural-language-processing-nlp.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>Andrew (AJ) Rader</name></author><id>tag:pyvideo.org,2018-10-12:pydata-indy-2018/brief-intro-to-natural-language-processing-nlp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Natural Language Processing (NLP) is a broad domain that deals with analyzing and understanding human text and words. Some typical areas of application for NLP involve text classification, speech recognition, machine translation, chatbots, and caption generation. Fundamentally, NLP involves converting words into numbers and doing math on these numbers in order to identify relationships between the words and documents they live in. The goal of this talk is to present the basic theory of what NLP is and demonstrate how to utilize machine learning approaches in Python to extract insights from text. An example text classification problem is presented; illustrating the steps required to ingest, preprocess, build and test a model for an example text corpus.&lt;/p&gt;
</summary><category term="nlp"></category></entry><entry><title>Building IoT Data Pipelines with Python</title><link href="https://pyvideo.org/pydata-indy-2018/building-iot-data-pipelines-with-python.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>Logan Wendholt</name></author><id>tag:pyvideo.org,2018-10-12:pydata-indy-2018/building-iot-data-pipelines-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;So you've learned about the data analytics capabilities of Python, and now you're ready to start churning through data -- great! But do you know how to turn your snippet of code into a system capable of taking in streams of raw sensor data and spitting out insights? This presentation will lay out the basic components of a Python-based data pipeline built for Internet-of-Things (IoT) applications, and will highlight some of the common challenges associated with putting together an efficient data analytics and storage system.&lt;/p&gt;
&lt;p&gt;Key topics include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;An overview of cloud-based &amp;quot;serverless&amp;quot; data pipelines;&lt;/li&gt;
&lt;li&gt;Pros and cons of locally-hosted or &amp;quot;edge computing&amp;quot; systems;&lt;/li&gt;
&lt;li&gt;Tradeoffs between cost, scalability, complexity, and development time for different architectures&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By the end of this presentation, you will have gained a broad overview of the ecosystem needed to support a Python analytics solution: how to get data in and out, how to write and deploy scalable code, and how to manage system cost and complexity.&lt;/p&gt;
</summary></entry><entry><title>Data Visualization with Bokeh</title><link href="https://pyvideo.org/pydata-indy-2018/data-visualization-with-bokeh.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>James Alexander</name></author><id>tag:pyvideo.org,2018-10-12:pydata-indy-2018/data-visualization-with-bokeh.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Learn how to create interactive charts and graphs without writing any JavaScript. We'll use Python to generate simple interactive graphs and plots within Jupyter notebooks, and embedded in a running Django site. I'll show examples of streaming data to a Bokeh instance, and interactively intuit about a large dataset using Datashader.&lt;/p&gt;
</summary><category term="Bokeh"></category></entry><entry><title>Keynote by Calvin Hendryx-Parker</title><link href="https://pyvideo.org/pydata-indy-2018/keynote-by-calvin-hendryx-parker.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>Calvin Hendryx-Parker</name></author><id>tag:pyvideo.org,2018-10-12:pydata-indy-2018/keynote-by-calvin-hendryx-parker.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Calvin Hendryx-Parker opens the PyData Indy 2018 conference with a keynote explaining the different presentations of the day.&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>Policy on a Page: Operational Workflow for Ad-Hoc Analyses</title><link href="https://pyvideo.org/pydata-indy-2018/policy-on-a-page-operational-workflow-for-ad-hoc-analyses.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>Aaron Burgess</name></author><id>tag:pyvideo.org,2018-10-12:pydata-indy-2018/policy-on-a-page-operational-workflow-for-ad-hoc-analyses.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The majority of requests to FSSA Data &amp;amp; Analytics are ad-hoc analyses. Ad-hoc analyses had suffered from two major issues. One was an inferred belief that stakeholders wanted data points when they really wanted a statement of fact that could be cited or applied. The other issue was an over-simplified workflow for ad-hoc requests that featured no version control and &amp;quot;wild west&amp;quot; peer review.&lt;/p&gt;
&lt;p&gt;The solution to managing the workflow for ad-hoc analyses was to immediately implement Git and (due to existing licenses) BitBucket policies and procedures. This included a standard ad-hoc repo template, repeated training on best practices such as immediately opening a pull request upon branch creation and peer review responsibilities. JIRA was already in place for task management. In addition, Bamboo was utilized to introduce the concept of continuous integration where ad-hoc requests should be ran automatically on every data refresh with change detection scripts as an early warning system. Finally, making use of Jupyter Notebooks and the respective extensions to deliver well-groomed html exports of deliverables became a standard practice. These deliverables focused on clearly defining an objective, methodology, results, and a &amp;quot;statement of fact&amp;quot; for use by stakeholders.&lt;/p&gt;
&lt;p&gt;Implemented changes have resulted in clearer expectations for Data &amp;amp; Analytics team members. The standard Jupyter Notebook html extracts have been well-received by stakeholders and greatly reduced the level of &amp;quot;data heavy; information light&amp;quot; deliverables. The resulting trust from stakeholders has increased our request load and opened up opportunities to work on more complex modeling.&lt;/p&gt;
</summary></entry><entry><title>Simplifying OAuth2.0 Authentication</title><link href="https://pyvideo.org/pydata-indy-2018/simplifying-oauth20-authentication.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>Mark Sikora</name></author><id>tag:pyvideo.org,2018-10-12:pydata-indy-2018/simplifying-oauth20-authentication.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;OAuth2.0, has been a favorite option to authenticate a system-to-system interaction over classic System ID/password combination via authentication servers. As popular as using an ID/password combination might be, the old authentication process poses a number of limitations: stateful cookie-caching of the authentication, incompatibility with mobile clients, highly coupled app/auth server architecture and incongruence with third-party identity providers. OAuth2.0 addresses these issues but, still have a misname for being complex and cumbersome to implement.&lt;/p&gt;
&lt;p&gt;This presentation will demonstrate a simple mean of implementing a system-to-system OAuth2.0 authentication via Python and its native libraries. You will be able to see a call that attains token from the Identity Provider, Okta, and authenticates itself to a Flask app - all made possible with minimal, straightforward coding. In addition to the live coding demo, the presenters will walk through how this can be used in an open-sourced Big Data Ecosystem.&lt;/p&gt;
</summary><category term="OAuth2"></category></entry><entry><title>Sparkflow: Utilizing Pyspark for Training Tensorflow Models on Large Datasets</title><link href="https://pyvideo.org/pydata-indy-2018/sparkflow-utilizing-pyspark-for-training-tensorflow-models-on-large-datasets.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>Derek Miller</name></author><id>tag:pyvideo.org,2018-10-12:pydata-indy-2018/sparkflow-utilizing-pyspark-for-training-tensorflow-models-on-large-datasets.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As more public, large datasets are becoming available, distributed data processing tools such as Apache Spark are vital for data scientists. While SparkML provides many machine learning algorithms, standard pipelines, and a basic linear algebra library, it does not support training deep learning models. Due to the rise of Tensorflow in the last two years, Lifeomic built the Sparkflow library to combine the power of the Pipeline api from Spark with training Deep Learning models in Tensorflow. Sparkflow uses the Hogwild algorithm to train deep learning models in a distributed manor, which underneath leverages the driver/executor architecture in Spark to manage copied networks and gradients. In this session, we describe some of the lessons learned in building Sparkflow, the pros and cons of asynchronous distributed deep learning, how to use Spark Pipelines with Tensorflow with very few lines of code, and where we are headed with the library in the near future.&lt;/p&gt;
</summary><category term="Pyspark"></category><category term="Tensorflow"></category></entry><entry><title>SQL Server 2017 Support for Machine Learning in Python</title><link href="https://pyvideo.org/pydata-indy-2018/sql-server-2017-support-for-machine-learning-in-python.html" rel="alternate"></link><published>2018-10-12T00:00:00+00:00</published><updated>2018-10-12T00:00:00+00:00</updated><author><name>Cathy Wyss</name></author><id>tag:pyvideo.org,2018-10-12:pydata-indy-2018/sql-server-2017-support-for-machine-learning-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;SQL Server is Microsoft’s flagship relational database product. A database is the best platform for storing data – even big data – and Structured Query Language (SQL) is unparalleled in terms of supporting data access and manipulation. Python is one of the de facto languages of choice for machine learning today, having many libraries facilitating all aspects, from data frames (Pandas) to neural nets (Keras or Tensorflow). Machine learning is crucial for deduction and prediction from vast datasets and underpins everything from web search to self-driving cars. So far, relational databases and machine learning have been at arm’s length, even though both are intimately tied to data. This division is no more. SQL Server 2017 is the first version to support Python natively, so database scripts can contain Python and SQL side-by-side. This talk gives examples and methods of how this exciting merging of technology can apply to real-world data.&lt;/p&gt;
</summary><category term="sql server"></category></entry></feed>