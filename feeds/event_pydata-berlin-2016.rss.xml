<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Tue, 07 Jun 2016 00:00:00 +0000</lastBuildDate><item><title>Setting up predictive analytics services with Palladium</title><link>https://pyvideo.org/pydata-berlin-2016/setting-up-predictive-analytics-services-with-palladium.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;We will introduce Palladium, an open source framework for setting up predictive analytics services. It supports tasks like fitting, evaluating, storing, and distributing (predictive) models. Core ML processes are compatible with scikit-learn and a large number of scikit-learn’s features can be used. Besides the use of Palladium we will also show how to use it with Docker and Mesos / Marathon.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In this talk, we will introduce Palladium, an open source framework for easily setting up predictive analytics services (&lt;a class="reference external" href="https://github.com/ottogroup/palladium"&gt;https://github.com/ottogroup/palladium&lt;/a&gt;). It supports tasks like fitting, evaluating, storing, distributing, and updating (predictive) models. Core machine learning processes are compatible with the open source machine learning library scikit-learn and thus, a large number of scikit-learn’s features can be used with Palladium. Although being implemented in Python, Palladium provides support for other languages and is shipped with examples how to integrate and expose R and Julia models. For an efficient deployment of services based on Palladium, a script to create Docker images automatically is provided. This talk will cover the use of Palladium including an example where a simple classification service is set up. We will also show how Docker and Mesos / Marathon can be used to deploy and scale Palladium-based services. Having basic knowledge about Machine Learning and/or scikit-learn would be an advantage when attending this talk.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andreas Lattner</dc:creator><pubDate>Tue, 07 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-07:pydata-berlin-2016/setting-up-predictive-analytics-services-with-palladium.html</guid><category>palladium</category><category>scikit-learn</category><category>docker</category><category>mesos</category><category>marathon</category><category>machine learning</category></item><item><title>ExpAn - A Python library for advanced statistical analysis of A/B tests</title><link>https://pyvideo.org/pydata-berlin-2016/expan-a-python-library-for-advanced-statistical-analysis-of-ab-tests.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;A/B tests have been adopted by various companies in different industries to drive the data-driven decision making process. Therefore, a statistically solid analytic framework is of common interest to a large community. We'll introduce the ExpAn library developed for the statistical evaluation of A/B tests, it has a generic data structure and all functions are standalone.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;A/B tests, or randomized controlled experiments, have been widely applied in different industries to optimize the business process and the user experience. Here we'll introduce a Python library, ExpAn, intended for the statistical analysis of A/B tests.&lt;/p&gt;
&lt;p&gt;The input data to ExpAn has a standard format, which is defined to interface with different data sources. The main statistical functions in ExpAn are all standalone and work with either the library-specific input data structure or some Python built-in data types. Among others, the functions can be used to assess whether the randomization is appropriate, and measure the expectation and error margin of the uplift due to the treatment. We also implemented a robust discretization algorithm to handle typical heavy-tailed distributions in the real world. Finally, a generic result structure is designed to incorporate results from different types of analyses.&lt;/p&gt;
&lt;p&gt;One can easily feed data from other domain-specific data fetching modules into ExpAn. Other advanced algorithms for the analysis of A/B test data can be implemented and plugged into ExpAn, eg. a Bayesian hypothesis testing scheme instead of the frequentist approach. The generality of the result structure also makes it handy to apply different kinds of visualization on top of the data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jie Bao</dc:creator><pubDate>Mon, 06 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-06:pydata-berlin-2016/expan-a-python-library-for-advanced-statistical-analysis-of-ab-tests.html</guid><category>testing</category><category>expan</category></item><item><title>Removing Soft Shadows with Hard Data</title><link>https://pyvideo.org/pydata-berlin-2016/removing-soft-shadows-with-hard-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;Find out how we used recent advances in photorealistic rendering and machine learning to teach a Random Forest how soft shadows look like. See how this model can then be used to to remove and modify shadows in RGB photographs with minimal user input.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Manipulated images lose believability if the user's edits fail to account for shadows. We propose a method that makes removal and editing of soft shadows easy. Soft shadows are ubiquitous, but remain notoriously difficult to extract and manipulate. We posit that soft shadows can be segmented, and therefore edited, by learning a mapping function for image patches that generates shadow mattes. We validate this premise by removing soft shadows from photographs with only a small amount of user input.&lt;/p&gt;
&lt;p&gt;Given only broad user brush strokes that indicate the region to be processed, our new supervised regression algorithm automatically unshadows an image, removing the umbra and penumbra. The resulting lit image is frequently perceived as a believable shadow-free version of the scene. We tested the approach on a large set of soft shadow images, and performed a user study that compared our method to the state of the art and to real lit scenes. Our results are more difficult to identify as being altered, and are perceived as preferable compared to prior work.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maciej Gryka</dc:creator><pubDate>Mon, 06 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-06:pydata-berlin-2016/removing-soft-shadows-with-hard-data.html</guid></item><item><title>A full Machine learning pipeline in Scikit-learn vs in scala-Spark: pros and cons</title><link>https://pyvideo.org/pydata-berlin-2016/a-full-machine-learning-pipeline-in-scikit-learn-vs-in-scala-spark-pros-and-cons.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;The machine learning libraries in Apache Spark are an impressive piece of software engineering, and are maturing rapidly. What advantages does Spark.ml offer over scikit-learn? At Data Science Retreat we've taken a real-world dataset and worked through the stages of building a predictive model -- exploration, data cleaning, feature engineering, and model fitting; which would you use in production?&lt;/p&gt;
&lt;p&gt;The machine learning libraries in Apache Spark are an impressive piece of software engineering, and are maturing rapidly. What advantages does Spark.ml offer over scikit-learn?&lt;/p&gt;
&lt;p&gt;At Data Science Retreat we've taken a real-world dataset and worked through the stages of building a predictive model -- exploration, data cleaning, feature engineering, and model fitting -- in several different frameworks. We'll show what it's like to work with native Spark.ml, and compare it to scikit-learn along several dimensions: ease of use, productivity, feature set, and performance.&lt;/p&gt;
&lt;p&gt;In some ways Spark.ml is still rather immature, but it also conveys new superpowers to those who know how to use it.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jose Quesada</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/a-full-machine-learning-pipeline-in-scikit-learn-vs-in-scala-spark-pros-and-cons.html</guid><category>apache spark</category><category>spark.ml</category><category>scikit-learn</category></item><item><title>Bayesian Optimization and it's application to Neural Networks</title><link>https://pyvideo.org/pydata-berlin-2016/bayesian-optimization-and-its-application-to-neural-networks.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;This talk will be about the fundamentals of Bayesian Optimization and how it can be used to train ML Algorithms in Python. To this end we'll consider it's application to Neural Networks. The NNs will be implemented in keras, the Bayesian Optimization will be optimized with hyperas/hyperopt.&lt;/p&gt;
&lt;p&gt;Have you ever failed to train a Neural Network? Spent hours, to get it even learn anything? If you ask an expert on how she does this, the answer might be something like: &amp;quot;It needs a lot of experience and some luck&amp;quot;. If you know this problem, then this talk is for you. Also, let's steer our luck with ML!&lt;/p&gt;
&lt;p&gt;When tuning hyperparameters, an expert has built a model, that means some expectations on how the output might change on a certain parameter adaption. For example, what happens to your Convolutional Neural Network if you set the dropout from 0.5 to 0.25.&lt;/p&gt;
&lt;p&gt;Bayesian Optimization is a method that is able to build exactly this kind of model. It uses for example Gaussian Processes to take decisions on which parameter-change might bring you the most benefit, and if it does not, the model is adapted accodingly.&lt;/p&gt;
&lt;p&gt;This talk will be about the fundamentals of Bayesian Optimization and how it can be used to train ML Algorithms in Python.&lt;/p&gt;
&lt;p&gt;To this end we'll consider it's application to Neural Networks. The NNs will be implemented in keras, the Bayesian Optimization will be optimized with hyperas/hyperopt.&lt;/p&gt;
&lt;p&gt;I am planning to split this talk 50:50 into theory and practice.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Moritz Neeb</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/bayesian-optimization-and-its-application-to-neural-networks.html</guid></item><item><title>Brand recognition in real-life photos</title><link>https://pyvideo.org/pydata-berlin-2016/brand-recognition-in-real-life-photos.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Talk will be about brand recognition system that detects logos in real-life photos. It allows for better brand monitoring in social media independently from text descriptions and hashtags. Software was implemented using state-of-the-art Python deep learning tools (Theano, Lasagne, Caffe). Project was created during a 3-months full-time data science program called Data Science Retreat.&lt;/p&gt;
&lt;p&gt;Project was created during a 3-months full-time data science program called Data Science Retreat. I’ve developed a brand recognition system that detects logos in real-life photos. It uses Deep Learning and SVM on Instagram images uploaded by users. It allows for better brand monitoring in social media independently from text descriptions and hashtags. I’ve used state-of-the-art Deep Learning tools (Theano, Lasagne, Caffe).&lt;/p&gt;
&lt;p&gt;Deep Learning is an advanced Machine Learning technic that usually needs lot of data and powerful machines. I would like to show how to use it when you don't have huge dataset and your resources are limited in time and money.&lt;/p&gt;
&lt;p&gt;I would also like to talk about a new era in which developers can start to write code which understands visual context of images and is not limited to descriptions and tags.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://de.slideshare.net/ukaszCzarnecki/brand-recognition-in-reallife-photos-using-deep-learning-lukasz-czarnecki-pydata-berlin-2016/"&gt;http://de.slideshare.net/ukaszCzarnecki/brand-recognition-in-reallife-photos-using-deep-learning-lukasz-czarnecki-pydata-berlin-2016/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lukasz Czarnecki</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/brand-recognition-in-real-life-photos.html</guid></item><item><title>Building a polyglot Data Science Platform on Big Data systems</title><link>https://pyvideo.org/pydata-berlin-2016/building-a-polyglot-data-science-platform-on-big-data-systems.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Slides: &lt;a class="reference external" href="https://speakerdeck.com/fkaufer/polyglot-data-science-platforms-on-big-data-systems"&gt;https://speakerdeck.com/fkaufer/polyglot-data-science-platforms-on-big-data-systems&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Frank Kaufer</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/building-a-polyglot-data-science-platform-on-big-data-systems.html</guid></item><item><title>Connecting Keywords to Knowledge Base Using Search Keywords and Wikidata</title><link>https://pyvideo.org/pydata-berlin-2016/connecting-keywords-to-knowledge-base-using-search-keywords-and-wikidata.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;The development of large-scale Knowledge Base (KB) has drawn lots of attentions and efforts from both academy and industries recently . In this talk I will introduce how to use keywords and public available data to build our structural KB, and build knowledge retrieval system for different languages using python.&lt;/p&gt;
&lt;p&gt;Many large-scale Knowledge Bases (KB), such as Yago, Wikidata, Freebase, and Google’s Knowledge Graph, have been build by extracting facts fro structural Wikipedia and/or natural language Web documents.&lt;/p&gt;
&lt;p&gt;The main observation of using knowledge base is that not all facts are useful and have enough information. To tackle this problem I will introduce how we build various data sources to help facts and keywords selection. We will also discuss important questions of KB applications including,&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Architecture of a KB processing and extraction system using Wikipedia and two public available KB including Wikidata and Yago.&lt;/li&gt;
&lt;li&gt;Method for calculating contextual relevance between facts.&lt;/li&gt;
&lt;li&gt;How to present different facts to users.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Resources:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Yago: &lt;a class="reference external" href="https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/"&gt;https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wikidata: &lt;a class="reference external" href="https://www.wikidata.org/wiki/Wikidata:Main_Page"&gt;https://www.wikidata.org/wiki/Wikidata:Main_Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Freebase: &lt;a class="reference external" href="https://developers.google.com/freebase/"&gt;https://developers.google.com/freebase/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Google’s Knowledge Graph: &lt;a class="reference external" href="https://developers.google.com/knowledge-graph/"&gt;https://developers.google.com/knowledge-graph/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fang Xu</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/connecting-keywords-to-knowledge-base-using-search-keywords-and-wikidata.html</guid></item><item><title>Estimating stock price correlations using Wikipedia</title><link>https://pyvideo.org/pydata-berlin-2016/estimating-stock-price-correlations-using-wikipedia.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Building an equities portfolio is a challenging task for a finance professional as it requires, among others, future correlations between stock prices. As this data is not always available, in this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures based on Wikipedia data.&lt;/p&gt;
&lt;p&gt;According to Modern Portfolio Theory, assembling a portfolio involves forming expectations about the individual stock's future risk and return as well as future correlations between stock prices. These future correlations are typically estimated using historical stock price data. However, there are situations where this type of data is not available, such as the time preceding an IPO.&lt;/p&gt;
&lt;p&gt;In this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures in order to estimate the correlation between stock prices.&lt;/p&gt;
&lt;p&gt;The focus of the analysis will be on companies listed on the Frankfurt Stock Exchange which form the DAX. I am going to use Wikipedia articles in order to derive the textual description for each company. Additionally, I will use the Wikipedia category structure to derive a graph describing relations between companies.&lt;/p&gt;
&lt;p&gt;The analysis will be performed using the scikit-learn and networkX libraries and example code will be available to the audience.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/deliarusu/wikipedia-correlation"&gt;https://github.com/deliarusu/wikipedia-correlation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://speakerdeck.com/deliarusu/estimating-stock-price-correlations-using-wikipedia"&gt;https://speakerdeck.com/deliarusu/estimating-stock-price-correlations-using-wikipedia&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Delia Rusu</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/estimating-stock-price-correlations-using-wikipedia.html</guid></item><item><title>Plumbing in Python: Pipelines for Data Science Applications</title><link>https://pyvideo.org/pydata-berlin-2016/plumbing-in-python-pipelines-for-data-science-applications.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Bringing data science models from development to production can be a daunting task. To reduce the overhead in this process and to improve flexibility, we introduced a Python data flow library at Blue Yonder which we will present in this talk.&lt;/p&gt;
&lt;p&gt;The data flow library presented in this talk provides a thin abstraction layer between data pipeline declarations and specific execution backends. As exceptions are the rule, the library allows the user to introduce limited control flow into pipelines. At the same time, it also offers composability of pipelines, as many of our projects share similar building blocks.&lt;/p&gt;
&lt;p&gt;In this talk we will show how using this library leads to a more functional style of programming, which improved the speed of our iterations. This shift in development style, already in the early stages of model development, includes clear separation of I/O operations and data transformations as well as the separation of data flow control and actual computations. We will also look into some additional benefits of this paradigm change, namely concurrency and testability.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Reineking</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/plumbing-in-python-pipelines-for-data-science-applications.html</guid></item><item><title>Predicting political views from text</title><link>https://pyvideo.org/pydata-berlin-2016/predicting-political-views-from-text.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;An unbiased view on media reports requires understanding the political bias of a text. This talk shows how basic tools from machine learning and natural language processing combined with publicly available data can be turned into assistive technology for automatically determining the political bias of a text. Some common pitfalls and example applications will be discussed.&lt;/p&gt;
&lt;p&gt;Every day media generate large amounts of text. Getting an unbiased view on what media report on requires an unbiased sample of media content. In many cases it is obvious which political bias an author has. In other cases some expertise is required to judge the political bias of a text. Assistive technology for estimating the political bias of texts can be helpful in this context, especially for scaling things up. We investigated to what extent political party affiliation can be predicted from textual content with basic machine learning tools. We used the text of speeches and discussions in the German parliament as well as texts from party manifestos to train classifiers that predict political party affiliation or political views based on standard text features. Results indicate that automatic classification of political affiliation and political views is possible with well above chance accuracy. We hope that this work will eventually be helpful for unbiased political education in the presence of massive amounts of media content. We show some web applications of how the models can be used in combination with classical topic models to analyse texts for which the party affiliation is not clear, such as news articles.&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a class="reference external" href="https://github.com/felixbiessmann/"&gt;https://github.com/felixbiessmann/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Felix Biessmann</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/predicting-political-views-from-text.html</guid></item><item><title>PySpark in Practice</title><link>https://pyvideo.org/pydata-berlin-2016/pyspark-in-practice.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;In this talk we will share our best practices of using PySpark in numerous customer facing data science engagements. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Unit testing with PySpark&lt;/li&gt;
&lt;li&gt;Integration with SQL on Hadoop engines&lt;/li&gt;
&lt;li&gt;Data pipeline management and workflows&lt;/li&gt;
&lt;li&gt;Data Structures (RDDs vs. Data Frames vs. Data Sets)&lt;/li&gt;
&lt;li&gt;When to use MLlib vs scikit-learn&lt;/li&gt;
&lt;li&gt;Operationalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At Pivotal Labs we have many data science engagements on big data. Typical problems involve real-time data from sensors collected by telecom operators to GPS data produced by vehicle tracking systems. One widespread framework to solve those inherently difficult problems is Apache Spark. In this talk, we want to share our best practices with PySpark, Spark’s Python API, highlighting our experience as well as dos and dont's. In particular, we will focus on the whole data science pipeline from data ingestion, data munging and wrangling to the actual model building.&lt;/p&gt;
&lt;p&gt;Finally, many businesses have started to realise that there is no return on investment from data science if the models do not go into production. At Pivotal Labs, one our core principle is API first. Therefore, we will also talk how we put our models into production, sharing our hands-on knowledge in this field and also how this fits into test-driven development.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ronert Obst</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/pyspark-in-practice.html</guid><category>pyspark</category></item><item><title>Python based predictive analytics with GraphLab Create</title><link>https://pyvideo.org/pydata-berlin-2016/python-based-predictive-analytics-with-graphlab-create.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;One of the most exciting areas in data science is the development of new predictive applications; apps used to drive product recommendations, predict machine failures, forecast airfare etc. These applications output real-time predictions and recommendations in response to user and machine input to directly derive business value and create cool experience.&lt;/p&gt;
&lt;p&gt;The most interesting apps utilize multiple types of data (tables, graphs, text &amp;amp; images) in a creative way. In this talk, we will show how to quickly build and deploy a predictive app that exploits the power of combining different data types together using GraphLab Create, our open source based Python software.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Danny Bickson</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/python-based-predictive-analytics-with-graphlab-create.html</guid></item><item><title>Smart Banking - Real Time Driven</title><link>https://pyvideo.org/pydata-berlin-2016/smart-banking-real-time-driven.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Harnessing the power of machine intelligence to empower users with advanced and smarter banking solutions.&lt;/p&gt;
&lt;p&gt;Banking has been the one of the most ancient professions for humans. Yet, when it comes to advanced or smart banking solutions we haven’t experienced a radical shift even in the 21st century.&lt;/p&gt;
&lt;p&gt;We, at Number26, aim to enrich that experience and provide an advanced, mobile banking platform for our customers. Each of our features, revolves around users - their interests and personal preferences.
We achieve such flexibility by the use of state-of-the-art machine learning and statistical learning methodologies.&lt;/p&gt;
&lt;p&gt;In particular, we would like to present a deeper insight into two major features we use in our product:&lt;/p&gt;
&lt;ol class="loweralpha simple"&gt;
&lt;li&gt;automatic real time categorisation of transactions: how we combine the power of the cloud computing service (AWS), with the intelligence of machine learning based models to achieve this&lt;/li&gt;
&lt;li&gt;predictive modelling of users future financial status: how artificial neural networks do the smart thinking for our users to come up his predicted personal finances.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The tech stack ranges from Python (Keras, scikit-learn and more), Dockers, AWS lambda, EBS, NLP (stanford nltk) and machine learning classifiers and more.&lt;/p&gt;
&lt;p&gt;In this presentation, we would like to make the message clear, how machine learning can make even the age old traditional ways of banking a fun.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Arnab Dutta</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/smart-banking-real-time-driven.html</guid></item><item><title>Spotting trends and tailoring recommendations: PySpark on Big Data in fashion</title><link>https://pyvideo.org/pydata-berlin-2016/spotting-trends-and-tailoring-recommendations-pyspark-on-big-data-in-fashion.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Predicting what people like when they choose what to wear is a non-trivial task involving several ingredients. At Mallzee, the data is variegated, large and has to be processed quickly to produce recommendations on products, tailored to each user. We use PySpark to crunch large sets of different data and create models in order to generate robust and meaningful suggestions.&lt;/p&gt;
&lt;p&gt;Mallzee is a fashion app where people can see products (clothes, shoes and accessories) and decide whether they like them or not. They can also buy products and create feeds of preferred brands and categories of items. We have large amounts of data generated by the users when they scroll and search through products and we use it to understand the user. We want to give everyone meaningful recommendations on the items they might like, hence tailoring the experience to who they are. We build pseudo-intelligent algorithms capable of extracting the style profile of a user and we crunch products data to match items to the user based on a classification model. The model is validated and statistical analyses are performed to determine the tipping point when recommendations are valuable to the user. The talk will go through the steps we implement by showing how the full data stack of Python is used in achieving this goal and how it is interfaced with a Spark cluster through PySpark in order to run Machine Learning algorithms on big data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martina Pugliese</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/spotting-trends-and-tailoring-recommendations-pyspark-on-big-data-in-fashion.html</guid><category>pyspark</category></item><item><title>Visualizing FragDenStaat.de</title><link>https://pyvideo.org/pydata-berlin-2016/visualizing-fragdenstaatde.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;FragDenStaat.de is a website that helps people to make freedom of information requests in Germany. Starting in 2011, they collected over 12000 of these requests. Using pandas and matplotlib, I tried to bring life into this interesting data set.&lt;/p&gt;
&lt;p&gt;Motivation&lt;/p&gt;
&lt;p&gt;FragDenStaat.de offers an API with which you can scrape more than 12000 public freedom of information requests. Each request contains information about: when it was sent, to which public body and to which jurisdiction it was sent, how long it took the public body to complete the request, whether the request was successful and if not, why so. Visualizing all these aspects of the data set might lead to new insights about this transparency process.&lt;/p&gt;
&lt;p&gt;Data preparation&lt;/p&gt;
&lt;p&gt;How I scraped the data set and organized it using pandas.&lt;/p&gt;
&lt;p&gt;Data visualization&lt;/p&gt;
&lt;p&gt;How I visualized the data set using pandas and matplotlib.&lt;/p&gt;
&lt;p&gt;Visualizations&lt;/p&gt;
&lt;p&gt;The results of my work so far.&lt;/p&gt;
&lt;p&gt;Materials available here: &lt;a class="reference external" href="https://github.com/awakenting/fds-statistics"&gt;https://github.com/awakenting/fds-statistics&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrej Warkentin</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/visualizing-fragdenstaatde.html</guid></item><item><title>Visualizing research data: Challenges of combining different datasources</title><link>https://pyvideo.org/pydata-berlin-2016/visualizing-research-data-challenges-of-combining-different-datasources.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Your source data has multiple formats? You have multiple API’s to pull data from? This talk will go through some common problems with solutions that you will face when trying to combine multiple different research data sources in programmatic way. We go through a real world web project on that visualizes poverty data with JSON API's, Shapefiles and Excel spreadsheets as data sources.&lt;/p&gt;
&lt;p&gt;Introduction&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Give talk goals: This talk aims to give the tools to solve the engineering challenges related to combining different datasources&lt;/li&gt;
&lt;li&gt;Set the context: We use geodata from humanitarian projects as an example, but solutions will apply to other areas as well.&lt;/li&gt;
&lt;li&gt;Go through talk outline&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Part 2: Quickly introduce the project&lt;/p&gt;
&lt;p&gt;Give the audience idea of real world project in preparation for the part 3&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Show screenshots of the final project&lt;/li&gt;
&lt;li&gt;Go through the used technologies (ESRI shapefiles, geo/topo json, xls, API’s, python libraries)&lt;/li&gt;
&lt;li&gt;Introduce the data pipeline&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Part 3: Explain common problems and our solutions for them&lt;/p&gt;
&lt;p&gt;This is the meat of the talk, each point introduces problem and suggests at least one solution. Solutions are based on Python technologies&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Handling different data formats&lt;/li&gt;
&lt;li&gt;How to manage the data sources (validation, automation, etc)&lt;/li&gt;
&lt;li&gt;Normalizing units&lt;/li&gt;
&lt;li&gt;Mapping problems (different projects may follow different standards for the id’s)&lt;/li&gt;
&lt;li&gt;Normalizing data and metadata&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Part 4: Wrap up&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Quickly explain how we applied these problems in the project&lt;/li&gt;
&lt;li&gt;Sum up the things you should consider (check-list)&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juha Suomalainen</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/visualizing-research-data-challenges-of-combining-different-datasources.html</guid></item><item><title>Zero-Administration Data Pipelines using AWS Simple Workflow</title><link>https://pyvideo.org/pydata-berlin-2016/zero-administration-data-pipelines-using-aws-simple-workflow.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Floto is an open source tool to programmatically author, schedule and run scalable data pipelines using AWS Simple Workflow - without the need to maintain a master server or queue or the state of workers.&lt;/p&gt;
&lt;p&gt;There are quite a few great tools for building effective and robust distributed data processing pipelines, especially Luigi from Spotify and Airflow from AirBnB.&lt;/p&gt;
&lt;p&gt;For scaling out, they all require a queue or master server, though. And those need maintenance.&lt;/p&gt;
&lt;p&gt;We wrote floto (&lt;a class="reference external" href="https://github.com/babbel/floto"&gt;https://github.com/babbel/floto&lt;/a&gt;), an open source tool to programmatically author, schedule and run scalable data pipelines on AWS - without the maintenance overhead.&lt;/p&gt;
&lt;p&gt;It uses AWS Simple Workflow, but I'll talk most about some general topics regarding data workflow orchestration:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;separation of concerns&lt;/li&gt;
&lt;li&gt;managing complexity through dependency reduction&lt;/li&gt;
&lt;li&gt;idempotent (or re-runnable) jobs&lt;/li&gt;
&lt;li&gt;transactional jobs (either completely fail, or completely succeed)&lt;/li&gt;
&lt;li&gt;failures and reruns&lt;/li&gt;
&lt;li&gt;evolving changes&lt;/li&gt;
&lt;li&gt;organizational scaling&lt;/li&gt;
&lt;li&gt;heterogenous systems&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anne Matthies</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/zero-administration-data-pipelines-using-aws-simple-workflow.html</guid><category>Floto</category></item><item><title>Accelerating Python Analytics by In-Database Processing</title><link>https://pyvideo.org/pydata-berlin-2016/accelerating-python-analytics-by-in-database-processing.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Python Analytics can be accelerated using SQL- Pushdowns to benefit from in-database performance-enhancing features such as column-stores and parallel processing. We will show the benefits of this approach and present ibmdbpy, a prototype which provides a Python interface for data manipulation and in-database algorithms in IBM DB2 and IBM dashDB on Bluemix .&lt;/p&gt;
&lt;p&gt;The Python ecosystem is very rich and provides intuitive tools for data analysis. However, most Python libraries require the data to be extracted from the database to working memory and resources are limited by computational power and memory. Analyzing a large amount of data is often impractical or even impossible. Ibmdbpy is an open-source Python package, developed by IBM, which provides a Python interface for data manipulation and machine learning algorithms such as Kmeans or Linear Regression to make working with databases more efficient by seamlessly pushing operations into the underlying database for execution. This does not only lift the memory limit of Python, but also allows users to profit from performance-enhancing features of the database management system. Ibmdbpy is designed for IBM DB2 and IBM dashDB, a database system available on IBM BlueMix, the IBM cloud application development and analytics platform. Via remote connection, user operations can benefit from in-database specific features, such as columnar technology and parallel processing, without having to interact with the database explicitly. Some in-database functions additionally use lazy loading to load only parts of the data that are actually required to further increase efficiency. Keeping the data in the database also avoids security issues that are associated with extracting data and ensures that the data that is being analyzed is as current as possible. Ibmdbpy can be used by Python developers with very little additional knowledge, since it imitates the well-known interface of Pandas library for data manipulation and Scikit-learn library for machine learning algorithms. Ibmdbpy provides a great runtime advantage for operations on medium to large dataset, i.e. on tables that have 1 million rows or more. Providing a Python interface for databases allows to bridge the gap between data warehousing platform and end-user environment, so that developers can benefit both from the expressivity of Python and from the speed-up provided by SQL execution in the database, which can be run on a cluster. In this talk, we will show the advantages of such approach for scaling Python analytics and do a short demo of data analysis with ibmdbpy.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://ibmdbanalytics.github.io/pydata-berlin-2016-ibmdbpy.slides.html"&gt;https://ibmdbanalytics.github.io/pydata-berlin-2016-ibmdbpy.slides.html&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Edouard Fouché</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/accelerating-python-analytics-by-in-database-processing.html</guid><category>ibmdbpy</category></item><item><title>BigchainDB : a Scalable Blockchain Database, in Python</title><link>https://pyvideo.org/pydata-berlin-2016/bigchaindb-a-scalable-blockchain-database-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;This talk describes BigchainDB. BigchainDB fills a gap in the decentralization ecosystem: a decentralized database, at scale. It has big-data performance levels, a querying system, and a permissioning system that supports public and private versions. It's complementary to decentralized processing platforms like Ethereum, and decentralized file systems like IPFS. BigchainDB is written in Python.&lt;/p&gt;
&lt;p&gt;This talk describes BigchainDB. BigchainDB fills a gap in the decentralization ecosystem: a decentralized database, at scale. It points to performance of 1 million writes per second throughput, storing petabytes of data, and sub-second latency.&lt;/p&gt;
&lt;p&gt;The BigchainDB design starts with a distributed database (DB), and through a set of innovations adds blockchain characteristics: decentralized control, immutability, and creation &amp;amp; movement of digital assets. BigchainDB inherits characteristics of modern distributed databases: linear scaling in throughput and capacity with the number of nodes, a full-featured NoSQL query language, efficient querying, and permissioning. Being built on an existing distributed DB, it also inherits enterprise-hardened code for most of its codebase.&lt;/p&gt;
&lt;p&gt;Scalable capacity means that legally binding con- tracts and certificates may be stored directly on the blockchain database. The permissioning system enables configurations ranging from private enterprise blockchain databases to open, public blockchain databases. BigchainDB is complementary to decentralized processing platforms like Ethereum, and decentralized file systems like InterPlanetary File System (IPFS).&lt;/p&gt;
&lt;p&gt;This talk describes technology perspectives that led to the BigchainDB design: traditional blockchains, distributed databases, and a case study of the domain name system (DNS). We introduce a concept called blockchain pipelining, which is key to scalability when adding blockchainlike characteristics to the distributed DB. We present a thorough description of BigchainDB, a detailed analysis of latency, and experimental results. The talk concludes with a description of use cases.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://github.com/bigchaindb/bigchaindb"&gt;https://github.com/bigchaindb/bigchaindb&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Trent McConaghy</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/bigchaindb-a-scalable-blockchain-database-in-python.html</guid></item><item><title>Bridging the gap: from Data Science to service</title><link>https://pyvideo.org/pydata-berlin-2016/bridging-the-gap-from-data-science-to-service.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Recent years have brought an explosion of algorithms, models and software libraries for doing data science that allow unprecedented possibilities for solving problems. But providing a data science service as a consultant or a company involves more than just tools. In this talk, I will share the most useful lessons that I learned while working at a company providing these services.&lt;/p&gt;
&lt;p&gt;Providing consulting services or custom machine learning based tools involves many elements beyond choosing the right models, analysis methods and tools. While working at Machinalis, we learned a set of ideas that help us to make our projects successful, which have more to do with the human and engineering side of the service than the science and programming.&lt;/p&gt;
&lt;p&gt;The goal of the talk is to share that experience with people that are providing or planning to provide services based on data science. It presents some tips and explains how they work and the kind of problems that they avoid. The content of the talk is divided into two main areas:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Tips related to team collaboration. These include aspects to consider regarding how and what to communicate between different roles in the team (data scientists, developers, project managers, people doing labeling or entry, etc.), and how to make certain decisions that the team faces (choosing tools, knowing when to do a flimsy proof of concept vs when to create robust products using best practices, etc.).&lt;/li&gt;
&lt;li&gt;Tips related to collaboration with clients and users. There are different kinds of clients with varied levels of knowledge regarding the possibilities and limits of data science, and each kind requires an appropriate way of communicating these issues. It is also important to establish clarity about mutual expectations, about what is guaranteed and what is experimental, about how your work will be evaluated, and about how you will integrate with different stakeholders in the client's organization. This alignment with your client is a critical element of a successful service.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://github.com/machinalis/slides/tree/master/data-science-to-service"&gt;https://github.com/machinalis/slides/tree/master/data-science-to-service&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Moisset</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/bridging-the-gap-from-data-science-to-service.html</guid></item><item><title>Classifying Search Queries Without User Click Data</title><link>https://pyvideo.org/pydata-berlin-2016/classifying-search-queries-without-user-click-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;This talk discusses how machine learning/data mining techniques can be applied to classify search terms that people use in search engines like Google, Bing, Yahoo etc. The talk focuses on machine learning techniques such as LSTM (long short term memory) rather than traditional ways like analysing user-behaviour with the help of their search logs.&lt;/p&gt;
&lt;p&gt;Traditionally, search queries are classified into different categories by analysing user-behavior with the help of search logs. Instead of focussing on the search logs and by analysing queries with the help of machine learning models such as LSTM, it is possible to get a very decent model to classify the search queries.&lt;/p&gt;
&lt;p&gt;This talk focuses majorly on LSTMs and their usefulness when it comes to search query classification. We also discuss how we can accurately classify search queries in hundreds of categories using open source data available online and how this can be combined with LSTMs to provide a much stable and better result.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Abhishek Thakur</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/classifying-search-queries-without-user-click-data.html</guid></item><item><title>Data Integration in the World of Microservices</title><link>https://pyvideo.org/pydata-berlin-2016/data-integration-in-the-world-of-microservices.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Since its launch in 2008, Zalando has grown with tremendous speed. The road from startup to multinational corporation has been full of challenges, especially for Zalando's technology team. Distributed across Berlin, Helsinki, Dublin, Hamburg and Dortmund — with nearly 1000 professionals strong — Zalando Technology still plans to expand by adding 1,000 more developers through the end of 2016.&lt;/p&gt;
&lt;p&gt;This rapid growth has shown us that we need to remain flexible about developing processes and organizational structures, to allow us to continue scaling and experimenting. In March 2015, our team adopted Radical Agility: a tech management approach that emphasizes Autonomy, Purpose, and Mastery, with trust as the glue holding it all together.&lt;/p&gt;
&lt;p&gt;To make autonomy possible, teams can now choose their own technology stacks for the products they own. Microservices, speaking with each other using RESTful APIs, promise to minimize the costs of integration between autonomous teams. In addition, Isolated AWS accounts run on top of our own open-source Platform as a Service (called STUPS.io), give each autonomous team enough hardware to experiment and introduce new features without breaking our entire system.&lt;/p&gt;
&lt;p&gt;One small issue with having microservices isolated in their individual AWS accounts: Our teams keep local data for themselves. In this environment, building an ETL process for data analyses, or integrating data from different services becomes quite challenging.&lt;/p&gt;
&lt;p&gt;PostgreSQL's new logical replication features, however, now make it possible to stream all the data changes from the isolated databases to the data integration system so that it can collect this data, represent it in different forms, and prepare it for analysis.&lt;/p&gt;
&lt;p&gt;In this talk, I will discuss Zalando's open-source data collection prototype, which uses PostgreSQL's logical replication streaming capabilities to collect data from various PostgreSQL databases, and recreate it for different formats and systems (Data Lake, Operational Data Store, KPI calculation systems, automatic process monitoring). The audience will come away with new ideas for how to use Postgres streaming replication in a microservices environment.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Valentine Gogichashvili</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/data-integration-in-the-world-of-microservices.html</guid><category>stups.io</category><category>postgresql</category><category>replication</category><category>etl</category></item><item><title>Dealing with TBytes of Data in Realtime</title><link>https://pyvideo.org/pydata-berlin-2016/dealing-with-tbytes-of-data-in-realtime.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Data processing often splits into two disjunct categories: Classic access to RDBMS is well-understood, but often scales poorly after considerable GBytes of data. Big data approaches are powerful, but complex to set up and to maintain. In a test setup we tried a compromise of both: What happens if you glue more than 1000 single SQL databases into a huge cluster? We learned a whole lotta lessons!&lt;/p&gt;
&lt;p&gt;Data processing often splits into two disjunct categories: Classic access to RDBMS with SQL and ORMs is well-understood and convenient, but often scales poorly after considerable GBytes of data. Big data approaches are powerful, but complex to set up and to maintain. In a test setup we tried a compromise of both: What happens if you glue more than 1000 single SQL databases into a huge cluster?&lt;/p&gt;
&lt;p&gt;Thanks to access to an unused IaaS cluster, we had the opportunity to research the behavior of many nodes clustered together. Data loading becomes a real challenge, while maintenance and monitoring such a drove of containers was no longer possible manually. We investigated the effect of changing container-vm-ratios. For our experiments, we used Crate, an open source, highly scalable, shared-nothing distributed SQL database, that comes with Python client connectors and support for several ORMs.&lt;/p&gt;
&lt;p&gt;We share unexpected experiences about data schema design with the attendees, will explain some tweaking options that turned out to be effective, and would like to campaign for more open data projects.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nils Magnus</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/dealing-with-tbytes-of-data-in-realtime.html</guid></item><item><title>Designing spaCy: Industrial-strength NLP</title><link>https://pyvideo.org/pydata-berlin-2016/designing-spacy-industrial-strength-nlp.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;The spaCy natural language processing (NLP) library features state-of-the-art performance, and a high-level Python API. Efficiency is crucial for NLP, because job sizes are constantly increasing. This talk describes how we’ve met these challenges in spaCy, by implementing the library in Cython.&lt;/p&gt;
&lt;p&gt;The spaCy natural language processing (NLP) library features state-of-the-art performance, and a high-level Python API. Efficiency is crucial for NLP, because job sizes are constantly increasing. The key algorithms are also relatively complicated, and frequently subject to change, as new research is published. This talk describes how we’ve met these challenges in spaCy, by implementing the library in Cython. Unlike many Cython users, we did not write the library in Python first, and then optimize it. Instead, we designed the library as a C extension from the start, and added the Python API on top. This allows us to build the library on top of efficient, memory-managed data structures, without having to maintain a separate C or C++ codebase. The result is the fastest NLP library in the world, support for GIL-free multithreading, in a concise readable codebase, and with no compromise on user friendliness.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matthew Honnibal</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/designing-spacy-industrial-strength-nlp.html</guid><category>spacy</category><category>npl</category></item><item><title>Frontera: open source, large scale web crawling framework</title><link>https://pyvideo.org/pydata-berlin-2016/frontera-open-source-large-scale-web-crawling-framework.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;We've tried to crawl the Spanish (.es zone) internet, containing about ~600K websites to collect stats about hosts and their sizes. I'll describe the crawler architecture, storage, problems we faced with during the crawl and solutions found. Finally we released our solution as Frontera framework, allowing to build an online, scalable web crawlers using Python.&lt;/p&gt;
&lt;p&gt;In this talk I'm going to share our experience crawling the Spanish web. We aimed at crawling about ~600K websites in .es zone, to collect statistics about hosts and their sizes. I'll describe crawler architecture, storage, problems we faced during the crawl and solutions found.&lt;/p&gt;
&lt;p&gt;Our solution is accessible in open source, as Frontera framework. It provides pluggable document and queue storage: RDBMS or Key-Value based, crawling strategy management, communication bus to choose: Kafka or ZeroMQ, using Scrapy as a fetcher, or plugging your own fetching component.&lt;/p&gt;
&lt;p&gt;Frontera allows to build a scalable, distributed web crawler to crawl the Web at high rates and large volumes. Frontera is online by design, allowing to modify the crawler components without stopping the whole process. Also Frontera can be used to build a focused crawlers to crawl and revisit a finite set of websites.&lt;/p&gt;
&lt;p&gt;Talk is organized in fascinating form: problem description, solution proposed, and issues appeared during the development and running the crawl.&lt;/p&gt;
&lt;p&gt;Github Repo: &lt;a class="reference external" href="https://github.com/scrapinghub/frontera"&gt;https://github.com/scrapinghub/frontera&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alexander Sibiryakov</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/frontera-open-source-large-scale-web-crawling-framework.html</guid><category>frontera</category></item><item><title>Functional Programming in Python</title><link>https://pyvideo.org/pydata-berlin-2016/functional-programming-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;There should be one-- and preferably only one --obvious way to do it. And that is functional - at least in my opinion. I'm working with Python for a living since two years after coming from a background in Ruby, Haskell, Clojure, and some more languages. Since then I have tried to marry ideomatic Python to the functional style I learned to love. It's time to share my experience.&lt;/p&gt;
&lt;p&gt;I will talk about&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;higher order functions&lt;/li&gt;
&lt;li&gt;partial function application and currying&lt;/li&gt;
&lt;li&gt;function composition&lt;/li&gt;
&lt;li&gt;functional collection transformations (and why they are relevant for PySpark)&lt;/li&gt;
&lt;li&gt;(fake) lazy evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and how it all relates to Python (and what's missing).&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://github.com/kirel/functional-python"&gt;https://github.com/kirel/functional-python&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Kirsch</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/functional-programming-in-python.html</guid><category>functional</category></item><item><title>How to Trick a Neural Network</title><link>https://pyvideo.org/pydata-berlin-2016/how-to-trick-a-neural-network.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://jvns.ca/blog/2016/05/21/a-few-notes-from-my-pydata-berlin-keynote/"&gt;http://jvns.ca/blog/2016/05/21/a-few-notes-from-my-pydata-berlin-keynote/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Julia Evans</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/how-to-trick-a-neural-network.html</guid><category>keynote</category></item><item><title>Introduction to Julia for Python Developers</title><link>https://pyvideo.org/pydata-berlin-2016/introduction-to-julia-for-python-developers.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Julia is a performance oriented language written from the ground-up to support numerical processing and parallelisation. The basic syntax of Julia resembles a cross between Matlab and Python, but offers performance which is comparable to compiled C-code. I will present an overview of the language with particular emphasis on where Python users may benefit in using it in their daily work.&lt;/p&gt;
&lt;p&gt;Python users have long benefitted from the less verbose nature of Python, when compared with C and Fortran. However, Python was originally designed for scripting tasks, using dynamic types and widescale object orientation, neither of which features are necessarily beneficial when it comes to numerical computing. Thus, we have seen the widespread use of Python libraries for numerical computation (scipy, numpy, etc.).&lt;/p&gt;
&lt;p&gt;Julia is a new language, developed at MIT, which attempts to learn from the experience of development of Python and similar languages. The main goals are to provide a non-verbose, performance oriented language written from the ground-up to support numerical processing and parallelisation. In its most basic syntax Julia resembles a cross between Matlab and Python, but via compilation through an intermediate level representation (llvm) it offers performance which is comparable to compiled C-code.&lt;/p&gt;
&lt;p&gt;I am not going to argue that Julia is ready for primetime yet. However, it is definitely worth consideration by anyone currently resorting to cython or needing distributed access to large datasets.&lt;/p&gt;
&lt;p&gt;I will present an outline/introduction to the language, including the main benefits and current weaknesses. Of particular interest to the audience may be the fact that Python libraries are importable and callable from within Julia, allowing a continuity of existing workflow but from a Julia-based host environment. My main focus will be for a numerically literate audience who are already contending with the technical limitations of Python and are curious about the new language in town.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://github.com/daveh19/pydataberlin2016"&gt;https://github.com/daveh19/pydataberlin2016&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Higgins</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/introduction-to-julia-for-python-developers.html</guid><category>julia</category></item><item><title>Let's play Space Invaders!</title><link>https://pyvideo.org/pydata-berlin-2016/lets-play-space-invaders.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Neural Networks became recently very hot topic because of some very eye-catching developments like Deep Dream and successes like AlphaGo winning with Go master.&lt;/p&gt;
&lt;p&gt;I will show how I taught my Deep Q-Network (DQN) to play Space Invaders and explain principles behind DQN.&lt;/p&gt;
&lt;p&gt;Going beyond this example I will show some exciting advances and applications of Deep Reinforcement Learning like attention.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maciej Jaskowski</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/lets-play-space-invaders.html</guid></item><item><title>Machine Learning at Scale</title><link>https://pyvideo.org/pydata-berlin-2016/machine-learning-at-scale.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Python machine learning libraries like scikit-learn are a fantastic resource but not always well suited to large datasets. How can we use Python for machine learning in such cases? This talk will introduce PySpark and MLlib as tools for distributed machine learning. We will discuss what these tools are, how they work, and cover some basic code examples of machine learning on a cluster.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;Intro&lt;/dt&gt;
&lt;dd&gt;&lt;ol class="first last loweralpha"&gt;
&lt;li&gt;Why is scikit-learn not enough?&lt;/li&gt;
&lt;li&gt;What is Spark?&lt;/li&gt;
&lt;li&gt;What is MLlib?&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;Spark&lt;/dt&gt;
&lt;dd&gt;&lt;ol class="first last loweralpha"&gt;
&lt;li&gt;Overview of Spark&lt;/li&gt;
&lt;li&gt;Overview of PySpark&lt;/li&gt;
&lt;li&gt;PySpark code sample&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;MLlib&lt;/dt&gt;
&lt;dd&gt;&lt;ol class="first last loweralpha"&gt;
&lt;li&gt;Overview of MLlib&lt;/li&gt;
&lt;li&gt;MLlib code samples&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nathan Epstein</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/machine-learning-at-scale.html</guid><category>scikit-learn</category><category>pyspark</category><category>mllib</category></item><item><title>One in a billion: finding matching images in very large corpora</title><link>https://pyvideo.org/pydata-berlin-2016/one-in-a-billion-finding-matching-images-in-very-large-corpora.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;The goal was not only to support high write volumes of over 10k/s but also to support fast lookup of similar images around 1-2s for over 1B images. Though similar paid services and free image hashing libraries exist, this may be the first complete free open-source solution. Available at: &lt;a class="reference external" href="https://github.com/ascribe/image-match"&gt;https://github.com/ascribe/image-match&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;image-match started as an internal project. We needed a way, given some target image, to find similar images downloaded by our web-crawler (think Tineye).&lt;/p&gt;
&lt;p&gt;So not only did we need to support fast, accurate lookup for millions or even billions of images, we also needed to facilitate very high volume insertion -- around 10k images per second.&lt;/p&gt;
&lt;p&gt;In my talk, I will cover:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The Problem: why is finding similar images hard?&lt;/li&gt;
&lt;li&gt;Algorithm: based on this paper&lt;/li&gt;
&lt;li&gt;Performance: but does it scale?&lt;/li&gt;
&lt;li&gt;Alternatives&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ryan Henderson</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/one-in-a-billion-finding-matching-images-in-very-large-corpora.html</guid><category>image-match</category></item><item><title>Predictive modelling with Python</title><link>https://pyvideo.org/pydata-berlin-2016/predictive-modelling-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://ogrisel.github.io/decks/2016_pydata_berlin/"&gt;http://ogrisel.github.io/decks/2016_pydata_berlin/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/ogrisel/docker-distributed"&gt;https://github.com/ogrisel/docker-distributed&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Olivier Grisel</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/predictive-modelling-with-python.html</guid><category>keynote</category></item><item><title>pypet: A Python Toolkit for Simulations and Numerical Experiments</title><link>https://pyvideo.org/pydata-berlin-2016/pypet-a-python-toolkit-for-simulations-and-numerical-experiments.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;pypet manages exploration of the parameter space of any numerical simulation in Python, thereby storing your data into HDF5 files for you. The toolkit offers a new data container which lets you access all your parameters and results from a single source. Data I/O of your simulations and analyses become a piece of cake!&lt;/p&gt;
&lt;p&gt;pypet &lt;a class="reference external" href="http://pypet.readthedocs.org/"&gt;python parameter exploration toolkit&lt;/a&gt; is a new multi-platform Python toolkit for management of simulations and storage of numerical data. Exploring or sampling the space of model parameters is one key aspect of simulations and numerical experiments. pypet was especially designed to allow easy and arbitrary sampling of trajectories through a parameter space beyond simple grid searches.&lt;/p&gt;
&lt;p&gt;Simulation parameters as well as the obtained results are collected by pypet and stored in the widely used &lt;a class="reference external" href="http://www.hdfgroup.org/HDF5/"&gt;HDF5 file format&lt;/a&gt;. This allows fast and convenient loading of data for further analyses. Furthermore, pypet provides an environment with various features. For example, among these are multiprocessing for fast parallel simulations, dynamic loading of data, integration of Git version control, and supervision of experiments via the electronic lab notebook &lt;a class="reference external" href="https://pythonhosted.org/Sumatra/"&gt;Sumatra&lt;/a&gt;. A rich set of data formats is supported encompassing native Python types, Numpy and Scipy data, and &lt;a class="reference external" href="http://pandas.pydata.org/"&gt;pandas DataFrames&lt;/a&gt;. Moreover, the toolkit is easily extendable to allow the user to add customized data formats. pypet is a very flexible tool and suited for short Python scripts as well as large scale projects that involve simulations and numerical experiments.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Robert Meyer</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/pypet-a-python-toolkit-for-simulations-and-numerical-experiments.html</guid><category>pypet</category><category>HDF5</category></item><item><title>Python and TouchDesigner for Interactive Experiments</title><link>https://pyvideo.org/pydata-berlin-2016/python-and-touchdesigner-for-interactive-experiments.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;I will dismantle the guts of an interactive multiplayer brain data driven installation we made last year. It uses Python modules to handle data from the users to create a new experiences every time. I will discuss the benefits of using Python with visual programming to create very complex experiences quickly, which can be useful for interactive art and science experiments alike.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;TouchDesigner is a visual programming software made by a company called Deriviative. I use this software with Python to make complex interactive installations which are hybrid of art and science. I use the same technique to make science experiments so the lab can test the validity of neurofeedback paradigms. I want to share some of the techniques I used in our brain driven installation, My Virual Dream, because I think there is huge potential to make very practical and very fantastical things. Quickly. This mixture of touch designer and Python could give scientists access to powerful audio visual tools usually associated with complex game engines or complex coding from scratch, in order to create limitless experiments. Even better, one can create usable modules to string into new experiments or share with each other.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jessica Palmer</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/python-and-touchdesigner-for-interactive-experiments.html</guid><category>TouchDesigner</category></item><item><title>Python Data Ecosystem: Thoughts on Building for the Future</title><link>https://pyvideo.org/pydata-berlin-2016/python-data-ecosystem-thoughts-on-building-for-the-future.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://de.slideshare.net/wesm/python-data-ecosystem-thoughts-on-building-for-the-future"&gt;http://de.slideshare.net/wesm/python-data-ecosystem-thoughts-on-building-for-the-future&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Wes McKinney</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/python-data-ecosystem-thoughts-on-building-for-the-future.html</guid><category>keynote</category></item><item><title>Robot uses toddler-like self exploration for the development of body representations</title><link>https://pyvideo.org/pydata-berlin-2016/robot-uses-toddler-like-self-exploration-for-the-development-of-body-representations.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Can artificial childhood improve robot intelligence? For some animals, including humans, childhood experiences are crucial for the healthy development of the brain. We believe that robots can also benefit from having such an exploratory stage. By adopting biologically inspired developmental methods, we tried to train a neural network to develop body representations of a humanoid robot.&lt;/p&gt;
&lt;p&gt;The project is part of an attempt to study and improve a robot learning skills using developmental approach inspired by psychology research, which has the potential to allow more flexibility in uncertain environments. We focused on the development of body representations - orientation and movement. This ability has a crucial part in the mammalian sensorimotor and self perception systems. In infants,the process of acquiring body awareness involves active exploration of the sensorimotor space by babbling and self-touch. We tried using similar technics with the Nao humanoid robot. Using an artificial neural network, we believe the robot can develop and maintain a body schema that can also be used for recognizing unexpected situations such as external stimuli or body changes. We used different python tools, both for the activation of the robot and the analysis of the recorded data. Our main tool was Lasagne - a library used for creating feed-forward neural networks and train them using back propagation.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Idai Guertel</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/robot-uses-toddler-like-self-exploration-for-the-development-of-body-representations.html</guid></item><item><title>The "Kwargh!" Problem</title><link>https://pyvideo.org/pydata-berlin-2016/the-kwargh-problem.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">James Powell</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/the-kwargh-problem.html</guid></item><item><title>Usable A/B testing – A Bayesian approach</title><link>https://pyvideo.org/pydata-berlin-2016/usable-ab-testing-a-bayesian-approach.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;When it comes to developing a product, most people focus on “why” a feature should be implemented. I would like to look at the “how” A/B testing can support such efforts in order to improve your users experience on your platform. We will then touch on the implementation of an experiment evaluation approach using Bayesian statistics that will help your product managers to make the right decisions.&lt;/p&gt;
&lt;p&gt;When it comes to developing a product, most people focus on “why” a feature should be implemented. I would like to look at the “how” A/B testing can support such efforts in order to improve your users experience on your platform. Many companies lose a good amount of conversion and therefore money because they struggle to understand the A/B testing tools that they are using and how to interpret the test results.&lt;/p&gt;
&lt;p&gt;We will take a look at why A/B testing experiment results using conventional approaches are often hard to interpret correctly and will explore an alternate, visually easy to understand approach that was implemented using Bayesian statistics. We will touch on the implementation, as well as the theory behind this approach.&lt;/p&gt;
&lt;p&gt;Finally I will present a new library for A/B testing experiment evaluation that will help you to effectively increase your conversion rates and help your product managers to make the right decisions.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://speakerdeck.com/nneu/b-testing-a-bayesian-approach"&gt;https://speakerdeck.com/nneu/b-testing-a-bayesian-approach&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nora Neumann</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/usable-ab-testing-a-bayesian-approach.html</guid></item><item><title>Using small data in the client instead of big data in the cloud</title><link>https://pyvideo.org/pydata-berlin-2016/using-small-data-in-the-client-instead-of-big-data-in-the-cloud.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Rather than using big data in the cloud, at Transit App we send data sets to the client and process it there. To keep the data small, we’ve switched to using binary files instead of plain text formats like ᴊsᴏɴ. In this talk we’ll discuss how you can turn data into binary, how it works, and our own approach to store complete transit schedules on the device and show their data instantly.
Abstract&lt;/p&gt;
&lt;p&gt;Rather than using big data in the cloud, we at Transit App found it useful to to give as much data to client as possible and process it client-side. In order to keep transfer volume and performance reasonable, it makes sense to move from text based interchange formats like ᴊsᴏɴ or xᴍʟ back back in time to using binary file formats. In this talk we will introduce some common libraries to make data compact, to make it fast, and how these libraries work. We will also discuss the approach of our own library for creating schema-less binary file formats, and how it helped us store complete transit schedules in small files on the phone and display their data instantly.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anton Dubrau</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/using-small-data-in-the-client-instead-of-big-data-in-the-cloud.html</guid></item><item><title>What every data scientist should know about data anonymization</title><link>https://pyvideo.org/pydata-berlin-2016/what-every-data-scientist-should-know-about-data-anonymization.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;There are numerous examples of data anonymization gone horribly wrong - the most prominent one might be the netflix prize, where researchers were able to uniquely identify users by combining netflix user data with imdb reviews. Let's learn from their mistakes and look at some of the measures you can take to better anonymize data before you share it with others.&lt;/p&gt;
&lt;p&gt;Outline:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Look at some of the examples where data anonymization was broken and identify what went wrong&lt;/li&gt;
&lt;li&gt;What is the state of the art for data anonymization and can you be sure to be safe if you follow it?&lt;/li&gt;
&lt;li&gt;How does anonymization affect the possibilities for data mining/machine learning on the data?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This talk is aimed at people who want release open data or want to share sensitive data between departments.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://github.com/krasch/presentations/blob/master/pydata_Berlin_2016.pdf"&gt;https://github.com/krasch/presentations/blob/master/pydata_Berlin_2016.pdf&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Katharina Rasch</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/what-every-data-scientist-should-know-about-data-anonymization.html</guid><category>data anonymization</category></item><item><title>What's new in Deep Learning?</title><link>https://pyvideo.org/pydata-berlin-2016/whats-new-in-deep-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;&amp;#64;karpathy's recent tweet &amp;quot;BatchNorm, STN, DCGAN, DRAW, soft/hard attention, char-rnn, DeepDream, NeuralStyle, TensorFlow, ResNet, AlphaGo.. a lot happened over 1 year&amp;quot; sums up the many new aspects of Deep Learning research.&lt;/p&gt;
&lt;p&gt;In this talk I will review some of the highlights of deep learning in the context of Python deep learning frameworks.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://www.dropbox.com/s/b6lgvq6ijlutii4/new-deep-learning.pdf?dl=0"&gt;https://www.dropbox.com/s/b6lgvq6ijlutii4/new-deep-learning.pdf?dl=0&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kashif Rasul</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/whats-new-in-deep-learning.html</guid><category>deep learning</category></item></channel></rss>