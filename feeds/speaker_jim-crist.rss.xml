<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 07 Dec 2019 00:00:00 +0000</lastBuildDate><item><title>Introducting Dask-Gateway: Dask clusters as a service</title><link>https://pyvideo.org/pydata-austin-2019/introducting-dask-gateway-dask-clusters-as-a-service.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask-Gateway provides a secure, multi-tenant server for managing Dask clusters. It allows users to launch and use Dask clusters in a shared, centrally managed environment, and supports a wide variety of backends (e.g. Kubernetes, Hadoop, HPC systems, etcâ€¦). In this talk we'll discuss the use and design of Dask-Gateway, as well as some of the issues we encountered while developing this tool.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jim Crist</dc:creator><pubDate>Sat, 07 Dec 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-12-07:pydata-austin-2019/introducting-dask-gateway-dask-clusters-as-a-service.html</guid><category>dask</category><category>dask-gateway</category></item><item><title>Make it Work, Make it Right, Make it Fast Debugging and Profiling in Dask</title><link>https://pyvideo.org/pydata-seattle-2017/make-it-work-make-it-right-make-it-fast-debugging-and-profiling-in-dask.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a pure python library for parallel and distributed computing. It's designed with flexibility in mind, making it easy to parallelize the complicated workflows often found in science. However, once you get something working, how do you debug or profile it? In this talk we'll cover the various tools Dask provides for diagnosing bugs and bottlenecks, as well as tips for resolving these issues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dask is a pure python library for parallel and distributed computing. It's designed with simplicity and flexibility in mind, making it easy to parallelize the complicated workflows often found in science. However, once you get something working, how do you debug or profile it? Debugging and profiling parallel code is notoriously hard! In this talk we'll cover the various tools Dask provides for diagnosing bugs and performance bottlenecks, as well as tips and techniques for resolving these issues.&lt;/p&gt;
&lt;p&gt;Starting with an example single-threaded probram, we'll walk through adding Dask to parallelize it, and then iterate on this example to gradually improve performance throughout the talk. Attendees should leave having a better understanding of:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What tools Dask provides for debugging on a single machine&lt;/li&gt;
&lt;li&gt;How Dask uses IPython to make debugging distributed computations easy&lt;/li&gt;
&lt;li&gt;How to profile Dask code both on a single machine and on a cluster&lt;/li&gt;
&lt;li&gt;How to interpret the graphs presented in the Dask Dashboard&lt;/li&gt;
&lt;li&gt;Performance techniques for attacking bottlenecks once they've been identified.&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jim Crist</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/make-it-work-make-it-right-make-it-fast-debugging-and-profiling-in-dask.html</guid></item><item><title>Parallelizing Scientific Python with Dask</title><link>https://pyvideo.org/pydata-seattle-2017/parallelizing-scientific-python-with-dask.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a flexible tool for parallelizing Python code on a single machine or across a cluster. It builds upon familiar tools in the PyData ecosystem (e.g. NumPy and Pandas) while allowing them to scale across multiple cores or machines. This tutorial will cover both the high-level use of dask collections, as well as the low-level use of dask graphs and schedulers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dask is a flexible tool for parallelizing Python code on a single machine or across a cluster.&lt;/p&gt;
&lt;p&gt;We can think of dask at a high and a low level&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;High level collections: Dask provides high-level Array, Bag, and DataFrame collections that mimic and build upon NumPy arrays, Python lists, and Pandas DataFrames, but that can operate in parallel on datasets that do not fit into main memory.&lt;/li&gt;
&lt;li&gt;Low Level schedulers: Dask provides dynamic task schedulers that execute task graphs in parallel. These execution engines power the high-level collections mentioned above but can also power custom, user-defined workloads to expose latent parallelism in procedural code. These schedulers are low-latency and run computations with a small memory footprint.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Different users operate at different levels but it is useful to understand both. This tutorial will cover both the high-level use of dask.array and dask.dataframe and the low-level use of dask graphs and schedulers. Attendees will come away&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Able to use dask.delayed to parallelize existing code&lt;/li&gt;
&lt;li&gt;Understanding the differences between the dask schedulers, and when to use one over another&lt;/li&gt;
&lt;li&gt;With a firm understanding of the different dask collections (dask.array and dask.dataframe) and how and when to use them.&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jim Crist</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/parallelizing-scientific-python-with-dask.html</guid></item></channel></rss>