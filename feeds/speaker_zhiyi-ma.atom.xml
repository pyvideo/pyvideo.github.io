<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Zhiyi Ma</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_zhiyi-ma.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2021-05-14T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Facebook sponsor workshop</title><link href="https://pyvideo.org/pycon-us-2021/facebook-sponsor-workshop.html" rel="alternate"></link><published>2021-05-14T00:00:00+00:00</published><updated>2021-05-14T00:00:00+00:00</updated><author><name>Zhiyi Ma</name></author><id>tag:pyvideo.org,2021-05-14:/pycon-us-2021/facebook-sponsor-workshop.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dynabench is a research platform for dynamic data collection and benchmarking. In the first part of this workshop, we introduce how we built the dynamic model evaluation pipeline for Dynabench using a collection of open source Python toolkits, e.g. pytorch, torchserve, boto3, etc, and how anyone in the â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dynabench is a research platform for dynamic data collection and benchmarking. In the first part of this workshop, we introduce how we built the dynamic model evaluation pipeline for Dynabench using a collection of open source Python toolkits, e.g. pytorch, torchserve, boto3, etc, and how anyone in the community can easily submit their models into the pipeline using our open source toolkit, Dynalab. We believe that the evaluation cloud of Dynabench will transform how the community thinks of models and benchmarks, and continuously push the community to innovate beyond the current status quo from a completely new angle of thinking.&lt;/p&gt;
&lt;p&gt;The two key components in a multi-task Reinforcement Learning codebase are (i) Multi-task RL algorithms and (ii) Multi-task RL environments. Facebook AI developed open-source libraries for both components. Part 2 of this workshop describes how one can use these two libraries to get started with multi-task reinforcement learning. By the end of the talk, the audience should be able to design their own multi-task RL agent using the components from the MTRL library (&lt;a class="reference external" href="https://github.com/facebookresearch/mtrl"&gt;https://github.com/facebookresearch/mtrl&lt;/a&gt;) and run them on environments provided by the MTEnv library (&lt;a class="reference external" href="https://github.com/facebookresearch/mtenv"&gt;https://github.com/facebookresearch/mtenv&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://drive.google.com/file/d/1FYRiyEojjGyRP8xifFzsuv9zU0nRx5rX/view?usp=sharing"&gt;https://drive.google.com/file/d/1FYRiyEojjGyRP8xifFzsuv9zU0nRx5rX/view?usp=sharing&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon US 2021"></category></entry></feed>