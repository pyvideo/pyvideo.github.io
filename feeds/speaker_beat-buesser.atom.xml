<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Beat Buesser</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_beat-buesser.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-09-16T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Adversarial Robustness Toolbox: How to attack and defend your machine learning models</title><link href="https://pyvideo.org/pycon-uk-2019/adversarial-robustness-toolbox-how-to-attack-and-defend-your-machine-learning-models.html" rel="alternate"></link><published>2019-09-16T00:00:00+00:00</published><updated>2019-09-16T00:00:00+00:00</updated><author><name>Beat Buesser</name></author><id>tag:pyvideo.org,2019-09-16:/pycon-uk-2019/adversarial-robustness-toolbox-how-to-attack-and-defend-your-machine-learning-models.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Adversarial samples and poisoning attacks are emerging threats to the security of AI systems. This talk demonstrates how to apply the Python library Adversarial Robustness Toolbox (ART) to create and deploy robust AI systems.&lt;/p&gt;
</content><category term="PyCon UK 2019"></category></entry></feed>