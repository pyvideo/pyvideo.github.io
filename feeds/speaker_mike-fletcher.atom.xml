<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_mike-fletcher.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2015-11-08T00:00:00+00:00</updated><entry><title>Embrace the Singularity</title><link href="https://pyvideo.org/pycon-ca-2015/embrace-the-singularity-mike-fletcher.html" rel="alternate"></link><published>2015-11-08T00:00:00+00:00</published><updated>2015-11-08T00:00:00+00:00</updated><author><name>Mike Fletcher</name></author><id>tag:pyvideo.org,2015-11-08:pycon-ca-2015/embrace-the-singularity-mike-fletcher.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Machine Learning has been making fantastic advances lately. Problems of classification, recognition, modelling and prediction are currently being optimized to almost-human-level accuracy. Python is used in many research groups to explore Machine Learning approaches, and provides many libraries that allow novices to code non-trivial networks with minimal work. Large corporations have moved from research to deployment. Machine Learning has proven itself useful, and we seem to be getting tantalisingly close to something that could be a “true” strong A.I.&lt;/p&gt;
&lt;p&gt;But shadowing these achievements there is a rising fear in society that we will open Pandora’s box, that our creation will decide that we need to be eliminated. We cannot dismiss those concerns out-of-hand as our track-record as a profession is littered with cases where we have lacked foresight. We should address the concerns now, before the box is sitting in our hands.&lt;/p&gt;
&lt;p&gt;Where are we on the road to creating a strong A.I.? How would we recognize a strong A.I.? What can we currently do with Machine Learning, what are the approaches we currently use, and where are they likely to lead over the next few years?&lt;/p&gt;
&lt;p&gt;Are there fundamental missing pieces that prevent a “human-like” strong A.I. from developing? Are there scale or resources standing in the way? What are the (moral) considerations in developing a strong A.I.? What about in shutting one down? What are the threat models involved in “standing up” a strong A.I.? What can be done to mitigate those threats?&lt;/p&gt;
&lt;p&gt;This talk may include code samples, for which a passing familiarity with Numpy or Theano might be helpful, but the focus for the discussion will be on the algorithms,  epistemology and how we as programmers should approach our attempt to develop an artificial mind.&lt;/p&gt;
</summary></entry><entry><title>Profiling for Performance</title><link href="https://pyvideo.org/pycon-ca-2012/profiling-for-performance.html" rel="alternate"></link><published>2012-11-10T00:00:00+00:00</published><updated>2012-11-10T00:00:00+00:00</updated><author><name>Mike Fletcher</name></author><id>tag:pyvideo.org,2012-11-10:pycon-ca-2012/profiling-for-performance.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;We will discuss how to profile Python code, how to interpret profiles,
and how (and how not) to use profiling to improve your code's run-time
performance. We will look at both built-in and external tools (including
RunSnakeRun and SnakeViz). We will also discuss the wider issues of how
to approach optimization in your code base.&lt;/p&gt;
</summary></entry></feed>