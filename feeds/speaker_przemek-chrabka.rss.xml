<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Thu, 12 Dec 2019 00:00:00 +0000</lastBuildDate><item><title>How to structure PySpark application</title><link>https://pyvideo.org/pydata-warsaw-2019/how-to-structure-pyspark-application.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A lot of the Data Scientists and Engineers donâ€™t come from Software
Engineering background and even they have an experience with writing
spark code they might luck the knowledge about application structure
principals. This talk is designed to help them write better and more
readable code.&lt;/p&gt;
&lt;p&gt;PySpark has become really popular for last couple of years and is now a
go-to tool for building and managing data-heavy applications. One of the
most common ways how Spark is used is moving some data around by writing
ETL/ELT jobs. Doing that your code should be manageable and
understandable to others. In this talk I will try to introduce good
practice how to structure PySpark application and write jobs and also
some naming conventions.&lt;/p&gt;
&lt;p&gt;I will start this talk with an example of bad way of writing PySpark job
and during the course of it we will gradually improve it so at the end
our application is going to be production ready, easy to manage and
share with other developers.&lt;/p&gt;
&lt;p&gt;During this talk I will try to answer this questions: - How to structure
PySpark ETL application - How to write ETL job - How to package your
code and dependencies - What are some coding and naming conventions&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Przemek Chrabka</dc:creator><pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-12-12:pydata-warsaw-2019/how-to-structure-pyspark-application.html</guid></item></channel></rss>