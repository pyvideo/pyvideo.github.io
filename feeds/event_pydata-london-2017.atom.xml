<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-london-2017.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-05-07T00:00:00+00:00</updated><entry><title>An Algorithm of Style</title><link href="https://pyvideo.org/pydata-london-2017/an-algorithm-of-style.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Ed Snelson</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/an-algorithm-of-style.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
How do eight stylists style half a million clients? At Thread we use machine-learning to help our stylists make personalised clothing recommendations to our users. In this talk I will give some insight into how we blend our stylists' expertise together with ML models that are continually learning from user feedback and sales data.&lt;/p&gt;
&lt;p&gt;Abstract
I will go into some detail about the types of algorithm and recommender system that have worked well, as well as those that have not! Interestingly we've found that some standard stalwarts of recommender systems have not thus far been a good fit for our data. I will describe some of the common pitfalls of deploying machine-learning systems in production (some of which we've avoided!), and emphasise the importance of using simple debuggable models when a stylist wants to know why The AI thinks it's a good idea to put a horizontal striped t-shirt on a larger gent.&lt;/p&gt;
</summary></entry><entry><title>Analyzing 3D objects with power of Deep Learning and Cython</title><link href="https://pyvideo.org/pydata-london-2017/analyzing-3d-objects-with-power-of-deep-learning-and-cython.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Alexandr Notchenko</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/analyzing-3d-objects-with-power-of-deep-learning-and-cython.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
Deep Learning taken world by storm in recent years, and most of the time it's powered by Python language. Working with 3D data is computationally demanding even by most powerful GPUs. A lot of times 3D data is in sparse form, or can be turned without loosing too much of it's usefulness. We combined high performance of CUDA library with ease of use and power of Python by getting good with Cython.&lt;/p&gt;
&lt;p&gt;Abstract
We live in great time for development of Machine Learning algorithms. There is an abundance of ways to implement models, a lot of them are in Python or have a python API. Python is a great way to implement high level APIs and connect your code to other parts of data processing. Learning how to write in Theano or Tensorflow can be very helpful for most of the people facing problems with complex data. But for a niche area like Deep Learning for sparse 3D data there was no solutions. Our python module enables creation of deep neural networks to process sparse data quickly using combination of Python and fast C++/CUDA code underneath. In my talk I'll explain how to connect high performance code with practical user level abstractions using Cython, as it was done in our project.&lt;/p&gt;
&lt;p&gt;You can see code in a github repository or checkout our paper.
&lt;a class="reference external" href="https://github.com/gangiman/PySparseConvNet"&gt;https://github.com/gangiman/PySparseConvNet&lt;/a&gt;
&lt;a class="reference external" href="https://arxiv.org/abs/1611.09159"&gt;https://arxiv.org/abs/1611.09159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary></entry><entry><title>Astrophysics to data Science: how the Milky Way is like my company.</title><link href="https://pyvideo.org/pydata-london-2017/astrophysics-to-data-science-how-the-milky-way-is-like-my-company.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Kathryn Harris</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/astrophysics-to-data-science-how-the-milky-way-is-like-my-company.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
The Milky Way is your average galaxy. There are thousands of them out there. Just like a medium size company. Not too young, not too old but has its own set of unique challenges no one ever mentioned. Hard earned lessons of moving from Academia to data science in industry in your average sized company.&lt;/p&gt;
&lt;p&gt;Abstract
What happens when you move to a company which is no longer a start-up but still not established? When I moved to Data science, I choose a company which had been around for 10 years, as a small-medium sized Enterprise (SME) but the data science team was brand new (and both of us had moved from academia for this, our first data science job). You don't have the excitement of a start-up: everything is new, let's try that, you can build the structures from the ground up. You don't have the structures of a bigger older company: there are things in place for you to work with, you know what's expected. Instead you have the best and worst of both worlds: starting a data team but no one understands how to use data. You get to put those structures in place, but have to shoe horn them into everyone else's. But middle sized companies are like our Milky Way in many ways: numerous, survive in many different environments, and you find them everywhere! So what can you use from academia? Some things (like being able to communicate with a wide range of people) will be exceptionally useful. In a SME you will be expected to do this straight away, more than you might in a large company and with more people than you would find in a start-up. But you have a great mixture of guidance (like from a large company) and leeway to invent (like from a start-up) on projects but you need to prove your worth quickly, which means creating useful products. So you need to break that habit of spending hours researching and not getting anything &amp;quot;done&amp;quot;. So here are some lessons learnt about the differences and how to use your academic experience to get the best of both worlds, not the worst&lt;/p&gt;
</summary></entry><entry><title>Bayesian Deep Learning with Edward (and a trick using Dropout)</title><link href="https://pyvideo.org/pydata-london-2017/bayesian-deep-learning-with-edward-and-a-trick-using-dropout.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Andrew Rowan</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/bayesian-deep-learning-with-edward-and-a-trick-using-dropout.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Bayesian neural networks have seen a resurgence of interest as a way of generating model uncertainty estimates. I use Edward, a new probabilistic programming framework extending Python and TensorFlow, for inference on deep neural nets for several benchmark data sets. This is compared with dropout training, which has recently been shown to be formally equivalent to approximate Bayesian inference.&lt;/p&gt;
&lt;p&gt;Abstract
Deep learning methods represent the state-of-the-art for many applications such as speech recognition, computer vision and natural language processing. Conventional approaches generate point estimates of deep neural network weights and hence make predictions that can be overconfident since they do not account well for uncertainty in model parameters. However, having some means of quantifying the uncertainty of our predictions is often a critical requirement in fields such as medicine, engineering and finance. One natural response is to consider Bayesian methods, which offer a principled way of estimating predictive uncertainty while also showing robustness to overfitting.&lt;/p&gt;
&lt;p&gt;Bayesian neural networks have a long history. Exact Bayesian inference on network weights is generally intractable and much work in the 1990s focused on variational and Monte Carlo based approximations [1-3]. However, these suffered from a lack of scalability for modern applications. Recently the field has seen a resurgence of interest, with the aim of constructing practical, scalable techniques for approximate Bayesian inference on more complex models, deep architectures and larger data sets [4-10].&lt;/p&gt;
&lt;p&gt;Edward is a new, Turing-complete probabilistic programming language built on Python [11]. Probabilistic programming frameworks typically face a trade-off between the range of models that can be expressed and the efficiency of inference engines. Edward can leverage graph frameworks such as TensorFlow to enable fast distributed training, parallelism, vectorisation, and GPU support, while also allowing composition of both models and inference methods for a greater degree of flexibility.&lt;/p&gt;
&lt;p&gt;In this talk I will give a brief overview of developments in Bayesian deep learning and demonstrate results of Bayesian inference on deep architectures implemented in Edward for a range of publicly available data sets. Dropout is an empirical technique which has been very successfully applied to reduce overfitting in deep learning models [12]. Recent work by Gal and Ghahramani [13] has demonstrated a surprising formal equivalence between dropout and approximate Bayesian inference in neural networks. I will compare some results of inference via the machinery of Edward with model averaging over neural nets with dropout training.&lt;/p&gt;
</summary></entry><entry><title>Building robust machine learning systems</title><link href="https://pyvideo.org/pydata-london-2017/building-robust-machine-learning-systems.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Stephen Whitworth</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/building-robust-machine-learning-systems.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description
With the growth of AI, ever growing parts of products we build are changing from the deterministic to the probabilistic. The accuracy of machine learning applications can deteriorate in the wild without strategies for testing, monitoring and introspection. You'll leave this talk knowing how to combine the best of software engineering and machine learning to build robust machine learning products.&lt;/p&gt;
&lt;p&gt;Abstract
As machine learning becomes more prevalent, ever growing parts of the systems we build are changing from the deterministic to the probabilistic. The accuracy of machine learning applications can quickly deteriorate in the wild without strategies for testing models, instrumenting their behaviour and the ability to introspect and debug incorrect predictions.&lt;/p&gt;
&lt;p&gt;This session will take an applied view from my experience of building production machine learning infrastructure at Ravelin. You’ll learn useful practices and tips to help ensure your machine learning systems are robust. We’ll go into:&lt;/p&gt;
&lt;p&gt;Labels and Data - can you trust it? Can you infer them?
Testing - how do you ensure your model is doing the basics, up to the more complicated examples?
Auditing and versioning - what's the provenance of your model? What data was it trained on? With which hyper parameters? Can you reproduce it?
Debugging and introspection when deployed - when you make an awful prediction, can you figure out why that happened and prevent it happening again?
And more, with the aim of helping you sleep a little better at night knowing your model is out there in the wild.&lt;/p&gt;
</summary></entry><entry><title>Dimension Reduction and Extracting Topics - A Gentle Introduction</title><link href="https://pyvideo.org/pydata-london-2017/dimension-reduction-and-extracting-topics-a-gentle-introduction.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Tariq Rashid</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/dimension-reduction-and-extracting-topics-a-gentle-introduction.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData 2017&lt;/p&gt;
&lt;p&gt;Description
Text mining has many powerful methods for unlocking insights into the messy, ambiguous, but interesting text created by people.&lt;/p&gt;
&lt;p&gt;Singular value decomposition (SVD) is a useful method for reducing the many dimensions of text data, and distill out key themes in that text - called topic modelling or latent semantic analysis.&lt;/p&gt;
&lt;p&gt;This talk for beginners will gently explain SVD and how to use it.&lt;/p&gt;
&lt;p&gt;Abstract
Text mining and natural language processing are hugely powerful fields that can unlock insights into the vast amounts of human knowledge, creativity and drivel (!) for automated computing. Examples include the fun of highlighting trends in internet chatter through to more serious analysis of finding patterns and links in leaked data sets of public interest.&lt;/p&gt;
&lt;p&gt;One key tool is to reduce the many dimensions of text data, and distill out the key themes in that text. People call this topic modelling, latent semantic analysis, and a few other names too. The powerful method at the heart of this is called singular value decomposition (SVD).&lt;/p&gt;
&lt;p&gt;This talk will gently introduce singular valued decomposition (SVD), explaining the mathematics in an accessible manner, and demonstrate how it can be used, using the Chilcot Iraq Report as an example dataset.&lt;/p&gt;
&lt;p&gt;Example code, notebooks and data sets are public on GitHub, and there is a blog for more discussion of this, and other text mining ideas &lt;a class="reference external" href="http://makeyourowntextminingtoolkit.blogspot.co.uk"&gt;http://makeyourowntextminingtoolkit.blogspot.co.uk&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Efficient and portable DataFrame storage with Apache Parquet</title><link href="https://pyvideo.org/pydata-london-2017/efficient-and-portable-dataframe-storage-with-apache-parquet.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/efficient-and-portable-dataframe-storage-with-apache-parquet.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
Apache Parquet is the most used columnar data format in the big data processing space and recently gained Pandas support. It leverages various techniques to store data in a CPU and I/O efficient way and provides capabilities to push-down queries to the I/O layer. In this talk, it is shown how to use it in Python, detail its structure and present the portable usage with other tools.&lt;/p&gt;
&lt;p&gt;Abstract
Since its creation in 2013, Apache Parquet has risen to be the most widely used binary columnar storage format in the big data processing space. While supporting basic attributes of a columnar format like reading a subset of columns, it also leverages techniques to store the data efficiently while providing fast access. In addition the format is structured in such a fashion that when supplied to a query engine, Parquet provides indexing hints and statistics to quickly skip over chunks of irrelevant data.&lt;/p&gt;
&lt;p&gt;In recent months, efficient implementations to load and store Parquet files in Python became available, bringing the efficiency of the format to Pandas DataFrames. While this provides a new option to store DataFrames, it especially allows us to share data between Pandas and a lot of other popular systems like Apache Spark or Apache Impala. In this talk we will show the improvements that Parquet bring performance-wise but also will highlight important aspects of the format that make it portable and efficient for queries on large amount of data. As not all features are yet available in Python, an overview of the upcoming Python-specific improvements and how the Parquet format will be extended in general is given at the end of the talk&lt;/p&gt;
</summary></entry><entry><title>Extracting Insight From A Muslim Marriage App</title><link href="https://pyvideo.org/pydata-london-2017/extracting-insight-from-a-muslim-marriage-app.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Laila Alabidi</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/extracting-insight-from-a-muslim-marriage-app.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
I use various Python libraries and methods from Graph theory and Information Retrieval to garner insight into the social dynamics of people searching for love and marriage on the Muslim marriage app 'MuzMatch'.&lt;/p&gt;
&lt;p&gt;Abstract
Dating apps have become the norm in modern society, with companies like Tinder, Bumble, Happn etc. boasting millions of users. OKCupid, an early entry into the online world of dating, were one of the leading companies to take advantage of the data at their fingertips and conducted several interesting experiments and subsequent data analysis which were published on their blog and subsequently in the book 'Cataclysm' by their lead data scientist Christian Rudder.&lt;/p&gt;
&lt;p&gt;I was curious to see how people interact when searching for love online when they are part of a (1) organized religion and (2) members of a minority. I teamed up with Shahzad Younus, founder of the muslim marriage app 'MuzMatch' to gather highly anonymized data about their users and extract insight into how people behave and what is important to them. For example, as an app targeted at members of a specific religious community, does levels of religiosity really matter? Does religion trump ethnicity?&lt;/p&gt;
&lt;p&gt;I hope to answer these questions and more&lt;/p&gt;
</summary></entry><entry><title>Forecasting social inequality using agent-based modelling</title><link href="https://pyvideo.org/pydata-london-2017/forecasting-social-inequality-using-agent-based-modelling.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>James Allen</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/forecasting-social-inequality-using-agent-based-modelling.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 201
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
Agent-based models, which simulate an entire population of interacting people or other agents, offer a promising approach to assessing the future social impact of government policy. I will describe one such case, looking at a proposed change in how wealth is inherited.&lt;/p&gt;
&lt;p&gt;Abstract
How can we assess the future impact of changes to government policy? One tool that is gaining in popularity is agent-based modelling, in which a population of agents - typically representing individual people - is simulated together with an environment they can interact with. The agents' actions, which can in turn influence the other agents and the environment, are governed by a predetermined set of rules or heuristics. After much use in fields such as ecology and epidemiology these models are gaining increasing recognition in economics and policy, and were recently featured in the Bank of England's Quarterly Bulletin1. By running parallel simulations with modifications to the environment or the behavioural rules, a policy-maker can compare the likely outcomes of different options available to them.&lt;/p&gt;
&lt;p&gt;We have used an agent-based model to assess the outcome of a change to the way in which wealth is inherited within families. The proposed change favours putting wealth into trust funds for grandchildren and great-grandchildren, instead of passing it directly to children. A similar proposal covering only houses was made last year by Gavin Barwell, the housing minister, but was not taken forward as government policy.&lt;/p&gt;
&lt;p&gt;We developed a simulation of the demographic makeup of the UK, based on data from the census and the ONS, and inserted the different inheritance methods into it. We could then see what the long-term outcome of each scenario would be, in terms of the distribution of wealth and level of inequality in the country, allowing a quantitative assessment of its impact on individuals and society as a whole. The model also provides a base for developing assessments of more complex policies and interventions&lt;/p&gt;
</summary></entry><entry><title>Interactively Analyse 100GB of Data using Spark, Amazon EMR and Zeppelin</title><link href="https://pyvideo.org/pydata-london-2017/interactively-analyse-100gb-of-data-using-spark-amazon-emr-and-zeppelin.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Raoul-Gabriel Urma</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/interactively-analyse-100gb-of-data-using-spark-amazon-emr-and-zeppelin.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
In this highly interactive session, you will learn how to leverage Apache Spark, Amazon Elastic Map Reduce (EMR) and Apache Zeppelin to rapidly mine a large real-world data set. You will learn how to apply common Spark patterns to extract insights as well as learn useful performance and monitoring tips.&lt;/p&gt;
&lt;p&gt;Abstract
You may have been hearing a lot of buzz around Big Data, Apache Spark, Amazon Elastic Map Reduce (EMR) and Apache Zeppelin. What’s the fuss about, and how can you benefit from these state of the art technologies?&lt;/p&gt;
&lt;p&gt;In this highly interactive session, you will learn how to leverage Spark to rapidly mine a large real-world data set. We will conduct the analysis live entirely using an iPython Notebook to show you how easy it can be to get to grips with these technologies.&lt;/p&gt;
&lt;p&gt;In the first part of the session, we will use a sample of data from the Open Library dataset, and you will learn how to apply common Spark patterns to extract insights and aggregate data. In the second part of the session, you will see how to leverage Spark on Amazon EMR to scale your data processing queries over a cluster of machines and interactively analyse a large data set (100GB) with a Zeppelin Notebook. Along the way you will learn gotchas as well as useful performance and monitoring tips&lt;/p&gt;
</summary></entry><entry><title>Is having dementia linked to where you live?</title><link href="https://pyvideo.org/pydata-london-2017/is-having-dementia-linked-to-where-you-live.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Frank Kelly</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/is-having-dementia-linked-to-where-you-live.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
In this talk we will explore the results from a short, collaborative ‘hack’ project with a focus on using Python machine learning tools and open data to find evidence linking increased diesel engine and tyre particulate emissions (PM10, PM2.5 and NO2) in certain residential areas with greater likelihood of diagnosis of dementia amongst the residents of that area.&lt;/p&gt;
&lt;p&gt;Abstract
Overview
Alzheimer’s is a neurodegenerative disease that currently affects over 55 million people worldwide and this is set to increase with trends in population growth and demographics. Meanwhile, particulate emission levels in large cities across Europe and the world have been under scrutiny, as diesel emissions related to a huge uptick in diesel vehicle purchases (due to governments' push to reduce CO2 emission levels) are found to have caused many medical problems in the area of breathing difficulties.&lt;/p&gt;
&lt;p&gt;Breathing function and the lungs may not be the only parts affected; there is now more than one scientific study (see link [1] for an example below) that has found a link between higher incidence of dementia (in particular Alzheimer’s) in those who have lived in highly polluted urban areas for large periods of their lives.&lt;/p&gt;
&lt;p&gt;This talk is about a short, collaborative ‘hack’ project with a focus on using Python machine learning tools and open data to test this hypothesis and look for evidence either confirming or denying the link between increased diesel engine and tyre particulate emissions (PM10, PM2.5 and NO2) in a given residential area with a greater likelihood of diagnosis of dementia amongst the residents of that area.&lt;/p&gt;
&lt;p&gt;The idea behind this endeavour is that this ‘pilot’ study might enable a funded venture to take root that, for example, increases awareness of the true impact of high diesel car pollution in dense conurbations.&lt;/p&gt;
&lt;p&gt;Questions asked
Can we visualize the likelihood of incidence of dementia per person on a heat map for a city or for the UK? And similarly visualize annual particulate exposure per resident? Do they look similar?
Do various sources of data, when placed under the magnifying glass of data science corroborate the evidence gathered by recent studies?
Can a classifier reasonably determine the likelihood of your contracting dementia based on where you have lived mostly during your life?
Where should you live and work that gives you a lower chance of contracting Alzheimer’s later in life?
How can the findings be used to push city officials to improve city air quality? How can we raise awareness?
Challenges encountered on the way
How to interpolate, in order to cover 'gaps' in-between emission monitoring stations, incorporating additional data sources (for example road network structure and traffic intensity levels)?
Sensitive medical data; how to merge open medical datasets securely without revealing personal information?
Which types of machine learning models are best suited to this problem?
Approach
Starting at a high level, it is possible to determine if there is a higher incidence of dementia cases linked with an increase in airborne pollutants (diesel source):&lt;/p&gt;
&lt;p&gt;At a country level (e.g. for the Netherlands or the UK)
At a city level (e.g. for Bristol, Eindhoven and / or London)
At a borough level (e.g. for Camden, London)
At a street level (by postcode)
At individual sufferer level (with de-personalized datasets, based on 1,000 blog posters)
Other research inputs (articles and papers)
[1] &lt;a class="reference external" href="http://www.sciencemag.org/news/2017/01/brain-pollution-evidence-builds-dirty-air-causes-alzheimer-s-dementia"&gt;http://www.sciencemag.org/news/2017/01/brain-pollution-evidence-builds-dirty-air-causes-alzheimer-s-dementia&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>KEYNOTE: Data for good: Lessons from the frontline</title><link href="https://pyvideo.org/pydata-london-2017/keynote-data-for-good-lessons-from-the-frontline.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Emma Deraze</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/keynote-data-for-good-lessons-from-the-frontline.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
DataKind UK share their experiences and critical success factors from 3 years of learning-by-doing in using data in the service of humanity.&lt;/p&gt;
&lt;p&gt;Abstract
Most of us will know data for business as the art and science of using proprietary data to solve high value commercial problems. But how can those same techniques be used to solve societal problems as well? From tackling money laundering through network statistics to establishing the true scale of youth homelessness through Bayesian inference, DataKind UK will share examples of how they’ve partnered with charities to use data for good. Reflecting on what has and hasn’t worked in three years, they will share the critical success factors they’ve identified along the way.&lt;/p&gt;
</summary></entry><entry><title>Leveraging recommender systems to personalise search results</title><link href="https://pyvideo.org/pydata-london-2017/leveraging-recommender-systems-to-personalise-search-results.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Soraya Hausl</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/leveraging-recommender-systems-to-personalise-search-results.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
This talk discuses an approach to personalise search results by leveraging techniques of recommender systems .We use Pyspark to set up the data and calculate recommendation and preference elements and Elasticsearch as a search engine&lt;/p&gt;
&lt;p&gt;Abstract
These days a lot of companies are building recommendations engines. The techniques for doing so are widely known and open source technology is accessible. We were experimenting to investigate how we can leverage our recommendations engine to extend personalisation to search results. Besides surfacing items that are relevant based on the search term our approach aims to increase relevancy for each user by considering their personal preferences. We use Pyspark to set up the data and calculate recommendation and preference elements and Elasticsearch as a search engine where we integrate our recommendations approach. We are looking into various ways of how to incorporate customer preferences such as item similarities, matrix factorisation output and preference inference&lt;/p&gt;
</summary></entry><entry><title>Lightning Talks - Sunday</title><link href="https://pyvideo.org/pydata-london-2017/lightning-talks-sunday.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/lightning-talks-sunday.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary><category term="lightning talks"></category></entry><entry><title>"Lights, camera, AI!" - Automated sports videography</title><link href="https://pyvideo.org/pydata-london-2017/lights-camera-ai-automated-sports-videography.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Zack Akil</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/lights-camera-ai-automated-sports-videography.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Amateur sports video quality usually depends on how good your friends are with their camera-phone. In this talk I walk through the stages of how I designed and developed a machine vision model for tracking where the action is happening in a rugby match, and how I made it lightweight enough to run on a Raspberry Pi in real-time .&lt;/p&gt;
&lt;p&gt;Abstract
Inspired by the shaky camera work of friends and parents alike in school sports events; I've build a Raspberry Pi controlled robot that automatically points a camera at where the action is happening in a rugby match.&lt;/p&gt;
&lt;p&gt;The talk will discuss the stages of developing an embedded machine vision model. Starting with a simple heuristic approach in order to collect the training data. Then how you can compress the feature space in order to reduce the computational load on the Raspberry Pi. Along with that, how using python multi-processing can help in sharing the load of the machine vision work and servo control.&lt;/p&gt;
&lt;p&gt;After the talk people should know a little about embedding machine learning using Raspberry Pi's as well as some insight in to SimpleCV (machine vision library for python) and multi-processing with python.&lt;/p&gt;
</summary></entry><entry><title>Next generation of word embeddings in Gensim</title><link href="https://pyvideo.org/pydata-london-2017/next-generation-of-word-embeddings-in-gensim.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Lev Konstantinovskiy</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/next-generation-of-word-embeddings-in-gensim.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
There are many ways to find similar words/docs with an open-source Natural Language processing library Gensim that I maintain. I will give an overview of modern word embeddings like Google's Word2vec, Facebook's FastText, GloVe, WordRank, VarEmbed and discuss what business tasks fit them best.&lt;/p&gt;
&lt;p&gt;Abstract
What is the most similar word to &amp;quot;king&amp;quot;? It depends on what you mean by similar. &amp;quot;King&amp;quot; can be interchanged with &amp;quot;Canute&amp;quot;, but it's attribute is &amp;quot;crown&amp;quot;. We will discuss how to achieve these two kinds of similarity from word embeddings. Also touch on how to deal with the common issues of rare, frequent and out of vocabulary words&lt;/p&gt;
</summary></entry><entry><title>Outlier detection methods for detecting cheaters in mobile gaming</title><link href="https://pyvideo.org/pydata-london-2017/outlier-detection-methods-for-detecting-cheaters-in-mobile-gaming.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Andrew Patterson</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/outlier-detection-methods-for-detecting-cheaters-in-mobile-gaming.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
By coming to this tutorial you will learn:&lt;/p&gt;
&lt;p&gt;What event-driven Python Frameworks are and why they are useful
How to overcome any fears you may have about event-driven Python programming
Enough examples to be able to consider projects like Dask/Distributed and Tornado for your next project
Abstract
At some point in a career in data science, software development or data engineering you will be looking to develop a piece of code that runs alongside another piece of code. For example, you might want to:&lt;/p&gt;
&lt;p&gt;Use all of your processors when running a script
Consume social media data as it is created
Create a super simple task scheduler without an ugly while True loop
Understand how Jupyter notebooks work
Write to a database and move on to the next task without waiting for confirmation
Call a function and throw an exception if it takes too long
There are numerous ways of achieving such concurrency. If creating a project to run on one or a few machines which needs good exception handling then I find tools like Twisted, Tornado and Dask/Distributed ideal for the above tasks.&lt;/p&gt;
&lt;p&gt;As a python developer I have always tended to work with Celery and felt intimidated by the different flow control when using async tools like Twisted and Tornado. In this tutorial I will walk through the examples above and talk about how I went from Async novice to Dask/Distributed contributor in 6 months. The tutorial will concentrate mainly on practical use cases. Along the way we will encounter some bumps in the road which will hopefully aid attendees' understanding of the basic dos and don'ts around these tools.&lt;/p&gt;
&lt;p&gt;By coming to this tutorial you will learn:&lt;/p&gt;
&lt;p&gt;What event-driven Python Frameworks are and why they are useful
How to overcome any fears you may have about event-driven Python programming
Enough examples to be able to consider projects like Dask/Distributed and Tornado for your next project&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary></entry><entry><title>Ranking hotel images using deep learning</title><link href="https://pyvideo.org/pydata-london-2017/ranking-hotel-images-using-deep-learning.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Nuno Castro</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/ranking-hotel-images-using-deep-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description
In this talk we will cover how Expedia has been ranking hotel images using deep learning techniques in Python.&lt;/p&gt;
&lt;p&gt;Abstract
Attractive hotel images can influence the hotels that customers book online. However,selecting the best images is not a trivial problem, as there are tens of million images,and each customer can have their own image preferences (e.g. family vs. business customer). Displaying the best images for each hotel would have a significant impact on conversion. In this talk we will cover how Expedia has been ranking hotel images using deep learning techniques in Python.&lt;/p&gt;
</summary></entry><entry><title>Recommender systems with Tensorflow</title><link href="https://pyvideo.org/pydata-london-2017/recommender-systems-with-tensorflow.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Guillaume Allain</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/recommender-systems-with-tensorflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
This talk will demonstrate how to harness a deep-learning framework such as Tensorflow, together with the usual suspects such as Pandas and Numpy, to implement recommendation models for news and classified ads.&lt;/p&gt;
&lt;p&gt;Abstract
Recommender systems are used across the digital industry to model users' preferences and increase engagement. Popularised by the seminal Netflix prize, collaborative filtering techniques such as matrix factorisation are still widely used, with modern variants using a mix of meta-data and interaction data in order to deal with new users and items. We will demonstrate how to implement a variety of models using Tensorflow, from simple bi-linear models expressed as shallow neural nets to the latest deep incarnations of Amazon DSSTNE and Youtube neural networks. We will also use TensorBoard and particularly the embedding projector to visualise the latent space for items and metadata.&lt;/p&gt;
</summary></entry><entry><title>Smelly London: visualising historical smells through text-mining, geo-referencing and mapping.</title><link href="https://pyvideo.org/pydata-london-2017/smelly-london-visualising-historical-smells-through-text-mining-geo-referencing-and-mapping.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Deborah Leem</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/smelly-london-visualising-historical-smells-through-text-mining-geo-referencing-and-mapping.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Smelly London project brings together historical data with modern digitisation and visualisation to give us a unique, revealing and visceral glimpse into a London of the past and what it tells us about London today. This is a collaborative, interdisciplinary project to demonstrate the capabilities of innovative text mining tools we design to facilitate new kinds of humanities research.&lt;/p&gt;
</summary></entry><entry><title>WTF am I doing? An introduction to NLP and ANN's</title><link href="https://pyvideo.org/pydata-london-2017/wtf-am-i-doing-an-introduction-to-nlp-and-anns.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Jeff Abrahamson</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/wtf-am-i-doing-an-introduction-to-nlp-and-anns.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
This talk will be a playful but serious introduction to natural language processing and image processing with (artificial) neural networks.&lt;/p&gt;
&lt;p&gt;Abstract
Collecting data is more fun when it's about myself: I'm an expert! So I program my computer to spy on me. And then I play with the data, the better to understand both ML techniques and what I've been up to. In the limit, I try not only to understand how I spend my time but also to predict how I am about to spend my time.&lt;/p&gt;
&lt;p&gt;Consider the active window on my computer: the window with which I am currently interacting. Watching the window name creates a sequence of window titles and a great NLP playground. (&amp;quot;Natural language&amp;quot; is here a discovered irony.) Snapshotting the contents of the window in turn creates long sequences of images. To save space, I reduce the images to postage stamp size, and yet it turns out I can learn some very interesting things nonetheless. Some reasonably simple neural networks are enough to discern how I spend my time, when I work on which projects. Indeed, well beyond providing a great playground for learning about ML techniques, the project also provides a good reminder (warning?) about how much we can learn from what appear to be relatively scant data. Of course, the links in the final slides will share all the code to help you get started with projects of your own&lt;/p&gt;
</summary></entry><entry><title>A beginner's guide to data analysis in cosmology using Jupyter Notebook</title><link href="https://pyvideo.org/pydata-london-2017/a-beginners-guide-to-data-analysis-in-cosmology-using-jupyter-notebook.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Dr Caroline Clark</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/a-beginners-guide-to-data-analysis-in-cosmology-using-jupyter-notebook.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Current cosmology experiments face exciting computational challenges in statistics and machine learning, with large amounts of data to process. In this beginner's session, we will use Jupyter Notebook to visualise and analyse real cosmological data, using easy to implement code examples from well known python packages such as AstroML, scipy, healpy, numpy, pymc, scikit-learn and matplotlib.&lt;/p&gt;
&lt;p&gt;Abstract
As cosmology has entered the era of precision measurement, academics face the exciting challenges of analysing large datasets, many of these common to other areas of Big Data. In recent years Python has evolved as a standard tool in astronomy and cosmology due to the availability of open source statistical analysis and machine learning packages that allow the development of robust data analysis pipelines and powerful visualisations. In this session, I’d like to demonstrate how a beginner to the field of cosmology can use open data along with the Jupyter Notebook to visualise and analyse real cosmological data, using easy to implement code examples from well known python packages such as AstroML, scipy, healpy, numpy, pymc, scikit-learn and matplotlib.&lt;/p&gt;
</summary></entry><entry><title>❤ Analyzing the ElectroCardioGram (ECG) and classifying what's healthy and what's not.</title><link href="https://pyvideo.org/pydata-london-2017/analyzing-the-electrocardiogram-ecg-and-classifying-whats-healthy-and-whats-not.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Dr. Emlyn Clay</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/analyzing-the-electrocardiogram-ecg-and-classifying-whats-healthy-and-whats-not.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
The ElectroCardioGram (ECG) is the electrical activity of your heart. By recording it, classifying fiducial markers and analysing these features we can make assessments about the healthy state of the heart, diagnose certain diseases of the heart and predict whether a subject will go on to develop certain diseases. Python and the scientific stack provide all the tools you need.&lt;/p&gt;
&lt;p&gt;Abstract
The ElectroCardioGram (ECG) is a periodic waveform that describes the action of heart as it moves through 3 electromechanical phases:&lt;/p&gt;
&lt;p&gt;Depolarization and contraction of the atria
Depolarization and contraction of the ventricles
Repolarization of the ventricles and atria
It is an enormous area of study and the ECG is tractable and effective way of detecting healthy sinus rythmn, diagnosing arrthymia and potentially predicting the decline of the heart from a healthy state to a disease state.&lt;/p&gt;
&lt;p&gt;Python and the scientific stack offers everything a researcher or a hobbyist would need to conduct sophisticated analysis and in this talk we'll describe how to store and load the ECG, process the signal, classify fiducial markers and make interpretations about the state of the heart.&lt;/p&gt;
&lt;p&gt;The talk will be presented in an ipython notebook and involve h5py for reading ECG data in from disk as well as using the python-wfdb to get data from the Physionet repositories. scipy.signal for smoothing, processing and classifying parts of the ECG as well as peakutils to classify peaks. matplotlib and seaborn will be used for visualisation and statsmodels will be used to describe the data. This will ultimately generate features that can be used as the basis or an ML model.&lt;/p&gt;
</summary></entry><entry><title>Bayesian optimisation with scikit-learn</title><link href="https://pyvideo.org/pydata-london-2017/bayesian-optimisation-with-scikit-learn.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Thomas Huijskens</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/bayesian-optimisation-with-scikit-learn.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Join Full Fact, the UK's independent factchecking charity, to discuss how they plan to make factchecking dramatically more effective with technology that exists now.&lt;/p&gt;
&lt;p&gt;Abstract
Factchecking is just one solution to the multifaceted problem of fake news. The factcheckers fight is valiant but how can they keep up in such tumultuous times? Join Full Fact, the UK's independent factchecking charity, to discuss how they plan to make factchecking dramatically more effective with technology that exists now.&lt;/p&gt;
</summary></entry><entry><title>Diversity and Data: Cases in the Music Industry</title><link href="https://pyvideo.org/pydata-london-2017/diversity-and-data-cases-in-the-music-industry.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Delger Enkhbayar</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/diversity-and-data-cases-in-the-music-industry.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description
With lower barriers to entry and rise of streaming, the volume of new songs and artists is exponentially increasing. Given these trends in the music industry, I would like to look at a few challenging problems I encountered at PPL. Solving these problems will improve the efficiency with which we churn through millions of observations and therefore minimise any delay in allocations of royalties.&lt;/p&gt;
&lt;p&gt;Abstract
I would like to present a few challenging questions that I have encountered at PPL:&lt;/p&gt;
&lt;p&gt;Rolling up music and musicians: for instance, Beyonce, Bey, Queen Bee, Beyonce feat. Jay-Z, Beyoncé feat jayz all should ideally come under the parent of Beyoncé. A lot has to do with normalising the strings. But an additional layer, with large potential for improving the predictive power, is introducing aliases and leveraging PPL's as well as external third-party data to identify potential aliases.&lt;/p&gt;
&lt;p&gt;Identifying turning-point collaborations: which partnerships among musicians are most likely to change their careers significantly? This involves looking at: firstly, whether music collaborations (including band memberships) are changing in nature. Secondly, asking how to identify the value added of collaborators, i.e. the effect of certain collaborations on the artist's profile, recognition and success.&lt;/p&gt;
&lt;p&gt;How to identify outliers at the artist level? The distribution of many variables of interest (band memberships, active years etc.) is highly heterogeneous across artists, and contain a few suspicious outliers that are hard to identify from aggregate distributions.&lt;/p&gt;
</summary></entry><entry><title>High-Performance Distributed Tensorflow: Request Batching and Model Post-Processing Optimizations</title><link href="https://pyvideo.org/pydata-london-2017/high-performance-distributed-tensorflow-request-batching-and-model-post-processing-optimizations.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Chris Fregly</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/high-performance-distributed-tensorflow-request-batching-and-model-post-processing-optimizations.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Title High-Performance Distributed Tensorflow: Request Batching and Model Post-Processing Optimizations&lt;/p&gt;
&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
In this completely demo-based talk, Chris will demonstrate various techniques to post-process and optimize trained Tensorflow AI models to reduce deployment size and increase prediction performance.&lt;/p&gt;
&lt;p&gt;Abstract
In this completely demo-based talk, Chris will demonstrate various techniques to post-process and optimize trained Tensorflow AI models to reduce deployment size and increase prediction performance.&lt;/p&gt;
&lt;p&gt;First, we'll use various techniques such as 8-bit quantization, weight-rounding, and batch-normalization folding, we will simplify the path of forward propagation and prediction.&lt;/p&gt;
&lt;p&gt;Next, we'll loadtest and compare our optimized and unoptimized models - in addition to enabling and disabling request batching.&lt;/p&gt;
&lt;p&gt;Last, we'll dive deep into Google's Tensorflow Graph Transform Tool to build custom model optimization functions.&lt;/p&gt;
</summary></entry><entry><title>How distributed representations make chatbots work (at least a bit)</title><link href="https://pyvideo.org/pydata-london-2017/how-distributed-representations-make-chatbots-work-at-least-a-bit.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Nils Hammerla</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/how-distributed-representations-make-chatbots-work-at-least-a-bit.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Chatbots are all the hype right now, like it or not. In this talk I want to take a look under the hood and show you how simple it is today to incorporate sophisticated language understanding in your applications. The tool of choice are distributed representations, also known as word vectors, which allow us to answer the most crucial question: which of these words mean similar things?&lt;/p&gt;
&lt;p&gt;Abstract
It is hard to go anywhere on the web these days without encountering chatbots and other natural language interfaces. But how do these bots actually understand what you say? It turns out it can be boiled down to a simple recipe: you need to know which words mean similar things! This sounds straight-forward, but efficient ways of doing this, namely distributed representations, were only just discovered in the past few years of machine learning research. They have immense potential and we are only beginning to realise what we can do with them.&lt;/p&gt;
&lt;p&gt;In this talk I want to outline how distributed representations are used at babylon health, and how everyone can incorporate sophisticated language understanding into their applications with just a few lines of python. Furthermore I will give a glimpse of the research we are doing, some of which we just published in a paper at ICLR.&lt;/p&gt;
</summary></entry><entry><title>Journeys through JuPyteR</title><link href="https://pyvideo.org/pydata-london-2017/journeys-through-jupyter.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Alex Glaser</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/journeys-through-jupyter.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
There have been many talks about Jupyter notebooks but rather than concentrating on the notebook this talk will look at three data science languages available within it: Jula, Python and R. This talk will be aimed at beginner coders where we will write code in all three languages and discuss the differences between them.&lt;/p&gt;
&lt;p&gt;Abstract
In the last few years notebooks have become de rigeur within data science, and the ability to mix &amp;amp; match different languages is one of many exciting elements within them. Using as a motto ‘find the best tool for the job’ we will take the Python code from Tariq Rashid’s book “Make your own Neural Network” and, using the MNIST data set, rewrite it in R and Julia. Of particular interest will be the idiosyncrasies that we see within each language as well as the readability and speed of each.&lt;/p&gt;
</summary></entry><entry><title>Julia: A Fresh Approach to Machine Learning</title><link href="https://pyvideo.org/pydata-london-2017/julia-a-fresh-approach-to-machine-learning.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Mike Innes</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/julia-a-fresh-approach-to-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Julia's well-known combination of ease-of-use, performance and powerful features make it uniquely suited to the toughest machine learning problems. We'll illustrate how Julia can accelerate your current workflow, show you the groups running intelligent Julia code in production, and discuss our plans for the future.&lt;/p&gt;
&lt;p&gt;Abstract
Existing machine learning frameworks are complex &amp;quot;black boxes&amp;quot; which are designed to work at a certain level of abstraction. If you need higher-level (e.g. complex models) or lower-level (e.g. custom gradients or GPU kernels) control than the framework provides, you get stuck.&lt;/p&gt;
&lt;p&gt;Julia's mix of performance and ease of use opens a radical alternative; a single language that can work at all levels, from GPUs to data processing pipelines to clusters. Different approaches can be freely mixed and matched, with transparency and control at all levels of the stack. Come see how these ideas can accelerate your current workflow, and get a glimpse of the future of ML in Julia.&lt;/p&gt;
</summary></entry><entry><title>KEYNOTE: Automated factchecking in the era of fake news</title><link href="https://pyvideo.org/pydata-london-2017/keynote-automated-factchecking-in-the-era-of-fake-news.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Will Moy</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/keynote-automated-factchecking-in-the-era-of-fake-news.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Join Full Fact, the UK's independent factchecking charity, to discuss how they plan to make factchecking dramatically more effective with technology that exists now.&lt;/p&gt;
&lt;p&gt;Abstract
Factchecking is just one solution to the multifaceted problem of fake news. The factcheckers fight is valiant but how can they keep up in such tumultuous times? Join Full Fact, the UK's independent factchecking charity, to discuss how they plan to make factchecking dramatically more effective with technology that exists now.&lt;/p&gt;
</summary></entry><entry><title>KEYNOTE: Picasso's terminal; data science and AI in the visual arts</title><link href="https://pyvideo.org/pydata-london-2017/keynote-picassos-terminal-data-science-and-ai-in-the-visual-arts.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Gene Kogan</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/keynote-picassos-terminal-data-science-and-ai-in-the-visual-arts.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A Keynote talk filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
A talk about the flourishing intersection between machine learning and art, a survey of recent works emerging from it, and a primer on how to get started with it for seasoned developers and newcomers alike.&lt;/p&gt;
&lt;p&gt;Abstract
Over the past several years, two trends in machine learning have converged to pique the curiosity of artists working with code: the proliferation of powerful open source deep learning frameworks like TensorFlow and Torch, and the emergence of data-intensive generative models for hallucinating images, sounds, and text as though they came from the oeuvre of Shakespeare, Picasso, or just a gigantic database of digitized cats. This talk will review these developments and offer a set of interdisciplinary tools and learning resources for artists and data scientists alike, if ever there was a difference to begin with.&lt;/p&gt;
</summary></entry><entry><title>Lightning Talks - Saturday</title><link href="https://pyvideo.org/pydata-london-2017/lightning-talks-saturday.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/lightning-talks-saturday.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary><category term="lightning talks"></category></entry><entry><title>Machine Learning in Financial Credit Risk Assessment</title><link href="https://pyvideo.org/pydata-london-2017/machine-learning-in-financial-credit-risk-assessment.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Soledad Galli</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/machine-learning-in-financial-credit-risk-assessment.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description
Risk management is paramount to any lending institution, allowing it to perform well-informed decisions while originating loans. In this talk, I will describe our research and development approach to build our Credit Risk Prediction Model. I will browse over our target definition, feature optimisation, model building and tuning and our experience with model stacking.&lt;/p&gt;
&lt;p&gt;Abstract
Credit Risk assessment is a general term used among financial institutions to describe the methodology used to determine the likelihood of loss on a particular asset, investment or loan. The objective of assessing credit risk is to determine if an investment is worthwhile, what steps should be taken to mitigate risk, and what the return rate should be to make an investment successful.&lt;/p&gt;
&lt;p&gt;Building a Credit Risk Prediction Model as accurate as possible becomes essential, as it allows the institution to provide fair prices to the customers while ensuring predictable and minimal losses. We build our Credit Risk Model by combining data gathered from the customer’s application on our online platform with their credit history provided by different credit agencies.&lt;/p&gt;
&lt;p&gt;In this talk, we will cover the research and development behind our recently created Credit Risk Model. We will discuss the definition of the target, the variable selection procedure, the different machine learning models built and how we optimise their hyper-parameters, as well us some of our latest research in model stacking and deep learning.&lt;/p&gt;
&lt;p&gt;Our development and Modelling pipeline is built in Python, using Pandas, Numpy, Scikit-Learn, XGBboost, Keras, Matplotlib and Seaborn. We combine the use of machine learning algorithms with data visualisation to better understand the variables and our customers, and to convey the message to different stakeholders within and outside the company. Throughout the talk, we will focus both on the intellectual rationale of the research and the utilisation of the different python tools to accomplish each task, highlighting both the problems encountered and the solutions devised.&lt;/p&gt;
</summary></entry><entry><title>Machine learning with ventilator data to improve reporting on critically ill newborn infants</title><link href="https://pyvideo.org/pydata-london-2017/machine-learning-with-ventilator-data-to-improve-reporting-on-critically-ill-newborn-infants.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Ian Ozsvald</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/machine-learning-with-ventilator-data-to-improve-reporting-on-critically-ill-newborn-infants.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description
Mechanical ventilators are widely used in intensive care, they are sophisticated but Doctors do not have time to analyse the copious traces of data in a neonatal unit. We are providing an easy-to-interpret summary of this time-series data using visualisation and machine learning. This is an open source collaboration with the NHS, All results are open.&lt;/p&gt;
&lt;p&gt;Abstract
Mechanical ventilators are widely used in intensive care. Even two decades ago they were be primarily mechanical devices whose &amp;quot;only&amp;quot; task was to inflate the patient’s lung. Recently, however, they have become equipped with powerful computers that provide sophisticated ventilator modes. Data provided by the ventilators are almost never downloaded, stored or analysed. The data is complex, high frequency and requires time-intensive scrutiny to review. Doctors do not have time to analyse these traces in a neonatal unit.&lt;/p&gt;
&lt;p&gt;We are providing a simple and easy-to-interpret summary of 100Hz dual-channel ventilator data to improve the quality of care of young infants by time-poor staff. This involves signal processing, visualisation, building a gold standard and machine learning to segment breaths and summarise a baby's behaviour. This builds on our talk at PyDataLondon Meetup 30 in January 2017. Our goal is to open source the research so that others can benefit from the processes that we develop. We invite feedback from the audience to help improve our methods.&lt;/p&gt;
&lt;p&gt;Anyone interested in time series data, automated labeling, scikit-learn, Bokeh and medical applications will find this talk of interest. Both the highs and lows of our current approaches will be discussed.&lt;/p&gt;
&lt;p&gt;This is a collaboration between Dr Gusztav Belteki (Cambridge University Hopsitals NHS Foundation Turst), Ian Ozsvald (ModelInsight) and Giles Weaver (ModelInsight).&lt;/p&gt;
</summary></entry><entry><title>Pythonic Polling Analysis and Comments on 2016's Polling Surprises</title><link href="https://pyvideo.org/pydata-london-2017/pythonic-polling-analysis-and-comments-on-2016s-polling-surprises.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Aileen Nielsen</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/pythonic-polling-analysis-and-comments-on-2016s-polling-surprises.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
2016 was a whopper of a year both for political upsets and debates about traditional polling and its relevance. In this talk I will discuss Pythonic survey analysis and will also highlight the pitfalls of polling and sampling generally. I will close with some thoughts on polling surprises from Brexit and the US Presidential election.&lt;/p&gt;
&lt;p&gt;Abstract
Between Brexit and the US Presidential election, it has been quite a year in the English-speaking world for pollsters. In this talk I will first introduce basic concepts of political polling design methodologies and traditional analytical techniques for dealing with the necessarily skewed data that results from traditional sampling. I will then give an overview of existing Python packages for tackling survey design and demonstrate sample code applying existing packages and also roll-your-own approaches. I will discuss current industry best practice for polling and explain how these traditional methods were deployed to monitor the year's two biggest political votes: the US Presidential election and the UK Brexit referendum.&lt;/p&gt;
&lt;p&gt;I will explore how, and whether, the outcomes of these two votes was as much as a surprise to pollsters as the media indicated and what might have led to more accurate predictions. Finally, I will close with comments about how polling methodology is likely to change in the coming years and what, if anything, could have been done differently analytically to better predict the actual results of these important 2016 votes.&lt;/p&gt;
</summary></entry><entry><title>SaaaS - Sampling as an Algorithm Service</title><link href="https://pyvideo.org/pydata-london-2017/saaas-sampling-as-an-algorithm-service.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Vincent D. Warmerdam</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/saaas-sampling-as-an-algorithm-service.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description
In this talk I will explain the idea of sampling to get to your model and I will demonstrate it with examples. The goal is to start with a for loop and to end with understanding how MCMC algorithms work.&lt;/p&gt;
&lt;p&gt;Abstract
A lot of people understand the scikit-learn models of todays world but feel uneasy about the whole MCMC method of training. Why are these algorithms different? How is it that you don't use a gradient method but a sampler instead? It can feel a bit misterious if you've not properly been introduced to this other way of thinking.&lt;/p&gt;
&lt;p&gt;In this talk I will explain the idea of sampling to get to your model and I will demonstrate it with examples. The goal is to start with a for loop and to end with understanding how MCMC algorithms work. As a consequence the audience will also get a proper introduction to PyMC3. In particular I will discuss the following;&lt;/p&gt;
&lt;p&gt;why markov chain sampling can be equivalent to direct sampling
how to build your own MCMC sampler with a for loop
how this for loop can be run faster by using PyMC3 instead
the key idea of inference and how i was briefly able apply it in a santa kaggle competition
how to analyse timeseries with MCMC in PyMC3
Parts of this talk are readily available on my blog;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://koaning.io/switching-to-sampli"&gt;http://koaning.io/switching-to-sampli&lt;/a&gt;...
&lt;a class="reference external" href="http://koaning.io/elimination-via-inf"&gt;http://koaning.io/elimination-via-inf&lt;/a&gt;...&lt;/p&gt;
</summary></entry><entry><title>Scale out from the very beginning</title><link href="https://pyvideo.org/pydata-london-2017/scale-out-from-the-very-beginning.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Jens Nie</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/scale-out-from-the-very-beginning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Most companies a very well aware of the potential behind Big Data solutions today and happily start collecting every piece of information building huge pools of Dark Data. How could Data Science teams create an initial overview on what's available? A simple search strategy, optimised and refined to scale could be a promising way to start.&lt;/p&gt;
&lt;p&gt;Abstract
In this talk the authors journey of making the pool of Dark Data available to teams with quite different goals is reflected, emphasising on creating a simple and robust set of tools matching each other and addressing the several needs of the teams based mainly on solutions such as dask distributed, dask based dataframes, bokeh and flask.&lt;/p&gt;
&lt;p&gt;The key to success was to prevent structuring too much at the very beginning and postpone this task into the several projects of the users consuming the results of these services giving them the freedom to create and use their own models.&lt;/p&gt;
&lt;p&gt;It is shown how we implemented a distributed filesystem scanning utility to crawl for data in our 1.5 PB storage system every night ending up in a simple, yet useful table of contents, and how this result set is processed further to fulfill all the project teams requirements.&lt;/p&gt;
&lt;p&gt;These services are for example used to&lt;/p&gt;
&lt;p&gt;find expensive duplicates of datasets
create customer as well as product and service orientated views on the available data
find data suitable to test algorithms, software and procedures, and to derive current performance
serve training and education material
show the usage frequency of the datasets to support an optimised data tiering process
Finally the involved procedures helped to gain more awareness of the value the available data had, both helping to build more trust in Big Data based solutions and to reduce the volume of the data itself that is available online, which in turn keeps the corresponding costs at a reasonable rate.&lt;/p&gt;
</summary></entry><entry><title>Show me the failures! Data products for manufacturing at shop floor</title><link href="https://pyvideo.org/pydata-london-2017/show-me-the-failures-data-products-for-manufacturing-at-shop-floor.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Thomas Alisi</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/show-me-the-failures-data-products-for-manufacturing-at-shop-floor.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
The Data Science and Analytics group at Pirelli has to deal with factories' day to day that can't be further from the aseptic crunching of data from a keyboard in an office. Our group took the lift, went down at shop floor and started asking questions to try and make their life better: turns out questions flowed the other way round and results were startling.&lt;/p&gt;
&lt;p&gt;Abstract
Pirelli has a 140 year old tradition of manufacturing with 20 factories across 14 countries and headquarter office in Milan. Production flows, logistic, machinery and the whole extended value chain has morphed through decades across a broad range of needs and circumstances.&lt;/p&gt;
&lt;p&gt;The creation of a Data Science and Analytics department at the beginning of 2016 has the goal of speeding up change and innovation, starting from areas that are harder to tackle. Some of the most interesting challenges include:&lt;/p&gt;
&lt;p&gt;bring data products at shop floor to increase efficiency while being aware of UX principles
keep 2-sided communication alive with wide number of actors, particularly with IT, quality and engineering
encourage active participation by providing accessible analytics tools and an internal Academy training program
activate the virtuous circle of prototyping, feasibility check and production releases for sound product lifecycles
introduce Agile development methodologies in traditional waterfall environments
shape a roadmap with principal stakeholders starting from off-line through live analysis and heading to ahead-of-time predictions
opening a steady communication channel across groups is progressively eroding barriers between white and blue collars, allowing teams to better understand each other requirements and kicking off a broader conversation. At the end of the first year since releasing the first prototype, there is much more on the plate, and groups are now more familiar with concepts of User Experience, release lifecycle, data exploration and agile development.&lt;/p&gt;
</summary></entry><entry><title>Static Type Analysis for Robust Data Products</title><link href="https://pyvideo.org/pydata-london-2017/static-type-analysis-for-robust-data-products.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Marco Bonzanini</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/static-type-analysis-for-robust-data-products.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
This talk discusses static type analysis applied to Python data products, its pros and cons, and overall how to adopt type checking tools (i.e. mypy) in your workflow.&lt;/p&gt;
&lt;p&gt;Abstract
As a dynamically typed language, Python is an extremely flexible tool that allows to write code quickly and concisely. This flexibility makes Python a popular tool for R&amp;amp;D and prototyping, but what about bringing Data Science in production? When comparing Python to statically typed languages, one of the downsides is that many type-related errors are not captured until runtime.&lt;/p&gt;
&lt;p&gt;This talk discusses the steps taken by the Python community to promote static type analysis, in particular the semantic definition of type hints and the adoption of mypy as type checking tool.&lt;/p&gt;
&lt;p&gt;The audience will learn about static typing for Python, its pros and cons, and how to adopt static type analysis in your workflow. Since the focus is on building and deploying data products, static type analysis is discussed as a mean to improve the robustness of your data products.&lt;/p&gt;
</summary></entry><entry><title>To explain or to predict?</title><link href="https://pyvideo.org/pydata-london-2017/to-explain-or-to-predict.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Nick Sorros</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/to-explain-or-to-predict.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Data science and machine learning in particular has a preference towards the kind of statistical modelling that optimises for prediction rather than explanation. Understanding which variables correlate with the phenomenon you are investigating is an inherent part of both approaches but models with high explanatory power are not necessarily the ones that yield the best predictions.&lt;/p&gt;
&lt;p&gt;Abstract
The goal of statistical modelling is to build a relationship between the observational data and the phenomenon under investigation. Predictive and explanatory modelling are sub-branches of statistical modelling that optimise for prediction or explaination respectively.&lt;/p&gt;
&lt;p&gt;The data science and machine learning literature is filled with examples of predictive modelling approaches while other disciples like economists are almost solely reliant on explanatory methods.&lt;/p&gt;
&lt;p&gt;In theory a model that explains your dataset well should come with high predictive capabilities but in reality it has been shown that this is not the case. This leads to a different set of optimisations and tradeoffs during modelling.&lt;/p&gt;
&lt;p&gt;The goal of this talk is to discuss these different tradeoffs and showcase the similarities and differences between these approaches as well as to discuss when is best to use one over the other.&lt;/p&gt;
</summary></entry><entry><title>Using Random Forests in Python</title><link href="https://pyvideo.org/pydata-london-2017/using-random-forests-in-python.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Nathan Epstein</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/using-random-forests-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
This talk will be an exposition of machine learning with random forests for Python programmers. The talk will cover the internals of how random forests are implemented, applications that are well suited to the use of random forests, and Python code samples to demonstrate their use.&lt;/p&gt;
&lt;p&gt;Abstract
Outline&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Intro (5 minutes)
What are random forests, how are they used, and what Python software is available for using them?
What strengths do they have relative to other models (scalability and applicability to a broad range of problems)?&lt;/li&gt;
&lt;li&gt;Forest Internals (15 minutes)&lt;/li&gt;
&lt;li&gt;Decision Trees (5 minutes)&lt;ul&gt;
&lt;li&gt;Presentation of the decision tree model, the building block of random forests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Entropy Minimization (5 minutes)&lt;ul&gt;
&lt;li&gt;Explanation of how decision trees are tuned using entropy minimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Building Forests from Decision Trees (5 minutes)&lt;ul&gt;
&lt;li&gt;Explanation of how decision trees are aggregated to form random forests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Illustrative Examples (10 minutes)&lt;/li&gt;
&lt;li&gt;Regression on non-linear functions (5 minutes)&lt;/li&gt;
&lt;li&gt;Classification with unscaled features (5 minutes)&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Variational Inference and Python</title><link href="https://pyvideo.org/pydata-london-2017/variational-inference-and-python.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Peadar Coyle</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/variational-inference-and-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Recent improvements in Probabilistic Programming have led to a new method called Variational Inference. This is an alternative method to the standard method of Markov Chain Monte-Carlo. We'll discuss these methods in PyMC3 and Edward, explain the theory and the limitations and apply these methods to realistic examples.&lt;/p&gt;
&lt;p&gt;Abstract
The state of the nation
There are currently three big trends in machine learning: Probabilistic Programming, Deep Learning and &amp;quot;Big Data&amp;quot;. Inside of PP, a lot of innovation is in making things scale using Variational Inference. In this talk , I will show how to use Variational Inference in PyMC3 to fit a simple Bayesian Neural Network. I will also discuss how bridging Probabilistic Programming and Deep Learning can open up very interesting avenues to explore in future research.&lt;/p&gt;
&lt;p&gt;Probabilistic Programming
Probabilistic Programming allows very flexible creation of custom probabilistic models and is mainly concerned with insight and learning from your data. The approach is inherently Bayesian so we can specify priors to inform and constrain our models and get uncertainty estimation in form of a posterior distribution. Using MCMC sampling algorithms we can draw samples from this posterior to very flexibly estimate these models. PyMC3 and Stan are the current state-of-the-art tools to consruct and estimate these models.&lt;/p&gt;
&lt;p&gt;One major drawback of sampling, however, is that it's often very slow, especially for high-dimensional models. That's why more recently, variational inference algorithms have been developed that are almost as flexible as MCMC but much faster. Instead of drawing samples from the posterior, these algorithms instead fit a distribution (e.g. normal) to the posterior turning a sampling problem into and optimization problem. ADVI -- Automatic Differentation Variational Inference -- is implemented in PyMC3 and Stan, as well as a new package called Edward which is mainly concerned with Variational Inference.&lt;/p&gt;
&lt;p&gt;In this talk we'll apply these methods of Variational Inference to regression and neural network problems, and explain the advantages for solving big data problems in probabilistic programming. You'll leave this talk with methods you can apply in your own work, and will showcase some of the new features in PyMC3 and Edward.&lt;/p&gt;
&lt;p&gt;The speakers are both contributors to PyMC3.&lt;/p&gt;
</summary></entry><entry><title>Yellowbrick: Steering Machine Learning with Visual Transformers</title><link href="https://pyvideo.org/pydata-london-2017/yellowbrick-steering-machine-learning-with-visual-transformers.html" rel="alternate"></link><published>2017-05-06T00:00:00+00:00</published><updated>2017-05-06T00:00:00+00:00</updated><author><name>Rebecca Bilbro</name></author><id>tag:pyvideo.org,2017-05-06:pydata-london-2017/yellowbrick-steering-machine-learning-with-visual-transformers.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Yellowbrick is a new library that extends Scikit-Learn's API to incorporate visualizations into machine learning. While the machine learning workflow is increasingly being automated with gridsearch, APIs, and GUIs, in practice, human intuition outperforms exhaustive search. By visualizing model selection, we can not only steer towards robust models, but also avoid common pitfalls and traps.&lt;/p&gt;
&lt;p&gt;Abstract
In machine learning, model selection is a bit more nuanced than simply picking the 'right' or 'wrong' algorithm. In practice, the workflow includes (1) selecting and/or engineering the smallest and most predictive feature set, (2) choosing a set of algorithms from a model family, and (3) tuning the algorithm hyperparameters to optimize performance. Recently, much of this workflow has been automated through grid search methods, standardized APIs, and GUI-based applications. In practice, however, human intuition and guidance can more effectively hone in on quality models than exhaustive search.&lt;/p&gt;
&lt;p&gt;This talk presents a new Python library, Yellowbrick, which extends the Scikit-Learn API with a visual transfomer (visualizer) that can incorporate visualizations of the model selection process into pipelines and modeling workflow. Yellowbrick is an open source, pure Python project that extends Scikit-Learn with visual analysis and diagnostic tools. The Yellowbrick API also wraps matplotlib to create publication-ready figures and interactive data explorations while still allowing developers fine-grain control of figures. For users, Yellowbrick can help evaluate the performance, stability, and predictive value of machine learning models, and assist in diagnosing problems throughout the machine learning workflow.&lt;/p&gt;
&lt;p&gt;In this talk, we'll explore not only what you can do with Yellowbrick, but how it works under the hood (since we're always looking for new contributors!). We'll illustrate how Yellowbrick extends the Scikit-Learn and Matplotlib APIs with a new core object: the Visualizer. Visualizers allow visual models to be fit and transformed as part of the Scikit-Learn Pipeline process - providing iterative visual diagnostics throughout the transformation of high dimensional data.&lt;/p&gt;
</summary></entry><entry><title>Building a ChatBot with Python, NLTK and scikit</title><link href="https://pyvideo.org/pydata-london-2017/building-a-chatbot-with-python-nltk-and-scikit.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Edward Bullen</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/building-a-chatbot-with-python-nltk-and-scikit.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData 2017&lt;/p&gt;
&lt;p&gt;Description
Introducing the basics of Natural Language Processing using Python NLTK and Machine Learning packages to classify language in order to create a simple Q&amp;amp;A bot.&lt;/p&gt;
&lt;p&gt;Abstract
Working code samples and a basic ChatBot framework (written in Python) will be provided and explained so that a simple Q&amp;amp;A bot that learns from previous experience and responds to questions with appropriate answers can be created. In this talk we will cover:&lt;/p&gt;
&lt;p&gt;Build a basic ChatBot Framework using core Python and a SQL database.
Demonstrate and experiment with a Learning-by-Example bot using ranking functions in Python and SQL to get some basic chat functionality working.
Introduce the Python NLTK to extract features from the chat sentences and words stored in the chatbot database.
Work through a feature engineering example using NLTK and Sci-Kit and Numpy to show how we can classify sentences using Supervised Learning and estimate the accuracy of our classification model.
Apply the sentence classification ML model to our chatbot engine to target responses more accurately.
Prerequisites&lt;/p&gt;
&lt;p&gt;Attendees will need:
+ Anaconda for Python 3.5 or 3.6
+ NLTK (Python Natural Language Toolkit - pip install nltk)
+ The Stanford Java CoreNLP Parser (&lt;a class="reference external" href="https://stanfordnlp.github.io/CoreNLP/"&gt;https://stanfordnlp.github.io/CoreNLP/&lt;/a&gt; or wget &lt;a class="reference external" href="http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip"&gt;http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip&lt;/a&gt; and un-zip)
+ Java rel 8&lt;/p&gt;
&lt;p&gt;Theoretically all of this could be installed on the day but it would just help to save time by preparing in advance. Most of what I am demonstrating will probably work against Python 2.7, but it hasn’t been tested with 2.7.&lt;/p&gt;
</summary></entry><entry><title>Building Web-based Analysis &amp; Simulation Platforms with React/Redux, Flask, Celery, Bokeh, and Numpy</title><link href="https://pyvideo.org/pydata-london-2017/building-web-based-analysis-simulation-platforms-with-reactredux-flask-celery-bokeh-and-numpy.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/building-web-based-analysis-simulation-platforms-with-reactredux-flask-celery-bokeh-and-numpy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Titles - Building Web-based Analysis &amp;amp; Simulation Platforms with React/Redux, Flask, Celery, Bokeh, and Numpy&lt;/p&gt;
&lt;p&gt;Filmed at PyData 2017&lt;/p&gt;
&lt;p&gt;Description
What use is analytical code if it can't be integrated into a business workflow to solve real problems? This tutorial is about integrating analytical work into a real production system that can be used by business users. It focuses on building a web-based platform for managing long-running analytical code and presenting results in a convenient format, using cutting-edge combination of tools.&lt;/p&gt;
&lt;p&gt;Abstract
The purpose of this stack is to be able to rapidly create web-based environments for users to interact with the results of analytical and simulation processes (without needing to retrain one's self as a web programmer!)&lt;/p&gt;
&lt;p&gt;This tutorial is composed of the following pieces:&lt;/p&gt;
&lt;p&gt;building a simple simulation using Numpy. For the purposes of this tutorial, we model a very simple Monte Carlo simulation with a number of user-controllable, tweakable algorithm inputs and model parameters. The simulation is chosen to be simple enough to present and code quickly. The purpose of this tutorial is not building Monte Carlo simulations but packaging them into lightweight production systems.&lt;/p&gt;
&lt;p&gt;Celery for launching and managing the above simulation jobs. This tutorial will not cover all aspects of Celery. It will merely show how the tool can be used as a job management system.&lt;/p&gt;
&lt;p&gt;Flask as a very thin JSON API layer. The tutorial will make use of Flask plugins for quickly building JSON APIs. This is the thinnest and least interesting component of the tutorial and won't be covered in great depth.&lt;/p&gt;
&lt;p&gt;React + Redux for a slick, simple single-page app. Attendees are expected to be least familiar with Javascript and the React ecosystem. The tutorial will spend a fair amount of time on this component, and will cover setting up build environment using Babel (for JSX transpilation) and Gulp as a build system.&lt;/p&gt;
&lt;p&gt;Bokeh for presenting graphical results from the simulation. This component may be cut based on time considerations.&lt;/p&gt;
&lt;p&gt;If time permits, it might also be possible to discuss the use of React Native to quickly build mobile apps using the same infrastructure.&lt;/p&gt;
</summary></entry><entry><title>Easy Bayesian regularization for fitting financial time series and curves</title><link href="https://pyvideo.org/pydata-london-2017/easy-bayesian-regularization-for-fitting-financial-time-series-and-curves.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Dr. Egor Kraev</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/easy-bayesian-regularization-for-fitting-financial-time-series-and-curves.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary></entry><entry><title>Introduction to Convolutional Neural Networks using TensorFlow and Keras</title><link href="https://pyvideo.org/pydata-london-2017/introduction-to-convolutional-neural-networks-using-tensorflow-and-keras.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Oliver Zeigermann</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/introduction-to-convolutional-neural-networks-using-tensorflow-and-keras.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Introduction to image classification using Tensorflow. I will provide a Notebook for you to try things out.&lt;/p&gt;
&lt;p&gt;Abstract
We will discuss how Convolutional Neural Networks work and how you can apply them to the task of image classification. This will be illustrated by an example using real pictures of German speed limit signs which we will process with Tensorflow and Keras as a frontend. I will provide you with a Notebook so you can follow along without installation or try out the examples later at home.&lt;/p&gt;
</summary></entry><entry><title>Jupyter Notebooks for geospatial data analysis</title><link href="https://pyvideo.org/pydata-london-2017/jupyter-notebooks-for-geospatial-data-analysis.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Julia Wagemann</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/jupyter-notebooks-for-geospatial-data-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
Learn how Jupyter Notebooks are a highly beneficial tool for modern geospatial data analysis and for creating reproducible workflows, both for web-based access of large geospatial data and its effective manipulation and geospatial data visualisation with bokeh or jupyter widgets.&lt;/p&gt;
&lt;p&gt;Abstract
Jupyter Notebooks benefit geospatial data analysis in multiple ways since geo data access, manipulation and (interactive) visualisation can be combined in one workflow and programming environment.&lt;/p&gt;
&lt;p&gt;This tutorial session presents practical examples of workflows with Jupyter Notebooks for Earth Observation and Climate data analysis. An introduction to Python’s owslib package is given, which will showcase how standardised web services provide a time and cost-efficient way to access and process large volumes of environmental data. Hands-on examples will show how terabytes of open environmental data can be accessed, manipulated and visualized within one workflow without requiring data download. Different Python packages for data visualisation and geospatial data analysis will be harnessed.&lt;/p&gt;
&lt;p&gt;The first part of the tutorial is a walk-through session, where participants are able to run the example workflows alongside. In a second part, the participants will get challenged to setup their own geospatial workflow, from data access and manipulation up to data visualization, based on the tools and data services that were presented during the walk-through session&lt;/p&gt;
&lt;p&gt;Session participants require their own laptop with Jupyter installed. We are investigating the feasibility of setting up mybinder or JupyterHub for this workshop. Notebooks will be provided.&lt;/p&gt;
</summary></entry><entry><title>Make your research interactive with Jupyter Dashboards</title><link href="https://pyvideo.org/pydata-london-2017/make-your-research-interactive-with-jupyter-dashboards.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Pavlo Andriychenko</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/make-your-research-interactive-with-jupyter-dashboards.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
I will show the tools and processes for building interactive web apps using the Jupyter stack. Simple, easy, beautiful.&lt;/p&gt;
&lt;p&gt;Abstract
Jupyter is great for tinkering and research. But what if you are not the only consumer of that notebook? What if other consumers want to slice and visualise your data differently? I will show how to turn fresh ideas or existing Jupyter notebooks into beautiful web apps without the need to worry about the web server, HTTP and content deployment.&lt;/p&gt;
</summary></entry><entry><title>One workshop that data scientists don't want you to attend...</title><link href="https://pyvideo.org/pydata-london-2017/one-workshop-that-data-scientists-dont-want-you-to-attend.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Oliver Laslett</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/one-workshop-that-data-scientists-dont-want-you-to-attend.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;By Oliver Laslett &amp;amp; Andraz Hribernik&lt;/p&gt;
&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
With this one weird trick you can build a text processing pipeline!&lt;/p&gt;
&lt;p&gt;We've all fallen for clickbait articles online. They pollute our news feeds and make it harder to filter out valuable information. In this workshop we'll stream news articles in real-time and detect clickbait using simple machine learning techniques. You won't believe what happened next...&lt;/p&gt;
&lt;p&gt;Abstract
By the end of the workshop you'll have your very own python app for streaming real-time news and detecting click bait. In the workshop we'll cover:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Streaming data from a REST API&lt;/li&gt;
&lt;li&gt;Preprocessing textual data&lt;/li&gt;
&lt;li&gt;Training a simple machine learning classifier for clickbait&lt;/li&gt;
&lt;li&gt;Putting everything together in a scikit-learn pipeline&lt;/li&gt;
&lt;li&gt;Analysing our results (which news source is the most clickbaity?)&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>PyData London 2017 - Closing Notes</title><link href="https://pyvideo.org/pydata-london-2017/pydata-london-2017-closing-notes.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Unknown</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/pydata-london-2017-closing-notes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary></entry><entry><title>Ten Steps to Keras</title><link href="https://pyvideo.org/pydata-london-2017/ten-steps-to-keras.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Valerio Maggio</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/ten-steps-to-keras.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description
In this tutorial we will learn Keras in ten steps (a.k.a. Jupyter Notebooks). We will warm up by learning how to create a multi layer network, and then we will go through more sophisticated topics such as implementing different types of networks (e.g. RNN, CNN), creating custom layers and discovering Keras internals. numpy proficiency and basic knowledge of Machine/Deep Learning are assumed.&lt;/p&gt;
&lt;p&gt;Abstract
Goal of the Tutorial&lt;/p&gt;
&lt;p&gt;Introduce main features of Keras APIs to build Neural Networks.
Learn how to implement simple and complex Deep Neural Networks Architectures using Keras.
Discover Keras Implementation and Internals.&lt;/p&gt;
&lt;p&gt;Note: examples and hands-on exercises will be provided along the way.&lt;/p&gt;
&lt;p&gt;Outline (in ten Notebooks)
Multi-layer Fully Connected Networks (and the backends)
Bottleneck features and Embeddings
Convolutional Networks
Transfer Learning and Fine Tuning
Residual Networks
Recursive Neural Networks
[Variational] AutoEncoders and Adversarials
Multi-Modal Networks
Custom Layers and Optimisers
Interactive Networks and Callbacks
Description
Multi-layer Fully Connected Networks In this notebook we will learn the basic building blocks of Keras APIs to create deep neural networks. We will learn how to use the Sequential object as well as Functional and keras.backend APIs. First examples of MLP and Fully Connected Networks will be presented.&lt;/p&gt;
</summary></entry><entry><title>Test-Driven Data Analysis</title><link href="https://pyvideo.org/pydata-london-2017/test-driven-data-analysis.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Nick Radcliffe</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/test-driven-data-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;This tutorial will introduce attendees to the concepts of test-driven data analysis and practical, hands-on use of the tdda library (available from Github and PyPI) for:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;writing reference tests, and&lt;/li&gt;
&lt;li&gt;generating and verifying constraints from data, using Pandas data frames.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;TDDA aims to bring the ideas and benefits of test-driven development to the arena of data analysis, augmenting those ideas as appropriate. There are two central planks of TDDA at present:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;The idea of a reference test, which is a lot like a system or integration test for an analytical process&lt;/li&gt;
&lt;li&gt;The idea of using constraints to verify input, intermediate and output data for/from analytical processes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The tdda library (available from Github and PyPI) provides tooling support for both of these, major current components being:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Support for writing tests, under unittest or pytest, than involve comparison of complex objects (e.g. graphs, dataframes etc.), possibly with variable components, and regenerating reference (&amp;quot;expected&amp;quot;) results easily when they have changed (after verification!)&lt;/li&gt;
&lt;li&gt;Support for automatically generating suggested constraints from example datasets/data frames (including Pandas DataFrames)&lt;/li&gt;
&lt;li&gt;Support for verifying a dataset/dataframe against a set of constraints&lt;/li&gt;
&lt;li&gt;Support for generating regular expressions from example strings, for possible use as constraints (or otherwise). (It will probably do more by May, but these things are there now!)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial will introduce users to using these ideas through the tdda library. Users will be able to use their own analytical processes and/or datasets, or to use example data that will be provided.&lt;/p&gt;
</summary></entry><entry><title>Twisted up in a Distributed Tornado - a beginners guide to async frameworks in python</title><link href="https://pyvideo.org/pydata-london-2017/twisted-up-in-a-distributed-tornado-a-beginners-guide-to-async-frameworks-in-python.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Andrew Stretton</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/twisted-up-in-a-distributed-tornado-a-beginners-guide-to-async-frameworks-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Twisted up in a Distributed Tornado - a beginners guide to async frameworks in python&lt;/p&gt;
&lt;p&gt;Filmed at PyData 2017&lt;/p&gt;
&lt;p&gt;Description
By coming to this tutorial you will learn:&lt;/p&gt;
&lt;p&gt;What event-driven Python Frameworks are and why they are useful
How to overcome any fears you may have about event-driven Python programming
Enough examples to be able to consider projects like Dask/Distributed and Tornado for your next project
Abstract
At some point in a career in data science, software development or data engineering you will be looking to develop a piece of code that runs alongside another piece of code. For example, you might want to:&lt;/p&gt;
&lt;p&gt;Use all of your processors when running a script
Consume social media data as it is created
Create a super simple task scheduler without an ugly while True loop
Understand how Jupyter notebooks work
Write to a database and move on to the next task without waiting for confirmation
Call a function and throw an exception if it takes too long
There are numerous ways of achieving such concurrency. If creating a project to run on one or a few machines which needs good exception handling then I find tools like Twisted, Tornado and Dask/Distributed ideal for the above tasks.&lt;/p&gt;
&lt;p&gt;As a python developer I have always tended to work with Celery and felt intimidated by the different flow control when using async tools like Twisted and Tornado. In this tutorial I will walk through the examples above and talk about how I went from Async novice to Dask/Distributed contributor in 6 months. The tutorial will concentrate mainly on practical use cases. Along the way we will encounter some bumps in the road which will hopefully aid attendees' understanding of the basic dos and don'ts around these tools.&lt;/p&gt;
&lt;p&gt;By coming to this tutorial you will learn:&lt;/p&gt;
&lt;p&gt;What event-driven Python Frameworks are and why they are useful
How to overcome any fears you may have about event-driven Python programming
Enough examples to be able to consider projects like Dask/Distributed and Tornado for your next project.&lt;/p&gt;
</summary></entry><entry><title>Word Embedding For Fun and Profit</title><link href="https://pyvideo.org/pydata-london-2017/word-embedding-for-fun-and-profit.html" rel="alternate"></link><published>2017-05-05T00:00:00+00:00</published><updated>2017-05-05T00:00:00+00:00</updated><author><name>Lev Konstantinovskiy</name></author><id>tag:pyvideo.org,2017-05-05:pydata-london-2017/word-embedding-for-fun-and-profit.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017&lt;/p&gt;
&lt;p&gt;Word Embedding For Fun and Profit&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary></entry></feed>