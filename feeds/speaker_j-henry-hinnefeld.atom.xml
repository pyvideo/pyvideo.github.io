<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_j-henry-hinnefeld.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-05-04T17:10:00+00:00</updated><entry><title>Measuring Model Fairness</title><link href="https://pyvideo.org/pycon-us-2019/measuring-model-fairness.html" rel="alternate"></link><published>2019-05-04T17:10:00+00:00</published><updated>2019-05-04T17:10:00+00:00</updated><author><name>J. Henry Hinnefeld</name></author><id>tag:pyvideo.org,2019-05-04:pycon-us-2019/measuring-model-fairness.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;When machine learning models make decisions that affect people’s lives,
how can you be sure those decisions are fair? When you build a machine
learning product, how can you be sure your product isn't biased? What
does it even mean for an algorithm to be ‘fair’? As machine learning
becomes more prevalent in socially impactful domains like policing,
lending, and education these questions take on a new urgency.&lt;/p&gt;
&lt;p&gt;In this talk I’ll introduce several common metrics which measure the
fairness of model predictions. Next I’ll relate these metrics to
different notions of fairness and show how the context in which a model
or product is used determines which metrics (if any) are applicable. To
illustrate this context-dependence I'll describe a case study of
anonymized real-world data. Next, I'll highlight some open source tools
in the Python ecosystem which address model fairness. Finally, I'll
conclude by arguing that if your job involves building these kinds
models or products then it is your responsibility to think about the
answers to these questions.&lt;/p&gt;
</summary><category term="talk"></category></entry><entry><title>Measuring Model Fairness</title><link href="https://pyvideo.org/pydata-new-york-city-2018/measuring-model-fairness.html" rel="alternate"></link><published>2018-08-17T00:00:00+00:00</published><updated>2018-08-17T00:00:00+00:00</updated><author><name>J. Henry Hinnefeld</name></author><id>tag:pyvideo.org,2018-08-17:pydata-new-york-city-2018/measuring-model-fairness.html</id><summary type="html"></summary></entry><entry><title>Git Risky Using git metadata to predict code bug risk</title><link href="https://pyvideo.org/pydata-new-york-city-2017/git-risky-using-git-metadata-to-predict-code-bug-risk.html" rel="alternate"></link><published>2017-11-27T00:00:00+00:00</published><updated>2017-11-27T00:00:00+00:00</updated><author><name>J. Henry Hinnefeld</name></author><id>tag:pyvideo.org,2017-11-27:pydata-new-york-city-2017/git-risky-using-git-metadata-to-predict-code-bug-risk.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Git is a powerful tool for code versioning. If you follow its best practices and have good ‘commit hygiene’ it can also be a source of valuable data about your coding practices. In this talk I’ll describe a system we built at Civis that uses the metadata git collects, along with its logging and ‘blaming’ functionality to score commits in real time on their likelihood of introducing a bug.&lt;/p&gt;
</summary></entry><entry><title>High Frequency Trading in MMORPG Markets using Luigi, Pandas, and Scikit learn</title><link href="https://pyvideo.org/pydata-chicago-2016/high-frequency-trading-in-mmorpg-markets-using-luigi-pandas-and-scikit-learn.html" rel="alternate"></link><published>2016-08-27T00:00:00+00:00</published><updated>2016-08-27T00:00:00+00:00</updated><author><name>J. Henry Hinnefeld</name></author><id>tag:pyvideo.org,2016-08-27:pydata-chicago-2016/high-frequency-trading-in-mmorpg-markets-using-luigi-pandas-and-scikit-learn.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;In this talk I’ll describe the system I developed to implement a basic algorithmic trading strategy in the in-game market of an online, multi-player video game. Using this toy model, I’ll walk through the steps involved in setting up a data pipeline with Luigi, analyzing the resulting data with pandas, and identifying important factors and features with scikit-learn.&lt;/p&gt;
</summary><category term="scikit"></category></entry></feed>