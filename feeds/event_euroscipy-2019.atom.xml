<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_euroscipy-2019.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-09-06T00:00:00+00:00</updated><entry><title>EuroSciPy 2019 Bilbao - Last moments - Good Bye EuroSciPy 2019</title><link href="https://pyvideo.org/euroscipy-2019/euroscipy-2019-bilbao-last-moments-good-bye-euroscipy-2019.html" rel="alternate"></link><published>2019-09-06T00:00:00+00:00</published><updated>2019-09-06T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2019-09-06:euroscipy-2019/euroscipy-2019-bilbao-last-moments-good-bye-euroscipy-2019.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;EuroSciPy 2019 Bilbao
September 5, Thursday
Mitxelena. Last Moments. 18.15&lt;/p&gt;
&lt;p&gt;Closing notes, raffles, thanks to attendees, sponsors and organizer and big applause to the volunteers.&lt;/p&gt;
</summary></entry><entry><title>Can we make Python fast without sacrificing readability? numba for Astrodynamics</title><link href="https://pyvideo.org/euroscipy-2019/can-we-make-python-fast-without-sacrificing-readability-numba-for-astrodynamics.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Juan Luis Cano Rodríguez</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/can-we-make-python-fast-without-sacrificing-readability-numba-for-astrodynamics.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We are lucky there are very diverse solutions to make Python faster that
have been in use for a while: from wrapping compiled languages (NumPy),
to altering the Python syntax to make it more suitable to compilers
(Cython), to using a subset of it which can in turn be accelerated
(numba). However, each of these options has a tradeoff, and there is no
silver bullet.&lt;/p&gt;
&lt;p&gt;poliastro is a library for Astrodynamics written in pure Python. All its
core algorithms are accelerated with numba, which allows poliastro to be
decently fast while having minimal code complexity and avoid using other
languages.&lt;/p&gt;
&lt;p&gt;However, even though numba is quite mature as a library and most of the
Python syntax and NumPy functions are supported, there are still some
limitations that affect its usage. In particular, we strive to offer a
high-level API with support for physical units and reusable functions
which can be passed as arguments, which sometimes require using complex
objects or introspective Python behavior which is not available.&lt;/p&gt;
&lt;p&gt;In this talk we will discuss the strategies and workarounds we have
developed to overcome these problems, and what advanced numba features
we can leverage.&lt;/p&gt;
&lt;p&gt;There are several solutions to make Python faster, and choosing one is
not easy: we would want it to be fast without sacrificing its
readability and high-level nature. We tried to do it for an
Astrodynamics library using numba. How did it turn out?&lt;/p&gt;
</summary></entry><entry><title>Data sciences in a polyglot world with xtensor and xframe</title><link href="https://pyvideo.org/euroscipy-2019/data-sciences-in-a-polyglot-world-with-xtensor-and-xframe.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Sylvain Corlay</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/data-sciences-in-a-polyglot-world-with-xtensor-and-xframe.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this presentation, we demonstrate how xtensor can be used to
implement numerical methods very efficiently in C++, with a high-level
numpy-style API, and expose it to Python, Julia, and R for free. The
resulting native extension operates in-place on Python, Julia, and R
infrastructures without overhead.&lt;/p&gt;
&lt;p&gt;We then dive into the xframe package, a dataframe project for the C++
programming language, exposing an API very similar to Python's xarray.&lt;/p&gt;
&lt;p&gt;Features of xtensor and xframe will be demonstrated using the xeus-cling
jupyter kernel, enabling interactive use of the C++ programming language
in the notebook.&lt;/p&gt;
&lt;p&gt;The main scientific computing programming languages have different
models the main data structures of data science such as dataframes and
n-d arrays. In this talk, we present our approach to reconcile the data
science tooling in this polyglot world.&lt;/p&gt;
</summary></entry><entry><title>Deep Learning for Understanding Human Multi-modal Behavior</title><link href="https://pyvideo.org/euroscipy-2019/deep-learning-for-understanding-human-multi-modal-behavior.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Ricardo Manhães Savii</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/deep-learning-for-understanding-human-multi-modal-behavior.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Multimedia automatic learning has drawn attention from companies and
governments for a significant number of applications for automated
recommendations, classification, and human brain understatement. In
recent years, and an increased amount of research has explored using
deep neural networks for multimedia related tasks.&lt;/div&gt;
&lt;div class="line"&gt;Some government security and surveillance applications are automated
detections of illegal and violent behaviors, child pornography and
traffic infractions. Companies worldwide are looking for content-based
recommendation systems that can personalize clients consumption and
interactions by understanding the human perception of memorability,
interestingness, attractiveness, aesthetics. For these fields like
event detection, multimedia affect and perceptual analysis are turning
towards Artificial Neural Networks. In this talk, I will present the
theory behind multi-modal fusion using deep learning and some open
challenges and their state-of-the-art.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Multi-modal sources of information are the next big step for AI. In this
talk, I will present the use of deep learning techniques for automated
multi-modal applications and some open benchmarks.&lt;/p&gt;
</summary></entry><entry><title>Deep Learning without a PhD</title><link href="https://pyvideo.org/euroscipy-2019/deep-learning-without-a-phd.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Paige Bailey</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/deep-learning-without-a-phd.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;EuroSciPy 2019 Bilbao
September 5, Thursday
Mitxelena. Talk. 11.00&lt;/p&gt;
&lt;p&gt;Deep Learning without a PhD
Paige Bailey&lt;/p&gt;
</summary></entry><entry><title>Driving a 30m Radio Telescope with Python</title><link href="https://pyvideo.org/euroscipy-2019/driving-a-30m-radio-telescope-with-python.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Francesco Pierfederici</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/driving-a-30m-radio-telescope-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The IRAM 30m radio telescope is one of the best in the world. It has
been in operation non-stop since the mid 80s and is used to observe
24-hours a day, 365 days a year. All of the high-level telescope control
software, monitoring, data archiving as well as some of the data
processing software is written in Python. This choice, controversial at
first, proved to be extremely successful making the IRAM 30m telescope
extremely efficient.&lt;/p&gt;
&lt;p&gt;This talk will describe how Python is used at the telescope, the reasons
behind these choices, lessons learned and future developments.&lt;/p&gt;
&lt;p&gt;The IRAM 30m radio telescope is one of the best in the world. The
telescope control software, monitoring, data archiving as well as some
of the data processing code is written in Python. We will describe how
and why Python is used at the telescope.&lt;/p&gt;
</summary></entry><entry><title>emzed: a Python based framework for analysis of mass-spectrometry data</title><link href="https://pyvideo.org/euroscipy-2019/emzed-a-python-based-framework-for-analysis-of-mass-spectrometry-data.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Uwe Schmitt</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/emzed-a-python-based-framework-for-analysis-of-mass-spectrometry-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many of the existing mass spectrometry data analysis tools are desktop
applications designed for specific applications without support for
customization. In addition, many of the commercial solutions offer no or
only limited functionality for exporting results.&lt;/p&gt;
&lt;p&gt;In addition, the existing programming libraries in this area are
scattered across different languages, mostly R, Java and Python.&lt;/p&gt;
&lt;p&gt;As a result, data analysis in this area often consists of manual
import/export steps from/to various tools and self-developed scripts
that prevent the reproducibility of results obtained or automated
execution on high-performance infrastructures.&lt;/p&gt;
&lt;p&gt;emzed tries to avoid these problems by integrating existing libraries
and tools from Python, R (and in the near future also Java) into an
easy-to-use API.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;To support workflow development and increase confidence in end results&lt;/div&gt;
&lt;div class="line"&gt;emzed also offers tools for interactive visualization of mass
spectrometry related data structures.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The presentation introduces basics and concepts of emzed, some lessons
learned and current development of the next version of emzed.&lt;/p&gt;
&lt;p&gt;This talk is about emzed, a Python library to support biologists with
little programming knowledge to implement ad-hoc analyses as well as
workflows for mass-spectrometry data.&lt;/p&gt;
</summary></entry><entry><title>Enhancing &amp; re-designing the QGIS user interface – a deep dive</title><link href="https://pyvideo.org/euroscipy-2019/enhancing-re-designing-the-qgis-user-interface-a-deep-dive.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Sebastian M. Ernst</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/enhancing-re-designing-the-qgis-user-interface-a-deep-dive.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Having been around for two decades, QGIS clearly is an organically grown
project. It has primarily been fulfilling the various special needs of
its developers. From an outsider's perspective, it is an amazingly rich
patchwork of features. However, some are deeply hidden in numerous
layers of user interface elements, requiring intense training for
getting used to. Others are only accessibly through APIs, requiring not
only training but also programming skills.&lt;/p&gt;
&lt;p&gt;Being confronted with QGIS as professional users on a regular basis, we
thought about what would make working with QGIS more attractive. What if
QGIS has a pleasant, coherent theme, including not only colors but also
icons? What if QGIS had the ability to store workbench configurations?
What if QGIS had dedicated interface configurations for specific
workflows? What if much more of the API's functionality was accessible
through the GUI in a well-organized way? How could QGIS work in a useful
manner with ribbons? How could the incredible amount of dialogs be tamed
into tabs?&lt;/p&gt;
&lt;p&gt;We demonstrate (live) a series of user interface experiments – all of
which are or will be &lt;a class="reference external" href="https://github.com/qgist"&gt;available online&lt;/a&gt; as
Python plugins.&lt;/p&gt;
&lt;p&gt;In this context, the current state of play with respect to Python and
QGIS is explained in detail. The way QGIS is typically being distributed
puts quite a few unusual limitations on Python plugin code. The case is
made that some of those limitations are simply out of date and must be
overcome, which may require help from the broader (scientific) Python
community.&lt;/p&gt;
&lt;p&gt;We seek a conversation with the audience.&lt;/p&gt;
&lt;p&gt;How can one of the largest code bases in open source Geographical
Information Science – QGIS – be enhanced and re-designed? Through the
powers of Python plugins. This talk demonstrates concepts on how to make
QGIS more user- friendly.&lt;/p&gt;
</summary></entry><entry><title>EuroSciPy 2019 Bilbao - Lightning talks - Thursday</title><link href="https://pyvideo.org/euroscipy-2019/euroscipy-2019-bilbao-lightning-talks-thursday.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/euroscipy-2019-bilbao-lightning-talks-thursday.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;EuroSciPy 2019 Bilbao
September 5, Thursday
Mitxelena. Lightning Talks. 17.00&lt;/p&gt;
&lt;p&gt;Thursday's lightning talks session&lt;/p&gt;
</summary></entry><entry><title>Exceeding Classical: Probabilistic Data Structures in Data Intensive Applications</title><link href="https://pyvideo.org/euroscipy-2019/exceeding-classical-probabilistic-data-structures-in-data-intensive-applications.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Andrii Gakhov</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/exceeding-classical-probabilistic-data-structures-in-data-intensive-applications.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Nowadays, research in every scientific domain, from medicine to
astronomy, is impossible without processing huge amounts of data to
check hypotheses, find new relations, and make discoveries. However, the
traditional technologies which include data structures and algorithms,
become ineffective or require too many resources. This creates a demand
for various optimization techniques, new data processing paradigms, and,
finally, appropriate algorithms.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The presentation is dedicated to &lt;em&gt;probabilistic data structures&lt;/em&gt; , that
is a common name for advanced data structures based mostly on different
hashing techniques. Unlike classical ones, these provide approximated
answers but with reliable ways to estimate possible errors and
uncertainty. They are designed for extremely low memory requirements,
constant query time, and scaling, the factors that are essential for
data applications. It is hard to imagine a branch that requires learning
from data, where they cannot be applicable.&lt;/p&gt;
&lt;p&gt;They are not necessarily new. Probably, everybody knows about the Bloom
filter data structure, designed in the 70s, it efficiently solves the
problem of performing membership queries (a task to decide whether some
element belongs to the dataset or not) in a constant time without
requirements to store all elements. This is an example of a
probabilistic data structure, but there are much more that have been
designed for various tasks in many domains.&lt;/p&gt;
&lt;p&gt;In this talk, I explain &lt;strong&gt;the five most important problems in data
processing&lt;/strong&gt; that occurred in different domains but &lt;strong&gt;can be efficiently
solved with probabilistic data structures and algorithms&lt;/strong&gt;. We cover the
&lt;em&gt;membership querying&lt;/em&gt; , &lt;em&gt;counting&lt;/em&gt; of unique elements, &lt;em&gt;frequency&lt;/em&gt; and
&lt;em&gt;rank&lt;/em&gt; estimation in data streams, and &lt;em&gt;similarity&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Everybody interested in such a topic is welcome to participate in
contributing a free and open-source Python (Cython) library called
&lt;a class="reference external" href="https://github.com/gakhov/pdsa"&gt;PDSA&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We interact with an increasing amount of data but classical data
structures and algorithms can't fit our requirements anymore. This talk
is to present the probabilistic algorithms and data structures and
describe the main areas of their applications.&lt;/p&gt;
</summary></entry><entry><title>High performance machine learning with dislib</title><link href="https://pyvideo.org/euroscipy-2019/high-performance-machine-learning-with-dislib.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Javier Álvarez</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/high-performance-machine-learning-with-dislib.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyCOMPSs is a distributed programming model and runtime for Python.
PyCOMPSs' main goal is to make distributed computing accessible to
non-expert developers by providing a simple programming model, and a
runtime that automates many aspects of the parallel execution. In
addition to this, PyCOMPSs is infrastructure agnostic, and can run on
top of a wide range of platforms, from HPC clusters to clouds, and from
GPUs to FPGAs.&lt;/p&gt;
&lt;p&gt;This talk will present dislib, a distributed machine learning library
built on top of PyCOMPSs. Inspired by scikit-learn, dislib programming
interface is based on the concept of &lt;em&gt;estimators&lt;/em&gt;. This provides a clean
and easy-to-use API that highly increases the productivity of building
large-scale machine learning pipelines. Thanks to PyCOMPSs, dislib can
run in multiple distributed platforms without changes in the source
code, and can handle up to billions of input samples using thousands of
CPU cores. This makes dislib a perfect tool for scientists (and other
users) that are not machine learning experts, but that still want to
extract useful knowledge from extremely large data sets.&lt;/p&gt;
&lt;p&gt;This talk will present dislib, a distributed machine learning library
built on top of PyCOMPSs programming model. One of the main focuses of
dislib is solving large-scale scientific problems on high performance
computing clusters.&lt;/p&gt;
</summary></entry><entry><title>High quality video experience using deep neural networks</title><link href="https://pyvideo.org/euroscipy-2019/high-quality-video-experience-using-deep-neural-networks.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Marco Bertini</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/high-quality-video-experience-using-deep-neural-networks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Video compression algorithms result in a reduction of image quality,
because of their lossy approach to reduce the required bandwidth. This
affects commercial streaming services such as Netflix, or Amazon Prime
Video, but affects also video conferencing and video surveillance
systems. In all these cases it is possible to improve the video quality,
both for human view and for automatic video analysis, without changing
the compression pipeline, through a post-processing that eliminates the
visual artefacts created by the compression algorithms. In this
presentation we show how deep convolutional neural networks implemented
in Python using TensorFlow, Scikit-Learn and Scipy can be used to reduce
compression artefacts and reconstruct missing high frequency details
that were eliminated by the compression algorithm.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;In particular, we follow an approach based on Generative Adversarial
Networks, that in the scientific literature have obtained extremely
high quality results in image enhancement tasks. However, to obtain
these results, typically, large generators are employed, resulting in
high computational costs and processing time, and thus the method can
be implemented using GPUs usually available only on desktop machines.&lt;/div&gt;
&lt;div class="line"&gt;In this presentation we show also an architecture that can be used to
reduce the computational cost and that can be implemented also on
mobile devices.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;A possible application is to improve video conferencing, or live
streaming. Since in these cases there is no original uncompressed video
stream available, we report results using no-reference video quality
metric showing high naturalness and quality even for efficient networks.&lt;/p&gt;
&lt;p&gt;Video compression algorithms used to stream videos are lossy, and when
compression rates increase they result in strong degradation of visual
quality. We show how deep neural networks can eliminate compression
artefacts and restore lost details.&lt;/p&gt;
</summary></entry><entry><title>Histogram-based Gradient Boosting in scikit-learn 0.21</title><link href="https://pyvideo.org/euroscipy-2019/histogram-based-gradient-boosting-in-scikit-learn-021.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Olivier Grisel</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/histogram-based-gradient-boosting-in-scikit-learn-021.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;scikit-learn 0.21 was recently released and this presentation will give
an overview its main new features in general and present the new
implementation of Gradient Boosted Trees.&lt;/p&gt;
&lt;p&gt;Gradient Boosted Trees (also known as Gradient Boosting Machines) are
very competitive supervised machine learning models especially on
tabular data.&lt;/p&gt;
&lt;p&gt;Scikit-learn offered a traditional implementation of this family of
methods for many years. However its computational performance was no
longer competitive and was dramatically dominated by specialized state
of the art libraries such as XGBoost and LightGBM. The new
implementation in version 0.21 uses histograms of binned features to
evaluate the tree node spit candidates. This implementation can
efficiently leverage multi-core CPUs and is competitive with XGBoost and
LightGBM.&lt;/p&gt;
&lt;p&gt;We will also introduce pygbm, a numba-based implementation of gradient
boosted trees that was used as prototype for the scikit-learn
implementation and compare the numba vs cython developer experience.&lt;/p&gt;
&lt;p&gt;In this presentation we will present some recently introduced features
of the scikit-learn Machine Learning library with a particular emphasis
on the new implementation of Gradient Boosted Trees.&lt;/p&gt;
</summary></entry><entry><title>How to process hyperspectral data from a prototype imager using Python</title><link href="https://pyvideo.org/euroscipy-2019/how-to-process-hyperspectral-data-from-a-prototype-imager-using-python.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Matti Eskelinen</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/how-to-process-hyperspectral-data-from-a-prototype-imager-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Our lab specializes in hyperspectral imaging using a spectral imager
that combines tunable filters with colour sensors. Compared to simpler,
more established imaging systems, this results in some unique challenges
for the data processing. Especially, many of the original imaging
parameters need to be preserved an d joined with calibration-derived
values to actually compute radiance values from the raw sensor data
since they are not automatically handled by the hardware. Handling this
metadata with the resulting hyperspectral images results in combined
datasets of large 3-dimensional datacube, and multiple smaller 2D and 1D
arrays with linked dimensions.&lt;/p&gt;
&lt;p&gt;We have built our solution to this problem utilizing Xarray for handling
the multiple arrays of data as well as the existing Dask integration for
providing easy parallelization for the required preprocessing. Xarray
also provides us many other advantages, such as:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Exploration of very complex multi-dimensional datasets (especially
when utilizing holoviews)&lt;/li&gt;
&lt;li&gt;Interoperability with the scikit ecosystem&lt;/li&gt;
&lt;li&gt;Serialization to NetCDF preserving all the data in a single file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, our extensive and somewhat non-conventional use of Xarray does
also bring out it's shortcomings when trying to develop such a library
as ours, such as indexing issues with multiple possible overlapping
coordinates and performance issues with complex datasets.&lt;/p&gt;
&lt;p&gt;We present a collection of software for handling hyperspectral data
acquisition and preprocessing fully in Python utilising Xarray for
metadata preservation from start to finish.&lt;/p&gt;
</summary></entry><entry><title>HPC and Python: Intel’s work in enabling the scientific computing community</title><link href="https://pyvideo.org/euroscipy-2019/hpc-and-python-intels-work-in-enabling-the-scientific-computing-community.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>David Liu</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/hpc-and-python-intels-work-in-enabling-the-scientific-computing-community.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;EuroSciPy 2019 Bilbao
September 5, Thursday
Mitxelena. Keynote Talk. 9.15&lt;/p&gt;
&lt;p&gt;HPC and Python: Intel’s work in enabling the scientific computing community
David Liu&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>In the Shadow of the Black Hole</title><link href="https://pyvideo.org/euroscipy-2019/in-the-shadow-of-the-black-hole.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Sara Issaoun</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/in-the-shadow-of-the-black-hole.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;EuroSciPy 2019 Bilbao
September 5, Thursday
Mitxelena. Keynote Talk. 14.00&lt;/p&gt;
&lt;p&gt;In the Shadow of the Black Hole
Sara Issaoun&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>Inside NumPy: preparing for the next decade</title><link href="https://pyvideo.org/euroscipy-2019/inside-numpy-preparing-for-the-next-decade.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Matti Picus</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/inside-numpy-preparing-for-the-next-decade.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Over the past year, and for the first time since its creation, NumPy has
been operating with dedicated funding. NumPy developers think it has
invigorated the project and its community. But is that true, and how can
we know?&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;We will give an overview of the actions we’ve taken, both successful
and unsuccessful, to improve sustainability of the NumPy project and
its community. We will draw some lessons from a first year of
grant-funded activity, discuss key obstacles faced, attempt to
quantify what we need to operate sustainably, and present a vision for
the project and how we plan to realize it.&lt;/div&gt;
&lt;div class="line"&gt;Topics we will cover include the following:&lt;/div&gt;
&lt;div class="line"&gt;- Invigorating the community - what did we do, and are we correct in
our opinion that it invigorated the community?&lt;/div&gt;
&lt;div class="line"&gt;- doing things in the open as much as possible&lt;/div&gt;
&lt;div class="line"&gt;- creating a roadmap&lt;/div&gt;
&lt;div class="line"&gt;- NumPy Enhancement Proposal process&lt;/div&gt;
&lt;div class="line"&gt;- commit rights&lt;/div&gt;
&lt;div class="line"&gt;- in-person meetings&lt;/div&gt;
&lt;/div&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Measuring community/project health. We will use a number of published
or proposed metrics to quantify this. Which ones do we think
accurately represent the state of the project?&lt;/li&gt;
&lt;li&gt;Lessons from the first grant and introducing paid work into a
previously fully volunteer-driven project.&lt;/li&gt;
&lt;li&gt;What is the best profile for a salaried employee?&lt;ul&gt;
&lt;li&gt;Social profile&lt;/li&gt;
&lt;li&gt;From inside or outside?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Have we succeeded in encouragin diversity?&lt;/li&gt;
&lt;li&gt;A vision for future sustainabity&lt;/li&gt;
&lt;li&gt;Models for obtaining and funneling funding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Over the past year, and for the first time since its creation, NumPy has
been operating with dedicated funding. NumPy developers think it has
invigorated the project and its community. But is that true, and how can
we know?&lt;/p&gt;
</summary></entry><entry><title>Kubeflow Kale: from Jupyter Notebook to Complex Pipelines</title><link href="https://pyvideo.org/euroscipy-2019/kubeflow-kale-from-jupyter-notebook-to-complex-pipelines.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Valerio Maggio</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/kubeflow-kale-from-jupyter-notebook-to-complex-pipelines.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I will present a new solution to automatically scale
Jupyter notebooks to complex and reproducibility pipelines based on
Kubernetes and KubeFlow.&lt;/p&gt;
&lt;p&gt;Nowadays, most of the High Performance Computing (HPC) tasks are carried
out in the Cloud, and this is as much as in industry as in research.&lt;/p&gt;
&lt;p&gt;Main advantages provided by the adoption of Cloud services include (a)
constant up-to-date hardware resources; (b) automated infrastructure
setup; (c) simplified resource management. Therefore, new solutions have
been recently released to the community (e.g. &lt;em&gt;Kubernetes&lt;/em&gt; by Google)
providing custom integrations to specifically support the migration of
existing Machine/Deep Learning pipelines to the Cloud.&lt;/p&gt;
&lt;p&gt;However, a shift towards a complete Cloud-based computational paradigm
imposes new challenges in terms of data and model reproducibility,
privacy, accountability, and (efficient) resource configuration and
monitoring. Moreover, the adoption of these technologies still imposes
additional workloads requiring significant software and system
engineering expertise (e.g. set up of containerised environments,
storage volumes, clusters nodes).&lt;/p&gt;
&lt;p&gt;In this talk, I will present &lt;strong&gt;kale&lt;/strong&gt; (&lt;tt class="docutils literal"&gt;/ˈkeɪliː/&lt;/tt&gt;) - a new Python
solution to ease and support ML workloads for HPC in the Cloud is
presented.&lt;/p&gt;
&lt;p&gt;Kale leverages on the combination of Jupyter &lt;tt class="docutils literal"&gt;notebooks&lt;/tt&gt;, and
&lt;em&gt;Kubernetes/Kubeflow Pipelines&lt;/em&gt; (&lt;tt class="docutils literal"&gt;KFP&lt;/tt&gt;) as core components in order
to:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;(&lt;tt class="docutils literal"&gt;R1&lt;/tt&gt;) automate the setup and deployment procedures by automating
the creation of (distributed) computation environments in the Cloud;&lt;/li&gt;
&lt;li&gt;(&lt;tt class="docutils literal"&gt;R2&lt;/tt&gt;) democratise the execution of machine learning models at
scale by instrumented and reusable environments;&lt;/li&gt;
&lt;li&gt;(&lt;tt class="docutils literal"&gt;R3&lt;/tt&gt;) provide a simple interface (UI, and SDK) to enable
researchers to deploy ML models without requiring extensive
engineering expertise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Technical features of Kale as well as open challenges and future
development will be presented, along with working examples integrating
&lt;tt class="docutils literal"&gt;kale&lt;/tt&gt; with the complete ML/DL workflows for pipeline reproducibility.&lt;/p&gt;
&lt;div class="section" id="domains"&gt;
&lt;h4&gt;Domains:&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Jupyter&lt;/li&gt;
&lt;li&gt;Machine Learning&lt;/li&gt;
&lt;li&gt;DevOps&lt;/li&gt;
&lt;li&gt;Parallel Computing/HPC&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="github"&gt;
&lt;h4&gt;GitHub:&lt;/h4&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/orgs/kubeflow-kale"&gt;https://github.com/orgs/kubeflow-kale&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</summary></entry><entry><title>Matrix calculus with SymPy</title><link href="https://pyvideo.org/euroscipy-2019/matrix-calculus-with-sympy.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Francesco Bonazzi</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/matrix-calculus-with-sympy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The recent popularization of libraries relying on tensor algebra
operations has led to a rise in the requirement of computational tools
to calculate the gradient and hessian of tensorial expressions. The
derivative of a tensor &lt;em&gt;A&lt;/em&gt; by tensor &lt;em&gt;B&lt;/em&gt; is the tensor containing all
combinations of the elements of &lt;em&gt;A&lt;/em&gt; derived by the elements of &lt;em&gt;B&lt;/em&gt;.
While tensor derivative operations are commonly supported by most
computer algebra systems and frameworks through iterative algorithms,
these derivatives can be expressed mathematically in closed-form
solutions, which are computationally many orders of magnitude faster.&lt;/p&gt;
&lt;p&gt;SymPy has been recently extended in order to support the computation of
symbolic matrix derivatives, and is currently the only computer algebra
system endowed with this feature (lacking even in Wolfram Mathematica).
Matrix calculus plays indeed a central role in optimization and machine
learning, but was unfortunately often limited to pen on papers or chalk
on blackboards.&lt;/p&gt;
&lt;p&gt;In this talk, we will introduce matrix expressions in SymPy, and address
the three ways they can be represented:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;explicit matrices with symbolic entries,&lt;/li&gt;
&lt;li&gt;indexed symbols with proper summation convention,&lt;/li&gt;
&lt;li&gt;implicit matrix expressions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We illustrate the way matrix derivatives are implemented for all three
representations, with special emphasis to the third way, the fastest and
most elegant. The derived expressions can then be passed to SymPy's code
generation utilities and the resulting code can be compared in speed
with other frameworks, such as TensorFlow.&lt;/p&gt;
&lt;p&gt;The support of matrix derivatives can turn SymPy into a simple tool to
create the code for optimization algorithms or the code to train machine
learning algorithms. The code generation utilities of SymPy are indeed
aware of how to export matrix expressions into other programming
languages and frameworks. We will give some examples using maximum
likelihood estimation and the expectation-maximization algorithms.&lt;/p&gt;
&lt;p&gt;In this talk we explore a recent addition to SymPy which allows to find
closed-form solutions to matrix derivatives. As a consequence,
generation of efficient code for optimization problems is now much
easier.&lt;/p&gt;
</summary></entry><entry><title>PSYDAC: a parallel finite element solver with automatic code generation</title><link href="https://pyvideo.org/euroscipy-2019/psydac-a-parallel-finite-element-solver-with-automatic-code-generation.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Yaman Güçlü</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/psydac-a-parallel-finite-element-solver-with-automatic-code-generation.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PSYDAC is a Python 3 library for the solution of partial differential
equations. Its current focus is on isogeometric analysis using B-spline
finite elements, but extensions to other methodologies are under
consideration. In order to use PSYDAC, the user defines geometry and
model equations in an abstract form using SymPDE, an extension of Sympy
that provides the mathematical expressions and checks their semantic
validity. Once a finite element discretization has been chosen, PSYDAC
maps the abstract concepts into concrete objects, the basic building
blocks being MPI-distributed vectors and matrices. Python code is
generated for all the computationally intensive operations (matrix and
vector assembly, matrix-vector products, etc.), and it is accelerated
using either Numba, Pythran, or Pyccel. We present the library design,
the user interface, and the performance results.&lt;/p&gt;
&lt;p&gt;PSYDAC takes input from SymPDE (a SymPy extension for partial
differential equations), applies a finite-element discretization,
generates MPI-parallel code, and accelerates it with Numba, Pythran, or
Pyccel. We present design, usage and performance.&lt;/p&gt;
</summary></entry><entry><title>PyPy meets SciPy</title><link href="https://pyvideo.org/euroscipy-2019/pypy-meets-scipy.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Ronan Lamy</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/pypy-meets-scipy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyPy is a fast and compliant implementation of Python. In other words,
it's an interpreter for the Python language that can act as a full
replacement for the reference interpreter, CPython. It's optimised to
enable efficient just-in- time compilation of Python code to machine
code, and has releases matching versions 2.7, and 3.6. It now also
supports the main pillars of the scientific ecosystem (numpy, Cython,
scipy, pandas, ...) thanks to its emulation layer for the C API of
CPython.&lt;/p&gt;
&lt;p&gt;Performance is a major concern for Python programmers. When using
CPython, this leads to splitting out the performance-sensitive parts of
the computation and rewriting them in a faster, but less convenient,
language such as C or Cython. With PyPy, there is no need to choose
between clear, Pythonic code and good performance. This talk aims to
convince the audience that PyPy should be part of every scientific
programmer's toolbox.&lt;/p&gt;
&lt;p&gt;PyPy, the fast and compliant alternative implementation of Python, is
now compatible with the SciPy ecosystem. We'll explore how scientific
programmers can use it.&lt;/p&gt;
</summary></entry><entry><title>Recent advances in python parallel computing</title><link href="https://pyvideo.org/euroscipy-2019/recent-advances-in-python-parallel-computing.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Pierre Glaser</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/recent-advances-in-python-parallel-computing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;&lt;em&gt;Modern hardware is multi-core&lt;/em&gt;. It is crucial for Python to provide&lt;/div&gt;
&lt;div class="line"&gt;high-performance parallelism. This talk will expose to both
data-scientists and&lt;/div&gt;
&lt;div class="line"&gt;library developers the current state of affairs and the recent
advances for&lt;/div&gt;
&lt;div class="line"&gt;parallel computing with Python. The goal is to help practitioners and&lt;/div&gt;
&lt;div class="line"&gt;developers to make better decisions on this matter.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;I will first cover how Python can interface with parallelism, from
leveraging&lt;/div&gt;
&lt;div class="line"&gt;external parallelism of C-extensions –especially the BLAS family– to
Python's&lt;/div&gt;
&lt;div class="line"&gt;multiprocessing and multithreading API. I will touch upon use cases,
e.g single&lt;/div&gt;
&lt;div class="line"&gt;vs multi machine, as well as and pros and cons of the various
solutions for&lt;/div&gt;
&lt;div class="line"&gt;each use case. Most of these considerations will be backed by
benchmarks from&lt;/div&gt;
&lt;div class="line"&gt;the &lt;a class="reference external" href="https://scikit-learn.org/stable/"&gt;scikit-learn&lt;/a&gt; machine&lt;/div&gt;
&lt;div class="line"&gt;learning library.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;From these low-level interfaces emerged higher-level parallel
processing&lt;/div&gt;
&lt;div class="line"&gt;libraries, such as concurrent.futures,&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://joblib.readthedocs.io/en/latest/"&gt;joblib&lt;/a&gt; and&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://loky.readthedocs.io/en/latest/"&gt;loky&lt;/a&gt; (used by&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://dask.org/"&gt;dask&lt;/a&gt; and &lt;a class="reference external" href="https://dask.org/"&gt;scikit-learn&lt;/a&gt;)
These&lt;/div&gt;
&lt;div class="line"&gt;libraries make it easy for Python programmers to use safe and reliable&lt;/div&gt;
&lt;div class="line"&gt;parallelism in their code. They can even work in more exotic
situations, such&lt;/div&gt;
&lt;div class="line"&gt;as interactive sessions, in which Python’s native multiprocessing
support tends&lt;/div&gt;
&lt;div class="line"&gt;to fail. I will describe their purpose as well as the canonical
use-cases they&lt;/div&gt;
&lt;div class="line"&gt;address.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;The last part of this talk will focus on the most recent advances in
the Python&lt;/div&gt;
&lt;div class="line"&gt;standard library, addressing one of the principal performance
bottlenecks of&lt;/div&gt;
&lt;div class="line"&gt;multi-core/multi-machine processing, which is data communication. We
will&lt;/div&gt;
&lt;div class="line"&gt;present a &lt;a class="reference external" href="https://docs.python.org/3.8/library/multiprocessing.shared_memory.html"&gt;new
API&lt;/a&gt;&lt;/div&gt;
&lt;div class="line"&gt;for shared-memory management between different Python processes, and&lt;/div&gt;
&lt;div class="line"&gt;performance improvements for the serialization of large Python objects
(&lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0574/"&gt;PEP
574&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/cloudpipe/cloudpickle"&gt;pickle
extensions&lt;/a&gt;). These
performance&lt;/div&gt;
&lt;div class="line"&gt;improvements will be leveraged by distributed data science frameworks
such as&lt;/div&gt;
&lt;div class="line"&gt;dask, &lt;a class="reference external" href="https://ray.readthedocs.io/en/latest/"&gt;ray&lt;/a&gt; and&lt;/div&gt;
&lt;div class="line"&gt;&lt;a class="reference external" href="https://spark.apache.org/docs/latest/api/python/index.html"&gt;pyspark&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;&lt;em&gt;Modern hardware is multi-core&lt;/em&gt;. It is crucial for Python to provide&lt;/div&gt;
&lt;div class="line"&gt;efficient parallelism. This talk exposes the current state and
advances&lt;/div&gt;
&lt;div class="line"&gt;in Python parallelism, in order to help practitioners and developers
take&lt;/div&gt;
&lt;div class="line"&gt;better decisions on this matter.&lt;/div&gt;
&lt;/div&gt;
</summary></entry><entry><title>TelApy a Python module to compute free surface flows and sediments transport in geosciences</title><link href="https://pyvideo.org/euroscipy-2019/telapy-a-python-module-to-compute-free-surface-flows-and-sediments-transport-in-geosciences.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Yoann Audouin</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/telapy-a-python-module-to-compute-free-surface-flows-and-sediments-transport-in-geosciences.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk is focused on the application of TelApy module
(&lt;a class="reference external" href="http://www.opentelemac.org"&gt;www.opentelemac.org&lt;/a&gt;). TelApy aims to
provide a Python wrapper of TELEMAC-MASCARET API (Application Program
Interface). The goal of TelApy is to have a full control on the
simulation while running a case. For example, it must allow the user to
stop the simulation at any time step, get values of some variables and
change them. In order to make this possible, a Fortran structure called
instantiation was developed with the API. It contains a list of strings
pointing to TELEMAC variables. This gives direct access to the physical
memory of variables, and allows therefore to get and set their values.
Furthermore, changes have been made in TELEMAC-MASCARET main subroutines
to make hydraulic cases execution possible time step by time step. It is
useful to drive the TELEMAC-MASCARET SYSTEM APIs using Python
programming language. In fact, Python is a portable, dynamic,
extensible, free language, which allows (without imposing) a modular
approach and object oriented programming. In addition of benefits of
this programming language, Python offers a large amounts of
interoperable libraries. The link between various interoperable
libraries with TELEMAC-MASCARET SYSTEM APIs allows the creation of an
ever more efficient computing chain able to more finely respond to
various complex problems. Therefore, the TelApy module has the ambition
to enable a new way of use for the TELEMAC-MASCARET system. In
particular one can think about high performance computing for the
calculation of uncertainties, optimization, code coupling and so on. The
objectives of this talk is to present some examples of the TelApy module
in the case of Uncertainty Quantification, Optimization, Reduced Order
Model.&lt;/p&gt;
&lt;p&gt;TelApy a Python module to compute free surface flows and sediments
transport in geosciences and examples of how it is used to inter-operate
with other Python libraries for Uncertainty Quantification,
Optimization, Reduced Order Model.&lt;/p&gt;
</summary></entry><entry><title>The Magic of Neural Embeddings with TensorFlow 2</title><link href="https://pyvideo.org/euroscipy-2019/the-magic-of-neural-embeddings-with-tensorflow-2.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Oliver Zeigermann</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/the-magic-of-neural-embeddings-with-tensorflow-2.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Symbols, words, categories etc. need to be converted into numbers before
they can be processed by neural networks or used into other ML methods
like clustering or outlier detection.&lt;/p&gt;
&lt;p&gt;It is desirable to have the converted numbers represent semantics of the
encoded categories. That means, numbers close to each other indicate
similar semantics.&lt;/p&gt;
&lt;p&gt;In this session you will learn what you need to train a neural network
for such embeddings. I will bring a complete example including code that
I will share using TensorFlow 2 functional API and the Colab service.&lt;/p&gt;
&lt;p&gt;I will also share some tricks how to stabilize embeddings when either
the model changes or you get more training data.&lt;/p&gt;
&lt;p&gt;Neural Embeddings are a powerful tool of turning categorical into
numerical values. Given reasonable training data semantics present in
the categories can be preserved in the numerical representation.&lt;/p&gt;
</summary></entry><entry><title>Understanding Numba</title><link href="https://pyvideo.org/euroscipy-2019/understanding-numba.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Valentin Haenel</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/understanding-numba.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;In this talk I will take you on a whirlwind tour of Numba, the
just-in-time,&lt;/div&gt;
&lt;div class="line"&gt;type-specializing, function compiler for accelerating
numerically-focused&lt;/div&gt;
&lt;div class="line"&gt;Python. Numba can compile the computationally intensive functions of
your&lt;/div&gt;
&lt;div class="line"&gt;numerical programs and libraries from Python/NumPy to highly optimized
binary&lt;/div&gt;
&lt;div class="line"&gt;code. It does this by inferring the data types used inside these
functions and&lt;/div&gt;
&lt;div class="line"&gt;uses that information to generate code that is specific to those data
types&lt;/div&gt;
&lt;div class="line"&gt;and specialised for your target hardware. On top of that, it does all
of this&lt;/div&gt;
&lt;div class="line"&gt;on-the-fly---or just-in-time---as your program runs. This
significantly reduces&lt;/div&gt;
&lt;div class="line"&gt;the potential complexity that traditionally comes with pre-compiling
and&lt;/div&gt;
&lt;div class="line"&gt;shipping numerical code for a variety of operating systems, Python
versions and&lt;/div&gt;
&lt;div class="line"&gt;hardware architectures. All you need in principle, is to
&lt;tt class="docutils literal"&gt;conda install numba&lt;/tt&gt;&lt;/div&gt;
&lt;div class="line"&gt;and decorate your compute intensive functions with &lt;tt class="docutils literal"&gt;&amp;#64;nuba.jit&lt;/tt&gt;!&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;This talk will equip you with a mental model of how Numba is
implemented and&lt;/div&gt;
&lt;div class="line"&gt;how it works at the algorithmic level. You will gain a deeper
understanding of&lt;/div&gt;
&lt;div class="line"&gt;the types of use-cases where Numba excels and why. Also, you will
understand&lt;/div&gt;
&lt;div class="line"&gt;the limitations and caveats that exist within Numba, including any
potential&lt;/div&gt;
&lt;div class="line"&gt;ideas and strategies that might alleviate these. At the end of the
talk you&lt;/div&gt;
&lt;div class="line"&gt;will be in a good position to decide if Numba is for you and you will
have&lt;/div&gt;
&lt;div class="line"&gt;learnt about the concrete steps you need to take to include it as a
dependency&lt;/div&gt;
&lt;div class="line"&gt;in your program or library.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In this talk I will take you on a whirlwind tour of Numba and you will
be quipped with a mental model of how Numba works and what it is good
at. At the end, you will be able to decide if Numba could be useful for
you.&lt;/p&gt;
</summary></entry><entry><title>VeloxChem: Python meets quantum chemistry and HPC</title><link href="https://pyvideo.org/euroscipy-2019/veloxchem-python-meets-quantum-chemistry-and-hpc.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Olav Vahtras</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/veloxchem-python-meets-quantum-chemistry-and-hpc.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Zilvinas Rinkevicius, Xin Li, Olav Vahtras, Manuel Brand, Karan
Ahmadzadeh, Magnus&lt;/div&gt;
&lt;div class="line"&gt;Ringholm, Nanna List, and Patrick Norman&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;With the ease of Python library modules, VeloxChem offers a front end
to quantum chemical&lt;/div&gt;
&lt;div class="line"&gt;calculations on contemporary high-performance computing (HPC) systems
and aims at&lt;/div&gt;
&lt;div class="line"&gt;harnessing the future compute power within the EuroHPC initiative. At
the heart of this&lt;/div&gt;
&lt;div class="line"&gt;software lies a module for the evaluation of electron-repulsion
integrals (ERIs) using the ObaraSaika recurrence scheme, where a high
degree of efficiency is achieved by employing&lt;/div&gt;
&lt;div class="line"&gt;architecture-independent vectorization via OpenMP SIMD pragmas in the
auto- generated C++&lt;/div&gt;
&lt;div class="line"&gt;source code. The software is topology aware and with a
Python-controlled work and task flow,&lt;/div&gt;
&lt;div class="line"&gt;the idle time is minimized using an MPI/OpenMP partitioning of
resources.&lt;/div&gt;
&lt;div class="line"&gt;In the second software layer, we have implemented a highly accurate
SCF start guess based&lt;/div&gt;
&lt;div class="line"&gt;on atomic densities and a first-level of iterations in a reduced
version of the user-defined basis&lt;/div&gt;
&lt;div class="line"&gt;set, leading to a very smooth convergence in the subsequent standard
DIIS scheme. This layer&lt;/div&gt;
&lt;div class="line"&gt;also includes vectorized and OpenMP/MPI parallelized modules for
efficient generation of DFT&lt;/div&gt;
&lt;div class="line"&gt;grid points and weights as well as kernel integration.&lt;/div&gt;
&lt;div class="line"&gt;In the third software layer, we present real and complex response
functions as to address&lt;/div&gt;
&lt;div class="line"&gt;dispersive and absorptive molecular properties in spectroscopy. The
kernel module in this layer&lt;/div&gt;
&lt;div class="line"&gt;is the iterative linear response equation solver that we have
formulated and implemented for a&lt;/div&gt;
&lt;div class="line"&gt;combination of multiple optical frequencies and multiple perturbation
operators. With efficient&lt;/div&gt;
&lt;div class="line"&gt;use of computer memory, we enable the simultaneous reference to, and
solving of, in the order&lt;/div&gt;
&lt;div class="line"&gt;of 1,000 response equations for sizable biochemical systems without
spatial symmetry, and we&lt;/div&gt;
&lt;div class="line"&gt;can thereby determine electronic response spectra in arbitrary
wavelength regions, including&lt;/div&gt;
&lt;div class="line"&gt;UV/vis and X-Ray, without resolving the sometimes embedded excited
states in the spectrum.&lt;/div&gt;
&lt;div class="line"&gt;E.g. the electronic CD spectrum (involving the Cartesian sets of
electric and magnetic&lt;/div&gt;
&lt;div class="line"&gt;perturbations) over a range of some 10 eV is obtained at a
computational cost comparable to&lt;/div&gt;
&lt;div class="line"&gt;that of determining the transition energy of the lowest excited state,
or optimizing the electronic&lt;/div&gt;
&lt;div class="line"&gt;structure of the reference state.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;A new and efficient Python/C++ modular library for real and complex
response functions at the&lt;/div&gt;
&lt;div class="line"&gt;level of Kohn-Sham density functional theory&lt;/div&gt;
&lt;/div&gt;
</summary></entry><entry><title>Visual Diagnostics at Scale</title><link href="https://pyvideo.org/euroscipy-2019/visual-diagnostics-at-scale.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Dr. Rebecca Bilbro</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/visual-diagnostics-at-scale.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Even with a modestly-sized dataset, the hunt for the most effective
machine learning model is &lt;em&gt;hard&lt;/em&gt;. Arriving at the optimal combination of
features, algorithm, and hyperparameters frequently requires significant
experimentation and iteration. This leads some of us to stay inside
algorithmic comfort zones, some to trail off on random walks, and others
to resort to automated processes like gridsearch. But whatever path we
take, we are often left in doubt about whether our final solution really
is the optimal one. And as our datasets grow in size and dimension, so
too does this ambiguity.&lt;/p&gt;
&lt;p&gt;Fortunately, many of us have developed strategies for steering model
search. Open source libraries like
&lt;a class="reference external" href="https://seaborn.pydata.org/"&gt;seaborn&lt;/a&gt;,
&lt;a class="reference external" href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt; and
&lt;a class="reference external" href="https://www.scikit-%20yb.org/en/latest/"&gt;yellowbrick&lt;/a&gt; can help make
machine learning more informed with visual diagnostic tools like
histograms, correlation matrices, parallel coordinates, manifold
embeddings, validation and learning curves, residuals plots, and
classification heatmaps. These tools enable us to tune our models with
visceral cues that allow us to be more strategic in our choices.
Visualizing feature transformations, algorithmic behavior,
cross-validation methods, and model performance allows us a peek into
the multi-dimensional realm in which our models operate.&lt;/p&gt;
&lt;p&gt;However, large, high-dimensional datasets can prove particularly
difficult to explore. Not only do the majority of people struggle to
visualize anything beyond two- or three-dimensional space, many of our
favorite open source Python tools are not designed to be performant with
arbitrarily big data. So how well &lt;em&gt;do&lt;/em&gt; our favorite visualization
techniques hold up to large, complex datasets?&lt;/p&gt;
&lt;p&gt;In this talk, we'll consider a suite of visual diagnostics — some
familiar and some new — and explore their strengths and weaknesses with
several publicly available datasets of varying size. Which suffer most
from the curse of dimensionality in face of increasingly big data? What
are the workarounds (e.g. sampling, brushing, filtering, etc.) and when
should we use them? And most importantly, how can we continue to steer
the machine learning process — not only purposefully but at scale?&lt;/p&gt;
&lt;p&gt;Machine learning is a search for the best combination of features,
model, and hyperparameters. But as data grow, so does the search space!
Fortunately, visual diagnostics can focus our search and allow us to
steer modeling purposefully, and at scale.&lt;/p&gt;
</summary></entry><entry><title>vtext: fast text processing in Python using Rust</title><link href="https://pyvideo.org/euroscipy-2019/vtext-fast-text-processing-in-python-using-rust.html" rel="alternate"></link><published>2019-09-05T00:00:00+00:00</published><updated>2019-09-05T00:00:00+00:00</updated><author><name>Roman Yurchak</name></author><id>tag:pyvideo.org,2019-09-05:euroscipy-2019/vtext-fast-text-processing-in-python-using-rust.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Scientific Python has historically relied on compiled extensions for
performance critical parts of the code. In this talk, we outline how
to write Rust extensions for Python using
&lt;a class="reference external" href="https://github.com/rust-%20numpy/rust-numpy"&gt;rust-numpy&lt;/a&gt;,&lt;/div&gt;
&lt;div class="line"&gt;project. Advantages and limitations of this approach as compared to
Cython or wrapping Fortran, C or C++ are also discussed.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In the second part, we introduce the
&lt;a class="reference external" href="https://github.com/rth/vtext"&gt;vtext&lt;/a&gt; project that allows fast text
processing in Python using Rust. In particular, we consider the problems
of text tokenization, and (parallel) token counting resulting in a
sparse vector representation of documents. These can then be used as
input in machine learning or information retrieval applications. We
outline the approach used in vtext and compare to existing solutions of
these problems in the Python ecosystem.&lt;/p&gt;
&lt;p&gt;In this talk, we present some of the benefits of writing extensions for
Python in Rust. We then illustrate this approach on the
&lt;a class="reference external" href="https://github.com/rth/vtext"&gt;vtext&lt;/a&gt; project, that aims to be a
high- performance library for text processing.&lt;/p&gt;
</summary></entry><entry><title>Apache Arrow: a cross-language development platform for in-memory data</title><link href="https://pyvideo.org/euroscipy-2019/apache-arrow-a-cross-language-development-platform-for-in-memory-data.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Joris Van den Bossche</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/apache-arrow-a-cross-language-development-platform-for-in-memory-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk discusses Apache Arrow project and how it already interacts
with the Python ecosystem.&lt;/p&gt;
&lt;p&gt;The Apache Arrow project specifies a standardized language-independent
columnar memory format for flat and nested data, organized for efficient
analytic operations on modern hardware. On top of that standard, it
provides computational libraries and zero-copy streaming messaging and
interprocess communication protocols, and as such, it provides a
cross-language development platform for in-memory data. It has support
for many languages, including C, C++, Java, JavaScript, MATLAB, Python,
R, Rust, ..&lt;/p&gt;
&lt;p&gt;The Apache Arrow project, although still in active development, has
already several applications in the Python ecosystem. For example, it
provides the IO functionality for pandas to read the Parquet format (a
columnar, binary file format used a lot in the Hadoop ecosystem). Thanks
to the standard memory format, it can help improve interoperability
between systems, and this is already seen in practice for the Spark /
Python interface, by increasing the performance of PySpark. Further, it
has the potential to provide a more performant string data type and
nested data types (like dicts or lists) for Pandas dataframes, which is
already being experimented with in the fletcher package (using the
pandas ExtensionArray interface).&lt;/p&gt;
&lt;p&gt;Apache Arrow, defining a columnar, in-memory data format standard and
communication protocols, provides a cross-language development platform
with already several applications in the PyData ecosystem.&lt;/p&gt;
</summary></entry><entry><title>Best Coding Practices in Jupyterlab</title><link href="https://pyvideo.org/euroscipy-2019/best-coding-practices-in-jupyterlab.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Alexander CS Hendorf</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/best-coding-practices-in-jupyterlab.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Jupyter notebooks are often a mess. The code produced is working for
one notebook, but it's hard to maintain or to re-use.&lt;/div&gt;
&lt;div class="line"&gt;In this talks I will present some best practices to make code more
readable, better to maintain and re-usable.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;This will include:&lt;/div&gt;
&lt;div class="line"&gt;- versioning best practices&lt;/div&gt;
&lt;div class="line"&gt;- how to use submodules&lt;/div&gt;
&lt;div class="line"&gt;- coding methods to avoid (e.g. closures)&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Jupyter notebooks are often a mess. The code produced is working for one
notebook, but it's hard to maintain or to re-use. In this talks I will
present some best practices to make code more readable, better to
maintain and re- usable.&lt;/p&gt;
</summary></entry><entry><title>Caterva: A Compressed And Multidimensional Container For Big Data</title><link href="https://pyvideo.org/euroscipy-2019/caterva-a-compressed-and-multidimensional-container-for-big-data.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/caterva-a-compressed-and-multidimensional-container-for-big-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/Blosc/Caterva"&gt;Caterva&lt;/a&gt; is a C library on top of
&lt;a class="reference external" href="https://github.com/Blosc/c-blosc2"&gt;C-Blosc2&lt;/a&gt; that implements a
simple multidimensional container for compressed binary data. It adds
the capability to store, extract, and transform data in these
containers, either in-memory or on-disk.&lt;/p&gt;
&lt;p&gt;While there are several existing solutions for this scenario (HDF5 is
one of the most known), Caterva brings novel features that, when taken
toghether, set it appart from them:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Leverage important features of C-Blosc2&lt;/strong&gt;. C-Blosc2 is the next
generation of the well-know, high performance C-Blosc compression
library (see below for a more in-depth description).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast and seamless interface with the compression engine&lt;/strong&gt;. While in
other solutions compression seems an after-thought and can implies
several copies of buffers internally, the interface of Caterva and
C-Blosc2 (its internal compression engine) is meant to be as direct
as possible minimizing copies and hence, increasing performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Both in-memory and on-disk paradigms are supported the same way&lt;/strong&gt;.
This allows for using the same API for data that can be either
in-memory or on-disk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Support for a plain buffer data layout&lt;/strong&gt;. This allows for
essentially no-copy data sharing among existing libraries (NumPy),
allowing to use existing functionality to be used directly in Caterva
without loosing performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Along this features, there is an important 'mis-feature': Caterva is
&lt;strong&gt;type- less&lt;/strong&gt;. Lacking the notion of data type means that Caterva
containers are not meant to be used in computations directly, but rather
in combination with other higher-level libraries. While this can be seen
as a drawback, it actually favors simplicity and leaves up to the user
the addition of the types that he is more interested in, which is far
more flexible than typed-aware libraries (HDF5, NumPy and many others).&lt;/p&gt;
&lt;p&gt;During our talk, we will describe all these Caterva features by using
&lt;a class="reference external" href="https://github.com/Blosc/cat4py"&gt;cat4py&lt;/a&gt;, a Python wrapper for
Caterva. Among the points to be discussed would be:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to the main features of Caterva.&lt;/li&gt;
&lt;li&gt;Description of the basic data container and its usage.&lt;/li&gt;
&lt;li&gt;Short discussion of different use cases:&lt;/li&gt;
&lt;li&gt;Create and fill high dimensional arrays.&lt;/li&gt;
&lt;li&gt;Get multi-dimensional slices out of the arrays.&lt;/li&gt;
&lt;li&gt;How different compression codecs and filters in the pipeline affect
store/retrieval performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have been using Caterva in one of our internal projects for several
months now, and we are pretty happy with the flexibility and easy-of-use
that it brings to us. This is why we decided to open-source it in the
hope that it would benefit others, but also that others may help us in
developing it further ;-)&lt;/p&gt;
&lt;div class="section" id="about-c-blosc-and-c-blosc2"&gt;
&lt;h4&gt;About C-Blosc and C-Blosc2&lt;/h4&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/Blosc/c-blosc"&gt;C-Blosc&lt;/a&gt; is a high performance
compressor optimized for binary data. It has been designed to transmit
data to the processor cache faster than the traditional, non-compressed,
direct memory fetch approach via a memcpy() OS call. Blosc is the first
compressor (that we are aware of) that is meant not only to reduce the
size of large datasets on- disk or in-memory, but also to accelerate
memory-bound computations.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/Blosc/c-blosc2"&gt;C-Blosc2&lt;/a&gt; is the new major
version of C-Blosc, with a revamped API and support for new compressors
and new filters (data transformations), including filter pipelining,
that is, the capability to apply different filters during the
compression pipeline, allowing for more adaptability to the data to be
compressed. Dictionaries are also introduced, allowing better handling
of redundancies among independent blocks and generally increasing
compression ratio and performance. Last but not least, there are new
data containers that are meant to overcome the 32-bit limitation of the
original C-Blosc. Furthermore, the new data containers are available in
various formats, including in-memory and on-disk implementations.&lt;/p&gt;
&lt;p&gt;Caterva is a library on top of the Blosc2 compressor that implements a
simple multidimensional container for compressed binary data. It adds
the capability to store, extract, and transform data in these
containers, either in-memory or on-disk.&lt;/p&gt;
&lt;/div&gt;
</summary></entry><entry><title>Constrained Data Synthesis</title><link href="https://pyvideo.org/euroscipy-2019/constrained-data-synthesis.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Nick Radcliffe</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/constrained-data-synthesis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Synthetic data is useful in many contexts, including&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;providing &amp;quot;safe&amp;quot;, non-private alternatives to data containing
personally identifiable information&lt;/li&gt;
&lt;li&gt;software and pipeline testing&lt;/li&gt;
&lt;li&gt;software and service development&lt;/li&gt;
&lt;li&gt;enhancing datasets for machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Synthetic data is often created on a bespoke basis, and since the advent
of generative adverserial networks (GANs) there has been considerable
interest and experimentation with using those as the basis for creating
synthetic data.&lt;/p&gt;
&lt;p&gt;We have taken a different approach. We have worked for some years on
developing methods for automatically finding constraints that
characterise data, and which can be used for testing data validity
(so-called &amp;quot;test-driven data analysis&amp;quot;, TDDA). Such constraints form (by
design) a useful characterisation of the data from which they were
generated. As a result, methods that generate datasets that match the
constraints necessarily construct datasets that match many of the
original characteristics of the data from which the constraints were
extracted.&lt;/p&gt;
&lt;p&gt;An important aspect of datasets is the relationship between &amp;quot;good&amp;quot; (~
valid) and &amp;quot;bad&amp;quot; (~ invalid) data, both of which are typically present.
Systems for creating useful, realistic synthetic data generally need to
be able to synthesize both kinds, in realistic mixtures.&lt;/p&gt;
&lt;p&gt;This talk will discuss data synthesis from constraints, describing what
has been achieved so far (which includes synthesizing good and bad data)
and future research directions.&lt;/p&gt;
&lt;p&gt;We introduce a method for creating synthetic data &amp;quot;to order&amp;quot; based on
learned (or provided) constraints and data classifications. This
includes &amp;quot;good&amp;quot; and &amp;quot;bad&amp;quot; data.&lt;/p&gt;
</summary></entry><entry><title>Controlling a confounding effect in predictive analysis.</title><link href="https://pyvideo.org/euroscipy-2019/controlling-a-confounding-effect-in-predictive-analysis.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Darya Chyzhyk</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/controlling-a-confounding-effect-in-predictive-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For instance, when predicting the salary to offer given the descriptions
of professional experience, the risk is to capture indirectly a gender
bias present in the distribution of salaries. Another example is found
in biomedical applications, where for an automated radiology diagnostic
system to be useful, it should use more than socio-demographic
information to build its prediction.&lt;/p&gt;
&lt;p&gt;Here I will talk about confounds in predictive models. I will review
classic deconfounding techniques developed in a well-established
statistical literature, and how they can be adapted to predictive
modeling settings. Departing from deconfounding, I will introduce a
non-parametric approach –that we named “confound-isolating
cross-validation”– adapting cross-validation experiments to measure the
performance of a model independently of the confounding effect.&lt;/p&gt;
&lt;p&gt;The examples are mentioned in this work are related to the common issues
in neuroimage analysis, although the approach is not limited to
neuroscience and can be useful in another domains.&lt;/p&gt;
&lt;p&gt;Confounding effects are often present in observational data: the effect
or association studied is observed jointly with other effects that are
not desired.&lt;/p&gt;
</summary></entry><entry><title>Dashboarding with Jupyter notebooks, voila and widgets</title><link href="https://pyvideo.org/euroscipy-2019/dashboarding-with-jupyter-notebooks-voila-and-widgets.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Maarten Breddels</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/dashboarding-with-jupyter-notebooks-voila-and-widgets.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Sharing the result of a Jupyter notebook is currently not an easy
path. With voila we are changing this. Voila is a small but important
ingredient in the Jupyter ecosystem. Voila can execute notebooks,
keeping the kernel connected but does not allow for arbitrary code
execution, making it safe to share your notebooks with others.&lt;/div&gt;
&lt;div class="line"&gt;With new libraries built on top of Jupyter widgets/ipywidgets
(ipymaterialui and ipyvuetify) we allow beautiful modern React and Vue
components to enter the Jupyter notebook. Using voila we can integrate
the ipywidgets seamlessly into modern React and Vue pages, to build
modern dashboards directly from a Jupyter notebook.&lt;/div&gt;
&lt;div class="line"&gt;I will give a live example on how to transform a Jupyter notebook into
a fully functional single page application with a modern (Material
Design) look.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Turn your Jupyter notebook into a beautiful modern React or Vue based
dashboard using voila and Jupyter widgets.&lt;/p&gt;
</summary></entry><entry><title>Distributed GPU Computing with Dask</title><link href="https://pyvideo.org/euroscipy-2019/distributed-gpu-computing-with-dask.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Peter Andreas Entschev</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/distributed-gpu-computing-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The need for speed remains important for scientific computing.
Historically, computers were limited to few dozens of processors, but
with modern GPUs, we can have thousands, or even millions of cores
running in parallel on distributed systems.&lt;/p&gt;
&lt;p&gt;However, developing software for distributed GPU systems can be
difficult, both because writing GPU code can be challenging for
non-experts, and because distributed systems are inherently complex. We
can work to address these challenges by using GPU-enabled libraries that
mimic parts of the SciPy ecosystem, such as CuPy, RAPIDS, and Numba,
abstracting GPU programming complexity, combined with Dask to abstract
distributed computing complexity.&lt;/p&gt;
&lt;p&gt;We talk about how Dask has come a long way to support distributed
GPU-enabled systems by leveraging community standards and protocols,
reusing open source libraries for GPU computing, and keeping it simple
and complication-free to build highly-configurable accelerated
distributed software.&lt;/p&gt;
&lt;p&gt;Dask has evolved over the last year to leverage multi-GPU computing
alongside its existing CPU support. We present how this is possible with
the use of NumPy-like libraries and how to get started writing
distributed GPU software.&lt;/p&gt;
</summary></entry><entry><title>EuroSciPy 2019 Bilbao - Open Source projects updates</title><link href="https://pyvideo.org/euroscipy-2019/euroscipy-2019-bilbao-open-source-projects-updates.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/euroscipy-2019-bilbao-open-source-projects-updates.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;EuroSciPy 2019 Bilbao
September 4, Wednesday
Mitxelena. Main Talks. 14.00&lt;/p&gt;
&lt;p&gt;Open source project updates&lt;/p&gt;
</summary></entry><entry><title>EuroSciPy 2019 Bilbao - Welcome talk</title><link href="https://pyvideo.org/euroscipy-2019/euroscipy-2019-bilbao-welcome-talk.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Alexandre Savio</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/euroscipy-2019-bilbao-welcome-talk.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;EuroSciPy 2019 Bilbao
September 4, Wednesday
Main Track&lt;/p&gt;
&lt;p&gt;Welcome talk
Alexandre Savio&lt;/p&gt;
&lt;p&gt;Introduction talk to EuroSciPy 2019 Bilbao.&lt;/p&gt;
</summary></entry><entry><title>High Voltage Lab Common Code Basis library: a uniform user-friendly object-oriented API for a high voltage engineering research.</title><link href="https://pyvideo.org/euroscipy-2019/high-voltage-lab-common-code-basis-library-a-uniform-user-friendly-object-oriented-api-for-a-high-voltage-engineering-research.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Mikołaj Rybiński</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/high-voltage-lab-common-code-basis-library-a-uniform-user-friendly-object-oriented-api-for-a-high-voltage-engineering-research.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;At the heart of ETH High Voltage Lab's (HVL) research are industrial
devices put&lt;/div&gt;
&lt;div class="line"&gt;together into code-automated experiments. It's a zoo of industrial
communication&lt;/div&gt;
&lt;div class="line"&gt;protocols one needs to handle when controlling these devices. HVL
decided to switch from&lt;/div&gt;
&lt;div class="line"&gt;MATLAB to Python as a programming and analysis tool. Python community
provides solutions&lt;/div&gt;
&lt;div class="line"&gt;to majority of technicalities involved in handling multitude of
industrial communication&lt;/div&gt;
&lt;div class="line"&gt;protocols used to control high voltage research experiment devices.
Moreover&lt;/div&gt;
&lt;div class="line"&gt;Python seems to be a more future-proof choice, meeting industry demand
for a more&lt;/div&gt;
&lt;div class="line"&gt;cost-effective and collaborative solution.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;The HVL Common Code Basis library (&lt;tt class="docutils literal"&gt;hvl_ccb&lt;/tt&gt;) provides a uniform
user-friendly&lt;/div&gt;
&lt;div class="line"&gt;object-oriented API as well as implementation for multiple of high
voltage engineering&lt;/div&gt;
&lt;div class="line"&gt;devices and their respective communication protocols. The library
leverages Python's&lt;/div&gt;
&lt;div class="line"&gt;open source community - implementations of specific communication
protocols, but also&lt;/div&gt;
&lt;div class="line"&gt;relies heavily on some of the languages newer features such as typing
hints, dataclasses&lt;/div&gt;
&lt;div class="line"&gt;or enums.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Python typing hints are used not only for their static type checking
and autocompletion&lt;/div&gt;
&lt;div class="line"&gt;support from IDEs, but also for dynamic type checking of the
communication protocol's&lt;/div&gt;
&lt;div class="line"&gt;and devices' configurations. The configurations themselves are a
customized&lt;/div&gt;
&lt;div class="line"&gt;implementation of Python's 3.7 dataclasses. Configurations properties
rely heavily on&lt;/div&gt;
&lt;div class="line"&gt;Python (advanced) enumerations.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Currently, the library supports serial port, VISA over TCP, Modbus
TCP, LabJack LJM and&lt;/div&gt;
&lt;div class="line"&gt;OPC UA communication protocols. These protocols are used within code
abstraction of&lt;/div&gt;
&lt;div class="line"&gt;devices such MBW973 SF6 Analyzer / dew point mirror, LabJack (T7-PRO)
device, Schneider&lt;/div&gt;
&lt;div class="line"&gt;Electric ILS2T stepper motor drive, Elektro-Automatik PSI9000 DC power
supply, Rhode &amp;amp;&lt;/div&gt;
&lt;div class="line"&gt;Schwarz RTO 1024 oscilloscope, or the Lab's state-of-the-art Supercube
platform, which&lt;/div&gt;
&lt;div class="line"&gt;encapsulates safety components, the voltage source, as well as other
auxiliary devices.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The library leverages Python richness to provide a uniform user-friendly
API for a zoo of industrial communication protocols used to control high
voltage engineering devices, together with abstraction and
implementations for such devices.&lt;/p&gt;
</summary></entry><entry><title>JupyterLab debugger</title><link href="https://pyvideo.org/euroscipy-2019/jupyterlab-debugger.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Jeremy Tuloup</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/jupyterlab-debugger.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;EuroSciPy 2019 Bilbao
September 4, Wednesday
Baroja Track. Talk. 15.45&lt;/p&gt;
&lt;p&gt;JupyterLab debugger
Jeremy Tuloup&lt;/p&gt;
&lt;p&gt;A talk about debugging jupyterLab.&lt;/p&gt;
</summary></entry><entry><title>Lessons learned from comparing Numba-CUDA and C-CUDA</title><link href="https://pyvideo.org/euroscipy-2019/lessons-learned-from-comparing-numba-cuda-and-c-cuda.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Lena Oden</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/lessons-learned-from-comparing-numba-cuda-and-c-cuda.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Numba allows the development of GPU code in Python style. When a Python
script using Numba is executed, the code is compiled just-in-time (JIT)
using the LLVM framework. Using Python for GPU programming can mean a
considerable simplification in the development of parallel applications
compared to C and C-CUDA.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Python, however, has to live with the prejudice of low performance,
especially in HighPerformance Computing.&lt;/div&gt;
&lt;div class="line"&gt;We wanted to get to the bottom of whether this is really true and
where these differences come from. For this reason, we first analyzed
the performance of typical micro benchmarks used in HPC. By analyzing
the assembly codes, we learned a lot about the difference between
codes produced by C-CUDA and NUMBA- CUDA. Some of these insights have
helped us to improve the performance of our application - and also of
Numba-CUDA. With a few tricks it is possible to achieve very good
performance with our Numba-Codes, which are very close - or sometimes
even better than the C-CUDA versions.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We compared the performance of GPU-Applications written in C-CUDA and
Numba- CUDA. By analyzing the GPU assembly code, we learned about the
reasons for the differences. This helped us to optimize our codes
written in NUMBA-CUDA and NUMBA itself.&lt;/p&gt;
</summary></entry><entry><title>Make your Python code fly at transonic speeds!</title><link href="https://pyvideo.org/euroscipy-2019/make-your-python-code-fly-at-transonic-speeds.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Pierre Augier</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/make-your-python-code-fly-at-transonic-speeds.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Slides available at &lt;a class="reference external" href="https://tiny.cc/euroscipy2019-transonic"&gt;https://tiny.cc/euroscipy2019-transonic&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://transonic.readthedocs.io/"&gt;Transonic&lt;/a&gt; is a pure Python
package (requiring Python &amp;gt;= 3.6) to easily accelerate modern
Python-Numpy code with different accelerators (like Cython,
&lt;a class="reference external" href="https://github.com/serge-sans-%20paille/pythran"&gt;Pythran&lt;/a&gt;, Numba,
Cupy, etc...) opportunistically (i.e. if/when they are available).&lt;/p&gt;
&lt;p&gt;We will first present the context of the creation of this package, i.e.
the Python's High Performance Computing (HPC) Landscape. We will show
how Transonic can be used to write elegant and very efficient HPC codes
with Python, with examples taken from real-life research simulation
codes (&lt;a class="reference external" href="https://fluidfft.readthedocs.io"&gt;fluidfft&lt;/a&gt; and
&lt;a class="reference external" href="https://fluidsim.readthedocs.io"&gt;fluidsim&lt;/a&gt;). We will discuss the
advantages of using Transonic instead of writing big Cython extensions
or using Numba or Pythran directly.&lt;/p&gt;
&lt;p&gt;A strategy to quickly develop a very efficient scientific
application/library with Python and Transonic could be:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Use modern Python coding, standard Numpy/Scipy for the computations
and all the cool libraries you want.&lt;/li&gt;
&lt;li&gt;Profile your applications on real cases, detect the bottlenecks and
apply standard optimizations with Numpy.&lt;/li&gt;
&lt;li&gt;Add few lines of Transonic to compile the hot spots.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We won't forget to also discuss some limitations of Transonic, and more
generally of Python and its numerical ecosystem for High Performance
Computing.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://transonic.readthedocs.io"&gt;Transonic&lt;/a&gt; is a new pure Python
package to easily accelerate modern Python-Numpy code with different
accelerators (like Cython, Pythran, Numba, Cupy, etc...).&lt;/p&gt;
</summary></entry><entry><title>Modern Data Science: A new approach to DataFrames and pipelines</title><link href="https://pyvideo.org/euroscipy-2019/modern-data-science-a-new-approach-to-dataframes-and-pipelines.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Jovan Veljanoski</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/modern-data-science-a-new-approach-to-dataframes-and-pipelines.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Working with datasets comprising millions or billions of samples is an
increasingly common task, one that is typically tackled with distributed
computing. Nodes in high-performance computing clusters have enough RAM
to run intensive and well-tested data analysis workflows. More often
than not, however, this is preceded by the scientific process of
cleaning, filtering, grouping, and other transformations of the data,
through continuous visualizations and correlation analysis. In today’s
work environments, many data scientists prefer to do this on their
laptops or workstations, as to more effectively use their time and not
to rely on spotty internet connection to access their remote data and
computation resources. Modern laptops have sufficiently fast I/O SSD
storage, but upgrading RAM is expensive or impossible.&lt;/p&gt;
&lt;p&gt;Applying the combined benefits of computational graphs, which are common
in neural network libraries, with delayed (a.k.a lazy) evaluations to a
DataFrame library enables efficient memory and CPU usage. Together with
memory-mapped storage (Apache Arrow, hdf5) and out-of-core algorithms,
we can process considerably larger data sets with fewer resources. As an
added bonus, the computational graphs ‘remember’ all operations applied
to a DataFrame, meaning that data processing pipelines can be generated
automatically.&lt;/p&gt;
&lt;p&gt;In this talk, we will demonstrate Vaex, an open-source DataFrame library
that embodies these concepts. Using data from the New York City
YellowCab taxi service comprising 1.1 billion samples and taking up over
170 GB on disk, we will showcase how one can conduct an exploratory data
analysis, complete with filtering, grouping, calculations of statistics
and interactive visualisations on a single laptop in real time. Finally
we will show an example of how one can automatically build a machine
learning pipeline as a by-product of the exploratory data analysis using
the computational graphs in Vaex.&lt;/p&gt;
&lt;p&gt;We will demonstrate how to explore and analyse massive datasets (&amp;gt;150GB)
on a laptop with the Vaex library in Python. Using computational graphs,
efficient algorithms and storage (Apache Arrow / hdf5) Vaex can easily
handle up to a billion rows.&lt;/p&gt;
</summary></entry><entry><title>Modin: Scaling the Capabilities of the Data Scientist, not the machine</title><link href="https://pyvideo.org/euroscipy-2019/modin-scaling-the-capabilities-of-the-data-scientist-not-the-machine.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Devin Petersohn</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/modin-scaling-the-capabilities-of-the-data-scientist-not-the-machine.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Modern data systems tend to heavily focus on optimizing for the system’s
time. Some of these optimizations, however, are counterproductive to the
end user’s workflow and thought process. In this talk, we discuss the
design of Modin, a DataFrame library, and how to optimize for the human
system.&lt;/p&gt;
&lt;p&gt;Modin is a project at UC Berkeley's RISELab designed to optimize for the
data scientist’s time. Often when building a data system, the system
designers will follow a set of “best practices” in order to optimize
performance. These “best practices” often require data scientists to
understand and personally optimize concepts and system components that
are not central to extracting value from their data.&lt;/p&gt;
&lt;p&gt;The fundamental goal of data science is to extract value from data.
Despite this, data systems are being built with user requirements such
as: (1) knowledge of partitioning, (2) understanding laziness and what
triggers computation, (3) an entirely new API, and (4) where their code
is running (e.g. locally, on-prem cluster, cloud). This overhead is
passed to the data scientist, even though there is no overlap between
these new requirements and the fundamental goal of their profession.&lt;/p&gt;
&lt;p&gt;In this talk, we will discuss how we think about the problem of large
scale data science and optimizing for the human system. We will discuss
the system design of Modin, which enables pluggable backends, runtimes,
and APIs. The system is designed to solve the needs of the data science
community regardless of an individual user’s environment. Currently,
Modin supports the pandas API, and a proof of concept for SQL has been
implemented. Modin is completely open- source and can be found on
GitHub: &lt;a class="reference external" href="https://github.com/modin-project/modin"&gt;https://github.com/modin-project/modin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Modern data systems tend to heavily focus on optimizing for the system’s
time. In this talk, we discuss the design of Modin, a DataFrame library,
and how to optimize for the human system.&lt;/p&gt;
</summary></entry><entry><title>PyFETI - An easy and massively Dual Domain Decomposition Solver for Python</title><link href="https://pyvideo.org/euroscipy-2019/pyfeti-an-easy-and-massively-dual-domain-decomposition-solver-for-python.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Guilherme Jenovencio</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/pyfeti-an-easy-and-massively-dual-domain-decomposition-solver-for-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyFETI is a python implementation of
Finite-Element-Tearing-Interconnecting Methods. The library provides a
massive linear solver that uses Domain Decomposition Techniques. FETI
methods rely in the solution of a linear system, based on to linear
solver algorithm strategies, Direct and Iteratively. A big problem is
decomposed in subdomains, generating an additional set of constraints at
the interface among subdomains. The local problem solution is formulated
based on a new interface force at the interface that must connect the
subdomains. Therefore, given an interface force, the local problems are
solved based on a direct solver, e.g SuperLU, and the update of
interface force is performed by Preconditioned Conjunged Projected
Gradient. The library has been tested for large linear elastic problems
at the IT4I supercomputer center.&lt;/p&gt;
&lt;p&gt;PyFETI is a python implementation of
Finite-Element-Tearing-Interconnecting Methods. The library provides a
massive linear solver using Domain Decomposition method, where problems
are solved locally by Direct Solver and at the interface iteratively.&lt;/p&gt;
</summary></entry><entry><title>QuTiP: the quantum toolbox in Python as an ecosystem for quantum physics exploration and quantum information science</title><link href="https://pyvideo.org/euroscipy-2019/qutip-the-quantum-toolbox-in-python-as-an-ecosystem-for-quantum-physics-exploration-and-quantum-information-science.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Nathan Shammah</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/qutip-the-quantum-toolbox-in-python-as-an-ecosystem-for-quantum-physics-exploration-and-quantum-information-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;pre&gt;QuTiP is emerging as a library at the center of a lively ecosystem. In
this talk you will learn about the ongoing projects that have invested
this project, from providing the framework to simulate quantum machine
learning for quantum computers to the development of efficient numerical
solvers tackling dynamical problems that are inherently hard to simulate
classically.

It can be noted that
`Astropy &lt;https://www.astropy.org/affiliated/index.html&gt;`__ is a
community effort to develop a common core package for Astronomy in
Python and "foster an ecosystem of interoperable astronomy packages",

It seems an interesting model for the quantum tech landscape.
`Qiskit &lt;&gt;`__ did build its own ecosystem of sub-libraries for quantum
computing. The physics library for quantum tech is http://qutip.org .

About the idea of QuTiP as a super-library, here are some details:

-  | ``krotov``, a very recent package for optimal control built on top
     of QuTiP ( https://arxiv.org/abs/1902.11284).
   | [https://github.com/qucontrol/krotov].

-  ``piqs``, the permutational invariant quantum solver, now a QuTiP
   module (see also https://arxiv.org/abs/1805.05129 );

-  ``matsubara``, a plugin to study the ultrastrong coupling regime with
   structured baths, http://matsubara.readthedocs.io/

-  ``QNET``, a computer algebra package for quantum mechanics and
   photonic quantum networks, which actually calls QuTiP as a plugin,
   mainly developed at Stanford in Mabuchi Lab
   https://github.com/mabuchilab/QNET

-  ``qptomographer``,
   https://qptomographer.readthedocs.io/en/latest/install, a library to
   derive error bars for experiments in quantum computing and quantum
   information processing.

-  ``tiqs``, a library to study open quantum systems on extended
   lattices exploiting the symmetries of such systems,
   https://github.com/fminga/tiqs

-  other upcoming integrations relative to pulse control, such as
   ``qupulse``,
   https://github.com/qutech/qupulse/wiki/Architecture-Proposal

This talk will be of interest to the curious coder and researcher,
analyzing how QuTiP's impact in the research community has fostered a
`*lingua franca* for quantum tech
research &lt;https://twitter.com/goerz/status/1118739088595652611&gt;`__. We
will also draw comparisons with other larger ecosystems in Python-based
scientific projects, such as astropy and scikit-learn.

More about QuTiP
================

-  QuTiP is the open-source software to study quantum physics. It
   develops both an intuitive playground to understand quantum mechanics
   and cutting-edge tools to investigate it.
-  QuTiP provides the most comprehensive toolbox to characterize noise
   and dissipation –realistic processes– affecting quantum systems, as
   well as tools not only to monitor but also to minimize their impact
   (quantum optimal control, description of decoherence-free spaces).
-  For this reason QuTiP is a software born out of the quantum optics
   community and that has become increasingly relevant for the quantum
   computing community, as current quantum computing devices are noisy
   (NISQ definition by Preskill).
-  ``pypinfo`` data shows that QuTiP is popular in countries that are
   strong in quantum tech and quantum computing research, eg, The
   Netherlands in the top five, as well as countries that benefit in the
   use of open source software (OSS) for university coursework, eg,
   India.
-  In the past three years, there has been an evolution in the quantum
   tech community, which has embraced OSS.
-  OSS libraries are used as a means to grow the user base, as well as
   in a more structural way for quantum computers, as they provide cloud
   access to quantum devices, e.g., IBM Q.
-  QuTiP is the only major library that has continued to thrive in this
   ecosystem, competing with other library packages that are funded by
   corporations or VC-backed startups/
-  Since the tools of QuTiP provide a common ground to study quantum
   mechanics, it is important that this independent project is provided
   with the necessary support to thrive
-  As access to quantum computers becomes more and more widespread also
   for the use of data scientist and QuTiP's popularity grows even more
   for undergraduate and graduate courses, becoming the de-facto
   standard OSS to study quantum optical systems, it is imperative that
   the QuTiP library makes a quality jump to provide a comprehensive
   introduction to its tools for a much broader community of users.

-  QuTiP website: http://www.qutip.org/

-  GitHub repository: https://github.com/qutip
-  GitHub repository (QuTiP code): https://github.com/qutip/qutip
-  GitHub repository (QuTiP documentation):
   https://github.com/qutip/qutip-doc
-  GitHub repository (QuTiP tutorials):
   https://github.com/qutip/qutip-notebooks
-  | Latest version of the documetnation:
   | http://qutip.org/docs/latest/index.html

-  Historical archive of released documentation:
   http://qutip.org/documentation.html

QuTiP core development team
---------------------------

QuTiP core development team: (Alex Pitchford, alex.pitchford@gmail.com).
Additional mentors will be the project's core contributors Nathan
Shammah (nathan.shammah@gmail.com), Shahnawaz Ahmed
(shahnawaz.ahmed95@gmail.com) and Eric Giguere
(eric.giguere@usherbrooke.ca).

QuTiP is a project started by Robert J. Johansson and Paul Nation. Other
core developers have been Arne Grimso, Chris Granade and over other 44
contributors.

References
----------

[1] J. R. Johansson, P. D. Nation, and F. Nori: “QuTiP: An open-source
Python framework for the dynamics of open quantum systems.”, Comp. Phys.
Comm. 183, 1760–1772 (2012)

[2] J. Robert Johansson, Paul D. Nation, and Franco Nori: “QuTiP 2: A
Python framework for the dynamics of open quantum systems.”, Comp. Phys.
Comm. 184, 1234 (2013)

[3] J. Preskill, "Quantum Computing in the NISQ era and beyond." Quantum
**2** , 79 (2018)

[4] Mark Fingerhuth, Tomáš Babej, and Peter Wittek, Open source software
in quantum computing, PLoS ONE 13 (12): e0208561 (2018).

[5] N. Shammah, S. Ahmed, N. Lambert, S. De Liberato, and F. Nori, "Open
quantum systems with local and collective incoherent processes:
Efficient numerical simulation using permutational invariance " Phys.
Rev. A 98, 063815 (2018). Code at http://piqs.readthedocs.io

[6] N. Lambert, S. Ahmed, M. Cirio, and F. Nori, "Virtual excitations in
the ultra-strongly-coupled spin-boson model: physical results from
unphysical modes", arXiv preprint arXiv:1903.05892. Also
http://matsubara.readthedocs.io

**Other relevant material** :

-  Slides on QuTiP and the quantum-tech open source ecosystem (Nathan
   Shammah @ Berkeley Lab, 2019).
   `PDF &lt;https://conferences.lbl.gov/event/195/session/6/contribution/13/material/slides/0.pdf&gt;`__

-  `"The rise of open source in quantum physics
   research" &lt;http://blogs.nature.com/onyourwavelength/2019/01/09/the-rise-of-open-source-in-quantum-physics-research/&gt;`__,
   Nathan Shammah and Shahnawaz Ahmed, Nature's physics blog, January 9,
   2019.

-  "Bit to QuBit: Data in the age of quantum computers", Shahnawaz
   Ahmed, PyData 2018, Warsaw, Poland, 2019. `YouTube
   video &lt;https://www.youtube.com/watch?v=6GAXJhL1mSs&gt;`__.

In this talk you will learn how QuTiP, the quantum toolbox in Python
(http://qutip.org), has emerged from a library to an *ecosystem*. QuTiP
is used for education, to teach quantum physics. In research and
industry, for quantum computing simulation.
&lt;/pre&gt;</summary></entry><entry><title>Scientific DevOps: Designing Reproducible Data Analysis Pipelines with Containerized Workflow Managers</title><link href="https://pyvideo.org/euroscipy-2019/scientific-devops-designing-reproducible-data-analysis-pipelines-with-containerized-workflow-managers.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Nicholas Del Grosso</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/scientific-devops-designing-reproducible-data-analysis-pipelines-with-containerized-workflow-managers.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Open source and open science come together when the software is
accessible, transparent, and owned by all. For data analysis pipelines
that grow in complexity beyond a single Jupyter notebook, this can
become a challenge as the number of steps and software dependencies
increase. In this talk, Nicholas Del Grosso will review a variety of
tools for packaging and managing a data analysis pipeline, showing how
they fit together and benefit the development, testing, deployment, and
publication processes and the scientific community. In particular, this
talk will cover:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Workflow managers&lt;/strong&gt; (e.g. Snakemake, PyDoit, Luigi) to combine
complex pipelines into single applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container Solutions&lt;/strong&gt; (e.g. Docker and Singularity) to package and
deploy the software on others' computers, including high-performance
computing clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Scientific Filesystem&lt;/strong&gt; to build explorable and multi-purpose
applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Testing Frameworks&lt;/strong&gt; (e.g. PyTest, Hypothesis) to declare and
confirm the assumptions and functionality of the analysis pipeline.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ease-of-Use Utilities&lt;/strong&gt; to share the pipeline online and make it
accessible to non-programmers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By writing software that stays manageable, reproducible, and deployable
continuously throughout the development cycle, we can better fulfill the
goals of open science and good scientific practice in a digital era.&lt;/p&gt;
&lt;p&gt;A review of DevOps tools as applied to data analysis pipelines,
including workflow managers, software containers, testing frameworks,
and online repositories for performing reproducible science that scales.&lt;/p&gt;
</summary></entry><entry><title>The Rapid Analytics and Model Prototyping (RAMP) framework: tools for collaborative data science challenges</title><link href="https://pyvideo.org/euroscipy-2019/the-rapid-analytics-and-model-prototyping-ramp-framework-tools-for-collaborative-data-science-challenges.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Guillaume Lemaitre</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/the-rapid-analytics-and-model-prototyping-ramp-framework-tools-for-collaborative-data-science-challenges.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will give an overview of the RAMP framework, which provides a
platform to organize reproducible and transparent data challenges.&lt;/p&gt;
&lt;p&gt;RAMP workflow is a python package used to define and formalize the data
science problem to be solved. It can be used as a standalone package and
allows a user to prototype different solutions. In addition to RAMP
workflow, a set of packages have been developed allowing to share and
collaborate around the developer solutions. Therefore, RAMP database
provides a database structure to store the solutions of different users
and the performance of these solutions. RAMP engine is the package to
run the user solutions (possibly on the cloud) and populate the
database. Finally, RAMP frontend is the web frontend where users can
upload their solutions and which shows the leaderboard of the challenge.&lt;/p&gt;
&lt;p&gt;The project is open-source and can be deployed on any local server. The
framework has been used at the Paris-Saclay Center for Data Science for
setting up and solving about twenty scientific problems, for organizing
collaborative data challenges, for organizing scientific sub-communities
around these events, and for training novice data scientists.&lt;/p&gt;
&lt;p&gt;The RAMP (Rapid Analytics and Model Prototyping) framework provides a
platform to organize reproducible and transparent data challenges. We
will present the different framework bricks.&lt;/p&gt;
</summary></entry><entry><title>ToFu - an open-source python/cython library for synthetic tomography diagnostics on Tokamaks</title><link href="https://pyvideo.org/euroscipy-2019/tofu-an-open-source-pythoncython-library-for-synthetic-tomography-diagnostics-on-tokamaks.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Didier VEZINET</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/tofu-an-open-source-pythoncython-library-for-synthetic-tomography-diagnostics-on-tokamaks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Nuclear fusion comes along with great promises of almost limitless
energy with little risks and waste. But it also comes with significant
scientific and technological complexities. Decades of efforts may find
an echo in ITER, an international tokamak being built to address this
challenge. A tokamak is a particular kind of advanced experimental
nuclear fusion reactor. It is a torus-shaped vacuum vessel in which a
hydrogen plasma of very low density is heated up to temperatures
(10-100 millions of degrees Celsius) allowing nuclear fusion reactions
to occur. The torus-shaped plasma radiates light, which is measured in
various wavelength domains by dedicated sets of detectors (called
diagnostics), like 2D cameras observing visible light, 1D arrangements
of diodes sensitive to X-rays, ultra-violet spectrometers... Due to
the torus shape, the plasma is axisymmetric, and like in medical
imaging, tomography methods can be used to diagnose the light radiated
in a plasma cross-section.&lt;/div&gt;
&lt;div class="line"&gt;For all diagnostics, one can seek to solve the direct or the inverse
problem. The direct problem consists in computing the measurements
from a known plasma light emissivity, provided by a plasma simulation
for example.&lt;/div&gt;
&lt;div class="line"&gt;The inverse problem consists in computing the plasma light emissivity
from experimental measurements. The algorithms involved in solving
both the direct and inverse problem are very similar, no matter the
wavelength domain.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Like many, the fusion community tends to suffer from a lack of
reproducibility of the results it publishes. This problem is
particularly acute in the case of tomography diagnostics since the
inverse problem is ill-posed and the solution unicity is not guaranteed.
There are also many possible simplifying hypotheses that may, or may
not, be relevant for each diagnostic. In this regard, the historical
uses of the community display a large variety of single-user black- box
codes, each typically designed by a student, and often forgotten or left
as is until a new student is hired and starts all over again.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;In this context, a machine-independent, open-source and documented
python library, ToFu, was started to provide the fusion community with
a common and free reference tool.&lt;/div&gt;
&lt;div class="line"&gt;We thus aim at improving reproducibility by providing a known and
transparent tool, able to efficiently solve both the direct and
inverse problem for tomography diagnostics. It can use very simple
hypothesis or very complete diagnostics descriptions alike, one of the
ideas being that it should allow users to perform accurate
calculations easily, sparing them the need to simplify hypotheses that
are not always valid.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;A zero version of tofu, fully operational but not user-friendly
enough, was first developed between 2014 and 2016 when it was used for
published results. Strong with this first proof of principle, a
significant effort was initiated in 2017 to completely re-write the
code with a stronger emphasis on python community standards (PEP8),
version control (Github), performance (cython), packaging (pip and
conda), continuous integration (nosetests and travis), modularity
(architecture refurbishing), user-friendliness (renamings, utility
tools) and object-oriented coding (class inheritance).&lt;/div&gt;
&lt;div class="line"&gt;This effort is still ongoing to this day and is scheduled to go on for
the next 2.5 years. However, the first milestones have been reached,
and we would like to present the first re-written modules to the
python community, for publicity, advice, feedback, mutually enriching
exchanges and more generally because we feel tofu is part of the large
open-source python scientific community.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The code is composed of several modules: a geometry module, a data
visualization module, a meshing module, and an inversion module. We will
present the geometry module (containing ray-tracing tools, spatial
integration algorithms...) and the data module (making use of matplotlib
for pre-defined interactive figures). Using profiling tools, the
numerical core of the geometry module was optimized and parallelized
recently in &lt;tt class="docutils literal"&gt;Cython&lt;/tt&gt; making the code more than ten thousand times
faster than the previous version on some test cases. Memory usage has
also been reduced by half on the largest test cases.&lt;/p&gt;
&lt;p&gt;see &lt;a class="reference external" href="https://github.com/ToFuProject/tofu"&gt;ToFu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We present an open-source parallelized and cythonized python library,
ToFu, for modeling tomography diagnostics on nuclear fusion reactors.
Its functionalities (with realistic examples), its architecture and its
design will be shown.&lt;/p&gt;
</summary></entry><entry><title>What about tests in Machine Learning projects?</title><link href="https://pyvideo.org/euroscipy-2019/what-about-tests-in-machine-learning-projects.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Sarah Diot-Girard</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/what-about-tests-in-machine-learning-projects.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Once your machine learning POC seems promising and your development
environment is set up, the next step is to refactor your code and write
TESTS. We know that a lot of people think tests are too complicated and
boring to write and they are not very useful. Some manual checks can
address the need.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;It is not totally false. Tests can be really boring and time consuming
to write when you don't have the right tools, the right APIs, the
right environments or the right code structure.&lt;/div&gt;
&lt;div class="line"&gt;But it is always a bad idea to ignore tests or to perform them
manually. If you want to be involved in your project life cycle, if
you want to bring it from POC to production you need to care about
tests. After some years tackling production bugs, you can't feel safe
delivering without tests as you can't start driving until your seat
belt is fastened.&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;There is more than one way to test. Tests can be split on several levels
(unit, component, functional, performances, etc...) to be able to
quickly identify the faulty code/data/parameter. Tests must also be
automated in a Continuous Integration and run at least on each
experiment before merging it in the baseline pipeline as it is done in
software engineering (the CI is triggered on each feature branch).&lt;/p&gt;
&lt;p&gt;This talk is about how to easily write tests and testable code, how to
avoid most common traps and what are the benefits of tests on
unrealistic data in your Machine Learning project.&lt;/p&gt;
&lt;p&gt;(Tests on real data are also really important but they are not the main
purpose of this talk.)&lt;/p&gt;
&lt;p&gt;Slides are here:
&lt;a class="reference external" href="http://sdg.jlbl.net/slides/tests_for_datascientist/presentation.html"&gt;sdg.jlbl.net/slides/tests_for_datascientist/presentation.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Good practices tell you must write tests! But testing Machine Learning
projects can be really complicated. Test writing seems often
inefficient. Which kind of test should be written? How to write them?
What are the benefits?&lt;/p&gt;
</summary></entry><entry><title>Astronomical Image Processing</title><link href="https://pyvideo.org/euroscipy-2019/astronomical-image-processing.html" rel="alternate"></link><published>2019-09-03T00:00:00+00:00</published><updated>2019-09-03T00:00:00+00:00</updated><author><name>Samuel FARRENS</name></author><id>tag:pyvideo.org,2019-09-03:euroscipy-2019/astronomical-image-processing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="section" id="programme"&gt;
&lt;h4&gt;Programme&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The tutorial will begin with short introduction to the basic premise
of sparsity and highlight some problems in astronomical image
processing that can be solved using this methodology. (~15-20min;
slides)&lt;/li&gt;
&lt;li&gt;Tutees will then follow a hands-on demonstration of how the concept
of sparsity can be used to denoise signals. (~30-35min; interactive
jupyter notebook with exercises)&lt;/li&gt;
&lt;li&gt;Finally the tutees will learn how to denoise an astronomical image
and use their newfound skills to recover a nice picture of Saturn.
(~35-40min; interactive jupyter notebook with an exercise)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="requirements"&gt;
&lt;h4&gt;Requirements&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The tutorial contents are available on
&lt;a class="reference external" href="https://github.com/sfarrens/euroscipy"&gt;GitHub&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Provided tutees have a stable internet connection, the entire
tutorial can be run online using
&lt;a class="reference external" href="https://mybinder.org/v2/gh/sfarrens/euroscipy/master"&gt;Binder&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;However, to be safe, tutees should download and install the tutorial
materials beforehand.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tutorial will introduce the concept of &lt;em&gt;sparsity&lt;/em&gt; and demonstrate
how it can be used to remove noise from signals. These concepts will
then be expanded to demonstrate how noise can be removed from
astronomical images in particular.&lt;/p&gt;
&lt;/div&gt;
</summary></entry><entry><title>Sufficiently Advanced Testing with Hypothesis</title><link href="https://pyvideo.org/euroscipy-2019/sufficiently-advanced-testing-with-hypothesis.html" rel="alternate"></link><published>2019-09-03T00:00:00+00:00</published><updated>2019-09-03T00:00:00+00:00</updated><author><name>Zac Hatfield-Dodds</name></author><id>tag:pyvideo.org,2019-09-03:euroscipy-2019/sufficiently-advanced-testing-with-hypothesis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Hypothesis is a testing package that will search for counterexamples
to your&lt;/div&gt;
&lt;div class="line"&gt;assertions – so you can write tests that provide a high-level
description of your&lt;/div&gt;
&lt;div class="line"&gt;code or system, and let the computer attempt a Popperian
falsification. If it&lt;/div&gt;
&lt;div class="line"&gt;fails, your code is (probably) OK… and if it succeeds you have a
minimal input&lt;/div&gt;
&lt;div class="line"&gt;to debug.&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Come along and learn the principles of property-based testing, how to
use&lt;/div&gt;
&lt;div class="line"&gt;Hypothesis, and how to use it to check scientific code – whether
highly- polished&lt;/div&gt;
&lt;div class="line"&gt;or quick-and-dirty!&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;You can even use it to test 'black boxes', such as simulations, where
we have no&lt;/div&gt;
&lt;div class="line"&gt;way of independently verifying that some input leads to the right
output!&lt;/div&gt;
&lt;div class="line"&gt;Intrigued? Come and learn about the power of embedding assertions in
your&lt;/div&gt;
&lt;div class="line"&gt;code, and metamorphic relations in your tests!&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Testing research code can be difficult, but is essential for robust
results. Using Hypothesis, a tool for property-based testing, I'll show
how testing can be both easier and dramatically more powerful - even for
complex &amp;quot;black box&amp;quot; codes.&lt;/p&gt;
</summary></entry></feed>