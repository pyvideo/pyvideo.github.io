<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Matt White</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_matt-white.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-09-18T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Keynote: Welcome &amp; Opening Remarks</title><link href="https://pyvideo.org/pytorch-conference-2024/keynote-welcome-opening-remarks.html" rel="alternate"></link><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><author><name>Matt White</name></author><id>tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/keynote-welcome-opening-remarks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Keynote: Welcome &amp;amp; Opening Remarks - Matt White, Executive Director, PyTorch Foundation&lt;/p&gt;
&lt;p&gt;Over the past few years, and especially since the deployment of ChatGPT in November 2022,  neural language models with billions of parameters and trained on trillions of words are powering the fastest-growing computing applications in history and generating discussion â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Keynote: Welcome &amp;amp; Opening Remarks - Matt White, Executive Director, PyTorch Foundation&lt;/p&gt;
&lt;p&gt;Over the past few years, and especially since the deployment of ChatGPT in November 2022,  neural language models with billions of parameters and trained on trillions of words are powering the fastest-growing computing applications in history and generating discussion and debate across society. However, AI scientists cannot study or improve those state-of-the-art models because the models' parameters, training data, code, and even documentation are not openly available. In this talk, I present our OLMo project toward building strong language models and making them fully open to researchers along with open-source code for data management, training, inference, and interaction. In particular, I describe DOLMa, a 3T token open dataset curated for training language models, Tulu, our instruction-tuned language model, and OLMo v1, a fully-open 7B parameter language model trained from scratch.&lt;/p&gt;
</content><category term="PyTorch Conference 2024"></category></entry></feed>