<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Maria Florina Balcan</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 15 Jul 2024 00:00:00 +0000</lastBuildDate><item><title>Semi-bandit Optimization in the Dispersed Setting</title><link>https://pyvideo.org/uai-2020/semi-bandit-optimization-in-the-dispersed-setting.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Semi-bandit Optimization in the Dispersed SettingㅇㅇTravis Dick (University of Pennsylvania)*; Wesley Pegden (Carnegie Mellon University); Maria-Florina Balcan (Carnegie Mellon University)ㅇㅇThe goal of data-driven algorithm design is to obtain high-performing algorithms for specific application domains using machine learning and data. Across many fields in AI, science, and engineering, practitioners will often fix a family of parameterized algorithms and then optimize those parameters to obtain good performance on example instances from the application domain.  In the online setting, we must choose algorithm parameters for each instance as they arrive, and our goal is to be competitive with the best fixed algorithm in hindsight.&lt;/p&gt;
&lt;p&gt;There are two major challenges in online data-driven algorithm design. First, it can be computationally expensive to evaluate the loss functions that map algorithm parameters to performance, which often require the learner to run a combinatorial algorithm to measure its performance. Second, the losses can be extremely volatile and have sharp discontinuities. However, we show that in many applications, evaluating the loss function for one algorithm choice can sometimes reveal the loss for a range of similar algorithms, essentially for free. We develop online optimization algorithms capable of using this kind of extra information by working in the semi-bandit feedback setting. Our algorithms achieve regret bounds that are essentially as good as algorithms under full-information feedback and are significantly more computationally efficient. We apply our semi-bandit results to obtain the first provable guarantees for data-driven algorithm design for linkage-based clustering and we improve the best regret bounds for designing greedy knapsack algorithms.&amp;quot;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Travis Dick</dc:creator><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2020-08-03:/uai-2020/semi-bandit-optimization-in-the-dispersed-setting.html</guid><category>UAI 2020</category></item><item><title>UAI 2024 Oral Session 5: Learning Algorithms</title><link>https://pyvideo.org/uai-2024/uai-2024-oral-session-5-learning-algorithms.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Cost-Sensitive Uncertainty-Based Failure Recognition for Object Detection
Moussa Kassem Sbeyti, Michelle E. Karg, Christian Wirth, Nadja Klein, Sahin Albayrak
&lt;a class="reference external" href="https://openreview.net/pdf?id=HuibNFkaoi"&gt;https://openreview.net/pdf?id=HuibNFkaoi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Learning Accurate and Interpretable Decision Trees
Maria Florina Balcan, Dravyansh Sharma
&lt;a class="reference external" href="https://openreview.net/pdf?id=skdlnUYRzQ"&gt;https://openreview.net/pdf?id=skdlnUYRzQ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bayesian Active Learning in the Presence of Nuisance Parameters
Sabina J. Sloman, Ayush Bharti, Julien Martinelli, Samuel Kaski
&lt;a class="reference external" href="https://openreview.net/pdf?id=dRa16UhuZi"&gt;https://openreview.net/pdf?id=dRa16UhuZi&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Moussa Kassem Sbeyti</dc:creator><pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-07-15:/uai-2024/uai-2024-oral-session-5-learning-algorithms.html</guid><category>UAI 2024</category></item></channel></rss>