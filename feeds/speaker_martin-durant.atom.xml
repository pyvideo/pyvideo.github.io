<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_martin-durant.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-11-11T00:00:00+00:00</updated><entry><title>Intake - taking the pain out of data access</title><link href="https://pyvideo.org/pycon-ca-2018/intake-taking-the-pain-out-of-data-access.html" rel="alternate"></link><published>2018-11-11T00:00:00+00:00</published><updated>2018-11-11T00:00:00+00:00</updated><author><name>Martin Durant</name></author><id>tag:pyvideo.org,2018-11-11:pycon-ca-2018/intake-taking-the-pain-out-of-data-access.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Intake is a simple library providing one interface for cataloging, finding, describing and reading any data locally, in the cloud, or on an Intake server. Intake separates the definition of data sources from their analysis, so that Data Engineers and Data Scientists can get on with their jobs.&lt;/p&gt;
&lt;p&gt;Presentation page -- &lt;a class="reference external" href="https://2018.pycon.ca/talks/talk-PC-55191/"&gt;https://2018.pycon.ca/talks/talk-PC-55191/&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Intake: Taking the Pain Out of Data Access</title><link href="https://pyvideo.org/pydata-new-york-city-2018/intake-taking-the-pain-out-of-data-access.html" rel="alternate"></link><published>2018-08-17T00:00:00+00:00</published><updated>2018-08-17T00:00:00+00:00</updated><author><name>Martin Durant</name></author><id>tag:pyvideo.org,2018-08-17:pydata-new-york-city-2018/intake-taking-the-pain-out-of-data-access.html</id><summary type="html"></summary></entry><entry><title>Parallelizing Scientific Python with Dask</title><link href="https://pyvideo.org/scipy-2018/parallelizing-scientific-python-with-dask.html" rel="alternate"></link><published>2018-07-09T00:00:00+00:00</published><updated>2018-07-09T00:00:00+00:00</updated><author><name>James Crist</name></author><id>tag:pyvideo.org,2018-07-09:scipy-2018/parallelizing-scientific-python-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a flexible tool for parallelizing Python code on a single
machine or across a cluster. It builds upon familiar tools in the SciPy
ecosystem (e.g. NumPy and Pandas) while allowing them to scale across
multiple cores or machines. This tutorial will cover both the high-level
use of dask collections, as well as the low-level use of dask graphs and
schedulers.Presenter(s): Speaker: James Crist, Anaconda, Inc. Speaker:
Martin Durant, Anaconda Inc.&lt;/p&gt;
</summary></entry><entry><title>Parallel Data Analysis with Dask</title><link href="https://pyvideo.org/pycon-us-2018/parallel-data-analysis-with-dask.html" rel="alternate"></link><published>2018-05-09T00:00:00+00:00</published><updated>2018-05-09T00:00:00+00:00</updated><author><name>Tom Augspurger</name></author><id>tag:pyvideo.org,2018-05-09:pycon-us-2018/parallel-data-analysis-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The libraries that power data analysis in Python are essentially limited to a single CPU core and to datasets that fit in RAM. Attendees will see how dask can parallelize their workflows, while still writing what looks like normal python, NumPy, or pandas code.&lt;/p&gt;
&lt;p&gt;Dask is a parallel computing framework, with a focus on analytical computing. We'll start with &lt;cite&gt;dask.delayed&lt;/cite&gt;, which helps parallelize your existing Python code. We’ll demonstrate &lt;cite&gt;dask.delayed&lt;/cite&gt; on a small example, introducing the concepts at the heart of dask like the &lt;em&gt;task graph&lt;/em&gt; and the &lt;em&gt;schedulers&lt;/em&gt; that execute tasks. We’ll compare this approach to the simpler, but less flexible, parallelization methods available in the standard library like &lt;cite&gt;concurrent.futures&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Attendees will see the high-level collections dask provides for writing regular Python, NumPy, or Pandas code that is then executed in parallel on datasets that may be larger than memory. These high level collections provide a familiar API, but the execution model is very different. We'll discuss concepts like the GIL, serialization, and other headaches that come up with parallel programming. We’ll use dask’s various schedulers to illustrate the differences between multi-threaded, multi-processes, and distributed computing.&lt;/p&gt;
&lt;p&gt;Dask includes a distributed scheduler for executing task graphs on a cluster of machines. We’ll provide each person access to their own cluster.&lt;/p&gt;
</summary><category term="dask"></category></entry></feed>