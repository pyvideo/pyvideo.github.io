<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_marianne-stecklina.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-10-09T00:00:00+00:00</updated><entry><title>Why you should (not) train your own BERT model for...</title><link href="https://pyvideo.org/pydata-berlin-2019/why-you-should-not-train-your-own-bert-model-for.html" rel="alternate"></link><published>2019-10-09T00:00:00+00:00</published><updated>2019-10-09T00:00:00+00:00</updated><author><name>Marianne Stecklina</name></author><id>tag:pyvideo.org,2019-10-09:pydata-berlin-2019/why-you-should-not-train-your-own-bert-model-for.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker: Marianne Stecklina&lt;/p&gt;
&lt;p&gt;Track:PyData
Language models like BERT can capture general language knowledge and transfer it to new data and tasks. However, applying a pre-trained BERT to non-English text has limitations. Is training from scratch a good (and feasible) way to overcome them?&lt;/p&gt;
&lt;p&gt;Recorded at the PyConDE &amp;amp; PyData Berlin 2019 conference.
&lt;a class="reference external" href="https://pycon.de"&gt;https://pycon.de&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More details at the conference page: &lt;a class="reference external" href="https://de.pycon.org/program/YAJRGX"&gt;https://de.pycon.org/program/YAJRGX&lt;/a&gt;
Twitter:  &lt;a class="reference external" href="https://twitter.com/pydataberlin"&gt;https://twitter.com/pydataberlin&lt;/a&gt;
Twitter:  &lt;a class="reference external" href="https://twitter.com/pyconde"&gt;https://twitter.com/pyconde&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>