<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Tue, 08 Oct 2019 00:00:00 +0000</lastBuildDate><item><title>Introduction To Docker / Python Microservices</title><link>https://pyvideo.org/hsvpy-huntsvilles-python-meetup/introduction-to-docker-python-microservices.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Two talks: 1) How Docker helped save a company time and money by automated environment setup / A look at Python Microservices using celery&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kyle Galloway</dc:creator><pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-08:hsvpy-huntsvilles-python-meetup/introduction-to-docker-python-microservices.html</guid><category>Docker</category><category>microservices</category></item><item><title>Automate Your Integration Tests Using pytest-docker-compose</title><link>https://pyvideo.org/kiwi-pycon-2019/automate-your-integration-tests-using-pytest-docker-compose.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Sometimes unit tests aren't enough, and you need to actually deploy your solution to see how it behaves as a whole. Utilities like docker-compose make it easy to stand up an entire environment, but the actual testing part still has to be done manually... or does it? Learn how to automate your integration tests using pytest-docker-compose today!&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Phoenix Zerin</dc:creator><pubDate>Sat, 24 Aug 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-08-24:kiwi-pycon-2019/automate-your-integration-tests-using-pytest-docker-compose.html</guid><category>docker</category><category>docker-compose</category><category>pytest</category><category>testing</category></item><item><title>Docker meets Python - A look on the Docker SDK for Python</title><link>https://pyvideo.org/europython-2019/docker-meets-python-a-look-on-the-docker-sdk-for-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;My talk aims to introduce and have a closer look on the Docker SDK for
Python.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;I will cover:&lt;/div&gt;
&lt;div class="line"&gt;- How and where to get the SDK&lt;/div&gt;
&lt;div class="line"&gt;- How it works and how to use it in general&lt;/div&gt;
&lt;div class="line"&gt;- Possible use-cases like: Processing Container-Logs, Testing with
pytest on different Python Versions, Deploy via Python Script, etc..&lt;/div&gt;
&lt;/div&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;For my talk, you should know what Docker is and how to use it.&lt;/div&gt;
&lt;div class="line"&gt;A basic idea of pytest and server administration is nice to have, but
not necessarily needed to follow my talk.&lt;/div&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan Wagner</dc:creator><pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-12:europython-2019/docker-meets-python-a-look-on-the-docker-sdk-for-python.html</guid><category>Deployment/Continuous Integration and Delivery</category><category>DevOps general</category><category>Docker</category><category>Testing</category><category>Virtualization</category></item><item><title>How we run GraphQL APIs in production on our Kubernetes cluster</title><link>https://pyvideo.org/europython-2019/how-we-run-graphql-apis-in-production-on-our-kubernetes-cluster.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I would like to share the workflow and tools we use to
build, deploy and operate GraphQL APIs on our on-premise Kubernetes
cluster.&lt;/p&gt;
&lt;p&gt;I will share code and command examples explaining how we are operating
our applications since our recent transition from REST APIs on Web
servers to GraphQL APIs containers on Kubernetes.&lt;/p&gt;
&lt;p&gt;This talk will not be about the difference between REST and GraphQL but
focus on the workflow, tools and experience we gained in switching our
run time environments and API models.&lt;/p&gt;
&lt;p&gt;At Numberly, we have built and are operating our own on-premise
Kubernetes cluster so we will also be talking about its capabilities and
share some of the experience we gained in doing so.&lt;/p&gt;
&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Proposed agenda:&lt;/div&gt;
&lt;div class="line"&gt;- Our previous workflow and its limitations&lt;/div&gt;
&lt;div class="line"&gt;- How we designed our Kubernetes cluster, its capabilities and the
choices we made&lt;/div&gt;
&lt;div class="line"&gt;- Developer workflow, environments management and deployment&lt;/div&gt;
&lt;div class="line"&gt;- Our GraphQL stack, featuring a sample application&lt;/div&gt;
&lt;div class="line"&gt;- What we're still working on to improve&lt;/div&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alexys Jacob</dc:creator><pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-12:europython-2019/how-we-run-graphql-apis-in-production-on-our-kubernetes-cluster.html</guid><category>APIs</category><category>Best Practice</category><category>Case Study</category><category>Docker</category><category>Infrastructure</category></item><item><title>Optimizing Docker builds for Python applications</title><link>https://pyvideo.org/europython-2019/optimizing-docker-builds-for-python-applications.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;Do you deploy Python applications in Docker? Then this session is for
you!&lt;/div&gt;
&lt;div class="line"&gt;We will start by reviewing a simple Dockerfile to package a Python
application and move to more complex examples which speed up the build
process and reduce the size of the resulting Docker image for both
development and production builds.&lt;/div&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dmitry Figol</dc:creator><pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-12:europython-2019/optimizing-docker-builds-for-python-applications.html</guid><category>Deployment/Continuous Integration and Delivery</category><category>Docker</category></item><item><title>Basta problemi con tensorflow usando Docker &amp; Nvidia Docker</title><link>https://pyvideo.org/pycon-italia-2019/basta-problemi-con-tensorflow-usando-docker-nvidia-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Di sicuro vi sarà capitato che condividendo o effettuando un progetto
tensorflow questo non funzioni correttamente. Soprattutto non riesco a
far scalare la mia app perchè non ho abbastanza macchine con GPU e
eseguire lo scale su macchine con solo CPU è costoso per poi ottenere
scarsi benefici. La soluzione è utilizzare Docker e Nvidia Docker.
Vedremo perchè Docker è migliore di una macchina virtuale e come
cambiano le prestazioni rispetto ad andare direttamente sulla macchina.
Vedremo trucchi su come strutturare dei docker-compose file senza
duplicazione per poter sviluppare agilmente sia con GPU che senza, poter
effettuare un deploy con tranquillità e poter scalare facilmente anche
senza GPU. Slide Link: &amp;lt;&lt;a class="reference external" href="https://www.slideshare.net/NicolaLandro/basta"&gt;https://www.slideshare.net/NicolaLandro/basta&lt;/a&gt;-
problemicontensorflowusandodockernvidiadocker&amp;gt;&lt;/p&gt;
&lt;p&gt;Feedback form: &lt;a class="reference external" href="https://python.it/feedback-1528"&gt;https://python.it/feedback-1528&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Saturday 4 May&lt;/strong&gt; at 10:45 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicola Landro</dc:creator><pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-04:pycon-italia-2019/basta-problemi-con-tensorflow-usando-docker-nvidia-docker.html</guid><category>OSX</category><category>windows</category><category>devops</category><category>Machine Learning</category><category>GNU/Linux</category><category>cuda</category><category>tensorflow</category><category>docker</category></item><item><title>Geospatial analysis with Python</title><link>https://pyvideo.org/pycon-italia-2019/geospatial-analysis-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Due to its effectiveness and simplicity, Python is spreading as a choice
for handling geospatial data. From running algorithms capable of
extracting geo- referred insights or processing geo archives, Python
could represent a powerful tool to handle geo-related problems thanks to
an extensive set of libraries. The training will give an overview about
processing geospatial data with Python. An approximate agenda will
include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction&lt;ul&gt;
&lt;li&gt;setting up the the environment&lt;/li&gt;
&lt;li&gt;an introduction to geospatial world&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Working with geospatial data&lt;ul&gt;
&lt;li&gt;playing with geo archives (vector/rasters)&lt;/li&gt;
&lt;li&gt;extracting geo analytics&lt;/li&gt;
&lt;li&gt;Vector tiles big vector data&lt;/li&gt;
&lt;li&gt;Apache Spark for geospatial raster analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Python for Earth observation&lt;ul&gt;
&lt;li&gt;Classifying earth observation images&lt;/li&gt;
&lt;li&gt;Extracting insights from Copernicus products&lt;/li&gt;
&lt;li&gt;Use SNAP from Python&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No tools are required for attending this training. Bring your PC with
Docker installed. Further instructions will be provided.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1674"&gt;https://python.it/feedback-1674&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 10:30 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt; in __on &lt;strong&gt;Saturday 4
May&lt;/strong&gt; at 18:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesco Bruni</dc:creator><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-03:pycon-italia-2019/geospatial-analysis-with-python.html</guid><category>Jupyter</category><category>pyspark</category><category>geospatial</category><category>geopynotebook</category><category>spark</category><category>docker</category></item><item><title>PostgreSQL on the kube</title><link>https://pyvideo.org/pycon-italia-2019/postgresql-on-the-kube.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Kubernetes è un sistema di orchestrazione di container che permette di
gestire il deploy, lo scaling e l’aggiornamento di una applicazione e di
tutti i suoi componenti.&lt;/p&gt;
&lt;p&gt;In questo talk parleremo di quali strumenti sono a disposizione per
effettuare un deploy di un database PostgreSQL in un cluster Kubernetes.
Inoltre vedremo come sia implementabile l’alta disponibilità e la
disaster recovery, in maniera da avere i propri dati al sicuro e sempre
accessibili.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1784"&gt;https://python.it/feedback-1784&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 10:30 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marco Nenciarini</dc:creator><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-03:pycon-italia-2019/postgresql-on-the-kube.html</guid><category>postgresql</category><category>postgres</category><category>kubernetes</category><category>storage</category><category>containers</category><category>docker</category><category>k8s</category><category>cloud</category></item><item><title>DevOps di applicazioni Python (e non solo) su OpenShift</title><link>https://pyvideo.org/pycon-italia-2018/devops-di-applicazioni-python-e-non-solo-su-openshift.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="section" id="abstract"&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;OpenShift Origin è la Platform-as-a-Service opensource di riferimento.
Basata su Kubernetes e Docker, contiene features aggiuntive e
integrazioni con altri componenti che semplificano le pratiche di
DevOps.&lt;/p&gt;
&lt;p&gt;Dopo una breve introduzione ad Openshift ed alla sua architettura,
vedremo come:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;fare il setup di infrastrutture applicative microservice-based (es.
microservizi Python Flask/Django, single page application Angular,
ecc…)&lt;/li&gt;
&lt;li&gt;creare una piattaforma di Continuous Integration e Continuous
Delivery&lt;/li&gt;
&lt;li&gt;implementare e gestire la CI/CD di microservice-based application
sfruttando l’integrazione con Git e Jenkins&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="agenda"&gt;
&lt;h4&gt;Agenda&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;architettura di base di OpenShift&lt;/li&gt;
&lt;li&gt;come costruire un &lt;em&gt;project&lt;/em&gt; OpenShift: &lt;em&gt;builds&lt;/em&gt; e &lt;em&gt;deployments&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;automatizzare il setup mediante &lt;em&gt;template&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;utilizzare Git, Jenkins e Openshift per creare una semplice pipeline
di CI/CD&lt;/li&gt;
&lt;li&gt;strategie di deployment avanzate: &lt;em&gt;blue-green deployment&lt;/em&gt; , &lt;em&gt;A/B
deployment&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="prerequisiti"&gt;
&lt;h4&gt;Prerequisiti&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;conoscenza base di Git e Jenkins&lt;/li&gt;
&lt;li&gt;conoscenza base dei concetti CI/CD e DevOps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in __on &lt;strong&gt;venerdì 20 aprile&lt;/strong&gt; at 11:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesco Fiore</dc:creator><pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-04-20:pycon-italia-2018/devops-di-applicazioni-python-e-non-solo-su-openshift.html</guid><category>microservices</category><category>continuous-integration</category><category>git</category><category>continuous-delivery</category><category>kubernetes</category><category>devops</category><category>jenkins</category><category>docker</category><category>OpenShift</category></item><item><title>PaaS per tutti i gusti: CI/CD sotto controllo con Kubernetes e Dokku</title><link>https://pyvideo.org/pycon-italia-2018/paas-per-tutti-i-gusti-cicd-sotto-controllo-con-kubernetes-e-dokku.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In questo talk saranno illustrati processi e workflow tipici dei
paradigmi del DevOp e delle metodologie Agile. Vedremo quali
&lt;strong&gt;accorgimenti&lt;/strong&gt; devono essere presi con le applicazioni pacchettizzate
con &lt;strong&gt;Docker&lt;/strong&gt; , in particolare le applicazioni Django e come &lt;strong&gt;evitare
le problematiche principali che portano frustrazione e impediscono
un’adozione reale della CI/CD&lt;/strong&gt;. Saranno presentati degli esempi pratici
&lt;strong&gt;workflow&lt;/strong&gt; implementati con successo, in modo snello, versionato e
ripetibile, in ambienti che vanno dal test fino alla produzione. In
ultimo faremo una carrellata dei sistemi di &lt;strong&gt;PaaS&lt;/strong&gt; più in voga del
momento concentrandoci quindi su &lt;strong&gt;Dokku&lt;/strong&gt; e &lt;strong&gt;Kubernetes&lt;/strong&gt; , che
coprono tutto il ventaglio delle necessità di deploy, dal piccolo sito
fino al sistema ultra scalabile e ridondato.&lt;/p&gt;
&lt;p&gt;Prerequisito per il talk è conoscere i concetti base di Docker e
dell’uso di git. Durante il talk con 3 distinti esempi e demo di
complessità crescente esploreremo il mondo della CI/CD.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;venerdì 20 aprile&lt;/strong&gt; at 12:30 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Claudio Mignanti</dc:creator><pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-04-20:pycon-italia-2018/paas-per-tutti-i-gusti-cicd-sotto-controllo-con-kubernetes-e-dokku.html</guid><category>paas</category><category>continuous-integration</category><category>gitlab</category><category>kubernetes</category><category>testing</category><category>git</category><category>docker</category></item><item><title>Scaling your Data infrastructure</title><link>https://pyvideo.org/pycon-italia-2018/scaling-your-data-infrastructure.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;This talk aims to answer a few questions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What do you do when you need to move your model from your laptop to
production?&lt;/li&gt;
&lt;li&gt;Is &lt;tt class="docutils literal"&gt;big data == I need to use JVM&lt;/tt&gt; the right assumption?&lt;/li&gt;
&lt;li&gt;How can I put my jupyter notebook in production?&lt;/li&gt;
&lt;li&gt;How do you apply the best software engineering practices (testing and
ci for example) inside your data science process?&lt;/li&gt;
&lt;li&gt;How do you “decouple” your data scientists, developers and devops
teams?&lt;/li&gt;
&lt;li&gt;How do you guarantee the reproducibility of your models?&lt;/li&gt;
&lt;li&gt;How do you scale your training process when does not fit in memory
anymore?&lt;/li&gt;
&lt;li&gt;How do you serve your models and provide an easy rollback system?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Agenda:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The Data Science workflow&lt;/li&gt;
&lt;li&gt;Scaling is not just a matter of the size of your Data&lt;/li&gt;
&lt;li&gt;Scaling when the size of your Data matters&lt;/li&gt;
&lt;li&gt;DDS, Dockerized Data Science&lt;/li&gt;
&lt;li&gt;Cassiny&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll share my experience highlighting some of the challenges I faced and
the solutions I came up to answer these questions.&lt;/p&gt;
&lt;p&gt;During this presentation I will mention libraries like jupyter, atom,
scikit- learn, dask, ray, parquet, arrow and many others.&lt;/p&gt;
&lt;p&gt;The principles and best practices I will share are something that you
can apply, more or less easily, if you are running or in the process to
run a production system based on the Python stack.&lt;/p&gt;
&lt;p&gt;This talk will focus on (my) best practices to run the Python Data stack
together and I will also talk about Cassiny, an open source project I
started, that aims to simplify your life if you want to use a completely
Python based solution in your data science workflow.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 20 April&lt;/strong&gt; at 11:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Christian Barra</dc:creator><pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-04-20:pycon-italia-2018/scaling-your-data-infrastructure.html</guid><category>Jupyter</category><category>CloudComputing</category><category>pydata</category><category>#lessonslearned</category><category>Big-Data</category><category>S3</category><category>Data-Scientist</category><category>#amicodialessia</category><category>java</category><category>docker</category><category>cloud</category></item><item><title>Aplicações web com Flask e Docker</title><link>https://pyvideo.org/flask-conf-2018/aplicacoes-web-com-flask-e-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Aplicações web com Flask e Docker - Palestra de Felipe Alcantara na Flask Conf 2018.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Felipe Alcantara</dc:creator><pubDate>Sat, 25 Aug 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-08-25:flask-conf-2018/aplicacoes-web-com-flask-e-docker.html</guid><category>flask</category><category>docker</category></item><item><title>Launch Jupyter to the Cloud with Docker and Terraform</title><link>https://pyvideo.org/pycon-israel-2018/launch-jupyter-to-the-cloud-with-docker-and-terraform.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Launch Jupyter to the Cloud: an example of using Docker and Terraform&lt;/p&gt;
&lt;p&gt;In this talk, we will use a task: hiring a GPU on Google Cloud Platform to train neural network, as an example to show how an application can be deployed on a cloud platform with Docker and Terraform. The goal is to have Jupyter Notebook running in an environment with Tensorflow (GPU version) and other libraries installed on a Google Compute Engine.&lt;/p&gt;
&lt;p&gt;This talk is for people with no experience in application deployment on cloud service but would benefit form computational reproducibility and cloud service, potentially data scientists/ analysts or tech practitioners who didn't have a software developing background. We will use an example that is simple but useful in data science to demonstrate basic usage of Docker and Terraform which would be beneficial to beginners who would like to simplify their work flow with those tools.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Cheuk Ting Ho</dc:creator><pubDate>Tue, 05 Jun 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-06-05:pycon-israel-2018/launch-jupyter-to-the-cloud-with-docker-and-terraform.html</guid><category>jupyter</category><category>docker</category><category>terraform</category></item><item><title>Docker for Data Science</title><link>https://pyvideo.org/pycon-us-2018/docker-for-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter notebooks simplify the process of developing and sharing Data Science projects across groups and organizations. However, when we want to deploy our work into production, we need to extract the model from the notebook and package it up with the required artifacts (data, dependencies, configurations, etc) to ensure it works in other environments. Containerization technologies such as Docker can be used to streamline this workflow.&lt;/p&gt;
&lt;p&gt;This hands-on tutorial presents Docker in the context of Reproducible Data Science - from idea to application deployment. You will get a thorough introduction to the world of containers; learn how to incorporate Docker into various Data Science projects; and walk through the process of building a Machine Learning model in Jupyter and deploying it as a containerized Flask REST API.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aly Sivji</dc:creator><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-10:pycon-us-2018/docker-for-data-science.html</guid><category>jupyter</category><category>docker</category><category>data science</category></item><item><title>Containerize all the things</title><link>https://pyvideo.org/caipyra-2016/containerize-all-the-things.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Palestra do Andrews Medina no Caipyra 2016:&lt;/p&gt;
&lt;p&gt;Containerize all the things&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://talks.godoc.org/github.com/andrewsmedina/containerize-all-the-things/sample.slide#1"&gt;http://talks.godoc.org/github.com/andrewsmedina/containerize-all-the-things/sample.slide#1&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrews Medina</dc:creator><pubDate>Sat, 25 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-25:caipyra-2016/containerize-all-the-things.html</guid><category>docker</category><category>devops</category></item><item><title>Sparking Pandas: an experiment</title><link>https://pyvideo.org/pycon-italia-2017/sparking-pandas-an-experiment.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a good library to deal with tabular data. What if you need to
manage an amount of data that doesn’t fit into memory? What if you want
to “distribute” your computations among multiple machines?&lt;/p&gt;
&lt;p&gt;Starting from a real scenario, Apache Spark will be presented as the
main tool to read and process collected data. It will be shown how a
Pandas-like syntax will come in handy to run aggregations, filtering and
grouping using a Spark Dataframe.&lt;/p&gt;
&lt;p&gt;A previous knowledge of Docker and Docker Compose will be very useful
while knowing MongoDB (where data will be fetched from) is not
mandatory. Basics of functional programming will help to understand
Spark inner logic.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesco Bruni</dc:creator><pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-07:pycon-italia-2017/sparking-pandas-an-experiment.html</guid><category>microservices</category><category>Jupyter</category><category>mongodb</category><category>data-visualization</category><category>data-analysis</category><category>spark</category><category>docker</category></item><item><title>Dockerizing the Python</title><link>https://pyvideo.org/pycon-se-2017/dockerizing-the-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The focus of this talk will be on building and running python programs in a docker container. A major chunk of the talk will also concentrate on the docker setup &amp;amp; it’s complete echo system. The purpose behind the talk is the demonstrate the usage of docker in everyday python development &amp;amp; deployment environment.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ambreen Sheikh</dc:creator><pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-09-06:pycon-se-2017/dockerizing-the-python.html</guid><category>docker</category></item><item><title>Extend Docker using Python</title><link>https://pyvideo.org/pycon-israel-2017/extend-docker-using-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Docker is the most popular platform to run Python applications within containers. Many companies are using this platform to either deploy micro-services or test the code changes before merging it to production. Docker has 3 extension points: Drivers, Plugins and user-facing API. I am going to focus on the latter (user-facing API) and by the end of the talk, you will learn Docker's REST API and know how to extend Docker capabilities using Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Boaz Shuster</dc:creator><pubDate>Tue, 13 Jun 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-06-13:pycon-israel-2017/extend-docker-using-python.html</guid><category>docker</category></item><item><title>Running jupyter notebook remotely in a docker swarm cluster</title><link>https://pyvideo.org/pydata-barcelona-2017/running-jupyter-notebook-remotely-in-a-docker-swarm-cluster.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The current version of Jupyter Notebook computes the document state at the browser side, this is a problem if you run long jobs in a remote notebook from a laptop. If you close the browser you lose all the output of the current running cell. I will explain how we solved this problem in our lab. This solution it also allows a &amp;quot;walkie-talkie&amp;quot; like real-time collaboration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The solution is based on running a docker container with a browser and a VNC server. All the remote access to the notebooks is done using Apache Guacamole a clientless remote desktop gateway. Everything is running on a dynamic docker swarm cluster of 20 nodes. As a lateral effect, this solution it also allows a real-time collaboration between users in a way that multiple users can access at the same time the same desktop (but they have to fight for the mouse and the keyboard).&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/jordeu/pytalks/tree/master/20170520_pydata_jupyter_in_a_docker_cluster"&gt;https://github.com/jordeu/pytalks/tree/master/20170520_pydata_jupyter_in_a_docker_cluster&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jordi Deu-Pons</dc:creator><pubDate>Sat, 20 May 2017 15:00:00 +0200</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/running-jupyter-notebook-remotely-in-a-docker-swarm-cluster.html</guid><category>jupyter notebook</category><category>docker</category><category>swarm</category></item><item><title>DIY Serverless Platform with Python3 and Docker</title><link>https://pyvideo.org/pycon-jamaica-2016/diy-serverless-platform-with-python3-and-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A walkthrough on how I built my own serverless platform to run both ephemeral and long-lasting functions in Python on top of Docker, capable of handling REST and websocket connections. Will go over architecture, show code, and discuss pain points as well as next steps.  #pyconjamaica2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joir-dan Gumbs</dc:creator><pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-11-18:pycon-jamaica-2016/diy-serverless-platform-with-python3-and-docker.html</guid><category>Serverless</category><category>Docker</category></item><item><title>Dev Ops meets Data Science Taking models from prototype to production with Docker</title><link>https://pyvideo.org/pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We present the evolution of a model to a production API that can scale to large e-commerce needs. On the journey we discuss metrics of success and how to use the Kubernetes cluster manager and associated tools for deploy. In addition to the use of these tools we highlight how to make use of the cluster management system for further testing and experimentation with your models.&lt;/p&gt;
&lt;p&gt;The chasm between data science and dev ops is often wide and impenetrable, but the two fields have more in common than meets the eye. Every data scientist will be able to lean in and help their career by investing in a basic understanding the basic principles of dev ops. In this talk I present the notions of service level indicators, objectives, and agreements. I cover the rigorous monitoring and testing of services. Finally we demonstrate how to build a basic data science workflow and push to production level APIs with Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes is an opinionated container cluster manager with an easy to use, robust interface. It can be use on very small and very large clusters. Docker is a container system that allows one to build code in an isolated environment. Paired with a container manager such as Kubernetes we are able to manage millions of instances as needed for a production deployment. These tools are two of many different options but are considered among the best open source solutions available.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andy Terrel</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</guid><category>Data</category><category>data science</category><category>docker</category><category>models</category><category>science</category></item><item><title>Setting up predictive analytics services with Palladium</title><link>https://pyvideo.org/pydata-berlin-2016/setting-up-predictive-analytics-services-with-palladium.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;We will introduce Palladium, an open source framework for setting up predictive analytics services. It supports tasks like fitting, evaluating, storing, and distributing (predictive) models. Core ML processes are compatible with scikit-learn and a large number of scikit-learn’s features can be used. Besides the use of Palladium we will also show how to use it with Docker and Mesos / Marathon.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In this talk, we will introduce Palladium, an open source framework for easily setting up predictive analytics services (&lt;a class="reference external" href="https://github.com/ottogroup/palladium"&gt;https://github.com/ottogroup/palladium&lt;/a&gt;). It supports tasks like fitting, evaluating, storing, distributing, and updating (predictive) models. Core machine learning processes are compatible with the open source machine learning library scikit-learn and thus, a large number of scikit-learn’s features can be used with Palladium. Although being implemented in Python, Palladium provides support for other languages and is shipped with examples how to integrate and expose R and Julia models. For an efficient deployment of services based on Palladium, a script to create Docker images automatically is provided. This talk will cover the use of Palladium including an example where a simple classification service is set up. We will also show how Docker and Mesos / Marathon can be used to deploy and scale Palladium-based services. Having basic knowledge about Machine Learning and/or scikit-learn would be an advantage when attending this talk.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andreas Lattner</dc:creator><pubDate>Tue, 07 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-07:pydata-berlin-2016/setting-up-predictive-analytics-services-with-palladium.html</guid><category>palladium</category><category>scikit-learn</category><category>docker</category><category>mesos</category><category>marathon</category><category>machine learning</category></item><item><title>Building Python apps with Docker</title><link>https://pyvideo.org/pytexas-2015/building-python-apps-with-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you haven't heard of Docker yet, its a great tool that allows you
wrap up your app and everything it needs to run: code, runtime, and even
system libraries and guarantee that it will always run the same,
regardless of the environment (local machine, server, or even the
cloud). Whether you're deploying a web app, performing data analysis, or
creating local environments for your dev team or CI builds, Docker can
help.&lt;/p&gt;
&lt;p&gt;I'll give an introduction to Docker, an overview of some of the current
tools in the Docker ecosystem (Docker Machine and Docker Compose) and
demonstrate how to create, build, and deploy Python applications using
Docker.&lt;/p&gt;
&lt;p&gt;This talk is targeted towards web developers, data scientists, or really
anyone who develops using Python that would like to learn more about
Docker and how it can help their projects.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mark Adams</dc:creator><pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-10-09:pytexas-2015/building-python-apps-with-docker.html</guid><category>Docker</category></item></channel></rss>