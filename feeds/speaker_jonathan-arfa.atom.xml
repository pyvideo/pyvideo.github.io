<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_jonathan-arfa.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-10-04T00:00:00+00:00</updated><entry><title>You Don't Need Neural Nets - How Simpler Algorithms Can Solve Your Problems With Less Headache</title><link href="https://pyvideo.org/pygotham-2019/you-dont-need-neural-nets-how-simpler-algorithms-can-solve-your-problems-with-less-headache.html" rel="alternate"></link><published>2019-10-04T00:00:00+00:00</published><updated>2019-10-04T00:00:00+00:00</updated><author><name>Gabe Levine</name></author><id>tag:pyvideo.org,2019-10-04:pygotham-2019/you-dont-need-neural-nets-how-simpler-algorithms-can-solve-your-problems-with-less-headache.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You’ve heard a lot about Neural Networks (or “A.I.” as your company’s
marketing team likes to call it), and how they are solution to all of your
problems. Unfortunately, they’re also finicky and complex. And for most
problems that most people deal with, they’re not necessarily much better
than easier-to-use algorithms such as Gradient Boosted Trees (GBTs). We will
explain (briefly) what GBTs are, compare neural networks vs. GBTs on some
benchmark problems, demonstrate where Neural Nets are necessary
(images/sound/text), and share some tips for making GBTs working even on
those cases.&lt;/p&gt;
&lt;p&gt;This talk is meant for people with a cursory knowledge of machine learning,
but who aren’t necessarily experts.&lt;/p&gt;
</summary></entry><entry><title>Machine Music: Exploring Machine Learning By Generating Music Or How To Teach A Computer To Rock Out</title><link href="https://pyvideo.org/pygotham-2017/machine-music-exploring-machine-learning-by-generating-music-or-how-to-teach-a-computer-to-rock-out.html" rel="alternate"></link><published>2017-10-07T00:00:00+00:00</published><updated>2017-10-07T00:00:00+00:00</updated><author><name>Gabe Levine</name></author><id>tag:pyvideo.org,2017-10-07:pygotham-2017/machine-music-exploring-machine-learning-by-generating-music-or-how-to-teach-a-computer-to-rock-out.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will go through the ups and downs of a machine learning    beginner trying to create a Recurrent Neural Network (RNN) to implement    a music generating algorithm. We will build off of Gabe’s PyGotham 2016    talk (The Sounds Of Data: &lt;a class="reference external" href="https://www.youtube.com/watch?v=vb9c_WFMYeI"&gt;https://www.youtube.com/watch?v=vb9c_WFMYeI&lt;/a&gt;)    and will attempt to implement an RNN based on Alex Graves’ GeneratingSequences With Recurrent Neural Networks    (&lt;a class="reference external" href="https://arxiv.org/pdf/1308.0850.pdf"&gt;https://arxiv.org/pdf/1308.0850.pdf&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Gabe Levine is a &lt;a class="reference external" href="http://pitchfork.com/reviews/albums/12049-migration/"&gt;musician&lt;/a&gt;    turned &lt;a class="reference external" href="https://github.com/gabelev"&gt;software engineer&lt;/a&gt;, and Jonathan Arfa is a &lt;a class="reference external" href="https://github.com/jarfa"&gt;Statistics and     Machine Learning enthusiast&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Algorithms to Sample From Streams - Reservoir Sampling &amp; Variants</title><link href="https://pyvideo.org/pycon-israel-2016/algorithms-to-sample-from-streams-reservoir-sampling-variants.html" rel="alternate"></link><published>2016-09-20T00:00:00+00:00</published><updated>2016-09-20T00:00:00+00:00</updated><author><name>Jonathan Arfa</name></author><id>tag:pyvideo.org,2016-09-20:pycon-israel-2016/algorithms-to-sample-from-streams-reservoir-sampling-variants.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Those of us who build systems that deal with constant streams of incoming data (which includes anybody whose code touches the internet) often need algorithms that keep a fixed-size sample from the stream for on-the-fly analytics. Reservoir sampling is a commonly used algorithm for keeping a sample that's unbiased over all events in the stream. But what if you want your algorithm to keep a representative sample from the past 10 minutes instead of the entire stream? In this talk I'll start with a gentle introduction to Reservoir sampling for those who are not familiar with it, and then discuss several variants which keep a sample that is biased towards the present. One of these is the VIRB (Variable Incoming Rate Biased) sampler, which we have developed in-house at Magnetic. This talk covers the same material as this blog post: &lt;a class="reference external" href="http://tech.magnetic.com/2016/04/virbs-sampling-events-from-streams.html"&gt;http://tech.magnetic.com/2016/04/virbs-sampling-events-from-streams.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://il.pycon.org/2016/static/sessions/jonathan-arfa.pdf"&gt;http://il.pycon.org/2016/static/sessions/jonathan-arfa.pdf&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>