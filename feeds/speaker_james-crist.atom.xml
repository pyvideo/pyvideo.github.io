<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_james-crist.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-07-09T00:00:00+00:00</updated><entry><title>Parallelizing Scientific Python with Dask</title><link href="https://pyvideo.org/scipy-2018/parallelizing-scientific-python-with-dask.html" rel="alternate"></link><published>2018-07-09T00:00:00+00:00</published><updated>2018-07-09T00:00:00+00:00</updated><author><name>James Crist</name></author><id>tag:pyvideo.org,2018-07-09:scipy-2018/parallelizing-scientific-python-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a flexible tool for parallelizing Python code on a single
machine or across a cluster. It builds upon familiar tools in the SciPy
ecosystem (e.g. NumPy and Pandas) while allowing them to scale across
multiple cores or machines. This tutorial will cover both the high-level
use of dask collections, as well as the low-level use of dask graphs and
schedulers.Presenter(s): Speaker: James Crist, Anaconda, Inc. Speaker:
Martin Durant, Anaconda Inc.&lt;/p&gt;
</summary></entry><entry><title>Parallel Data Analysis with Dask</title><link href="https://pyvideo.org/pycon-us-2018/parallel-data-analysis-with-dask.html" rel="alternate"></link><published>2018-05-09T00:00:00+00:00</published><updated>2018-05-09T00:00:00+00:00</updated><author><name>Tom Augspurger</name></author><id>tag:pyvideo.org,2018-05-09:pycon-us-2018/parallel-data-analysis-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The libraries that power data analysis in Python are essentially limited to a single CPU core and to datasets that fit in RAM. Attendees will see how dask can parallelize their workflows, while still writing what looks like normal python, NumPy, or pandas code.&lt;/p&gt;
&lt;p&gt;Dask is a parallel computing framework, with a focus on analytical computing. We'll start with &lt;cite&gt;dask.delayed&lt;/cite&gt;, which helps parallelize your existing Python code. We’ll demonstrate &lt;cite&gt;dask.delayed&lt;/cite&gt; on a small example, introducing the concepts at the heart of dask like the &lt;em&gt;task graph&lt;/em&gt; and the &lt;em&gt;schedulers&lt;/em&gt; that execute tasks. We’ll compare this approach to the simpler, but less flexible, parallelization methods available in the standard library like &lt;cite&gt;concurrent.futures&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Attendees will see the high-level collections dask provides for writing regular Python, NumPy, or Pandas code that is then executed in parallel on datasets that may be larger than memory. These high level collections provide a familiar API, but the execution model is very different. We'll discuss concepts like the GIL, serialization, and other headaches that come up with parallel programming. We’ll use dask’s various schedulers to illustrate the differences between multi-threaded, multi-processes, and distributed computing.&lt;/p&gt;
&lt;p&gt;Dask includes a distributed scheduler for executing task graphs on a cluster of machines. We’ll provide each person access to their own cluster.&lt;/p&gt;
</summary><category term="dask"></category></entry><entry><title>Make it Work, Make it Right, Make it Fast Debugging and Profiling in Dask</title><link href="https://pyvideo.org/scipy-2017/make-it-work-make-it-right-make-it-fast-debugging-and-profiling-in-dask.html" rel="alternate"></link><published>2017-07-17T00:00:00+00:00</published><updated>2017-07-17T00:00:00+00:00</updated><author><name>James Crist</name></author><id>tag:pyvideo.org,2017-07-17:scipy-2017/make-it-work-make-it-right-make-it-fast-debugging-and-profiling-in-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a pure Python library for parallel and distributed computing. It's designed with simplicity and flexibility in mind, making it easy to parallelize the kind of messy custom workflows that often show up in science. However, once you get something working, how do you debug or profile it? Debugging and profiling parallel code is notoriously hard! In this talk we'll cover the various tools Dask provides for diagnosing bugs and performance bottlenecks, as well as tips and techniques for resolving these issues.&lt;/p&gt;
</summary><category term="dask"></category></entry><entry><title>Parallelizing Scientific Python with Dask</title><link href="https://pyvideo.org/scipy-2017/parallelizing-scientific-python-with-dask.html" rel="alternate"></link><published>2017-07-13T00:00:00+00:00</published><updated>2017-07-13T00:00:00+00:00</updated><author><name>James Crist</name></author><id>tag:pyvideo.org,2017-07-13:scipy-2017/parallelizing-scientific-python-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Tutorial materials found here: &lt;a class="reference external" href="https://scipy2017.scipy.org/ehome/220975/493423/"&gt;https://scipy2017.scipy.org/ehome/220975/493423/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dask is a flexible tool for parallelizing Python code on a single machine or across a cluster. We can think of dask at a high and a low level:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;High level collections: Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and Pandas but can operate in parallel on datasets that don't fit into main memory. Dask's high-level collections are alternatives to NumPy and Pandas for large datasets.&lt;/li&gt;
&lt;li&gt;Low Level schedulers: Dask provides dynamic task schedulers that execute task graphs in parallel. These execution engines power the high-level collections mentioned above but can also power custom, user-defined workloads. These schedulers are low-latency (around 200 us) and work hard to run computations in a small memory footprint. Dask's schedulers are an alternative to direct use of threading or multiprocessing libraries in complex cases or other task scheduling systems like Luigi or IPython parallel.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Different users operate at different levels but it is useful to understand both. This tutorial will cover both the high-level use of dask.array and dask.dataframe and low-level use of dask graphs and schedulers.&lt;/p&gt;
</summary><category term="tutorial"></category><category term="dask"></category></entry><entry><title>Dask Out of core NumPy:Pandas through Task Scheduling</title><link href="https://pyvideo.org/scipy-2015/dask-out-of-core-numpypandas-through-task-scheduling.html" rel="alternate"></link><published>2015-07-08T00:00:00+00:00</published><updated>2015-07-08T00:00:00+00:00</updated><author><name>James Crist</name></author><id>tag:pyvideo.org,2015-07-08:scipy-2015/dask-out-of-core-numpypandas-through-task-scheduling.html</id><summary type="html"></summary></entry><entry><title>Multibody Dynamics and Control with Python</title><link href="https://pyvideo.org/scipy-2015/multibody-dynamics-and-control-with-python.html" rel="alternate"></link><published>2015-07-08T00:00:00+00:00</published><updated>2015-07-08T00:00:00+00:00</updated><author><name>Jason Moore</name></author><id>tag:pyvideo.org,2015-07-08:scipy-2015/multibody-dynamics-and-control-with-python.html</id><summary type="html"></summary><category term="tutorial"></category></entry></feed>