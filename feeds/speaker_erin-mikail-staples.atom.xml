<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Erin Mikail Staples</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_erin-mikail-staples.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-04-17T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Improving Machine Learning from Human Feedback</title><link href="https://pyvideo.org/pydata-berlin-2023/improving-machine-learning-from-human-feedback.html" rel="alternate"></link><published>2023-04-17T00:00:00+00:00</published><updated>2023-04-17T00:00:00+00:00</updated><author><name>Erin Mikail Staples</name></author><id>tag:pyvideo.org,2023-04-17:/pydata-berlin-2023/improving-machine-learning-from-human-feedback.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large generative models rely upon massive data sets that are collected automatically. For example, GPT-3 was trained with data from &amp;quot;Common Crawl&amp;quot; and &amp;quot;Web Text&amp;quot;, among other sources. As the saying goes — bigger isn't always better. While powerful, these data sets (and the models that they create) often come …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large generative models rely upon massive data sets that are collected automatically. For example, GPT-3 was trained with data from &amp;quot;Common Crawl&amp;quot; and &amp;quot;Web Text&amp;quot;, among other sources. As the saying goes — bigger isn't always better. While powerful, these data sets (and the models that they create) often come at a cost, bringing their &amp;quot;internet-scale biases&amp;quot; along with their &amp;quot;internet-trained models.&amp;quot; While powerful, these models beg the question — is unsupervised learning the best future for machine learning?&lt;/p&gt;
&lt;p&gt;ML researchers have developed new model-tuning techniques to address the known biases within existing models and improve their performance (as measured by response preference, truthfulness, toxicity, and result generalization). All of this at a fraction of the initial training cost. In this talk, we will explore these techniques, known as Reinforcement Learning from Human Feedback (RLHF), and how open-source machine learning tools like PyTorch and Label Studio can be used to tune off-the-shelf models using direct human feedback.&lt;/p&gt;
</content><category term="PyData Berlin 2023"></category><category term="reinforcement learning"></category><category term="human feedback"></category><category term="pytorch"></category><category term="label studio"></category></entry></feed>