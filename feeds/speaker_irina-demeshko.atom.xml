<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Irina Demeshko</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_irina-demeshko.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2025-05-17T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Scale Smarter, Not Harder, with cuPyNumeric</title><link href="https://pyvideo.org/pycon-us-2025/scale-smarter-not-harder-with-cupynumeric.html" rel="alternate"></link><published>2025-05-17T00:00:00+00:00</published><updated>2025-05-17T00:00:00+00:00</updated><author><name>Irina Demeshko</name></author><id>tag:pyvideo.org,2025-05-17:/pycon-us-2025/scale-smarter-not-harder-with-cupynumeric.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many data and simulation scientists use NumPy for its ease of use and good performance on CPU. This approach works well for single-node tasks, but scaling to handle larger datasets or more resource-intensive computations introduces significant challenges. Not to mention, using GPUs requires another level of complexity. We present â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many data and simulation scientists use NumPy for its ease of use and good performance on CPU. This approach works well for single-node tasks, but scaling to handle larger datasets or more resource-intensive computations introduces significant challenges. Not to mention, using GPUs requires another level of complexity. We present the cuPyNumeric library. cuPyNumeric gives developers the same familiar NumPy interface, but seamlessly distributes work across CPUs and GPUs.&lt;/p&gt;
&lt;p&gt;A compelling example when scaling is necessary is when scientists at the SLAC National Accelerator Laboratory need to process a large amount of data within a fixed time window, called beam time. The full dataset generated during experiments is too large to be processed on a single CPU. Additionally, the code often must be modified during the beam time to adapt to changing experimental needs. Being able to use NumPy syntax rather than lower level distributed computing libraries makes these changes quick and easy, allowing researchers to focus on conducting more experiments rather than debugging or optimizing code.&lt;/p&gt;
&lt;p&gt;cuPyNumeric is designed to be a drop-in replacement to NumPy. Built on top of task-based distributed runtime from Stanford University, it automatically parallelizes NumPy APIs across all available resources, taking care of data distribution, communication, asynchronous and accelerated execution of compute kernels on both GPUs or multi-core CPUs. In addition, cuPyNumeric can be integrated with other popular Python libraries like SciPy, matplotlib, Jax. With cuPyNumeric, SLAC scientists successfully ran their data processing code distributed across multiple nodes and GPUs, processing the full dataset with a 6x speed-up compared to the original single-node implementation.&lt;/p&gt;
&lt;p&gt;In this talk we showcase the productivity and performance of cuPyNumeric library covering some detail on its implementation.&lt;/p&gt;
</content><category term="PyCon US 2025"></category></entry></feed>