<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Stefanie Stoppel</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_stefanie-stoppel.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2022-05-11T00:00:00+00:00</updated><subtitle></subtitle><entry><title>The Myth of Neutrality: How AI is widening social divides</title><link href="https://pyvideo.org/europython-2021/the-myth-of-neutrality-how-ai-is-widening-social-divides.html" rel="alternate"></link><published>2021-07-26T00:00:00+00:00</published><updated>2021-07-26T00:00:00+00:00</updated><author><name>Stefanie Stoppel</name></author><id>tag:pyvideo.org,2021-07-26:/europython-2021/the-myth-of-neutrality-how-ai-is-widening-social-divides.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Myth of Neutrality: How AI is widening social divides
[EuroPython 2021 - Talk - 2021-07-30 - Optiver]
[Online]&lt;/p&gt;
&lt;p&gt;By Stefanie Stoppel&lt;/p&gt;
&lt;p&gt;Imagine you're a Black woman having your face not recognized by a government photo booth, no matter how you position yourself in front of the camera.
Imagine you're a woman …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Myth of Neutrality: How AI is widening social divides
[EuroPython 2021 - Talk - 2021-07-30 - Optiver]
[Online]&lt;/p&gt;
&lt;p&gt;By Stefanie Stoppel&lt;/p&gt;
&lt;p&gt;Imagine you're a Black woman having your face not recognized by a government photo booth, no matter how you position yourself in front of the camera.
Imagine you're a woman getting your loan request rejected, while your partner - who has a similar income and credit history - gets his approved.
Imagine you're an African American man arrested by the police because your face was mistakenly matched to a guy involved in an armed robbery.&lt;/p&gt;
&lt;p&gt;In these real-world examples, the people affected might not know that they are being treated unfairly by Artificial Intelligence (AI). And even if they did, they would not be able to do anything about it.
While they may be used to handling discrimination by humans, algorithmic discrimination is a different story: you cannot argue with the algorithms and, due to their inherent scalability, you might be confronted with them wherever you go.
We often expect AI technology to be neutral, but it's far from it. The reason is that - especially when we are not aware of it - we transfer existing stereotypes into these systems through our current data collection practices, our development processes and by how we apply these technologies within our societies.
My talk will shed light on how algorithms become discriminatory, how difficult it is to build &amp;quot;fair and responsible&amp;quot; AI, and what we should do to prevent the systems we build from cementing existing injustices.&lt;/p&gt;
&lt;p&gt;License: This video is licensed under the CC BY-NC-SA 4.0 license: &lt;a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;https://creativecommons.org/licenses/by-nc-sa/4.0/&lt;/a&gt;
Please see our speaker release agreement for details: &lt;a class="reference external" href="https://ep2021.europython.eu/events/speaker-release-agreement/"&gt;https://ep2021.europython.eu/events/speaker-release-agreement/&lt;/a&gt;&lt;/p&gt;
</content><category term="EuroPython 2021"></category></entry><entry><title>The Myth of Neutrality: How AI is widening social divides</title><link href="https://pyvideo.org/pycon-de-2022/the-myth-of-neutrality-how-ai-is-widening-social-divides.html" rel="alternate"></link><published>2022-05-11T00:00:00+00:00</published><updated>2022-05-11T00:00:00+00:00</updated><author><name>Stefanie Stoppel</name></author><id>tag:pyvideo.org,2022-05-11:/pycon-de-2022/the-myth-of-neutrality-how-ai-is-widening-social-divides.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker:: Stefanie Stoppel&lt;/p&gt;
&lt;p&gt;Track: General: Ethics
Many people expect artificial intelligence to be neutral - or at least more objective than we humans are. But is it really? In recent years, researchers and activists have shown that it is in fact not, and that our biases end up becoming part …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker:: Stefanie Stoppel&lt;/p&gt;
&lt;p&gt;Track: General: Ethics
Many people expect artificial intelligence to be neutral - or at least more objective than we humans are. But is it really? In recent years, researchers and activists have shown that it is in fact not, and that our biases end up becoming part of AI systems. My talk will shed light on how algorithms become discriminatory, how difficult it is to build &amp;quot;fair and responsible&amp;quot; AI, and what we should do to prevent the systems we build from cementing existing injustices.&lt;/p&gt;
&lt;p&gt;Recorded at the PyConDE &amp;amp; PyData Berlin 2022 conference, April 11-13 2022.
&lt;a class="reference external" href="https://2022.pycon.de"&gt;https://2022.pycon.de&lt;/a&gt;
More details at the conference page: &lt;a class="reference external" href="https://2022.pycon.de/program/ML7XNX"&gt;https://2022.pycon.de/program/ML7XNX&lt;/a&gt;
Twitter: &lt;a class="reference external" href="https://twitter.com/pydataberlin"&gt;https://twitter.com/pydataberlin&lt;/a&gt;
Twitter: &lt;a class="reference external" href="https://twitter.com/pyconde"&gt;https://twitter.com/pyconde&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon DE 2022"></category><category term="PyCon"></category><category term="PyConDE"></category><category term="pyconde2022"></category><category term="pydata"></category><category term="PyDataBerlin"></category><category term="pydataberlin2022"></category></entry></feed>