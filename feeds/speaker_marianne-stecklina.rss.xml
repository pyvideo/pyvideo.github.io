<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Marianne Stecklina</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 17 Apr 2023 00:00:00 +0000</lastBuildDate><item><title>Why you should (not) train your own BERT model for...</title><link>https://pyvideo.org/pydata-berlin-2019/why-you-should-not-train-your-own-bert-model-for.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker: Marianne Stecklina&lt;/p&gt;
&lt;p&gt;Track:PyData
Language models like BERT can capture general language knowledge and transfer it to new data and tasks. However, applying a pre-trained BERT to non-English text has limitations. Is training from scratch a good (and feasible) way to overcome them?&lt;/p&gt;
&lt;p&gt;Recorded at the PyConDE &amp;amp; PyData Berlin 2019 conference.
&lt;a class="reference external" href="https://pycon.de"&gt;https://pycon.de&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More details at the conference page: &lt;a class="reference external" href="https://de.pycon.org/program/YAJRGX"&gt;https://de.pycon.org/program/YAJRGX&lt;/a&gt;
Twitter:  &lt;a class="reference external" href="https://twitter.com/pydataberlin"&gt;https://twitter.com/pydataberlin&lt;/a&gt;
Twitter:  &lt;a class="reference external" href="https://twitter.com/pyconde"&gt;https://twitter.com/pyconde&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marianne Stecklina</dc:creator><pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-09:/pydata-berlin-2019/why-you-should-not-train-your-own-bert-model-for.html</guid><category>PyData Berlin 2019</category></item><item><title>Using transformers â€“ a drama in 512 tokens</title><link>https://pyvideo.org/pydata-berlin-2023/using-transformers-a-drama-in-512-tokens.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;'Got an NLP problem nowadays? Use transformers! Just download a pretrained model from the hub!' - every blog article ever&lt;/p&gt;
&lt;p&gt;As if it's that easy, because nearly all pretrained models have a very annoying limitation: they can only process short input sequences. Not every NLP practitioner happens to work on tweets, but instead many of us have to deal with longer input sequences. What started as a minor design choice for BERT, got cemented by the research community over the years and now turns out to be my biggest headache: the 512 tokens limit.&lt;/p&gt;
&lt;p&gt;In this talk, we'll ask a lot of dumb questions and get an equal number of unsatisfying answers:&lt;/p&gt;
&lt;p&gt;How much text actually fits into 512 tokens? Spoiler: not enough to solve my use case, and I bet a lot of your use cases, too.&lt;/p&gt;
&lt;p&gt;I can feed a sequence of any length into an RNN, why do transformers even have a limit? We'll look into the architecture in more detail to understand that.&lt;/p&gt;
&lt;p&gt;Somebody smart must have thought about this sequence length issue before, or not? Prepare yourself for a rant about benchmarks in NLP research.&lt;/p&gt;
&lt;p&gt;So what can we do to handle longer input sequences? Enjoy my collection of mediocre workarounds.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marianne Stecklina</dc:creator><pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-04-17:/pydata-berlin-2023/using-transformers-a-drama-in-512-tokens.html</guid><category>PyData Berlin 2023</category><category>NLP</category><category>Transformers</category></item></channel></rss>