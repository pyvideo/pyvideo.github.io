<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_maxim-lapan.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-04-08T00:00:00+00:00</updated><entry><title>Deep Reinforcement Learning: theory, intuition, code</title><link href="https://pyvideo.org/pydata-amsterdam-2017/deep-reinforcement-learning-theory-intuition-code.html" rel="alternate"></link><published>2017-04-08T00:00:00+00:00</published><updated>2017-04-08T00:00:00+00:00</updated><author><name>Maxim Lapan</name></author><id>tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/deep-reinforcement-learning-theory-intuition-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;In this talk I'd like to give practical introduction into deep reinforcement learning methods, used to solve complex control problems in robotics, play Atari games, self-driving car control and lots more.&lt;/p&gt;
&lt;p&gt;Deep Reinforcement Learning is a very hot topic, successfully applied in lots of areas which require planning of actions in complex, noisy and partially-observed environments. Concrete examples vary from playing arcade games, navigating websites, helicopter, quadrocopter and car control, protein folding and lots of others.&lt;/p&gt;
&lt;p&gt;Surprisingly, during my own delving into this wide topic, I've discovered that (with rare exceptions) there is a lack of concrete, understandable explanation of most successful and useful algorithms and methods, such as Deep Q-Networks (DQN), Policy Gradients (PG) and Asynchronous Advantage Actor-Critic (A3C). The situation is even worse with simple code examples of the above methods.&lt;/p&gt;
&lt;p&gt;On the one side, there are lots of scientific papers on arxiv.org where researchers tune ideas and methods. On the other side there is a couple full-sized open-source projects implementing those methods plus dozens of &amp;quot;tricks&amp;quot; to improve stability and performance of those methods.&lt;/p&gt;
&lt;p&gt;In this talk, I'll try to fill the gap between them by showing the intuition behind the math and demonstrating how those three approaches (DQN, PG and A3C) can be implemented in less than 200 lines of python code using keras.&lt;/p&gt;
</summary></entry></feed>