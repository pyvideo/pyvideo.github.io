<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_skater.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-02-23T00:00:00+00:00</updated><entry><title>How did you know? Explaining Black Box Model Predictions in Python</title><link href="https://pyvideo.org/pycon-philippines-2019/how-did-you-know-explaining-black-box-model-predictions-in-python.html" rel="alternate"></link><published>2019-02-23T00:00:00+00:00</published><updated>2019-02-23T00:00:00+00:00</updated><author><name>Suzy Lee</name></author><id>tag:pyvideo.org,2019-02-23:pycon-philippines-2019/how-did-you-know-explaining-black-box-model-predictions-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As algorithms get more and more complex (i.e. Ensemble models - XGBoost, Random Forest, Neural Networks), it becomes harder to explain the predictions they make. These 'Black Box' models may produce more accurate results but may in fact hard to operationalize in the real world as it gets harder and harder to explain to business decision makers how a model came up with the prediction. In certain cases such as in credit scoring model interpretability is crucial particularly for regulatory compliance. This talk will highlight certain Python tools and libraries such as LIME, ELI5 and Skater, that would allow data scientists to finally be able to explain how their models came up with its predictions.&lt;/p&gt;
</summary><category term="machine learning"></category><category term="eli5"></category><category term="lime"></category><category term="skater"></category></entry></feed>