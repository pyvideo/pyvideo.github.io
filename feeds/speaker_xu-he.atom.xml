<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Xu He</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_xu-he.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Learning Behaviors with Uncertain Human Feedback</title><link href="https://pyvideo.org/uai-2020/learning-behaviors-with-uncertain-human-feedback.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Xu He</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/learning-behaviors-with-uncertain-human-feedback.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Learning Behaviors with Uncertain Human Feedback&lt;/p&gt;
&lt;p&gt;Xu He (Nanyang Technological University)*; Haipeng Chen (Dartmouth College); Bo An (Nanyang Technological University)&lt;/p&gt;
&lt;p&gt;Human feedback is widely used to train agents in many domains. However, previous works rarely consider the uncertainty when humans provide feedback, especially in cases that the optimal actions â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Learning Behaviors with Uncertain Human Feedback&lt;/p&gt;
&lt;p&gt;Xu He (Nanyang Technological University)*; Haipeng Chen (Dartmouth College); Bo An (Nanyang Technological University)&lt;/p&gt;
&lt;p&gt;Human feedback is widely used to train agents in many domains. However, previous works rarely consider the uncertainty when humans provide feedback, especially in cases that the optimal actions are not obvious to the trainers. For example, the reward of a sub-optimal action can be stochastic and sometimes exceeds that of the optimal action, which is common in games or real-world. Trainers are likely to provide positive feedback to sub-optimal actions, negative feedback to the optimal actions and even do not provide feedback in some confusing situations. Existing works, which utilize the Expectation Maximization (EM) algorithm and treat the feedback model as hidden parameters, do not consider uncertainties in the learning environment and human feedback. To address this challenge, we introduce a novel feedback model that considers the uncertainty of human feedback. However, this incurs intractable calculus in the EM algorithm. To this end, we propose a novel approximate EM algorithm, in which we approximate the expectation step with the Gradient Descent method. Experimental results in both synthetic scenarios and two real-world scenarios with human participants demonstrate the superior performance of our proposed approach.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>