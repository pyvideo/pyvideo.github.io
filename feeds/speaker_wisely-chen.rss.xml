<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Tue, 24 Jun 2014 00:00:00 +0000</lastBuildDate><item><title>PySpark: next generation cluster computing engine</title><link>https://pyvideo.org/pycon-apac-2014/pyspark-next-generation-cluster-computing-engine.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Sparkâ„¢ is a lightning fast engine for large-scale data
processing. It is an in-memory cluster computing framework, originally
developed in UC Berkeley. Base on it's project page's evaluation,
machine learning programming can run program 100x faster than Hadoop
MapReduce. And Spark can run on Hadoop 2's YARN cluster manager, and can
read any existing Hadoop data. Currently, it supports Scala, Java and
Python for writing spark programs.&lt;/p&gt;
&lt;p&gt;In this talk, I will introduce the General concept of Spark's
infrastructure, What is RDD (Resilient Distributed Datasets) in Spark,
Introduction on PySpark, Demo of PySpark's speed and power, Head-to-head
comparison between two programs doing same work - one written in Hadoop
MapReduce and the other written using PySpark.&lt;/p&gt;
&lt;p&gt;I will also conclude about the companies currently using Spark's use
cases.&lt;/p&gt;
&lt;p&gt;About the speaker&lt;/p&gt;
&lt;p&gt;Sr. Software Engineer for the Yahoo! (Taiwan) Data Team. He has been
responsible for data infrastructure, data solution, software release and
continuous integration management. He is a lifelong student of software
development/testing/deployment/CI processes and best practices and an
avid coding puzzle competition fanatic as well as Open Source evangelist&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Wisely Chen</dc:creator><pubDate>Tue, 24 Jun 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-06-24:pycon-apac-2014/pyspark-next-generation-cluster-computing-engine.html</guid></item></channel></rss>