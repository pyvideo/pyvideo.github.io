<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Woosuk Kwon</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 18 Sep 2024 00:00:00 +0000</lastBuildDate><item><title>vLLM: Easy, Fast, and Cheap LLM Serving for Everyone</title><link>https://pyvideo.org/pytorch-conference-2024/vllm-easy-fast-and-cheap-llm-serving-for-everyone.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We will present vLLM, an open-source high-performance LLM inference engine built on top of PyTorch. Starting as a research project at UC Berkeley, vLLM has been one of the fastest and most popular LLM inference solutions in industry, reaching 20K+ stars and 350+ contributors. In this talk, we will cover how vLLM adopts various LLM inference optimizations and how it supports various AI accelerators such as AMD GPUs, Google TPUs, and AWS Inferentia. Also, we will discuss how vLLM benefits from PyTorch 2 and its ecosystem.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Woosuk Kwon</dc:creator><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/vllm-easy-fast-and-cheap-llm-serving-for-everyone.html</guid><category>PyTorch Conference 2024</category></item></channel></rss>