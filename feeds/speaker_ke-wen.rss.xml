<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Ke Wen</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 18 Sep 2024 00:00:00 +0000</lastBuildDate><item><title>Introduction to Torch.Distributed.Pipelining</title><link>https://pyvideo.org/pytorch-conference-2024/introduction-to-torchdistributedpipelining.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pipeline parallelism is a technique employed in distributed deep learning that enhances model execution by dividing the model into distinct segments, or &amp;quot;stages.&amp;quot; As large language models and other memory-intensive models become more common, pipeline parallelism has grown increasingly important for several key areas: - Executing large-scale training jobs. - Enhancing performance in bandwidth-limited clusters. - Supporting large model inference. In this talk, we will introduce the &lt;cite&gt;torch.distributed.pipelining&lt;/cite&gt; package which provides users a seamless way of applying pipeline parallelism. We will demonstrate the following features: - Splitting of model code based on simple specification. - Support for pipeline schedules, including GPipe, 1F1B, Interleaved 1F1B and Looped BFS, and providing the infrastructure for writing customized schedules. - Composability with other PyTorch parallel techniques such as data parallel (DDP, FSDP) or tensor parallel. - Out of the box integration with Hugging Face models for efficient inference.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Howard Huang</dc:creator><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/introduction-to-torchdistributedpipelining.html</guid><category>PyTorch Conference 2024</category><category>Lightning Talk</category></item></channel></rss>