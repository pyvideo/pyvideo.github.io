<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sun, 09 Oct 2016 00:00:00 +0000</lastBuildDate><item><title>PostgreSQL is Web-Scale (Really :) )</title><link>https://pyvideo.org/europython-2013/postgresql-is-web-scale-really.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I show you how to set up a python and PostgreSQL based
system which is easy to set up and easy to scale, provides ACID
guarantees where they are needed and delays time-consistency between
unrelated objects for scalability and availability where the latter are
deemed more important.&lt;/p&gt;
&lt;p&gt;The best thing is that this kind of scalability work for both OLTP and
OLAP workloads, so with some planning you can have just a single large
“database” which can take almost any type of load.&lt;/p&gt;
&lt;p&gt;Also, if you hate SQL, you can do all the OLTP stuff in a pythonic way
using an automagically generated ORM layer inside the database, near the
data. If you are really masochistic, you can use the same ORM also for
map-reduce type distributed data processing, though on this side the
small effort of learning SQL usually pays off when queries get more
complex. But as I said, everything runs inside the databse, near the
data and thus even the ORM &amp;amp; map-reduce analytics works fast.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hannu Krosing</dc:creator><pubDate>Tue, 02 Jul 2013 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2013-07-02:europython-2013/postgresql-is-web-scale-really.html</guid><category>postgresql</category><category>nosql</category><category>datamining</category><category>parallelization</category><category>distributed</category><category>bigdata</category><category>scalability</category><category>pl/python</category><category>olap</category><category>optimization</category><category>orm</category><category>sql</category><category>performance</category></item><item><title>Uno sguardo agli internal di RestFS</title><link>https://pyvideo.org/europython-2013/uno-sguardo-agli-internal-di-restfs.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fabrizio Manfredi</dc:creator><pubDate>Tue, 02 Jul 2013 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2013-07-02:europython-2013/uno-sguardo-agli-internal-di-restfs.html</guid><category>clustering</category><category>HTTP</category><category>parallelization</category><category>distributed</category><category>twisted</category><category>REST</category><category>optimization</category><category>Algorithms</category><category>scalability</category><category>async</category><category>hpc</category><category>performance</category></item><item><title>Dask for ad hoc distributed computing</title><link>https://pyvideo.org/pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk discusses parallel and distributed computing in Python, particularly for ad-hoc and custom algorithms. It focuses on Dask, a Python solution for flexible distributed computing.&lt;/p&gt;
&lt;p&gt;The Python data science stack contains efficient algorithms with intuitive interfaces for sophisticated and friendly analysis. As the data science community tackles larger problems with larger hardware we naturally ask how best to parallelize this software stack both across many cores in a single computer and across computers in a cluster. This turns out to be harder than it looks, even with traditional Big Data tools like MapReduce, Storm, and Spark. Both the complexity of the algorithms and the high expectations for interactivity raise challenges for these systems. This talk lays out the benefits and challenges of parallelizing a numeric analytic stack, and then describes Dask, a parallel framework gaining traction within the Python community for interactive performant parallel computing, and finally goes through a few domains where this work is enabling novel science today.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matthew Rocklin</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html</guid><category>dask</category><category>distributed</category></item><item><title>Implementing distributed grid search for deep learning using scikit learn and joblib</title><link>https://pyvideo.org/pydata-chicago-2016/implementing-distributed-grid-search-for-deep-learning-using-scikit-learn-and-joblib.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://mheilman.github.io/pydata_chicago_2016/#/"&gt;https://mheilman.github.io/pydata_chicago_2016/#/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Grid search over hyperparameters is an important but computationally expensive process in machine learning, particularly for deep learning and tree ensembles. In this talk, I will describe how one can use joblib's recently added custom backend functionality to do distributed grid search on Amazon EC2 for a TensorFlow deep text classifier that follows the scikit-learn estimator API.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mike Heilman</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/implementing-distributed-grid-search-for-deep-learning-using-scikit-learn-and-joblib.html</guid><category>deep learning</category><category>distributed</category><category>learning</category><category>scikit</category><category>search</category></item><item><title>Playing tasks with Django-Celery</title><link>https://pyvideo.org/europython-2011/playing-tasks-with-django-celery.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Mauro Rocco - 22 June 2011 in &amp;quot;Track Tagliatelle &amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Celery is an open source task queueing system based on distributed
message passing.&lt;/p&gt;
&lt;p&gt;I will talk about the tools that Celery offers for task distribution and
how to monitor and manage the system using a Django web interface. This
talk will also focus on how we use Celery at Jamendo and our real
solutions to some common issues you may encounter when developing a
back-office based on Celery.&lt;/p&gt;
&lt;p&gt;The talk will cover the following topics:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A brief overview of Celery and the AMPQ protocol AMPQ protocol
overview, Celery introduction: Celery, RabbitMQ code examples&lt;/li&gt;
&lt;li&gt;The impact of Celery on the Jamendo work-flow; examples with real
tasks. Here I will talk about the Jamendo back-office infrastructure
and some of our common tasks. I will discuss the improvements made by
introducing a new back-office system based on Celery. I will show
some code snippets and go over some real scenarios.&lt;/li&gt;
&lt;li&gt;Overview of the Django Celery admin interface and some Jamendo
extensions. Let's talk about the Django-Celery interface that allows
one to monitor or schedule tasks directly from the Django admin. I
will explain which common additional features are necessary and how
to add them.&lt;/li&gt;
&lt;li&gt;Common &amp;quot;gotchas&amp;quot; we encountered while working with Celery and how we
solved them.&lt;/li&gt;
&lt;li&gt;Global task locks&lt;/li&gt;
&lt;li&gt;Centralized logging: be able to read all the logs of all celery
workers on different servers and filter them for real-time debugging&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mauro Rocco</dc:creator><pubDate>Thu, 21 Jul 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-07-21:europython-2011/playing-tasks-with-django-celery.html</guid><category>celery</category><category>distributed</category><category>django</category><category>infrastructure</category><category>queueing</category><category>rabbitmq</category><category>real-time</category><category>web</category></item><item><title>Developing cutting-edge applications with PyQt</title><link>https://pyvideo.org/europython-2011/developing-cutting-edge-applications-with-pyqt.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Lorenzo Mancini,Matteo Bertozzi - 23 June 2011 in
&amp;quot;Training Pizza Margherita &amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;(Presented with Matteo Bertozzi)&lt;/p&gt;
&lt;p&gt;Python's high development speed and Qt's gargantuan feature set allow
for comfortable development of complex desktop applications. Still, what
does one need to do to best leverage this awesome combination? And more
importantly, what crucial advantages exist to decide its adoption in
favour of more traditional tecniques?&lt;/p&gt;
&lt;p&gt;During this training, we'll start from a white canvas and show how to
develop a desktop application using the best tools Qt offers. You'll
learn how to compose complex GUIs from basic building blocks, and how to
use QPainter, one of the most advanced 2D painting systems in the open
source world. Qt's Webkit integration will be discussed, along with the
recent Qt Quick technology, which allows one to create smooth and fluid
user interfaces. Qt's solution to common deployment problems, like
handling a serious translation workflow, will be presented. At the end,
we'll package our PyQt application so it's ready to be distributed for
download.&lt;/p&gt;
&lt;p&gt;Walk away with a sound understanding of why you should choose PyQt for
your next desktop project.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lorenzo Mancini</dc:creator><pubDate>Wed, 20 Jul 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-07-20:europython-2011/developing-cutting-edge-applications-with-pyqt.html</guid><category>deployment</category><category>distributed</category><category>pyqt</category><category>qt</category></item><item><title>Getting ready for PostgreSQL 9.1</title><link>https://pyvideo.org/europython-2011/getting-ready-for-postgresql-91.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Gabriele Bartolini,Harald Armin Massa,Marco Nenciarini
- 22 June 2011 in &amp;quot;Training Pizza Napoli &amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PostgreSQL is an advanced, versatile open-source database management
system that integrates perfectly with Python. It is developed by a very
active international community and is distributed under the BSD-like
PostgreSQL License.&lt;/p&gt;
&lt;p&gt;Enterprise-class features (including SQL standard compliance, ACID
transactions, disaster recovery, high availability, replication,
partitioning and general extensibility) make PostgreSQL suitable for
business critical environments seeking to reduce the TCO of their
database solutions without altering their functional needs. PostgreSQL
9.0, released in September 2010, was the first version of PostgreSQL
with Hot Standby, a built-in master/slave replication mechanism.
Asynchronous replication through the standard and consolidated log
shipping technique (previously used with Warm Standby for high
availability) has been enhanced with streaming replication.&lt;/p&gt;
&lt;p&gt;Version 9.1, expected to be out later in 2011, will add synchronous
replication to PostgreSQL, making it the first DBMS that allows
developers and users to control the replication strategy at
transactional granularity. Come to the talk and discover all the major
new features of PostgreSQL 9.1, including extensions management,
writable common table expressions (WCTE), etc.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gabriele Bartolini</dc:creator><pubDate>Wed, 13 Jul 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-07-13:europython-2011/getting-ready-for-postgresql-91.html</guid><category>asynchronous</category><category>business</category><category>community</category><category>database</category><category>distributed</category><category>extensions</category><category>partitioning</category><category>postgresql</category><category>replication</category></item><item><title>Implementing distributed applications using ZeroMQ, Python and other bad guys...</title><link>https://pyvideo.org/europython-2011/implementing-distributed-applications-using-zerom.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Francesco Crippa - 24 June 2011 in &amp;quot;Track Spaghetti&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Cloud Computing and Large Scale environments require sometime
applications based on complex and distributed architectures… and this
usually means a huge overhead in the design and confusion out of control
in the code (network wise race conditions, single points of failure and
so on)&lt;/p&gt;
&lt;p&gt;Introducing elements like *MQ and IPC frameworks in this kind of
applications is the only way to reduce the complexity and enable a fluid
design (in other words: mess-under-control)&lt;/p&gt;
&lt;p&gt;The talk is focused on describing how to design a distributed
application in different scenarios, using ZeroMQ (a modern broker-less
MQ system) as core framework, with examples and demos.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesco Crippa</dc:creator><pubDate>Wed, 13 Jul 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-07-13:europython-2011/implementing-distributed-applications-using-zerom.html</guid><category>design</category><category>distributed</category><category>zeromq</category></item><item><title>Python MapReduce Programming with Pydoop</title><link>https://pyvideo.org/europython-2011/python-mapreduce-programming-with-pydoop.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;[EuroPython 2011] Simone Leo - 24 June 2011 in &amp;quot;Track Lasagne&amp;quot;&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Hadoop is the leading open source implementation of MapReduce, Google's
large scale distributed computing paradigm. Hadoop's native API is in
Java, and its built-in options for Python programming - Streaming and
Jython - have several drawbacks: the former allows to access only a
small subset of Hadoop's features, while the latter carries with it all
of the limitations of Jython with respect to CPython.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://pydoop.sourceforge.net"&gt;Pydoop&lt;/a&gt; is an API for Hadoop that
makes most of its features available to Python programmers while
allowing CPython development. Its core consists of Boost.Python wrappers
for Hadoop's C/C++ interface.&lt;/p&gt;
&lt;p&gt;The talk consists of a MapReduce/Hadoop tutorial and a presentation of
the Pydoop API, with the main goal of bridging the gap between the
Hadoop and Python communities. A basic knowledge of distributed
programming is helpful but not strictly required.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Simone Leo</dc:creator><pubDate>Wed, 13 Jul 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-07-13:europython-2011/python-mapreduce-programming-with-pydoop.html</guid><category>api</category><category>cpython</category><category>distributed</category><category>hadoop</category><category>jython</category><category>mapreduce</category><category>tutorial</category></item><item><title>mrjob: Distributed Computing for Everyone</title><link>https://pyvideo.org/pycon-us-2011/pycon-2011--mrjob--distributed-computing-for-ever.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;mrjob: Distributed Computing for Everyone&lt;/p&gt;
&lt;p&gt;Presented by Jimmy Retzlaff&lt;/p&gt;
&lt;p&gt;Have tons of data that needs analysis? Now it's as easy as 1-2-3! 1)
Sign up for an Amazon Web Services account. 2) Install Yelp's mrjob. 3)
Write as few as a dozen lines of Python code. This talk will show you
how to use mrjob and Amazon's Elastic MapReduce to easily process lots
of data in parallel on a potentially large cluster of computers that you
can rent for a dime per computer per hour.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In their 2004 paper, Google outlined MapReduce - one of the programming
models they use to process large data sets. MapReduce is a relatively
simple model to develop for that allows the underlying framework to
automatically parallelize the job, add fault tolerance, and scale the
job to many commodity computers.&lt;/p&gt;
&lt;p&gt;In 2009, Amazon Web Services introduced their Elastic MapReduce (EMR)
product. It layers the Hadoop open source package on top of their
Elastic Compute Cloud (EC2) to allow anyone to rent a cluster of
computers by the hour, starting at about a dime per computer per hour,
in order to run MapReduce jobs.&lt;/p&gt;
&lt;p&gt;Some of the significant issues with Amazon's solution involve starting
up machine instances, replicating your code and its dependancies to EMR,
running and monitoring the job, and gathering the results.&lt;/p&gt;
&lt;p&gt;So Yelp developed mrjob, which takes care of these details and lets the
developer focus on working with their data. Yelp uses mrjob to power
many internal jobs that work with its very large log files, for example:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;People Who Viewed This Also Viewed...&lt;/li&gt;
&lt;li&gt;A user clicked an ad over and over, but we only want to charge the
advertiser once&lt;/li&gt;
&lt;li&gt;We're thinking of a change, but want to simulate how that will affect
ad revenue&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now you can use that same power with just a few lines of Python.&lt;/p&gt;
&lt;p&gt;Useful links:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Install mrjob: sudo easy_install mrjob&lt;/li&gt;
&lt;li&gt;Documentation: &lt;a class="reference external" href="http://packages.python.org/mrjob/"&gt;http://packages.python.org/mrjob/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyPI: &lt;a class="reference external" href="http://pypi.python.org/pypi/mrjob"&gt;http://pypi.python.org/pypi/mrjob&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Development is hosted at github: &lt;a class="reference external" href="http://github.com/Yelp/mrjob"&gt;http://github.com/Yelp/mrjob&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jimmy Retzlaff</dc:creator><pubDate>Fri, 11 Mar 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-03-11:pycon-us-2011/pycon-2011--mrjob--distributed-computing-for-ever.html</guid><category>distributed</category><category>distributedcomputing</category><category>mrjob</category><category>pycon</category><category>pycon2011</category></item></channel></rss>