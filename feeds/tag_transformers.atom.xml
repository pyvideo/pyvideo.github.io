<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Transformers</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_transformers.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-04-17T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Using transformers – a drama in 512 tokens</title><link href="https://pyvideo.org/pydata-berlin-2023/using-transformers-a-drama-in-512-tokens.html" rel="alternate"></link><published>2023-04-17T00:00:00+00:00</published><updated>2023-04-17T00:00:00+00:00</updated><author><name>Marianne Stecklina</name></author><id>tag:pyvideo.org,2023-04-17:/pydata-berlin-2023/using-transformers-a-drama-in-512-tokens.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;'Got an NLP problem nowadays? Use transformers! Just download a pretrained model from the hub!' - every blog article ever&lt;/p&gt;
&lt;p&gt;As if it's that easy, because nearly all pretrained models have a very annoying limitation: they can only process short input sequences. Not every NLP practitioner happens to work on …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;'Got an NLP problem nowadays? Use transformers! Just download a pretrained model from the hub!' - every blog article ever&lt;/p&gt;
&lt;p&gt;As if it's that easy, because nearly all pretrained models have a very annoying limitation: they can only process short input sequences. Not every NLP practitioner happens to work on tweets, but instead many of us have to deal with longer input sequences. What started as a minor design choice for BERT, got cemented by the research community over the years and now turns out to be my biggest headache: the 512 tokens limit.&lt;/p&gt;
&lt;p&gt;In this talk, we'll ask a lot of dumb questions and get an equal number of unsatisfying answers:&lt;/p&gt;
&lt;p&gt;How much text actually fits into 512 tokens? Spoiler: not enough to solve my use case, and I bet a lot of your use cases, too.&lt;/p&gt;
&lt;p&gt;I can feed a sequence of any length into an RNN, why do transformers even have a limit? We'll look into the architecture in more detail to understand that.&lt;/p&gt;
&lt;p&gt;Somebody smart must have thought about this sequence length issue before, or not? Prepare yourself for a rant about benchmarks in NLP research.&lt;/p&gt;
&lt;p&gt;So what can we do to handle longer input sequences? Enjoy my collection of mediocre workarounds.&lt;/p&gt;
</content><category term="PyData Berlin 2023"></category><category term="NLP"></category><category term="Transformers"></category></entry><entry><title>Transformers</title><link href="https://pyvideo.org/riiaa-2021/transformers.html" rel="alternate"></link><published>2021-08-25T00:00:00+00:00</published><updated>2021-08-25T00:00:00+00:00</updated><author><name>Omar Sanseviero</name></author><id>tag:pyvideo.org,2021-08-25:/riiaa-2021/transformers.html</id><content type="html"></content><category term="RIIAA 2021"></category><category term="Artificial Intelligence"></category><category term="Natural Language Processing"></category><category term="Transformers"></category></entry></feed>