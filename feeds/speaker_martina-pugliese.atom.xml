<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_martina-pugliese.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-06-01T00:00:00+00:00</updated><entry><title>Spotting trends and tailoring recommendations: PySpark on Big Data in fashion</title><link href="https://pyvideo.org/pydata-berlin-2016/spotting-trends-and-tailoring-recommendations-pyspark-on-big-data-in-fashion.html" rel="alternate"></link><published>2016-06-01T00:00:00+00:00</published><updated>2016-06-01T00:00:00+00:00</updated><author><name>Martina Pugliese</name></author><id>tag:pyvideo.org,2016-06-01:pydata-berlin-2016/spotting-trends-and-tailoring-recommendations-pyspark-on-big-data-in-fashion.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Predicting what people like when they choose what to wear is a non-trivial task involving several ingredients. At Mallzee, the data is variegated, large and has to be processed quickly to produce recommendations on products, tailored to each user. We use PySpark to crunch large sets of different data and create models in order to generate robust and meaningful suggestions.&lt;/p&gt;
&lt;p&gt;Mallzee is a fashion app where people can see products (clothes, shoes and accessories) and decide whether they like them or not. They can also buy products and create feeds of preferred brands and categories of items. We have large amounts of data generated by the users when they scroll and search through products and we use it to understand the user. We want to give everyone meaningful recommendations on the items they might like, hence tailoring the experience to who they are. We build pseudo-intelligent algorithms capable of extracting the style profile of a user and we crunch products data to match items to the user based on a classification model. The model is validated and statistical analyses are performed to determine the tipping point when recommendations are valuable to the user. The talk will go through the steps we implement by showing how the full data stack of Python is used in achieving this goal and how it is interfaced with a Spark cluster through PySpark in order to run Machine Learning algorithms on big data.&lt;/p&gt;
</summary><category term="pyspark"></category></entry></feed>