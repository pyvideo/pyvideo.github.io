<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 18 Aug 2018 00:00:00 +0000</lastBuildDate><item><title>What, Where, When, How of Stream Processing</title><link>https://pyvideo.org/florida-pycon-2017/what-where-when-how-of-stream-processing.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Currently, some popular data processing frameworks such as Apache Spark
consider batch and stream processing jobs independently. The APIs across
different processing systems such as Apache Spark or Apache Flink are
also different. This forces the end user to learn a potentially new
system every time. Apache Beam [1] addresses this problem by providing a
unified programming model that can be used for both batch and streaming
pipelines. The Beam SDK allows the user to execute these pipelines
against different execution engines. Currently, Apache Beam provides a
Java and Python SDK.&lt;/p&gt;
&lt;p&gt;In the talk, we start off by providing an overview of Apache Beam using
the Python SDK and the problems it tries to address from an end user’s
perspective. We cover the core programming constructs in the Beam model
such as PCollections, ParDo, GroupByKey, windowing, and triggers. We
describe how these constructs make it possible for pipelines to be
executed in a unified fashion in both batch and streaming. Then we use
examples to demonstrate these capabilities. The examples showcase using
Beam for stream processing and real time data analysis, and how Beam can
be used for feature engineering in some Machine Learning applications
using Tensorflow. Finally, we end with Beam’s vision of creating runner
and execution independent graphs using the Beam FnApi [2].&lt;/p&gt;
&lt;p&gt;Apache Beam [1] is a top level Apache project and is completely open
source. The code for Beam can be found on Github [3].&lt;/p&gt;
&lt;p&gt;[1] &lt;a class="reference external" href="https://beam.apache.org/"&gt;https://beam.apache.org/&lt;/a&gt; [2] &lt;a class="reference external" href="http://s.apache.org/beam-fn-api"&gt;http://s.apache.org/beam-fn-api&lt;/a&gt; [3]
&lt;a class="reference external" href="https://github.com/apache/beam"&gt;https://github.com/apache/beam&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sourabh Bajaj</dc:creator><pubDate>Sat, 07 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-07:florida-pycon-2017/what-where-when-how-of-stream-processing.html</guid></item><item><title>Recent Advances in Deep Learning and Tensorflow</title><link>https://pyvideo.org/pybay-2018/recent-advances-in-deep-learning-and-tensorflow.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sourabh Bajaj</dc:creator><pubDate>Sat, 18 Aug 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-08-18:pybay-2018/recent-advances-in-deep-learning-and-tensorflow.html</guid></item><item><title>Big data processing with Apache Beam</title><link>https://pyvideo.org/pybay-2017/big-data-processing-with-apache-beam.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, we present the new Python SDK for Apache Beam - a parallel programming model that allows one to implement batch and streaming data processing jobs that can run on a variety of execution engines like Apache Spark and Google Cloud Dataflow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Currently, some popular data processing frameworks such as Apache Spark consider batch and stream processing jobs independently. The APIs across different processing systems such as Apache Spark or Apache Flink are also different. This forces the end user to learn a potentially new system every time. Apache Beam [1] addresses this problem by providing a unified programming model that can be used for both batch and streaming pipelines. The Beam SDK allows the user to execute these pipelines against different execution engines. Currently, Apache Beam provides a Java and Python SDK.&lt;/p&gt;
&lt;p&gt;In the talk, we start off by providing an overview of Apache Beam using the Python SDK and the problems it tries to address from an end user’s perspective. We cover the core programming constructs in the Beam model such as PCollections, ParDo, GroupByKey, windowing, and triggers. We describe how these constructs make it possible for pipelines to be executed in a unified fashion in both batch and streaming. Then we use examples to demonstrate these capabilities. The examples showcase using Beam for stream processing and real-time data analysis, and how Beam can be used for feature engineering in some Machine Learning applications using Tensorflow. Finally, we end with Beam's vision of creating runner and execution independent graphs using the Beam FnApi [2].&lt;/p&gt;
&lt;p&gt;Apache Beam [1] is a top-level Apache project and is completely open source. The code for Beam can be found on Github [3].&lt;/p&gt;
&lt;p&gt;[1] &lt;a class="reference external" href="https://beam.apache.org/"&gt;https://beam.apache.org/&lt;/a&gt; [2] &lt;a class="reference external" href="http://s.apache.org/beam-fn-api"&gt;http://s.apache.org/beam-fn-api&lt;/a&gt; [3] &lt;a class="reference external" href="https://github.com/apache/beam"&gt;https://github.com/apache/beam&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bio&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sourabh is a software engineer at Google interested in Data Infrastructure and Machine Learning. He currently works on Apache Beam. Prior to Google he was part of the Data Science team at Coursera working on everything from Recommendation System to Data warehousing.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sourabh Bajaj</dc:creator><pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-08-13:pybay-2017/big-data-processing-with-apache-beam.html</guid></item><item><title>Data processing with Apache Beam</title><link>https://pyvideo.org/pydata-seattle-2017/data-processing-with-apache-beam.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, we present the new Python SDK for Apache Beam - a parallel programming model that allows one to implement batch and streaming data processing jobs that can run on a variety of execution engines like Apache Spark and Google Cloud Dataflow. We will use examples to discuss some of the interesting challenges in providing a Pythonic API and execution environment for distributed processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Currently some popular data processing frameworks such as Apache Spark consider batch and stream processing jobs independently. The APIs across different processing systems such as Apache Spark or Apache Flink are also different. This forces the end user to learn a potentially new system every time. Apache Beam [1] addresses this problem by providing a unified programming model that can be used for both batch and streaming pipelines. The Beam SDK allows the user to execute these pipelines against different execution engines. Currently Apache Beam provides a Java and Python SDK.&lt;/p&gt;
&lt;p&gt;In the talk, we start off by providing an overview of Apache Beam using the Python SDK and the problems it tries to address from an end user’s perspective. We cover the core programming constructs in the Beam model such as PCollections, ParDo, GroupByKey, windowing and triggers. We describe how these constructs make it possible for pipelines to be executed in a unified fashion in both batch and streaming. Then we use examples to demonstrate these capabilities. The examples showcase using Beam for stream processing and real time data analysis, and how Beam can be used for feature engineering in some Machine Learning applications using Tensorflow. Finally, we end with Beam's vision of creating runner and execution independent graphs using the Beam FnApi [2].&lt;/p&gt;
&lt;p&gt;Apache Beam [1] is a top level Apache project and is completely open source. The code for Beam can be found on Github [3].&lt;/p&gt;
&lt;p&gt;[1] &lt;a class="reference external" href="https://beam.apache.org/"&gt;https://beam.apache.org/&lt;/a&gt; [2] &lt;a class="reference external" href="http://s.apache.org/beam-fn-api"&gt;http://s.apache.org/beam-fn-api&lt;/a&gt; [3] &lt;a class="reference external" href="https://github.com/apache/beam"&gt;https://github.com/apache/beam&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sourabh Bajaj</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/data-processing-with-apache-beam.html</guid></item></channel></rss>