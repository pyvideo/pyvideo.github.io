<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_felix-cheung.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-07-06T00:00:00+00:00</updated><entry><title>Scalable Data Science in Python and R on Apache Spark</title><link href="https://pyvideo.org/pydata-seattle-2017/scalable-data-science-in-python-and-r-on-apache-spark.html" rel="alternate"></link><published>2017-07-06T00:00:00+00:00</published><updated>2017-07-06T00:00:00+00:00</updated><author><name>Felix Cheung</name></author><id>tag:pyvideo.org,2017-07-06:pydata-seattle-2017/scalable-data-science-in-python-and-r-on-apache-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In the world of Data Science, Python and R are very popular. Apache Spark is a highly scalable data platform. How could a Data Scientist integrate Spark into their existing Data Science toolset? How does Python work with Spark? How could one leverage the rich 10000+ packages on CRAN for R?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the world of Data Science, Python and R are very popular. Apache Spark is a highly scalable data platform. How could a Data Scientist integrate Spark into their existing Data Science toolset? How does Python work with Spark? How could one leverage the rich 10000+ packages on CRAN for R?&lt;/p&gt;
&lt;p&gt;We will start with PySpark, beginning with a quick walkthrough of data preparation practices and an introduction to Spark MLLib Pipeline Model. We will also discuss how to integrate native Python packages with Spark.&lt;/p&gt;
&lt;p&gt;Compare to PySpark, SparkR is a new language binding for Apache Spark and it is designed to be familiar to native R users. In this talk we will walkthrough many examples how several new features in Apache Spark 2.x will enable scalable machine learning on Big Data. In addition to talking about the R interface to the ML Pipeline model, we will explore how SparkR support running user code on large scale data in a distributed manner, and give examples on how that could be used to work with your favorite R packages.&lt;/p&gt;
&lt;p&gt;Presentation: &lt;a class="reference external" href="https://www.slideshare.net/felixcss/scalable-data-science-in-python-and-r-on-apache-spark"&gt;https://www.slideshare.net/felixcss/scalable-data-science-in-python-and-r-on-apache-spark&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>