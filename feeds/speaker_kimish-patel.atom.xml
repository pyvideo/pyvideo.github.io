<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Kimish Patel</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_kimish-patel.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-09-18T00:00:00+00:00</updated><subtitle></subtitle><entry><title>PyTorch Edge: Vendor Integration Journey for Compilers and Backends</title><link href="https://pyvideo.org/pytorch-conference-2023/pytorch-edge-vendor-integration-journey-for-compilers-and-backends.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Kimish Patel</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/pytorch-edge-vendor-integration-journey-for-compilers-and-backends.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Join this technical session tailored for partners and hardware vendors aiming to provide high-performance solutions to their customers, including on-device PyTorch users. We will showcase how to integrate backend and compiler toolchain natively with PyTorch Edge IR without compromising on performance. Explore our well-defined entry points and API for …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Join this technical session tailored for partners and hardware vendors aiming to provide high-performance solutions to their customers, including on-device PyTorch users. We will showcase how to integrate backend and compiler toolchain natively with PyTorch Edge IR without compromising on performance. Explore our well-defined entry points and API for integrating compiler passes, delegates, and custom kernel implementations, all without any intermediate conversions.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>LLMs on Edge with AI Accelerators</title><link href="https://pyvideo.org/pytorch-conference-2024/llms-on-edge-with-ai-accelerators.html" rel="alternate"></link><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><author><name>Chen Lai</name></author><id>tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/llms-on-edge-with-ai-accelerators.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;LLMs are known to be compute heavy and consume lots of resources (almost all resources on phones), including memory and power. A natural thought is to leverage the AI hardware accelerators, for example, Apple Neural Engine (ANE) on Apple devices and HTP on Qualcomm SoCs, to make it run …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;LLMs are known to be compute heavy and consume lots of resources (almost all resources on phones), including memory and power. A natural thought is to leverage the AI hardware accelerators, for example, Apple Neural Engine (ANE) on Apple devices and HTP on Qualcomm SoCs, to make it run fast and efficiently. Only by optimizing the model latency, memory consumption and power usage to a certain level will users be interested in installing the models on their devices. In this session, we’d like to introduce how we leverage these AI accelerators within the PyTorch ecosystem to achieve the state-of-art performance for llama3 on device, via ExecuTorch and the partnership with Apple and Qualcomm. Hardware companies usually have their own AI accelerators. Likely they have different characteristics, one may support a list of different operators than others, and one may only support static shapes (like HTP). However, transformers-based optimization can be generic. We’ll discuss in more detail how we apply the generic optimization as well as the backend specific optimization. The techniques we applied here are not just for LLMs, but can be applied to other transformer-based models.&lt;/p&gt;
</content><category term="PyTorch Conference 2024"></category><category term="Lightning Talk"></category></entry></feed>