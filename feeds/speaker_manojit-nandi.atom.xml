<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_manojit-nandi.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-10-04T00:00:00+00:00</updated><entry><title>The Benefits and Dangers of Face Recognition Technology</title><link href="https://pyvideo.org/pygotham-2019/the-benefits-and-dangers-of-face-recognition-technology.html" rel="alternate"></link><published>2019-10-04T00:00:00+00:00</published><updated>2019-10-04T00:00:00+00:00</updated><author><name>Manojit Nandi</name></author><id>tag:pyvideo.org,2019-10-04:pygotham-2019/the-benefits-and-dangers-of-face-recognition-technology.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Biometric scanners, such as face recognition technology, have seen
widespread adoption in applications, such as identifying suspected
criminals, analyzing candidate's facial expressions during job interviews,
and [monitoring attendance at church](&lt;a class="reference external" href="https://churchix.com/"&gt;https://churchix.com/&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;As these technologies have become more pervasive, many organizations have
raised potential concerns about the way these technologies schematize faces.
Studies have shown commercial face recognition software has noticeably lower
accuracy on darker-skinned individuals, and automatic gender recognition
systems regularly misgender trans and non-binary individuals.&lt;/p&gt;
&lt;p&gt;In this talk, I will cover the known limitations of face recognition
technology and the privacy concerns raised about these systems. At the end,
I will cover how companies and government legislation aims to address these
concerns.&lt;/p&gt;
</summary></entry><entry><title>Measures and Mismeasures of algorithmic fairness</title><link href="https://pyvideo.org/pycon-us-2019/measures-and-mismeasures-of-algorithmic-fairness.html" rel="alternate"></link><published>2019-05-04T13:40:00+00:00</published><updated>2019-05-04T13:40:00+00:00</updated><author><name>Manojit Nandi</name></author><id>tag:pyvideo.org,2019-05-04:pycon-us-2019/measures-and-mismeasures-of-algorithmic-fairness.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Within the last few years, researchers have come to understand that
machine learning systems may display discriminatory behavior with
regards to certain protected characteristics, such as gender or race. To
combat these harmful behaviors, we have created multiple definitions of
fairness to enable equity in machine learning algorithms. In this talk,
I will cover these different definitions of algorithmic fairness and
discuss both the strengths and limitations of these formalizations. In
addition, I will cover other best practices to better mitigate the
unintended bias of data products.&lt;/p&gt;
</summary><category term="talk"></category></entry><entry><title>Interpretable Machine Learning: Methods for understanding complex models</title><link href="https://pyvideo.org/pygotham-2018/interpretable-machine-learning-methods-for-understanding-complex-models.html" rel="alternate"></link><published>2018-10-05T00:00:00+00:00</published><updated>2018-10-05T00:00:00+00:00</updated><author><name>Manojit Nandi</name></author><id>tag:pyvideo.org,2018-10-05:pygotham-2018/interpretable-machine-learning-methods-for-understanding-complex-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Today, businesses use algorithmic decision-making in various applications, such as determining who gets a bank loan, evaluating a teacher's performance, and other areas that greatly affect people's livelihood. In these applications, understanding why a statistical model makes a particular prediction can be as important as its accuracy. However often times, these models are complex black-boxes that are difficult or impossible to understand by humans.  For persons whose lives are impacted by these algorithms, this lack of interpretability creates serious problems as these individuals are unable to improve their outcome. In this talk, I will discuss various definitions of global and local interpretability for machine learning models.  Next, I will discuss methodologies for better understanding how a model created a prediction for a particular test instance.&lt;/p&gt;
</summary></entry><entry><title>Algorithmic fairness and algorithmic discrimination</title><link href="https://pyvideo.org/pygotham-2017/algorithmic-fairness-and-algorithmic-discrimination.html" rel="alternate"></link><published>2017-10-06T00:00:00+00:00</published><updated>2017-10-06T00:00:00+00:00</updated><author><name>Manojit Nandi</name></author><id>tag:pyvideo.org,2017-10-06:pygotham-2017/algorithmic-fairness-and-algorithmic-discrimination.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As data-driven models are more commonly used in decision-making and public policy, we as data practitioners must be aware of the systematic biases present in our data, so we do not discriminate or reinforce vicious cycles against vulnerable groups.  I will explain the concept of algorithmic fairness and how it relates to the traditional view of machine learning classifiers. I will discuss ways to measure the extent to which a classifier discriminates against a particular minority group and showcase special algorithms for mitigating the level of disparate impact of a classifier. At the end of the talk, I will point interested audience members to resources where they can learn more about emerging trends in this topic.&lt;/p&gt;
</summary></entry><entry><title>Anomaly Detection Algorithms and Techniques for Real-World Detection Systems</title><link href="https://pyvideo.org/pygotham-2016/anomaly-detection-algorithms-and-techniques-for-real-world-detection-systems.html" rel="alternate"></link><published>2016-07-17T00:00:00+00:00</published><updated>2016-07-17T00:00:00+00:00</updated><author><name>Manojit Nandi</name></author><id>tag:pyvideo.org,2016-07-17:pygotham-2016/anomaly-detection-algorithms-and-techniques-for-real-world-detection-systems.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Finding outliers in a dataset is a challenging problem in which traditional analytical methods often perform poorly. As a result, researchers have developed special algorithms for detecting anomalies. In this talk, I will take about three different families of anomaly detection algorithms: Density-based methods, data streaming methods, and time series methods. I will cover both the mathematical and statistical theory behind these algorithms and provide code implementations. Afterwards, I will discuss useful tips I have learned  while implementing threat detection systems in practice.&lt;/p&gt;
</summary></entry></feed>