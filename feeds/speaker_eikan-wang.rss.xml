<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Eikan Wang</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 18 Sep 2024 00:00:00 +0000</lastBuildDate><item><title>Lightning Talk: Adding Backends for TorchInductor: Case Study with Intel GPU</title><link>https://pyvideo.org/pytorch-conference-2023/lightning-talk-adding-backends-for-torchinductor-case-study-with-intel-gpu.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;There are two integration levels to add a new backend for the PyTorch compiler - AtenIR/PrimsIR level and Inductor loop IR level. The ATen/Prim level IR integration has been there via the custom backend registration infrastructure (&lt;a class="reference external" href="https://pytorch.org/docs/stable/dynamo/custom-backends.html"&gt;https://pytorch.org/docs/stable/dynamo/custom-backends.html&lt;/a&gt;). Yet, the latter offers an option to integrate backend compiler at the lower loop-level IR, which can benefit from the existing compiler infrastructure of the Inductor, such as the loop fusion and memory planning. We developed a dynamic registration mechanism on the Inductor side for a new backend. The mechanism allows a backend to register its codegen for a particular device at runtime. And the new backend just needs to focus on generating optimal code for the device. - Case Study – Intel GPU Backend for Inductor Take Intel GPU Backend for Inductor as an example to study how to support Intel GPU via the proposed registration mechanism to prove the idea. Intel GPU Backend for Inductor is on top of Triton, as we have enabled Triton to support any new HW backend. In this context, the case study will show the power of “Inductor + Triton” to easily support any new accelerator.&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eikan Wang</dc:creator><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-adding-backends-for-torchinductor-case-study-with-intel-gpu.html</guid><category>PyTorch Conference 2023</category><category>Lightning Talk</category></item><item><title>Intel GPU in Upstream PyTorch: Expanding GPU Choices and Enhancing Backend Flexibility</title><link>https://pyvideo.org/pytorch-conference-2024/intel-gpu-in-upstream-pytorch-expanding-gpu-choices-and-enhancing-backend-flexibility.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Intel GPU in Upstream PyTorch: Expanding GPU Choices and Enhancing Backend Flexibility - Eikan Wang &amp;amp; Min Jean Cho, Intel&lt;/p&gt;
&lt;p&gt;The integration of Intel GPU support into PyTorch marks a pivotal enhancement for PyTorch device and runtime. We generalized the PyTorch device and runtime to accommodate streaming devices. The generalization not only facilitates the deployment of PyTorch on ubiquitous hardware but also makes the integration of different HW backends easier. In addition, PyTorch with Intel GPU supports various Intel GPUs from the data center to the client. It enriches and democratizes PyTorch HW ecosystem. Particularly in AIPC scenarios where Intel's integrated and discrete GPUs are prevalent, Pytorch with Intel GPU can deliver promising performance and improved OOB experience in the AIPC domain that can extend PyTorch's applicability significantly.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eikan Wang</dc:creator><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/intel-gpu-in-upstream-pytorch-expanding-gpu-choices-and-enhancing-backend-flexibility.html</guid><category>PyTorch Conference 2024</category></item></channel></rss>