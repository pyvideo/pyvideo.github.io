<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Mark Saroufim</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 18 Sep 2024 00:00:00 +0000</lastBuildDate><item><title>Keynote: PyTorch 2.1 Technical Deep Dive</title><link>https://pyvideo.org/pytorch-conference-2023/keynote-pytorch-21-technical-deep-dive.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This Deep Dive provides an update on the PT2 development since last conference and dives into the key new features coming in PyTorch 2.1 This will provide high level updates on compile, distributed, inference, export and edge.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mario Lezcano</dc:creator><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-pytorch-21-technical-deep-dive.html</guid><category>PyTorch Conference 2023</category><category>Keynote</category></item><item><title>Lightning Talk: The Fastest Path to Production: PyTorch Inference in Python</title><link>https://pyvideo.org/pytorch-conference-2023/lightning-talk-the-fastest-path-to-production-pytorch-inference-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Historically for inference, users have had to rewrite their models to be jit scriptable which required model rewrites and familiarity with C++ services. This is frustrating especially when the vast majority of real world pytorch users actually deploy python in production. When torch.compile was introduced, it encouraged a UX of gradual model rewrites to optimize models but users would get value even without any. A C++ based option still represents a steep difficulty jump and torch.compile still suffers from long compile times which make it unsuited for server side inference where cold start times are critical. In this talk we introduce the options users have for the quickest possible path to production including new APIs to cache compilation artifacts across devices so users can compile models once for both training and inference and python bindings for AOT Inductor. We'll also end with some real world case studies inspired by users who faced the above problems within the context of torchserve. By which point we hope you'll be fully convinced that it's possible deploy python in production and retain performance.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mark Saroufim</dc:creator><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-the-fastest-path-to-production-pytorch-inference-in-python.html</guid><category>PyTorch Conference 2023</category><category>Lightning Talk</category></item><item><title>Slaying OOMs</title><link>https://pyvideo.org/pytorch-conference-2024/slaying-ooms.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever hit an OOM (and wished you had more VRAM)? Who hasn't! Hop on the bus with us and feel the road become smoother as we talk about stacking together techniques like FSDP2 + QLoRa + CPU Offloading + Fused ADAM (thanks Intel) + more in PyTorch native. We will give an overview of these techniques as well as the hard edges we solved in their composition. Curious for more? Or...still OOMing? We also plan on discussing our more researchy work on offloading, pagedness, and low precision optimizers.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mark Saroufim</dc:creator><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/slaying-ooms.html</guid><category>PyTorch Conference 2024</category></item></channel></rss>