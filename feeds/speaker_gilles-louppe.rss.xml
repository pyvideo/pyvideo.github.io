<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 08 Apr 2017 00:00:00 +0000</lastBuildDate><item><title>Bayesian optimization with Scikit-Optimize</title><link>https://pyvideo.org/pydata-amsterdam-2017/bayesian-optimization-with-scikit-optimize.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;You are given access to an espresso machine with many buttons and knobs to tweak. Your task is to brew the best cup of espresso possible. How do you find the best settings before dying of a caffeine overdose? The answer is bayesian optimisation: the art of minimising an extremely expensive to evaluate function in as few calls as possible.&lt;/p&gt;
&lt;p&gt;Bayesian optimisation can be applied to finding the optimal hyper-parameters for a deep neural network, optimizing the click-through-rate in online advertising or simulator settings of an optimal physics experiment. This talk will teach you about the basics of bayesian optimisation and how to use Scikit-Optimize to apply it to your own real world problems.&lt;/p&gt;
&lt;p&gt;In addition to learning how to use this toolkit, you will also learn the answers to questions like:&lt;/p&gt;
&lt;p&gt;When is bayesian optimisation useful? What is bayesian about bayesian optimisation? When can a statistical model be used for bayesian optimisation? What is an acquisition function and how does it help to exploit / explore the search space?&lt;/p&gt;
&lt;p&gt;We will end the talk with an example to optimize hyperparameters of a neural-network using bayesian optimisation.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gilles Louppe</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pydata-amsterdam-2017/bayesian-optimization-with-scikit-optimize.html</guid></item><item><title>Tree models with scikit-learn</title><link>https://pyvideo.org/pydata-paris-2015/tree-models-with-scikit-learn.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk gives an introduction to tree-based methods, both from a
theoretical and practical point of view. It covers decision trees,
random forests and boosting estimators, along with concrete examples
based on Scikit-Learn about how they work, when they work and why they
work.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gilles Louppe</dc:creator><pubDate>Mon, 13 Apr 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-04-13:pydata-paris-2015/tree-models-with-scikit-learn.html</guid></item><item><title>Accelerating Random Forests in Scikit Learn</title><link>https://pyvideo.org/euroscipy-2014/accelerating-random-forests-in-scikit-learn.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Random Forests are without contest one of the most robust, accurate and
versatile tools for solving machine learning tasks. Implementing this
algorithm properly and efficiently remains however a challenging task
involving issues that are easily overlooked if not considered with care.
In this talk, we present the Random Forests implementation developed
within the Scikit-Learn machine learning library. In particular, we
describe the iterative team efforts that led us to gradually improve our
codebase and eventually make Scikit-Learn's Random Forests one of the
most efficient implementations in the scientific ecosystem, across all
libraries and programming languages. Algorithmic and technical
optimizations that have made this possible include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;An efficient formulation of the decision tree algorithm, tailored for
Random Forests;&lt;/li&gt;
&lt;li&gt;Cythonization of the tree induction algorithm;&lt;/li&gt;
&lt;li&gt;CPU cache optimizations, through low-level organization of data into
contiguous memory blocks;&lt;/li&gt;
&lt;li&gt;Efficient multi-threading through GIL-free routines;&lt;/li&gt;
&lt;li&gt;A dedicated sorting procedure, taking into account the properties of
data;&lt;/li&gt;
&lt;li&gt;Shared pre-computations whenever critical.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, we believe that lessons learned from this case study extend to
a broad range of scientific applications and may be of interest to
anybody doing data analysis in Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gilles Louppe</dc:creator><pubDate>Wed, 22 Oct 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-10-22:euroscipy-2014/accelerating-random-forests-in-scikit-learn.html</guid></item></channel></rss>