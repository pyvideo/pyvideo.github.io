<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Scipy Japan 2020</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_scipy-japan-2020.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-10-30T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Apache Arrow 1.0 - A Cross-Language Development Platform for In-Memory Data</title><link href="https://pyvideo.org/scipy-japan-2020/apache-arrow-10-a-cross-language-development-platform-for-in-memory-data.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Kouhei Sutou</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/apache-arrow-10-a-cross-language-development-platform-for-in-memory-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Arrow is a cross-language development platform for in-memory data. You can use Apache Arrow to process large data effectively in Python and other languages such as R. Apache Arrow is the future of data processing. Apache Arrow 1.0, the first major version, will be released soon. It's …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Arrow is a cross-language development platform for in-memory data. You can use Apache Arrow to process large data effectively in Python and other languages such as R. Apache Arrow is the future of data processing. Apache Arrow 1.0, the first major version, will be released soon. It's a good time to know Apache Arrow and start using it.&lt;/p&gt;
&lt;p&gt;Apache Arrowはインメモリデータのための言語横断的な開発プラットフォームです。Apache Arrowを使って、PythonやRなどの他の言語で大容量データを効率的に処理することができます。Apache Arrowはデータ処理の未来です。最初のメジャーバージョンであるApache Arrow 1.0がまもなくリリースされます。この機会にApache Arrowを知って、使い始めてみてはいかがでしょうか。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Applying Machine Learning to R&amp;D for Semiconductor Process Development</title><link href="https://pyvideo.org/scipy-japan-2020/applying-machine-learning-to-rd-for-semiconductor-process-development.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Naoki Yoshii</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/applying-machine-learning-to-rd-for-semiconductor-process-development.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Naoki Yoshii, Kenji Matogawa&lt;/p&gt;
&lt;p&gt;To achieve efficient development of semiconductor processes/materials, a method for predicting optimal process/materials conditions using machine learning, open information (papers) and material databases was implemented using Python.&lt;/p&gt;
&lt;p&gt;半導体プロセス/材料の効率的な開発を実現するために、機械学習、オープン情報(論文)、マテリアルデータベースを利用した最適なプロセス/材料条件を予測する方法をPythonにより実施した。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Approx Vector Search @Scale with App Image Search</title><link href="https://pyvideo.org/scipy-japan-2020/approx-vector-search-scale-with-app-image-search.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Wakana Nogami</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/approx-vector-search-scale-with-app-image-search.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Mercari provides an image search feature, which makes it possible for users to find similar items by image. This talk describes how we implemented similar image search within 100s of millions of images, in a way that is accurate. We will also highlight the techniques we used to keep …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Mercari provides an image search feature, which makes it possible for users to find similar items by image. This talk describes how we implemented similar image search within 100s of millions of images, in a way that is accurate. We will also highlight the techniques we used to keep the system efficient and update to date.&lt;/p&gt;
&lt;p&gt;メルカリには画像検索機能があり、ユーザーは画像から類似品を探すことができます。本講演では、数百万枚の画像の中から類似画像検索をどのようにして正確に実装したかを説明します。また、システムを効率的に更新するために使ったテクニックも紹介します。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Building a Scalable AI Chatbot From Regex to Deep Learning</title><link href="https://pyvideo.org/scipy-japan-2020/building-a-scalable-ai-chatbot-from-regex-to-deep-learning.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Max Frenzel</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/building-a-scalable-ai-chatbot-from-regex-to-deep-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Using the example of a chatbot I will show how a complex ML system can be built in an iterative fashion with feasibility and scalability in mind, starting from simple but proven methods and incrementally constructing an increasingly powerful pipeline of algorithms. I will discuss a range of natural …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Using the example of a chatbot I will show how a complex ML system can be built in an iterative fashion with feasibility and scalability in mind, starting from simple but proven methods and incrementally constructing an increasingly powerful pipeline of algorithms. I will discuss a range of natural language processing tools and techniques in Python, from scikit-learn's inbuilt text processing capabilities, to the advanced NLP library spaCy, all the way to neural networks and deep learning using PyTorch. Besides the technical aspects, I will present real case studies of how we at Bespoke have applied this approach.&lt;/p&gt;
&lt;p&gt;チャットボットの例を用いて、複雑なMLシステムがどのようにして実現可能性とスケーラビリティを念頭に置いた反復的な方法で構築されるかを紹介します。私は、scikit-learnの組み込みテキスト処理機能から、高度なNLPライブラリspaCy、PyTorchを使ったニューラルネットワークと深層学習まで、Pythonを使った自然言語処理ツールとテクニックの範囲について議論します。技術的な側面に加えて、私たちがBESPOKEでどのようにこのアプローチを適用したかの実際のケーススタディを紹介します。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Building Machine Learning Pipelines with Kubeflow</title><link href="https://pyvideo.org/scipy-japan-2020/building-machine-learning-pipelines-with-kubeflow.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>William Horton</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/building-machine-learning-pipelines-with-kubeflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Kubeflow is a tool for managing machine learning workflows on Kubernetes. In this talk, I will explain how we can use Kubeflow to take on the challenges of machine learning beyond model training, including data preprocessing, model evaluation, and deployment--the steps that together make up a full machine learning …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Kubeflow is a tool for managing machine learning workflows on Kubernetes. In this talk, I will explain how we can use Kubeflow to take on the challenges of machine learning beyond model training, including data preprocessing, model evaluation, and deployment--the steps that together make up a full machine learning pipeline. I will introduce Kubeflow and explain the central concepts of the platform using basic examples. Then I will show how my team is using Kubeflow to tackle real-world problems in the real estate domain.&lt;/p&gt;
&lt;p&gt;Kubeflowは、Kubernetes上で機械学習のワークフローを管理するためのツールです。 本講演では、Kubeflowを使って、データの前処理、モデルの評価、デプロイメントなど、モデル学習の枠を超えた機械学習への挑戦、つまり、機械学習のパイプラインを構成するステップをまとめて解説します。Kubeflowを紹介し、基本的な例を用いてプラットフォームの中心的な概念を説明します。その後、私のチームがどのようにKubeflowを使用して不動産分野の実世界の問題に取り組んでいるかを紹介します。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>CuPy: A Numpy-Compatible Library for HPC with GPU</title><link href="https://pyvideo.org/scipy-japan-2020/cupy-a-numpy-compatible-library-for-hpc-with-gpu.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Masayuki Takagi</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/cupy-a-numpy-compatible-library-for-hpc-with-gpu.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;CuPy is an open-source library with NumPy-compatible API and brings high performance N-dimensional array computation with utilizing Nvidia GPU. Users can get several times speed improvement from drop-in replacement to their NumPy-based code in most cases. CuPy is actively being developed and is continuously well-maintained, resulting in 4,200 …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;CuPy is an open-source library with NumPy-compatible API and brings high performance N-dimensional array computation with utilizing Nvidia GPU. Users can get several times speed improvement from drop-in replacement to their NumPy-based code in most cases. CuPy is actively being developed and is continuously well-maintained, resulting in 4,200+ GitHub stars and 17,000+ commits. CuPy was also presented at PyCon 2018.&lt;/p&gt;
&lt;p&gt;CuPyは、NumPyと互換性のあるAPIを持つオープンソースのライブラリで、Nvidia GPUを利用した高性能なN次元配列計算を実現します。ほとんどの場合、NumPyベースのコードをドロップインで置き換えることで、数倍の速度向上を得ることができます。CuPyは積極的に開発されており、継続的によくメンテナンスされており、4,200以上のGitHubスターと17,000以上のコミットを得ています。CuPyはPyCon 2018でも発表されました。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Dask Image - Distributed Image Processing for Large Data</title><link href="https://pyvideo.org/scipy-japan-2020/dask-image-distributed-image-processing-for-large-data.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Genevieve Buckley</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/dask-image-distributed-image-processing-for-large-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk introduces dask-image, a python library for distributed image processing. Targeted towards applications involving large array data too big to fit in memory, dask-image is built on top of numpy, scipy, and dask allowing easy scalability and portability from your laptop to the supercomputing cluster. It is of …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk introduces dask-image, a python library for distributed image processing. Targeted towards applications involving large array data too big to fit in memory, dask-image is built on top of numpy, scipy, and dask allowing easy scalability and portability from your laptop to the supercomputing cluster. It is of broad interest to a diverse range of scientific fields including astronomy, geosciences, microscopy, and climate sciences. We will provide a general overview of the dask-image library, then discuss mixing and matching with your own custom functions, and present a practical case study of a python image processing pipeline.&lt;/p&gt;
&lt;p&gt;この講演では、分散画像処理のための python ライブラリである dask-image を紹介します。メモリに収まりきらないほど大きな配列データを扱うアプリケーションをターゲットにしており、dask-image は numpy, scipy, dask の上に構築されているため、ラップトップからスーパーコンピュータクラスタへの拡張性と移植性を容易にします。天文学、地球科学、顕微鏡、気候科学などの多様な科学分野に広く関心があります。dask-image ライブラリの一般的な概要を説明した後、独自のカスタム関数との混合やマッチングについて議論し、Python 画像処理パイプラインの実践的なケーススタディを紹介します。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Demystifying Self-Supervised Learning for Visual Recognition</title><link href="https://pyvideo.org/scipy-japan-2020/demystifying-self-supervised-learning-for-visual-recognition.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Sayak Paul</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/demystifying-self-supervised-learning-for-visual-recognition.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Deep learning models are powered by data, the abundance of it. Most of the deep learning models that empower many critical applications in our day-to-day lives depend on labeled data. Affording humongous amounts of labeled datasets is not always possible for a number of different factors like budget, human …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Deep learning models are powered by data, the abundance of it. Most of the deep learning models that empower many critical applications in our day-to-day lives depend on labeled data. Affording humongous amounts of labeled datasets is not always possible for a number of different factors like budget, human resources, lack of expert annotators, and so on. The internet is practically an infinite source of unlabeled data. So, at this point, an important question that gets raised is - how can we effectively utilize this large pool of unlabeled data for developing better deep learning models? Self-supervised learning can be helpful here to answer questions like this.&lt;/p&gt;
&lt;p&gt;ディープラーニングモデルは、豊富なデータを動力源としています。私たちの日常生活における多くの重要なアプリケーションに力を与えているディープラーニングモデルのほとんどは、ラベル付きデータに依存しています。膨大な量のラベル付きデータセットを提供することは、予算、人的資源、専門家のアノテータの不足など、さまざまな要因により常に可能とは限りません。インターネットには、事実上、ラベル付けされていないデータが無限に存在します。そこで、この時点で重要な疑問が浮かび上がってきます-どのようにして、より良いディープラーニングモデルを開発するために、この大規模な非ラベルデータのプールを効果的に利用することができるのでしょうか？自己教師付き学習は、このような疑問に答えるのに役立ちます。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Hyperparameters - Autotuning to Make Performance Sing with Optuna</title><link href="https://pyvideo.org/scipy-japan-2020/hyperparameters-autotuning-to-make-performance-sing-with-optuna.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Crissman Loomis</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/hyperparameters-autotuning-to-make-performance-sing-with-optuna.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Hyperparameters are manual, often hard-coded, settings in programming. Some examples are selection of optimizers or learning rates in data science, database performance tuning, or video compression settings. These values and choices may seem incidental to the programming task, but can be extremely important for performance. Tuning them to find …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Hyperparameters are manual, often hard-coded, settings in programming. Some examples are selection of optimizers or learning rates in data science, database performance tuning, or video compression settings. These values and choices may seem incidental to the programming task, but can be extremely important for performance. Tuning them to find the right values can be difficult and time-consuming. This talk introduces Optuna, an open-source, eager interface, Python framework that automates the process of tuning hyperparameters using blackbox optimization, and highlights the new features and integration modules for other open-source projects available in v1.0.&lt;/p&gt;
&lt;p&gt;ハイパーパラメータとは、プログラミングにおける手動の設定のことで、多くの場合はハードコーディングされています。例としては、データサイエンスにおけるオプティマイザや学習率の選択、データベースのパフォーマンスチューニング、動画圧縮の設定などがあります。これらの値や選択は、プログラミングのタスクには付随的に見えるかもしれませんが、パフォーマンスにとっては非常に重要です。これらの値をチューニングして正しい値を見つけるのは難しく、時間のかかる作業です。この講演では、ブラックボックス最適化を使用してハイパーパラメータのチューニングプロセスを自動化する、オープンソースのイーガーインターフェース、PythonフレームワークであるOptunaを紹介し、v1.0で利用可能になった他のオープンソースプロジェクトの新機能と統合モジュールをハイライトします。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>ML Technologies - Accelerating Discovery of Innovative Materials</title><link href="https://pyvideo.org/scipy-japan-2020/ml-technologies-accelerating-discovery-of-innovative-materials.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Ryo Yoshida</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/ml-technologies-accelerating-discovery-of-innovative-materials.html</id><content type="html"></content><category term="Scipy Japan 2020"></category><category term="Keynote"></category></entry><entry><title>napari, a fast n-dimensional array viewer for scientific Python</title><link href="https://pyvideo.org/scipy-japan-2020/napari-a-fast-n-dimensional-array-viewer-for-scientific-python.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Juan Nunez-Iglesias</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/napari-a-fast-n-dimensional-array-viewer-for-scientific-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;I will present napari (&lt;a class="reference external" href="https://napari.org"&gt;https://napari.org&lt;/a&gt;), a new n-dimensional array viewer for scientific Python. napari uses VisPy and OpenGL to do fast rendering on the GPU. Additionally, napari leverages &amp;quot;lazy arrays&amp;quot; such as Dask and Zarr to only load the part of your data being viewed, allowing easy …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;I will present napari (&lt;a class="reference external" href="https://napari.org"&gt;https://napari.org&lt;/a&gt;), a new n-dimensional array viewer for scientific Python. napari uses VisPy and OpenGL to do fast rendering on the GPU. Additionally, napari leverages &amp;quot;lazy arrays&amp;quot; such as Dask and Zarr to only load the part of your data being viewed, allowing easy exploration of larger-than-RAM datasets, remote datasets, and even data computed on the fly.&lt;/p&gt;
&lt;p&gt;今回紹介するのは、科学的なPythonのための新しいn次元配列ビューア、napari (&lt;a class="reference external" href="https://napari.org"&gt;https://napari.org&lt;/a&gt;)です。 napariはVisPyとOpenGLを使用してGPU上で高速なレンダリングを行います。さらに、napariはDaskやZarrのような &amp;quot;遅延配列 &amp;quot;を利用して、表示されているデータの一部だけをロードすることで、RAMよりも大きなデータセットやリモートデータセット、さらにはオンザフライで計算されたデータを簡単に探索することができます。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Process Informatics Tech - Accelerating Development of Semiconductor Crystal Growth Process</title><link href="https://pyvideo.org/scipy-japan-2020/process-informatics-tech-accelerating-development-of-semiconductor-crystal-growth-process.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Daisuke Uematsu</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/process-informatics-tech-accelerating-development-of-semiconductor-crystal-growth-process.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The discovery and utilizing of new materials has the potential to create the world that is more convenient and richer for our lives and more eco-friendly and cleaner for the natural environment.On the other hand, conventional materials research has required a great deal of time and money for …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The discovery and utilizing of new materials has the potential to create the world that is more convenient and richer for our lives and more eco-friendly and cleaner for the natural environment.On the other hand, conventional materials research has required a great deal of time and money for discovery and development, and this is the major issue that needs to be solved in order to realize the practical application of new materials.
As a solution to this problem, the use of informatics technology for materials research is considered to be effective.
In this presentation ,I’ll talk about my research of next-gen Semiconductor Material (Crystal) Growth Process with utilizing Process Informatics technologies mainly for next-gen automobile especially from the view point of how I utilized Python with the overview about how Informatics is utilized for R&amp;amp;D in materials science.&lt;/p&gt;
&lt;p&gt;新しい材料の発見と活用は、私達の暮らしに対しより豊かで便利な世界を、また自然環境にとって優しくクリーンな世界を実現する可能性を秘めています。
一方で、従来の材料研究は発見や開発に多大な時間・コストを必要することが多く、このことは新材料の実用化実現に対する解決すべき大きな課題となっています。
この課題の解決策として、我々は材料研究に対するインフォマティクス技術の活用が有効だと考えています。
この発表では、プロセス・インフォマティクスを活用した次世代半導体材料（結晶）の成長プロセスにおける私の研究について、特にどのようにPythonを活用したかという観点から、材料科学のR&amp;amp;Dへのインフォマティクス活用の俯瞰とともに発表します。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>PyNUFFT: experiences &amp; reflections on high-performance biomed imaging reconstruct</title><link href="https://pyvideo.org/scipy-japan-2020/pynufft-experiences-reflections-on-high-performance-biomed-imaging-reconstruct.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Jyh-Miin Lin</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/pynufft-experiences-reflections-on-high-performance-biomed-imaging-reconstruct.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Python non-uniform fast Fourier transform (PyNUFFT) provides a high-speed, yet accurate non-Cartesian data processing method to many research fields, including magnetic resonance imaging (MRI) and tomography.&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>PySnooper - Never use print for debugging again</title><link href="https://pyvideo.org/scipy-japan-2020/pysnooper-never-use-print-for-debugging-again.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Ram Rachum</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/pysnooper-never-use-print-for-debugging-again.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;I had an idea for a debugging solution for Python that doesn't require complicated configuration like PyCharm. I released PySnooper as a cute little open-source project that does that, and to my surprise, it became a huge hit overnight, hitting the top of Hacker News, r/python and GitHub …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;I had an idea for a debugging solution for Python that doesn't require complicated configuration like PyCharm. I released PySnooper as a cute little open-source project that does that, and to my surprise, it became a huge hit overnight, hitting the top of Hacker News, r/python and GitHub trending. In this talk I'll go into:
- How PySnooper can help you debug your code.
- How you can write your own debugging / code intelligence tools.
- How to make your open-source project go viral.
- How to use PuDB, another debugging solution, to find bugs in your code.
- A PEP idea for making debuggers easier to debug.&lt;/p&gt;
&lt;p&gt;私はPyCharmのような複雑な設定を必要としないPythonのデバッグソリューションのアイデアを持っていました。驚いたことに、PySnooperは一夜にして大ヒットとなり、Hacker News、r/python、GitHubのトレンドのトップに上りました。この講演では、次のような話をします。
- PySnooperがどのようにコードのデバッグに役立つのか。
- 独自のデバッグ/コードインテリジェンスツールの書き方。
- オープンソースのプロジェクトをバイラルにするには？
- 他のデバッグソリューションである PuDB を使ってコードのバグを見つける方法。
- デバッガをデバッグしやすくするためのPEPのアイデア。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>TFX Open Source Production ML Pipelines With TensorFlow</title><link href="https://pyvideo.org/scipy-japan-2020/tfx-open-source-production-ml-pipelines-with-tensorflow.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Robert Crowe</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/tfx-open-source-production-ml-pipelines-with-tensorflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;An increasing range of applications and services are applying machine learning to achieve amazing results, but putting an ML pipeline into production is fundamentally different from just training a model. Learn from Google's experience with ML in production and how you can apply TFX, Google's open-source ML pipeline, to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;An increasing range of applications and services are applying machine learning to achieve amazing results, but putting an ML pipeline into production is fundamentally different from just training a model. Learn from Google's experience with ML in production and how you can apply TFX, Google's open-source ML pipeline, to your deployments.&lt;/p&gt;
&lt;p&gt;機械学習を適用して驚くべき結果を出すアプリケーションやサービスが増えていますが、ML パイプラインを本番に投入することは、単なるモデルのトレーニングとは根本的に異なります。 本番環境での ML の経験と、Google のオープンソース ML パイプラインである TFX をどのようにデプロイメントに適用できるかを Google の経験から学びましょう。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Translation Project of Mayavi2 documents</title><link href="https://pyvideo.org/scipy-japan-2020/translation-project-of-mayavi2-documents.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Tetsuo Koyama</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/translation-project-of-mayavi2-documents.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Mayavi2 is a application and library for interactive scientific data visualization and 3D plotting in Python. We started a document translation project so that Japanese users can use this wonderful software. In this project, we translated all Mayavi2 documentation into Japanese.&lt;/p&gt;
&lt;p&gt;As you know, Mayavi2 uses sphinx as its …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Mayavi2 is a application and library for interactive scientific data visualization and 3D plotting in Python. We started a document translation project so that Japanese users can use this wonderful software. In this project, we translated all Mayavi2 documentation into Japanese.&lt;/p&gt;
&lt;p&gt;As you know, Mayavi2 uses sphinx as its documentation software. We have created an environment in which translators can easily contribute to the project by referring to the translation procedures of the sphinx project. This procedure can also be used for projects with other languages and other sphinx documents.&lt;/p&gt;
&lt;p&gt;Mayavi2は、Pythonでインタラクティブな科学データの可視化と3Dプロットを行うためのアプリケーションとライブラリです。私たちは、この素晴らしいソフトウェアを日本のユーザーに使ってもらうために、ドキュメント翻訳プロジェクトを始めました。このプロジェクトでは、Mayavi2のドキュメントをすべて日本語に翻訳しました。ご存知の通り、Mayavi2はドキュメントソフトとしてsphinxを使用しています。sphinxプロジェクトの翻訳手順を参考にして、翻訳者がプロジェクトに貢献しやすい環境を整えました。この手順は、他の言語のプロジェクトや他のsphinxのドキュメントでも利用することができます。&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry><entry><title>Tutorial: Building a Scalable AI Chatbot-From Regex to Deep Learning</title><link href="https://pyvideo.org/scipy-japan-2020/tutorial-building-a-scalable-ai-chatbot-from-regex-to-deep-learning.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Asir Saeed</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/tutorial-building-a-scalable-ai-chatbot-from-regex-to-deep-learning.html</id><content type="html"></content><category term="Scipy Japan 2020"></category></entry><entry><title>Tutorial: (D)Ask Me Anything - Munging, Wrangling &amp; Preprocessing Data @ Scale</title><link href="https://pyvideo.org/scipy-japan-2020/tutorial-dask-me-anything-munging-wrangling-preprocessing-data-scale.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>Ramon Perez</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/tutorial-dask-me-anything-munging-wrangling-preprocessing-data-scale.html</id><content type="html"></content><category term="Scipy Japan 2020"></category></entry><entry><title>Tutorial: Introduction to Conda for (Data) Scientists</title><link href="https://pyvideo.org/scipy-japan-2020/tutorial-introduction-to-conda-for-data-scientists.html" rel="alternate"></link><published>2020-10-30T00:00:00+00:00</published><updated>2020-10-30T00:00:00+00:00</updated><author><name>David Pugh</name></author><id>tag:pyvideo.org,2020-10-30:/scipy-japan-2020/tutorial-introduction-to-conda-for-data-scientists.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial is a Software Carpentry-style introduction to Conda for (data) scientists. This tutorial motivates the use of Conda as a development tool for building and sharing project specific software environments that facilitate reproducible (data) science workflows. Particular attention is given to using Conda to create reproducible environments with …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial is a Software Carpentry-style introduction to Conda for (data) scientists. This tutorial motivates the use of Conda as a development tool for building and sharing project specific software environments that facilitate reproducible (data) science workflows. Particular attention is given to using Conda to create reproducible environments with NVIDIA GPU dependencies (including environments for Horovod, TensorFlow, PyTorch, and NVIDIA RAPIDS) as well as a discussion of best practices for using Conda in HPC environments.&lt;/p&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;p&gt;Tutorial Prerequisites: Basic familiarity with Python programming and Bash shell concepts (i.e., basic commands, environment variables, etc). Familiarity installing NVIDIA CUDA Toolkit would be beneficial for NVIDIA GPU focused episodes.&lt;/p&gt;
</content><category term="Scipy Japan 2020"></category></entry></feed>