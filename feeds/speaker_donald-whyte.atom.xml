<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_donald-whyte.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-11-10T00:00:00+00:00</updated><entry><title>High Performance Data Processing in Python</title><link href="https://pyvideo.org/pycon-ireland-2018/high-performance-data-processing-in-python.html" rel="alternate"></link><published>2018-11-10T00:00:00+00:00</published><updated>2018-11-10T00:00:00+00:00</updated><author><name>Donald Whyte</name></author><id>tag:pyvideo.org,2018-11-10:pycon-ireland-2018/high-performance-data-processing-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;numpy and numba are popular Python libraries for processing large quantities of data. When running complex transformations on large datasets, many developers fall into common pitfalls that kill the performance of these libraries. This talk explains how numpy/numba work under the hood and how they use vectorisation to process large amounts of data extremely quickly. We use these tools to reduce the processing time of a dataset from 3 years to 12 hours, even when the code is run on a single Macbook Pro.&lt;/p&gt;
</summary><category term="numpy"></category><category term="numba"></category></entry><entry><title>High Performance Data Processing in Python</title><link href="https://pyvideo.org/pycon-russia-2018/high-performance-data-processing-in-python.html" rel="alternate"></link><published>2018-06-22T00:00:00+00:00</published><updated>2018-06-22T00:00:00+00:00</updated><author><name>Donald Whyte</name></author><id>tag:pyvideo.org,2018-06-22:pycon-russia-2018/high-performance-data-processing-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Donald Whyte&lt;/strong&gt; , Engineers Gate&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://pycon.ru/2018/en/program/content/whyte/"&gt;**High Performance Data Processing in
Python**&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Internet age generates vast amounts of data. Most of this data is
unstructured and needs to cleaned. Python has become the standard tool
for transforming this data into more useable forms.&lt;/p&gt;
&lt;p&gt;numpy and pandas are the most popular Python libraries for processing
large quantities of data. For small datasets, these libraries do the job
without much effort. However, when running complex transformations on
larger datasets, many developers fall into common pitfalls that kill the
performance of these libraries.&lt;/p&gt;
&lt;p&gt;This talk explains how numpy and pandas work under the hood and how they
use vectorisation to process large amounts of data extremely quickly. We
show an example dataset being processed using numpy/pandas. We
demonstrate how to use these libraries effectively, reducing the
processing time of this large dataset from several hours to seconds.&lt;/p&gt;
</summary></entry></feed>