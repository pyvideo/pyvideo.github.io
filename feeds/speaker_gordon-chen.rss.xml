<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Thu, 11 Jul 2019 00:00:00 +0000</lastBuildDate><item><title>Safe Handling Instructions for Probabilistic Classification</title><link>https://pyvideo.org/scipy-2019/safe-handling-instructions-for-probabilistic-classification.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In machine learning, a common task is to predict whether an unclassified observation belongs to one class or another. However, people are often actually more interested to know the probability of belonging to a class rather than just the most likely class. In such cases, a traditional classification problem becomes a probabilistic classification problem. This distinction is subtle yet crucial. Firstly, the probability-ish outputs of most classifiers are not true probabilities. Moreover, if we use traditional metrics such as the AUC score on probabilistic classification problems, we may end up selecting the wrong models. Fortunately, probability calibration techniques, advanced model stacking/blending methods, and more suitable metrics can fix these issues.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Gordon Chen</dc:creator><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-11:scipy-2019/safe-handling-instructions-for-probabilistic-classification.html</guid></item></channel></rss>