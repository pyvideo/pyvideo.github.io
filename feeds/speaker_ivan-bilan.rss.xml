<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 07 Jul 2018 00:00:00 +0000</lastBuildDate><item><title>Understanding and Applying Self-Attention for NLP</title><link>https://pyvideo.org/pydata-berlin-2018/understanding-and-applying-self-attention-for-nlp.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Understanding attention mechanisms and self-attention, presented in
Google's &amp;quot;Attention is all you need&amp;quot; paper, is a beneficial skill for
anyone who works on complex NLP problems. In this talk, we will go over
the main parts of the Google Transformer self-attention model and the
intuition behind it. Then we will look on how this architecture can be
used for other NLP tasks, i.e. slot filling.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ivan Bilan</dc:creator><pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-07-07:pydata-berlin-2018/understanding-and-applying-self-attention-for-nlp.html</guid></item></channel></rss>