<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Thu, 06 Jul 2017 00:00:00 +0000</lastBuildDate><item><title>Building a community fountain around your data stream</title><link>https://pyvideo.org/pydata-seattle-2017/building-a-community-fountain-around-your-data-stream.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With the trend towards data streams, building successful streaming analysis systems means building a community comfortable with streaming tech. But getting started with stream processing can be intimidating for anyone. In this talk, I’ll talk about designing and deploying a mini-testbed system to scale down the stream and how you can practice your favorite algorithm on an astronomical data stream.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The increasing availability of real-time data sources and the Internet of Things movement have pushed data analysis pipelines towards stream processing. But what does this really mean for my applications, and how do I have to change my code and workflow? In a new era of “Kappa architecture,” it’s easier than ever to use the same programming model for both batch and stream processing.&lt;/p&gt;
&lt;p&gt;For those interested in the design and operations side, I will cover high-level design considerations for architecting a modular and scalable stream processing infrastructure that can support the flexibility of different use cases and can welcome a community of users who are more familiar with batch processing.&lt;/p&gt;
&lt;p&gt;For the fast-batching Pythonistas, I’ll talk about some of the advantages of using streaming tech in a data processing pipeline and how to make your life easier with&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;built-in replication, scalability, and stream “rewind” for data distribution with Kafka,&lt;/li&gt;
&lt;li&gt;structured messages with strictly enforced schemas and dynamic typing for fast parsing with Avro, and&lt;/li&gt;
&lt;li&gt;a stream processing interface that is similar to batch with Spark that you can even use in a Jupyter notebook.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you’re ready to jump into the stream, or at least take a drink from the fountain, I’ll point you to an open source, containerized (with Docker), streaming ecosystem testbed that you can deploy to mock a stream of data and take your streaming analytics on a dry run over an astronomical data stream.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Maria Patterson</dc:creator><pubDate>Thu, 06 Jul 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-07-06:pydata-seattle-2017/building-a-community-fountain-around-your-data-stream.html</guid></item></channel></rss>