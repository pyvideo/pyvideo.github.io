<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 13 Jul 2019 00:00:00 +0000</lastBuildDate><item><title>Kafka in Finance: Over 1 Billion messages a day</title><link>https://pyvideo.org/pydata-london-2019/kafka-in-finance-over-1-billion-messages-a-day.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Man Alpha Technology provides the front-office technology for the investments engines within Man. The Data Engineering team is responsible for delivering live financial data to systematic trading strategies and maintaining historical data store for research purposes.&lt;/p&gt;
&lt;p&gt;We have multiple live data feeds from various vendors which tick real time data onto a live message bus. This message bus does not maintain any history which we require for both research and trading purposes. To satisfy these requirements we by and large use two technologies; Kafka and Mongo. Both of these technologies are driven from our in-house Python clients.&lt;/p&gt;
&lt;p&gt;Each &amp;quot;tick&amp;quot; is first written to Kafka under a topic that corresponds to the upstream data vendor by a set of tick collectors that consume from the live message bus and publish to Kafka. Each message is then transformed into a standard MarketData message structure which is encoded into binary payloads using MsgPack. Another set of Python processes consume from Kafka and write to an Arctic datastore backed by Mongo. Arctic is a high performance datastore for numeric time-series data which Man Alpha Technology has open sourced (&lt;a class="reference external" href="https://github.com/manahl/arctic"&gt;https://github.com/manahl/arctic&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;To make the most of our Kafka pipelines we have our own Python Kafka clients built on top of the open source clients that provide a number of additional features. For example, the client deterministically assigns market data symbols to a Kafka partitions to maintain time ordering of the messages for each symbol and it allows us to use timestamps as a first class concept when seeking and consuming from Kafka. Although this feature is now available natively at the broker level (Time Based Search), this has been implemented in our client since 2014.&lt;/p&gt;
&lt;p&gt;In the talk we will dive deeper into the details of our data pipelines as described above, as well as how we monitor, deploy and maintain the production and research pipelines.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matthew Hertz</dc:creator><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-13:pydata-london-2019/kafka-in-finance-over-1-billion-messages-a-day.html</guid></item></channel></rss>