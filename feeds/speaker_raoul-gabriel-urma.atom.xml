<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_raoul-gabriel-urma.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-11-06T00:00:00+00:00</updated><entry><title>Advanced Software Testing for Data Scientists</title><link href="https://pyvideo.org/pydata-new-york-city-2019/advanced-software-testing-for-data-scientists.html" rel="alternate"></link><published>2019-11-06T00:00:00+00:00</published><updated>2019-11-06T00:00:00+00:00</updated><author><name>Raoul-Gabriel Urma</name></author><id>tag:pyvideo.org,2019-11-06:pydata-new-york-city-2019/advanced-software-testing-for-data-scientists.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The journey to deploy a model to production starts with testing it rigorously, including its code implementation. In this tutorial, you will learn about state of the art software testing approach. You will learn how to write unit tests with enhanced diagnostics, leverage validation tools from numpy, pandas, scikit-learn, apply test doubles and generate test cases using property-based testing.&lt;/p&gt;
</summary><category term="tutorial"></category></entry><entry><title>Adv. Software Testing for Data Scientists</title><link href="https://pyvideo.org/pydata-london-2019/adv-software-testing-for-data-scientists.html" rel="alternate"></link><published>2019-07-13T00:00:00+00:00</published><updated>2019-07-13T00:00:00+00:00</updated><author><name>Raoul-Gabriel Urma</name></author><id>tag:pyvideo.org,2019-07-13:pydata-london-2019/adv-software-testing-for-data-scientists.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The journey to deploy a model to production starts with testing it rigorously, including its code implementation. In this tutorial, you will learn about state of the art software testing approach. You will learn how to write unit tests with enhanced diagnostics, leverage validation tools from numpy, pandas, scikit-learn, apply test doubles and generate test cases using property-based testing.&lt;/p&gt;
</summary></entry><entry><title>Introduction to Big Data Processing using Spark and Python</title><link href="https://pyvideo.org/pydata-new-york-city-2018/introduction-to-big-data-processing-using-spark-and-python.html" rel="alternate"></link><published>2018-08-17T00:00:00+00:00</published><updated>2018-08-17T00:00:00+00:00</updated><author><name>Raoul-Gabriel Urma</name></author><id>tag:pyvideo.org,2018-08-17:pydata-new-york-city-2018/introduction-to-big-data-processing-using-spark-and-python.html</id><summary type="html"></summary></entry><entry><title>Making Sense of Big Data File Formats: Avro and Parquet</title><link href="https://pyvideo.org/pycon-uk-2017/making-sense-of-big-data-file-formats-avro-and-parquet.html" rel="alternate"></link><published>2017-10-29T12:30:00+01:00</published><updated>2017-10-29T12:30:00+01:00</updated><author><name>Raoul-Gabriel Urma</name></author><id>tag:pyvideo.org,2017-10-29:pycon-uk-2017/making-sense-of-big-data-file-formats-avro-and-parquet.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Modern applications generate and manipulate a lot of data. The growth rate of the data is staggering. Unfortunately, large datasets can be expensive to store at large scale and also slow to process. In fact, memory speed has been evolving at a much lower rate in comparison to CPUs. Thankfully, there are various file formats suited for big data systems to help. In this talk, you will learn about two popular file formats suitable for big data systems: Avro and Parquet. Through live coded examples in Python, you will learn the good, the bad, the ugly, and how you can make use of Avro and Parquet in practice.&lt;/p&gt;
</summary><category term="avro"></category><category term="parquet"></category></entry><entry><title>Interactively Analyse 100GB of Data using Spark, Amazon EMR and Zeppelin</title><link href="https://pyvideo.org/pydata-london-2017/interactively-analyse-100gb-of-data-using-spark-amazon-emr-and-zeppelin.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Raoul-Gabriel Urma</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/interactively-analyse-100gb-of-data-using-spark-amazon-emr-and-zeppelin.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
In this highly interactive session, you will learn how to leverage Apache Spark, Amazon Elastic Map Reduce (EMR) and Apache Zeppelin to rapidly mine a large real-world data set. You will learn how to apply common Spark patterns to extract insights as well as learn useful performance and monitoring tips.&lt;/p&gt;
&lt;p&gt;Abstract
You may have been hearing a lot of buzz around Big Data, Apache Spark, Amazon Elastic Map Reduce (EMR) and Apache Zeppelin. Whatâ€™s the fuss about, and how can you benefit from these state of the art technologies?&lt;/p&gt;
&lt;p&gt;In this highly interactive session, you will learn how to leverage Spark to rapidly mine a large real-world data set. We will conduct the analysis live entirely using an iPython Notebook to show you how easy it can be to get to grips with these technologies.&lt;/p&gt;
&lt;p&gt;In the first part of the session, we will use a sample of data from the Open Library dataset, and you will learn how to apply common Spark patterns to extract insights and aggregate data. In the second part of the session, you will see how to leverage Spark on Amazon EMR to scale your data processing queries over a cluster of machines and interactively analyse a large data set (100GB) with a Zeppelin Notebook. Along the way you will learn gotchas as well as useful performance and monitoring tips&lt;/p&gt;
</summary></entry></feed>