<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sun, 23 Feb 2014 00:00:00 +0000</lastBuildDate><item><title>House Prices and Rents: Evidence from a Matched Dataset in Central London</title><link>https://pyvideo.org/pydata-london-2014/house-prices-and-rents-evidence-from-a-matched-dataset-in-central-london.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The average ratio between house prices and rents is an indicator of
housing market conditions. Since micro data on rents are difficult to
find, little is known about this ratio at the individual-property level.
In this project, I analyse a real estate agency's proprietary dataset
containing tens of thousands of rental transactions in Central London
during the 2006-2012 period. I merge it with the Land Registry and
isolate 1,922 properties which were both sold and rented out within a
six-month period. I measure their price-rent ratios and show that
price-rent ratios are higher for bigger and more central units.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Philippe Bracke</dc:creator><pubDate>Sun, 23 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-23:pydata-london-2014/house-prices-and-rents-evidence-from-a-matched-dataset-in-central-london.html</guid></item><item><title>Python for High Throughput Science</title><link>https://pyvideo.org/pydata-london-2014/python-for-high-throughput-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Diamond Light Source is the UK Synchrotron, a national facility
containing over 20 experimental stations or beamlines, many of which are
capable of generating Terra-bytes of raw data every day. In this data
rich environment, many scientists that come to the facility can be
daunted by the sheer quantity and complexity of the data on offer. The
scientific software group is charged with assisting with this deluge of
data and as a small team it is imperative that provides sustainable and
rapid solutions to problems. Python has proved to be well suited to this
and is now used heavily at the facility, from cutting edge research
projects, through general pipe-lining and data management, to simple
data manipulation scripts. And by a range of staff and facility users,
from experienced software engineers and scientists, to support staff and
PhD students simply wanting something to help make sense of the data or
experimental set-up.&lt;/p&gt;
&lt;p&gt;This presentation focuses on the current state of the scientific
management and data analysis within Diamond, and details the workhorses
which are relied on, as well as what the future holds.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mark Basham</dc:creator><pubDate>Sun, 23 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-23:pydata-london-2014/python-for-high-throughput-science.html</guid></item><item><title>Writing a simple backend framework for 1-line AB tests in Django</title><link>https://pyvideo.org/pydata-london-2014/writing-a-simple-backend-framework-for-1-line-ab-tests-in-django.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Making product decisions is hard - often, we take one step forward, but
two steps back. We need to become scientists about our own product - run
an AB test comparing two versions and measure which works better. We'll
talk through a simple but effective backend AB testing framework in
Python (using Django as an example), along with some of the issues,
gotchas and best practices of running AB tests in production.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Greg Detre</dc:creator><pubDate>Sun, 23 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-23:pydata-london-2014/writing-a-simple-backend-framework-for-1-line-ab-tests-in-django.html</guid></item><item><title>A Beginner's guide to Random Forests - R vs Python</title><link>https://pyvideo.org/pydata-london-2014/a-beginners-guide-to-random-forests-r-vs-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I will give an overview of Random Forests and will show
their versatility when attempting to predict song ratings using the EMI
music data set. I will present results using the randomForest library in
R and the scikit learn implementation in Python and discuss their
performance. I will also comment on their ease of use from a beginner's
point of view.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Linda Uruchurtu</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/a-beginners-guide-to-random-forests-r-vs-python.html</guid></item><item><title>Hierarchical Text Classification using Python (and friends)</title><link>https://pyvideo.org/pydata-london-2014/hierarchical-text-classification-using-python-and-friends.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I will describe a system that we've built for doing
hierarchical text classification. I will describe the logical setup of
the various steps involved: data processing, feature selection,
training, validation and labelling. To make this all work in practice
we've mapped the setup onto a Hadoop cluster. I'll discuss some of the
pro's and con's that we've run into when working with Python and Hadoop.
Finally, I'll discuss how we use crowdsourcing to continuously improve
the quality of our hierarchical classifier.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jurgen Van Gael</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/hierarchical-text-classification-using-python-and-friends.html</guid></item><item><title>Life after matplotlib: Harder, Better, Faster, Stronger</title><link>https://pyvideo.org/pydata-london-2014/life-after-matplotlib-harder-better-faster-stronger.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;AvoPlot is a simple graphical plotting program created by scientists,
for scientists. Born out of the frustrations of a multi-disciplinary
research group working in the lab, the field and a desk, it aims to
solve some of the greatest remaining problems in scientific data
handling, such as:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&amp;quot;I hate programming! I wish I could create an interactive plot of
this data file without having to write a script.&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;This plot that matplotlib created is beautiful! But I wish I could
just click and drag that title a bit to the left&amp;quot;.&lt;/li&gt;
&lt;li&gt;&amp;quot;I have this great bit of data processing code and I'd like to make
it available as a graphical application, but I have neither the time
nor the programming ability to do so.&amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot;Spreadsheets suck! I want to work with my data in a visual way,
rather than having to deal with tables of numbers.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Built on top of Python's excellent matplotlib plotting library,
AvoPlot is a graphical plotting tool designed for visualising and
analysing scientific data. In addition to providing a graphical user
interface to many of the capabilities of matplotlib, it also provides
a plug-in framework, allowing users to extend its standard feature
set to meet their specific requirements. Plug-ins can be written both
to import different data sets (e.g. binary data), and to provide
analysis tools for working with them (e.g. fitting routines,
background subtraction etc.). This talk will take a light-hearted
look at some of the data handling problems encountered by scientists,
and explain how we have brought together the capabilities of many
well established Python packages into a single convenient application
in an attempt to solve them.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kayla Iacovino</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/life-after-matplotlib-harder-better-faster-stronger.html</guid></item><item><title>Massively Parallel Processing with Procedural Python</title><link>https://pyvideo.org/pydata-london-2014/massively-parallel-processing-with-procedural-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Python data ecosystem has grown beyond the confines of single
machines to embrace scalability. Here we describe one of our approaches
to scaling, which is already being used in production systems. The goal
of in-database analytics is to bring the calculations to the data,
reducing transport costs and I/O bottlenecks. Using PL/Python we can run
parallel queries across terabytes of data using not only pure SQL but
also familiar PyData packages such as scikit- learn and nltk. This
approach can also be used with PL/R to make use of a wide variety of R
packages. We look at examples on Postgres compatible systems such as the
Greenplum Database and on Hadoop through Pivotal HAWQ. We will also
introduce MADlib, Pivotalâ€™s open source library for scalable in-database
machine learning, which uses Python to glue SQL queries to low level C++
functions and is also usable through the PyMADlib package.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ian Huston</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/massively-parallel-processing-with-procedural-python.html</guid></item><item><title>Measuring the digital economy using big data</title><link>https://pyvideo.org/pydata-london-2014/measuring-the-digital-economy-using-big-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The UK has one of strongest digital economies in the world, yet the size
and scope of this space is poorly understood through conventional
classification systems and datasets. This is not only important for
economists but for the people affected by Government policy. In this
talk, we will provide an overview of the work Growth Intelligence has
done in generating our own classification system to describe businesses
using web-based data and python tools and techniques â€“ including the
challenges we still face. We will also look at some situations in which
traditional methods fall short of a data driven approach.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Prash Majmudar</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/measuring-the-digital-economy-using-big-data.html</guid></item><item><title>Most Winning A/B Test Results are Illusory</title><link>https://pyvideo.org/pydata-london-2014/most-winning-ab-test-results-are-illusory.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many people have started to suspect that their A/B testing results are
not what they seem. A/B test reports an uplift of 20% and yet this
increase never seems to translate into increased profits. So whatâ€™s
going on? I'll use python simulations to show that A/B testing is often
conducted in such a way that it virtually guarantees false positive
results. I'll also mention some python functions that can be used to
avoid these problems.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Goodson</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/most-winning-ab-test-results-are-illusory.html</guid></item><item><title>Python and MongoDB as a platform for financial market data</title><link>https://pyvideo.org/pydata-london-2014/python-and-mongodb-as-a-platform-for-financial-market-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As businesses search for diversification by trading new financial
products, it is easy for market data infrastructure to become fragmented
and inconsistent. We describe how we have successfully used Python,
Pandas and MongoDB to build a market data system that stores a variety
of Timeseries-based financial data for research and live trading at a
large systematic hedge fund. Our system has a simple, high-performance
schema, a consistent API for all data access, and built-in support for
data versioning and deduplication. We support fast interactive access to
the data by quants, as well as clustered batch processing by running a
dynamic data flow graph on a cluster.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">James Blackburn</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/python-and-mongodb-as-a-platform-for-financial-market-data.html</guid></item><item><title>Python for Optimization</title><link>https://pyvideo.org/pydata-london-2014/python-for-optimization.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Much of what we want to do with data involves optimization: whether
it's to find a model that best fits the data, or to decide on the
optimal action given some information.&lt;/p&gt;
&lt;p&gt;We'll explore the embarrassment of riches Python offers to tackle
custom optimization problems: the scipy.optimize package, Sympy for
calculus and code generation, Cython for speedups and binding to
external libraries.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ben Moran</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/python-for-optimization.html</guid></item><item><title>Shared Memory Parallelism with Python</title><link>https://pyvideo.org/pydata-london-2014/shared-memory-parallelism-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python threads cannot utilize the power of multiple CPUs. Other
solutions such multiprocessing or MPI wrapper are based on message
passing, resulting substantial overhead for certain types of tasks.&lt;/p&gt;
&lt;p&gt;While pure Python does not support shared memory calculations, Cython
combined with OpenMP can provide full access to this type of parallel
data processing.&lt;/p&gt;
&lt;p&gt;This talk gives a whirlwind tour of Cython and introduces Cython's
OpenMP abilities focusing on parallel loops over NumPy arrays. Source
code examples demonstrate how to use OpenMP from Python. Results for
parallel algorithms with OpenMP show what speed-ups can be achieved for
different data sizes compared to other parallelizing strategies.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mike Mueller</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/shared-memory-parallelism-with-python.html</guid></item><item><title>The High Performance Python Landscape</title><link>https://pyvideo.org/pydata-london-2014/the-high-performance-python-landscape.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A Python programmer has many options to profile and optimize CPU-bound
and data-bound systems, common solutions include Cython, numpy and PyPy.
Increasingly we have single-core solutions that should take advantage of
many cores and clusters. This talk reviews the current state of the art,
looking at the compromises and outcomes of the current approaches and
reviews upcoming solutions like Numba, Pythran and PyPyâ€™s numpy.
Thoughts will be shared on how current hindrances might be improved.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ian Ozsvald</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/the-high-performance-python-landscape.html</guid></item><item><title>You give me data, I give you art.</title><link>https://pyvideo.org/pydata-london-2014/you-give-me-data-i-give-you-art.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data and algorithms are artistic materials just as much as paint and
canvas.&lt;/p&gt;
&lt;p&gt;A talk covering my recent work with The Tate's CC dataset, David
Cameron's deleted speeches and the role of the artist in the world of
Big Data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eric Drass</dc:creator><pubDate>Sat, 22 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-22:pydata-london-2014/you-give-me-data-i-give-you-art.html</guid></item><item><title>An introduction to video action recognition</title><link>https://pyvideo.org/pydata-london-2014/an-introduction-to-video-action-recognition.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At WIDE IO, we are specialists in image processing and video analytics;
we have individual experience using Python, Numpy and Scipy for Computer
Vision applications since 2007. Now, the environment has become much
mature. Our goal with this talk is to share our enthusiasm and to
present the basic steps required to perform image and video pattern
analysis with Python. In our tutorial, weâ€™ll investigate how to build an
action recognition framework and how to do video-tracking with
traditional vision models based on a bag-of- keypoints. By going through
examples, weâ€™ll discuss how in practice computer vision for
real-applications involve a trade-off between esthetical theories and
utilitarianism. We will explore the various tricks that allow engineers
to boost global performances, methods for running experiments and a
mechanism for how to prepare the data... All these points are just a
nice pretext to discuss our favorite tools: Numpy and Scipy of course,
but also more exotic ones such as MediaLovinToolkit, PyCUDA, Bob and
PyCVF... At the end of the talk, weâ€™ll conclude by briefly discussing
future imperatives, especially with respect to mobility and cloud
computing.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bertrand Nouvel</dc:creator><pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-21:pydata-london-2014/an-introduction-to-video-action-recognition.html</guid></item><item><title>Bokeh - Interactive Visualization for Large Datasets</title><link>https://pyvideo.org/pydata-london-2014/bokeh-interactive-visualization-for-large-datasets.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Bokeh is a Python interactive visualization library for large datasets
that natively uses the latest web technologies. Its goal is to provide
elegant, concise construction of novel graphics in the style of
Protovis/D3, while delivering high-performance interactivity over large
data to thin clients. This tutorial will walk users through the steps to
create different kinds of interactive plots using Bokeh. We will cover
using Bokeh for static HTML output, the IPython notebook, and plot
hosting and embedding using the Bokeh server.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bryan Van De Ven</dc:creator><pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-21:pydata-london-2014/bokeh-interactive-visualization-for-large-datasets.html</guid><category>tutorial</category></item><item><title>Eurex Tutorial - Interactive Financial Analytics with Python and IPython -- With Examples Based on the VSTOXX Volatility Index</title><link>https://pyvideo.org/pydata-london-2014/eurex-tutorial-interactive-financial-analytics-with-python-and-ipython-with-examples-based-on-the-vstoxx-volatility-index.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Today's financial market environment demands for ever shorter
times-to-insight when it comes to financial analytics tasks. For the
analysis of financial times series or for typical tasks related to
derivatives analytics and trading, Python has developed to the ideal
technology platform.&lt;/p&gt;
&lt;p&gt;Not only that Python provides powerful and efficient libraries for data
analytics, such as NumPy or pandas. With IPython there is a tool and
environment available that facilitates interactive, and even real-time,
financial analytics tremendously.&lt;/p&gt;
&lt;p&gt;The tutorial introduces into IPython and shows, mainly on the basis of
practical examples related to the VSTOXX volatility index, how Python
and IPython might re-define interactive financial analytics.&lt;/p&gt;
&lt;p&gt;Quants, traders, financial engineers, analysts, financial researchers,
model validators and the like all benefit from the tutorial and the new
technologies provided by the Python ecosystem.&lt;/p&gt;
&lt;div class="section" id="background"&gt;
&lt;h4&gt;BACKGROUND&lt;/h4&gt;
&lt;p&gt;For background information see the Python-based &amp;quot;VSTOXX Advanced
Services&amp;quot; and the related backtesting applications:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.eurexchange.com/vstoxx/"&gt;http://www.eurexchange.com/vstoxx/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.eurexchange.com/vstoxx/app1/"&gt;http://www.eurexchange.com/vstoxx/app1/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.eurexchange.com/vstoxx/app2/"&gt;http://www.eurexchange.com/vstoxx/app2/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="technical-requirements"&gt;
&lt;h4&gt;TECHNICAL REQUIREMENTS&lt;/h4&gt;
&lt;p&gt;To follow the tutorial, you should have installed the Anaconda Python
distribution on your notebook. Download and follow the instructions
here:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
http://continuum.io/downloads
&lt;/pre&gt;
&lt;p&gt;After installation, start IPython from the command line interface/shell
as follows:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ ipython notebook --pylab inline
&lt;/pre&gt;
&lt;p&gt;IPython should then start in your default Web browser.&lt;/p&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yves Hilpisch</dc:creator><pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2014-02-21:pydata-london-2014/eurex-tutorial-interactive-financial-analytics-with-python-and-ipython-with-examples-based-on-the-vstoxx-volatility-index.html</guid></item></channel></rss>