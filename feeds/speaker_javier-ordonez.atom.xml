<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_javier-ordonez.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-10-07T00:00:00+00:00</updated><entry><title>The bad guys in AI: atacando sistemas de machine learning</title><link href="https://pyvideo.org/pycon-es-2018/the-bad-guys-in-ai-atacando-sistemas-de-machine-learning.html" rel="alternate"></link><published>2018-10-07T00:00:00+00:00</published><updated>2018-10-07T00:00:00+00:00</updated><author><name>Alicia Pérez</name></author><id>tag:pyvideo.org,2018-10-07:pycon-es-2018/the-bad-guys-in-ai-atacando-sistemas-de-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Las redes generativas antagónicas (conocidas como GANs por sus siglas en inglés) son una de las tecnologías más revolucionarias de los últimos años, no obstante han sido incluidas por Forbes en la lista de las mejores innovaciones de los últimos tres años e identificadas por MIT Tech Review como una de las tecnologías más prometedoras en 2018.&lt;/p&gt;
&lt;p&gt;Técnicamente pueden definirse como un tipo específico de red neuronal, concebida especialmente para imitar la redes neuronales clásicas, aquellas que a día de hoy se conocen también como deep learning. Son capaces de aprender qué está modelando una red neuronal y generar datos válidos en su dominio desde cero. Esto significa que si una red neuronal categoriza imágenes de caras, una red generativa antagónica asociada aprenderá a generar nuevas imágenes de caras, ¡de gente que no existe!&lt;/p&gt;
&lt;p&gt;Una de las consecuencias de tener dicha capacidad de imitación es que se pueden configurar para engañar a las redes neuronales clásicas, generando datos que no es posible detectar como falsos. Este caso de uso tiene su lado oscuro, ya que el poder generar datos sintéticos indistinguibles de los datos reales puede usarse para atacar redes neuronales que estén integradas en sistemas críticos y de seguridad. La técnica para explotar esta vulnerabilidad se ha bautizado como ataque antagónico o adversarial attack.&lt;/p&gt;
&lt;p&gt;En esta charla veremos primero una pequeña introducción sobre redes neuronales y explicaremos qué son las redes generativas antagónicas. Después veremos cómo las redes generativas aprenden a imitar otras redes, hasta el punto de lograr engañarlas, y por qué esto supone una vulnerabilidad en nuestros algoritmos de machine learning. Para ilustrar un ataque antagónico usaremos un ejemplo en Python para engañar a un sistema de reconocimiento de imágenes. Por último discutiremos por qué este problema es importante y las enormes consecuencias que puede llegar a tener en nuestros sistemas de machine learning.&lt;/p&gt;
</summary></entry></feed>