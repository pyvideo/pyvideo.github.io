<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Negar Kiyavash</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_negar-kiyavash.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Model-Augmented Conditional Mutual Information Estimation for Feature Selection</title><link href="https://pyvideo.org/uai-2020/model-augmented-conditional-mutual-information-estimation-for-feature-selection.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Alan Yang</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/model-augmented-conditional-mutual-information-estimation-for-feature-selection.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Model-Augmented Conditional Mutual Information Estimation for Feature Selection&lt;/p&gt;
&lt;p&gt;Alan Yang (University of Illinois at Urbana-Champaign)*; AmirEmad Ghassami (UIUC); Maxim Raginsky (University of Illinois); Negar Kiyavash (École Polytechnique Fédérale de Lausanne); Elyse Rosenbaum (University of Illinois at Urbana-Champaign)&lt;/p&gt;
&lt;p&gt;Markov blanket feature selection, while theoretically optimal, is generally challenging to implement …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Model-Augmented Conditional Mutual Information Estimation for Feature Selection&lt;/p&gt;
&lt;p&gt;Alan Yang (University of Illinois at Urbana-Champaign)*; AmirEmad Ghassami (UIUC); Maxim Raginsky (University of Illinois); Negar Kiyavash (École Polytechnique Fédérale de Lausanne); Elyse Rosenbaum (University of Illinois at Urbana-Champaign)&lt;/p&gt;
&lt;p&gt;Markov blanket feature selection, while theoretically optimal, is generally challenging to implement. This is due to the shortcomings of existing approaches to conditional independence (CI) testing, which tend to struggle either with the curse of dimensionality or computational complexity. We propose a novel two-step approach which facilitates Markov blanket feature selection in high dimensions. First, neural networks are used to map features to low-dimensional representations. In the second step, CI testing is performed by applying the $k$-NN conditional mutual information estimator to the learned feature maps. The mappings are designed to ensure that mapped samples both preserve information and share similar information about the target variable if and only if they are close in Euclidean distance. We show that these properties boost the performance of the $k$-NN estimator in the second step. The performance of the proposed method is evaluated on both synthetic and real data.&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>