<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_gerben-oostra.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-11-30T00:00:00+00:00</updated><entry><title>Preventing churn like a bandit</title><link href="https://pyvideo.org/pydata-eindhoven-2019/preventing-churn-like-a-bandit.html" rel="alternate"></link><published>2019-11-30T00:00:00+00:00</published><updated>2019-11-30T00:00:00+00:00</updated><author><name>Gerben Oostra</name></author><id>tag:pyvideo.org,2019-11-30:pydata-eindhoven-2019/preventing-churn-like-a-bandit.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Losing customers, also referred to as churning, is something that any
company wants to prevent. But not by predicting churn, assuming
correlation is causation, or by acting on prescribed actions. Let me
show how to combine techniques from uplift modelling, causal inference
and reinforcement learning, into one contextual bandit system that
balances exploitation &amp;amp; exploration and deals with biases.&lt;/p&gt;
&lt;p&gt;Losing customers, also referred to as churning, is something that any
company wants to prevent, especially in industries with many subscribed
customers, like Telco, Media, Finance and Insurance. The challenge then
is to determine which, if any, intervention (also referred to as
treatment or countermeasure) can retain a customer. In the Telco case
this could be giving a discount or sending specific emails. In common
approaches I see three issues: to predict churn, to assume correlation
is causation, and to act on the prescribed actions. Please join me in
constructing a more effective solution, combining techniques from uplift
modelling, causal inference and reinforcement learning. Let us solve the
correct business problem, deal with biased historical data, and balance
exploration and exploitation. Along the way the audience will get a good
grasp of possible design choices and techniques, with their
implications. The end result will be a full contextual bandit solution,
utilizing various techniques like inverse propensity scaling, Bayesian
neural networks and Thompson sampling.&lt;/p&gt;
</summary></entry></feed>