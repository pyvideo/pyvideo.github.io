<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_apache.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-10-25T00:00:00+00:00</updated><entry><title>Connecting PyData to other Big Data Landscapes using Arrow and Parquet</title><link href="https://pyvideo.org/pycon-de-2017/connecting-pydata-to-other-big-data-landscapes-using-arrow-and-parquet.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/connecting-pydata-to-other-big-data-landscapes-using-arrow-and-parquet.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Uwe L. Korn&lt;/strong&gt; (&amp;#64;xhochy)&lt;/p&gt;
&lt;p&gt;Uwe Korn is a Data Scientist at the Karlsruhe-based RetailTec company Blue Yonder. His expertise is on building architectures for machine learning services that are scalably usable for multiple customers aiming at high service availability as well as rapid prototyping of solutions to evaluate the feasibility of his design decisions. As part of his work to provide an efficient data interchange he became a core committer to the Apache Parquet and Apache Arrow projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While Python itself hosts a wide range of machine learning and data tools, other ecosystems like the Hadoop world also provide beneficial tools that can be either connected via Apache Parquet files or in memory using Arrow. This talks shows recent developments that allow interoperation at speed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Python has a vast amount of libraries and tools in its machine learning and data analysis ecosystem. Although it is clearly in competition with R here about the leadership, the world that has sprung out of the Hadoop ecosystem has established itself in the space of data engineering and also tries to provide tools for distributed machine learning. As these stacks run in different environments and are mostly developed by distinct groups of people, using them together has been a pain. While Apache Parquet has already proven itself as the gold standard for the exchange of DataFrames serialized to files, Apache Arrow recently got traction as the in-memory format for DataFrame exchange between different ecosystems.&lt;/p&gt;
&lt;p&gt;This talk will outline how Apache Parquet files can be used in Python and how they are structured to provide efficient DataFrame exchange. In addition to small code sample, this also includes an explanation of some interesting details of the file format. Additionally, the idea of Apache Arrow will be presented and taking Apache Spark (2.3) as an example to showcase how performance increases once DataFrames can be efficiently shared between Python and JVM processes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="data-science"></category><category term="hadoop"></category><category term="apache"></category><category term="arrow"></category><category term="parquet"></category><category term="pandas"></category><category term="pydata"></category></entry><entry><title>Python/Django deployment</title><link href="https://pyvideo.org/chipy/python-django-deployment.html" rel="alternate"></link><published>2011-02-10T00:00:00+00:00</published><updated>2011-02-10T00:00:00+00:00</updated><author><name>Rohit Sankaran</name></author><id>tag:pyvideo.org,2011-02-10:chipy/python-django-deployment.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;I'll introduce the WSGI ecosystem. We'll then setup a Django app and
deploy it to a VM. I'll cover server setup/config and best practices and
cover software used like Fabric, nginx etc. Sample nginx, WSGI and
Apache configs will be provided. People can follow along and I'll
provide download links after so they can try it at home. This will be
the best meeting ever.&lt;/p&gt;
</summary><category term="apache"></category><category term="chipy"></category><category term="fabric"></category><category term="nginx"></category><category term="wsgi"></category></entry><entry><title>Django Deployment for the Average Bloke</title><link href="https://pyvideo.org/chipy/django-deployment-for-the-average-bloke.html" rel="alternate"></link><published>2011-01-13T00:00:00+00:00</published><updated>2011-01-13T00:00:00+00:00</updated><author><name>Carl Karsten</name></author><id>tag:pyvideo.org,2011-01-13:chipy/django-deployment-for-the-average-bloke.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Deploying a Django/Pinax site on the following stack: Linux Debian Lenny
Apache mod_wsgi PostgreSQL.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This is the config you want to use if you don't know what you want to
use. You don't need to make any choices, you don't have to do any
research, and you don't even need to read the docs if you don't want to.
And as an added bonus, I show how to wget a vm image to deploy into with
network ports exposed. It is very much like working with a hosting
provider like Slice Host, and it makes a great framework for testing
server installs.&lt;/p&gt;
</summary><category term="apache"></category><category term="chipy"></category><category term="deployment"></category><category term="mod_wsgi"></category><category term="pinax"></category><category term="postgresql"></category><category term="web"></category></entry></feed>