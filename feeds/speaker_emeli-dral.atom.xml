<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Emeli Dral</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_emeli-dral.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-04-17T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Detecting drift: how to evaluate and explore data drift in machine learning systems</title><link href="https://pyvideo.org/pycon-de-2022/detecting-drift-how-to-evaluate-and-explore-data-drift-in-machine-learning-systems.html" rel="alternate"></link><published>2022-05-12T00:00:00+00:00</published><updated>2022-05-12T00:00:00+00:00</updated><author><name>Emeli Dral</name></author><id>tag:pyvideo.org,2022-05-12:/pycon-de-2022/detecting-drift-how-to-evaluate-and-explore-data-drift-in-machine-learning-systems.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker:: Emeli Dral&lt;/p&gt;
&lt;p&gt;Track: PyData: Machine Learning &amp;amp; Stats
When your ML model is in production, you might observe input data and prediction drift. In absence of ground truth, drift can serve as a proxy for the model performance. But how exactly to evaluate it? In this talk, I will â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speaker:: Emeli Dral&lt;/p&gt;
&lt;p&gt;Track: PyData: Machine Learning &amp;amp; Stats
When your ML model is in production, you might observe input data and prediction drift. In absence of ground truth, drift can serve as a proxy for the model performance. But how exactly to evaluate it? In this talk, I will give an overview of the possible approaches, and how to implement and visualize the results.&lt;/p&gt;
&lt;p&gt;Recorded at the PyConDE &amp;amp; PyData Berlin 2022 conference, April 11-13 2022.
&lt;a class="reference external" href="https://2022.pycon.de"&gt;https://2022.pycon.de&lt;/a&gt;
More details at the conference page: &lt;a class="reference external" href="https://2022.pycon.de/program/ASW8CJ"&gt;https://2022.pycon.de/program/ASW8CJ&lt;/a&gt;
Twitter: &lt;a class="reference external" href="https://twitter.com/pydataberlin"&gt;https://twitter.com/pydataberlin&lt;/a&gt;
Twitter: &lt;a class="reference external" href="https://twitter.com/pyconde"&gt;https://twitter.com/pyconde&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon DE 2022"></category><category term="PyCon"></category><category term="PyConDE"></category><category term="pyconde2022"></category><category term="pydata"></category><category term="PyDataBerlin"></category><category term="pydataberlin2022"></category></entry><entry><title>Staying Alert - How to Implement Continuous Testing for Machine Learning Models</title><link href="https://pyvideo.org/pycon-de-2023/staying-alert-how-to-implement-continuous-testing-for-machine-learning-models.html" rel="alternate"></link><published>2023-04-17T00:00:00+00:00</published><updated>2023-04-17T00:00:00+00:00</updated><author><name>Emeli Dral</name></author><id>tag:pyvideo.org,2023-04-17:/pycon-de-2023/staying-alert-how-to-implement-continuous-testing-for-machine-learning-models.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Proper monitoring of machine learning models in production is essential to avoid performance issues. Setting up monitoring can be easy for a single model, but it often becomes challenging at scale or when you face alert fatigue based on many metrics and dashboards.&lt;/p&gt;
</content><category term="PyCon DE 2023"></category></entry></feed>