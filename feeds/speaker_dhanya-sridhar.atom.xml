<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Dhanya Sridhar</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_dhanya-sridhar.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-07-31T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Adapting Text Embeddings for Causal Inference</title><link href="https://pyvideo.org/uai-2020/adapting-text-embeddings-for-causal-inference.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Victor Veitch</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/adapting-text-embeddings-for-causal-inference.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Adapting Text Embeddings for Causal Inference&lt;/p&gt;
&lt;p&gt;Victor Veitch (Columbia University)*; Dhanya Sridhar (Columbia University); David Blei (Columbia University)&lt;/p&gt;
&lt;p&gt;Does adding a theorem to a paper affect its chance of acceptance? Does labeling a post with the author’s gender affect the post popularity? This paper develops a method to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Adapting Text Embeddings for Causal Inference&lt;/p&gt;
&lt;p&gt;Victor Veitch (Columbia University)*; Dhanya Sridhar (Columbia University); David Blei (Columbia University)&lt;/p&gt;
&lt;p&gt;Does adding a theorem to a paper affect its chance of acceptance? Does labeling a post with the author’s gender affect the post popularity? This paper develops a method to estimate such causal effects from observational text data, adjusting for confounding features of the text such as the subject or writing quality. We assume that the text suffices for causal adjustment but that, in practice, it is prohibitively high-dimensional. To address this challenge, we develop causally sufficient embeddings, low- dimensional document representations that preserve sufficient information for causal identification and allow for efficient estimation of causal effects. Causally sufficient embeddings combine two ideas. The first is supervised dimensionality reduction: causal adjustment requires only the aspects of text that are predictive of both the treatment and outcome. The second is efficient language modeling: representations of text are designed to dispose of linguistically irrelevant information, and this information is also causally irrelevant. Our method adapts language models (specifically, word embeddings and topic models) to learn document embeddings that are able to predict both treatment and outcome. We study causally sufficient embeddings with semi-synthetic datasets and find that they improve causal estimation over related embedding methods. We illustrate the methods by answering the two motivating questions---the effect of a theorem on paper acceptance and the effect of a gender label on post popularity. Code and data available at github.com/vveitch/causal-text-embeddings-tf2.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry><entry><title>Causal Representation Learning</title><link href="https://pyvideo.org/uai-2023/causal-representation-learning.html" rel="alternate"></link><published>2023-07-31T00:00:00+00:00</published><updated>2023-07-31T00:00:00+00:00</updated><author><name>Dhanya Sridhar</name></author><id>tag:pyvideo.org,2023-07-31:/uai-2023/causal-representation-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Causal Representation Learning&amp;quot;
Dhanya Sridhar, Jason Hartford&lt;/p&gt;
&lt;p&gt;Causal Representation Learning (CRL) is an emerging area of research that seeks to address an important gap in the field of causality: how can we learn causal models and mechanisms without direct measurements of all the variables? To this end, CRL combines …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Causal Representation Learning&amp;quot;
Dhanya Sridhar, Jason Hartford&lt;/p&gt;
&lt;p&gt;Causal Representation Learning (CRL) is an emerging area of research that seeks to address an important gap in the field of causality: how can we learn causal models and mechanisms without direct measurements of all the variables? To this end, CRL combines recent advances in machine learning withnew assumptions that guarantee that causal variables can be identified up to some indeterminacies from low-level observations such as text, images or biological measurements. In this tutorial, we will review the broad classes of assumptions driving CRL. We strive to build strong intuitions about the core technical problems underpinning CRL and draw connections across different results. We will conclude the tutorial by discussing open questions for CRL, motivated by the kind of methods we would need if we wanted to extend causal models to scientific discovery.&lt;/p&gt;
</content><category term="UAI 2023"></category><category term="tutorial"></category></entry></feed>