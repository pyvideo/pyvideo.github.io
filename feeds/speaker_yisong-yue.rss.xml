<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Yisong Yue</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 03 Aug 2020 00:00:00 +0000</lastBuildDate><item><title>PyTorch in Robotics</title><link>https://pyvideo.org/pytorch-conference-2019/pytorch-in-robotics.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Learn about the research work at Caltech focused on risk aware machine learning for dynamic robotics control, and how PyTorch is being used to build deep learning systems for projects like the neural lander.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yisong Yue</dc:creator><pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-16:/pytorch-conference-2019/pytorch-in-robotics.html</guid><category>PyTorch Conference 2019</category><category>AI</category><category>AI research</category><category>Artificial Intelligence</category><category>Caltech</category><category>Facebook</category><category>ML</category><category>Machine Learning</category><category>PyTorch</category><category>PyTorch 1.3</category><category>Robotics</category><category>dynamic robotics control</category></item><item><title>Dueling Posterior Sampling for Preference-Based Reinforcement Learning</title><link>https://pyvideo.org/uai-2020/dueling-posterior-sampling-for-preference-based-reinforcement-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Dueling Posterior Sampling for Preference-Based Reinforcement Learning&lt;/p&gt;
&lt;p&gt;Ellen Novoseller (California Institute of Technology)*; Yibing Wei (California Institute of Technology); Yanan Sui (Tsinghua University); Yisong Yue (Caltech); Joel Burdick (Caltech)&lt;/p&gt;
&lt;p&gt;In preference-based reinforcement learning (RL), an agent interacts with the environment while receiving preferences instead of absolute feedback. While there is increasing research activity in preference-based RL, the design of formal frameworks that admit tractable theoretical analysis remains an open challenge. Building upon ideas from preference-based bandit learning and posterior sampling in RL, we present DUELING POSTERIOR SAMPLING (DPS), which employs preference-based posterior sampling to learn both the system dynamics and the underlying utility function that governs the preference feedback. As preference feedback is provided on trajectories rather than individual state-action pairs, we develop a Bayesian approach for the credit assignment problem, translating  preferences to a posterior distribution over state-action reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a Bayesian linear regression credit assignment model. This is the first regret guarantee for preference-based RL to our knowledge. We also discuss possible avenues for extending the proof methodology to  other credit assignment models. Finally, we evaluate the approach empirically, showing competitive performance against existing baselines.&amp;quot;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ellen Novoseller</dc:creator><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2020-08-03:/uai-2020/dueling-posterior-sampling-for-preference-based-reinforcement-learning.html</guid><category>UAI 2020</category></item></channel></rss>