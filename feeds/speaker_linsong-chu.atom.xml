<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Linsong Chu</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_linsong-chu.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-09-18T00:00:00+00:00</updated><subtitle></subtitle><entry><title>A Distributed Stateful Dataloader for Large-Scale Pretraining</title><link href="https://pyvideo.org/pytorch-conference-2024/a-distributed-stateful-dataloader-for-large-scale-pretraining.html" rel="alternate"></link><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><author><name>Davis Wertheimer</name></author><id>tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/a-distributed-stateful-dataloader-for-large-scale-pretraining.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A Distributed Stateful Dataloader for Large-Scale Pretraining - Davis Wertheimer, IBM &amp;amp; Linsong Chu, IBM Research&lt;/p&gt;
&lt;p&gt;Large-scale model pretraining crucially relies on specialized and dedicated dataloaders that can, for example, partition and stream data asynchronously across multiple processes and physical nodes. In this talk we discuss one of the torch-native dataloaders …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A Distributed Stateful Dataloader for Large-Scale Pretraining - Davis Wertheimer, IBM &amp;amp; Linsong Chu, IBM Research&lt;/p&gt;
&lt;p&gt;Large-scale model pretraining crucially relies on specialized and dedicated dataloaders that can, for example, partition and stream data asynchronously across multiple processes and physical nodes. In this talk we discuss one of the torch-native dataloaders we built and use at IBM Research for addressing these needs. Intended for use in large-scale model pretraining, particularly in research settings where rapid iteration between datasets may be required, our dataloader is distributed, stateful, checkpointable, composable and rescalable – while remaining a simple extension of the existing PyTorch dataloading framework. It automatically and invisibly handles data sharding, shuffling, subdataset weighting, checkpoint saving and loading, and custom user-defined preprocessing functions, with minimal overhead and high throughput. We discuss these properties and how we achieved them, such as reducing overhead by implementing a custom LCG random number generator, and demonstrate proof of concept on production-scale training of a 7B parameter Llama model over 4 trillion tokens.&lt;/p&gt;
</content><category term="PyTorch Conference 2024"></category></entry><entry><title>Maximizing Training Throughput Using Torch.Compile and FSDP</title><link href="https://pyvideo.org/pytorch-conference-2024/maximizing-training-throughput-using-torchcompile-and-fsdp.html" rel="alternate"></link><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><author><name>Linsong Chu</name></author><id>tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/maximizing-training-throughput-using-torchcompile-and-fsdp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;torch.compile is a graph compilation technique that improves GPU utilization. A key challenge in getting torch.compile to perform well is to minimize (or eliminate) graph breaks, however, this isn't trivial as even the Llama implementation provided by Meta has many graph breaks resulting in reduced training throughput …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;torch.compile is a graph compilation technique that improves GPU utilization. A key challenge in getting torch.compile to perform well is to minimize (or eliminate) graph breaks, however, this isn't trivial as even the Llama implementation provided by Meta has many graph breaks resulting in reduced training throughput. In this talk we discuss 1. how we addressed these challenges in order to train a model using torch.compile 2. how we combined torch.compile with FSDP and selective activation checkpointing to achieve the maximum throughput for training 3. model quality comparison between models trained with compile and no-compile, and lastly 4. the best setup we have for different model sizes in the Llama family that achieves the maximum throughput and MFU number (e.g. 68% MFU for the 7B model on A100 GPUs!)&lt;/p&gt;
</content><category term="PyTorch Conference 2024"></category></entry><entry><title>Torchtitan: Large-Scale LLM Training Using Native PyTorch 3D Parallelism</title><link href="https://pyvideo.org/pytorch-conference-2024/torchtitan-large-scale-llm-training-using-native-pytorch-3d-parallelism.html" rel="alternate"></link><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><author><name>Wanchao Liang</name></author><id>tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/torchtitan-large-scale-llm-training-using-native-pytorch-3d-parallelism.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;torchtitan is a proof-of-concept for Large-scale LLM training using native PyTorch. It is a repo that showcases PyTorch's latest distributed training features in a clean, minimal codebase. We show-cased end to end large scale training features enablement: 1. 3D/4D Parallelism 2. Efficient distributed checkpoint save/load/resharding 3 …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;torchtitan is a proof-of-concept for Large-scale LLM training using native PyTorch. It is a repo that showcases PyTorch's latest distributed training features in a clean, minimal codebase. We show-cased end to end large scale training features enablement: 1. 3D/4D Parallelism 2. Efficient distributed checkpoint save/load/resharding 3. Many efficient training techniques including Float8, torch.compile, activation checkpoint, etc.&lt;/p&gt;
</content><category term="PyTorch Conference 2024"></category></entry></feed>