<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Sahil Manchanda</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 11 Jul 2022 00:00:00 +0000</lastBuildDate><item><title>Machine Translation engines evaluation framework</title><link>https://pyvideo.org/europython-2022/machine-translation-engines-evaluation-framework.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;EuroPython 2022 - Machine Translation engines evaluation framework - presented by Anton Masalovich &amp;amp; Sahil Manchanda&lt;/p&gt;
&lt;p&gt;[Liffey Hall 1 on 2022-07-15]&lt;/p&gt;
&lt;p&gt;Task of Machine Translation engine evaluation may be very challenging. Quality of Machine Translation varies greatly depending on domain and language pair. Different MT engines may have different interfaces or APIs and different requirements to run. To add to that, even definition of a good translation may be debatable, with any automatic MT quality metric providing only approximation of actual translation quality. That's why having universal evaluation framework for this task is very important. In our work we tried to create such framework.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;We defined base translation class that unified all file handling, batch creation and result processing. As a result of that, only work needed to support new MT engine was creation of small child class that implemented couple of simple functions. That allows us to easily extend our framework to MT engines and new language pairs.&lt;/li&gt;
&lt;li&gt;We defined set of test datasets and provided a way to add new datasets to this set. For our evaluation our aim was to create test data that covers both general and healthcare domains EMEA dataset (&lt;a class="reference external" href="https://opus.nlpl.eu/EMEA.php"&gt;https://opus.nlpl.eu/EMEA.php&lt;/a&gt;), OPUS-100 (&lt;a class="reference external" href="https://opus.nlpl.eu/opus-100.php"&gt;https://opus.nlpl.eu/opus-100.php&lt;/a&gt;), Paracrawl (&lt;a class="reference external" href="https://paracrawl.eu/"&gt;https://paracrawl.eu/&lt;/a&gt;) and several others. But our data preparations scripts can be easily extended to other domains and datasets as well.&lt;/li&gt;
&lt;li&gt;We defined a set of quality metrics to evaluate results of MT engines. Metrics that we used included BLEU (&lt;a class="reference external" href="https://github.com/mjpost/sacrebleu"&gt;https://github.com/mjpost/sacrebleu&lt;/a&gt;), BERTScore (&lt;a class="reference external" href="https://github.com/Tiiiger/bert_score"&gt;https://github.com/Tiiiger/bert_score&lt;/a&gt;), ROUGE (&lt;a class="reference external" href="https://github.com/pltrdy/rouge"&gt;https://github.com/pltrdy/rouge&lt;/a&gt;), TER and CHRF (both also from sacrebleu implementation).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Beside MT evaluation framework we will present our own evaluation results. For our evaluation we used cloud based engines - Azure Translator (&lt;a class="reference external" href="https://azure.microsoft.com/en-us/services/cognitive-services/translator/"&gt;https://azure.microsoft.com/en-us/services/cognitive-services/translator/&lt;/a&gt;), Google Translate (&lt;a class="reference external" href="https://cloud.google.com/translate/"&gt;https://cloud.google.com/translate/&lt;/a&gt;), as well as open-source engines - Marian MT (&lt;a class="reference external" href="https://huggingface.co/transformers/model_doc/marian.html"&gt;https://huggingface.co/transformers/model_doc/marian.html&lt;/a&gt;), NVIDIA's NeMo (&lt;a class="reference external" href="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/machine_translation.html"&gt;https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/machine_translation.html&lt;/a&gt;), Facebook's MBart 50 (&lt;a class="reference external" href="https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt"&gt;https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt&lt;/a&gt;), Facebook's M2M100 (&lt;a class="reference external" href="https://huggingface.co/facebook/m2m100_418M"&gt;https://huggingface.co/facebook/m2m100_418M&lt;/a&gt;). For open source engines we tried to use Huggingface's transformer implementation whenever possible. But as we mentioned our framework was designed in a way to be easily extendable to other MT engines and underlying frameworks.&lt;/p&gt;
&lt;p&gt;We also will present evaluation results for NeMo and MarianMT engines that we fine-tuned specifically for healthcare domain. While these particular results may rather specific to our use case, they help to highlight how our framework can be extended to custom MT engines as well.&lt;/p&gt;
&lt;p&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License &lt;a class="reference external" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;http://creativecommons.org/licenses/by-nc-sa/4.0/&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anton Masalovich</dc:creator><pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2022-07-11:/europython-2022/machine-translation-engines-evaluation-framework.html</guid><category>EuroPython 2022</category></item></channel></rss>