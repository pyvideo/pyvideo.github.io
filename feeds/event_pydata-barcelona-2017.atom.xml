<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-barcelona-2017.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-05-21T17:30:00+02:00</updated><entry><title>Security for Data Scientists</title><link href="https://pyvideo.org/pydata-barcelona-2017/security-for-data-scientists.html" rel="alternate"></link><published>2017-05-21T17:30:00+02:00</published><updated>2017-05-21T17:30:00+02:00</updated><author><name>David Arcos</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/security-for-data-scientists.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Handling confidential data attracts unwanted attention from hostile attackers.&lt;/p&gt;
&lt;p&gt;We will go over the basic concepts of security threats, show many examples of real attacks, and focus on how to defend ourselves and secure our data.&lt;/p&gt;
&lt;p&gt;Keep yourself safe!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Main topics:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Myths&lt;/li&gt;
&lt;li&gt;Why it's important to be safe&lt;/li&gt;
&lt;li&gt;Authentication &amp;amp; Password Managers&lt;/li&gt;
&lt;li&gt;Phishing &amp;amp; 2FA&lt;/li&gt;
&lt;li&gt;MITM &amp;amp; Encryption&lt;/li&gt;
&lt;li&gt;Internet tracking &amp;amp; Ad-blockers&lt;/li&gt;
&lt;li&gt;Internet Of Things&lt;/li&gt;
&lt;li&gt;Physical Security&lt;/li&gt;
&lt;li&gt;More Resources&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Data science for lazy people... genetics will work for you!</title><link href="https://pyvideo.org/pydata-barcelona-2017/data-science-for-lazy-people-genetics-will-work-for-you.html" rel="alternate"></link><published>2017-05-21T15:45:00+02:00</published><updated>2017-05-21T15:45:00+02:00</updated><author><name>Diego Hueltes</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/data-science-for-lazy-people-genetics-will-work-for-you.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data science is fun... right? Data cleaning, feature selection, feature preprocessing, feature construction, model selection, parameter optimization, model validation... oh wait... are you sure? What about automating 80% of the work using genetic algorithms that can make better choices than you? TPOT is a tool that automatically creates and optimizes machine learning pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We love Data Science, but sometimes we have to do some manual and repetitive work before starting with the interesting and fun parts, but that will change. TPOT is an open source tool built on top of scikit-learn for creating and optimizing machine learning pipelines. It can be considered a data science assistant. The library will automate from feature selection to parameter optimization, it is also able to do preprocessing or construct new features from existing ones. TPOT tests a huge number of pipelines to provide you with the optimal one, this work is done with genetic algorithms. It is easy to use, has a familiar syntax if you have used Pandas or scikit-learn, and it's very powerful. Let genetics work for you!&lt;/p&gt;
</summary><category term="tpot"></category></entry><entry><title>Marketing Data Science</title><link href="https://pyvideo.org/pydata-barcelona-2017/marketing-data-science.html" rel="alternate"></link><published>2017-05-21T15:45:00+02:00</published><updated>2017-05-21T15:45:00+02:00</updated><author><name>Joaquin Pais</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/marketing-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Marketing Data Science: how digital marketing needs data science to survive.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Digital Marketing is changing the way corporations and brands communicates with customers. The investments in Digital Marketing are skyrocketing around the world in a multi-billion industry.&lt;/p&gt;
&lt;p&gt;But consumers and customers (specially millenials) does not trust advertising. Different approaches are being made by corporations like Inbound Marketing Strategies. My presentation is about how Digital Marketing needs Data Science in order to better understand the customer needs and generate new niches of interest for companies. Companies investing in Digital Marketing should take a close look at Data Science Platforms like Python in order to better gather inisghts, create segments and personalice a customer experience.&lt;/p&gt;
&lt;p&gt;I will provide some short examples about how we are using python jupyter notebook environment in order to gain inisghts from customers using IBM Watson API, generating new segmentation and customer experiences.&lt;/p&gt;
</summary><category term="marketing"></category><category term="data science"></category><category term="pandas"></category></entry><entry><title>Using network community clustering algorithms to aid determination of protein structures</title><link href="https://pyvideo.org/pydata-barcelona-2017/using-network-community-clustering-algorithms-to-aid-determination-of-protein-structures.html" rel="alternate"></link><published>2017-05-21T15:45:00+02:00</published><updated>2017-05-21T15:45:00+02:00</updated><author><name>Claudia Millán</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/using-network-community-clustering-algorithms-to-aid-determination-of-protein-structures.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How can you best exploit the information on the protein structures that we already know in order to solve the (many more) that we don’t know? That’s the question we try to solve in our group and this talk will show our use of community clustering on the use case of aiding protein structure solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The study of proteins is extremely relevant to many fields, ranging from medicine to industrial processes. If a cell was a factory, proteins would be the workers, performing very specific tasks in a production chain, in which a failure at any of the steps might affect the others. In order to really understand the function of proteins, there are a number of biochemical techniques that we can apply, but to get a really accurate description of their mechanisms we need to have information up to their atomic level. Proteins are composed by different combinations of 20 amino acids which form chains and result in an intricate and functional structure. We know the final shape of the protein is dependent from its sequence but we can't (currently) predict the outcome of a given sequence, so we need experimental techniques to obtain this information. We can't directly look a protein with a microscope and see its three-dimensional atomic arrangement. There are biophysical techniques that allow us to get low-resolution shapes or surfaces of them, but the only current method that provides atomic detail is x-ray diffraction. In contrast to microscopy, were after illuminating your sample you can, with the appropriate lenses, transform back your reflected rays in an image, in x-ray diffraction we only get the Fourier transform of our protein's structure, that is, its diffraction pattern, but we lose the phase information from the diffracted rays. This problem, known as the phase problem, is an issue in structure solution, because, compared to the intensities of the diffraction pattern, phases provide much more information about our protein's structure. Phases can be computed from a coordinates model, so, if there is a structure of a protein that is expected to be similar (in terms of their coordinates) to an unknown one, it can be used to provide initial estimates of the phases and then improve them until the problem structure is solved.&lt;/p&gt;
&lt;p&gt;Proteins do not adopt random shapes. There are forces that act on them and, according to the chemical properties of the amino acids and the context they have around, they will arrange in particular forms. The first level of structure formed is what we call secondary structure, which is formed of alpha helices and beta strands. These are very general and are present in all kind of proteins. As letters in different books, which are general if looked at independently but can acquire a quite different meaning in 'El Quijote' than in '50 Shades of Grey', these structure fragments alone have a different meaning than when set in a context with other structure elements. Yet, even combinations of a few elements can still be general and frequent, and have a sense, such as the phrase 'Once upon a time' in so many tales. In the protein case, particular small combinations of alpha helices and beta strands should also appear more frequently and can be studied on their different contexts. In a way, we could say that, protein structures, from a top down view, look quite different between them, but from a bottom up one, when we go to small pieces, they look much more similar.&lt;/p&gt;
&lt;p&gt;In our group, we try to exploit these properties of protein structures for solving two types of problems. One is a search problem, consisting in understanding which is the best way in which to break down a larger model of a protein similar to an unknown one in order to refine it and get the correct phases and solve the structure. The other one is more of a prediction problem, in which we want to find structural units in the database of solved structures in order to help us interpret them and extract new information from this vast amount of data. In order to solve both problems we need a numerical description of our system than can help us describing its geometrical features in an accurate way. We have developed a description of such secondary structure fragments based on what we call characteristic vectors, that we also employ to represent the relations between such elements. This description can be implemented in a network graph that allows to use community clustering algorithms on it in order to evaluate simultaneously all the relations between the elements of the structure and find its communities.&lt;/p&gt;
&lt;p&gt;From the technical point of view, the language we use for our development is Python, and the tool in which our graphs are implemented is python-igraph. There are a number of community clustering algorithms available and deciding which is the best for our case and which metrics to use to describe our structures has been an interesting process that we want to share with you. In this talk I will explain our research and development on this topic and I will link the general descriptions to our current and successful use case, possibly leaving open questions on what can we do more!&lt;/p&gt;
</summary><category term="bioinformatics"></category><category term="protein"></category></entry><entry><title>Blockchain and smart contracts explained (and simplified)</title><link href="https://pyvideo.org/pydata-barcelona-2017/blockchain-and-smart-contracts-explained-and-simplified.html" rel="alternate"></link><published>2017-05-21T15:00:00+02:00</published><updated>2017-05-21T15:00:00+02:00</updated><author><name>Guillem Borrell Nogueras</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/blockchain-and-smart-contracts-explained-and-simplified.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Blockchain is the next big thing that will revolutionize everything, but no one knows how. The same can be said about smart contracts. Bitcoin and Ethereum are great technologies, but too complex for most cases. This talk will explain why smart contracts are hard, and why they don't need to be. Finally, a simple ledger for smart contracts in python (pyledger) will be presented.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Smart contracts are an old and simple concept. They are just computer programs that facilitate the negotiation and settlement of contracts by several parties. Their goal is to lower the transaction latency and cost associated with human middlemen. The blockchain technology brought the possibility to sanction contracts when peers do not trust each other, which is necessary for applications like virtual currencies. However, due to the CAP theorem and the cost of the Byzantine consensus, their performance tends to be quite bad.&lt;/p&gt;
&lt;p&gt;Current smart contract platforms such as Ethereum are general (one can issue any kind of contract), distributed (an updated copy is present at all the nodes within the same network), shared (an arbitrary amount of contracts are served within the same infrastructure) and secure (there is no way to tamper with the stored data). All those properties are undeniably good, but the consequence of keeping all those promises is that the infrastructure is exceedingly complex. Such an extense architecture does not add any functionality to a smart contract, it makes it trustworthy.&lt;/p&gt;
&lt;p&gt;If one understands the raw concepts behind the Blockchain consensus, and we are willing to sacrifice some of the properties mentioned earlier, one can design and implement a simple ledger for smart contracts like pyledger (pyledger.readthedocs.io). Pyledger was designed as a platform for fast prototyping of smart contracts, and for experimentation, but in some particular cases can be good enough to be used in real-world cases.&lt;/p&gt;
&lt;p&gt;Finally... Fun time! All the concepts will be exemplified with a Q&amp;amp;A contest implemented as a smart contract where all attendants can play.&lt;/p&gt;
</summary><category term="blockchain"></category><category term="pyledger"></category></entry><entry><title>How to map a protein structure into a network graph to understand protein folding</title><link href="https://pyvideo.org/pydata-barcelona-2017/how-to-map-a-protein-structure-into-a-network-graph-to-understand-protein-folding.html" rel="alternate"></link><published>2017-05-21T15:00:00+02:00</published><updated>2017-05-21T15:00:00+02:00</updated><author><name>Massimo Domenico Sammito</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/how-to-map-a-protein-structure-into-a-network-graph-to-understand-protein-folding.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In all living systems any biological function is carried out by the interactions of proteins. The specific function of a protein is determined by its own 3d shape that can be modeled in a list of single atom coordinates derived from experimental analysis. In our lab we use python to process the information of hundred thousands structures already known to predict common patterns in protein folding.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From DNA, the genetic code is translated into proteins being these macro molecular structures in charge to execute specific cellular functions. All proteins are composed by the combination of the same 20 amino acids, indeed, it is the 3D shape and detailed structure that differentiate a protein from another and so their functions. Determining protein structure is fundamental to understand their biological function; modify and alter their structures is the way in which drugs can block or activate a protein activity and doing so preventing or treating diseases. In our lab we are understanding protein and DNA structures to the level of atomic resolution contributing to the biomedical research. We process with python the information of hundred thousands 3D coordinate structures already known and predict patterns that can be used to determine structure of unknown proteins. To achieve this objective, proteins are first reduced to a series of specific 3D vectors (Characteristic Vectors &lt;a class="footnote-reference" href="#id2" id="id1"&gt;[1]&lt;/a&gt;) and then the geometrical relationships among them are extracted. We make use of biopython and python-igraph libraries to model each structure vectors in a network graph and we use supervised and unsupervised learning to classify and predict structure features. Python-igraph is a wrapper to a C code implementation of a set of graph algorithms of classical use in network analysis.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Sammito M1, Millán C, Rodríguez DD, de Ilarduya IM, Meindl K, De Marino I, Petrillo G, Buey RM, de Pereda JM, Zeth K, Sheldrick GM, Usón I. Exploiting tertiary structure through local folds for crystallographic phasing. Nat Methods. 2013 Nov;10(11):1099-101. doi: 10.1038/nmeth.2644. Epub 2013 Sep 15.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</summary><category term="biopython"></category><category term="python-igraph"></category></entry><entry><title>Python for visualization of HPC applications</title><link href="https://pyvideo.org/pydata-barcelona-2017/python-for-visualization-of-hpc-applications.html" rel="alternate"></link><published>2017-05-21T15:00:00+02:00</published><updated>2017-05-21T15:00:00+02:00</updated><author><name>Miguel Zavala-Ake</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/python-for-visualization-of-hpc-applications.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This work describes the usage of python in application that makes use of high performance application (HPC). Such applications comprise examples as computational fluid dynamics, where the analysis of large data bases is necessary in order to extract useful information. On the other hand, a common way used in the analysis of data is the visualization. It can be done by libraries as matplotlib or pandas, however more sophisticated 3D visualization can also be performed though vtk and mpi4py libraries, or even though Blender of 3d Max. The two latter case enable the use of high quality rendering.&lt;/p&gt;
</summary><category term="high performance application"></category></entry><entry><title>Guillotina - An Async REST Resource DB to manage millions of objects</title><link href="https://pyvideo.org/pydata-barcelona-2017/guillotina-an-async-rest-resource-db-to-manage-millions-of-objects.html" rel="alternate"></link><published>2017-05-21T12:45:00+02:00</published><updated>2017-05-21T12:45:00+02:00</updated><author><name>Ramon Navarro Bosch</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/guillotina-an-async-rest-resource-db-to-manage-millions-of-objects.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Guillotina is a new framework designed to be horizontal scalable, build on top of aiohttp and offering a Resource REST API to store, read and query object oriented datasets with millions of elements. Based on JSON schema types allows to define a Traversal API with strong security and granularity. With an abstracted storage layer its possible to provide support for Couch, Cassandra and Postgres.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the most common problems on my daily basis data management is a way to store and secure objects with information in a transactional scalable infrastructure. Having an async API that holds security, triggers and transactional operations allows to provide an interface for managing the data that will be processed by the machine learning engines on batch operations. Including async queues of operations allows to provide model inference on the objects on real time operations on objects on the stack.&lt;/p&gt;
&lt;p&gt;After 15 years on Plone/Zope framework we started to design a new concept using all the lessons learned on content management to provide a CRUD storage layer for big data objects.&lt;/p&gt;
</summary><category term="guillotina"></category></entry><entry><title>LympHOS2, Organizing and Sharing Biological Data</title><link href="https://pyvideo.org/pydata-barcelona-2017/lymphos2-organizing-and-sharing-biological-data.html" rel="alternate"></link><published>2017-05-21T12:45:00+02:00</published><updated>2017-05-21T12:45:00+02:00</updated><author><name>Óscar Gallardo Román</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/lymphos2-organizing-and-sharing-biological-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The study of the proteome (the set of proteins expressed by a cell,
tissue, or organism at a defined time and conditions) generates a large
amount of complex data. This data should be processed, stored, curated,
and made easily available to researchers so it can be studied to obtain
biomedical knowledge. In this talk we expose the Python tools we have
used and developed to accomplish it.&lt;/p&gt;
&lt;div class="section" id="abstract"&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;At the &lt;a class="reference external" href="http://proteomica.uab.cat"&gt;LP-CSIC/UAB&lt;/a&gt; we use a technology called &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Mass_spectrometry"&gt;Mass Spectrometry&lt;/a&gt; to
study the &lt;em&gt;phospho-proteome of human T-Lymphocytes&lt;/em&gt;; this is, the group
of proteins that are phosphorilated (modified with phosphate groups) in
the human &lt;a class="reference external" href="https://en.wikipedia.org/wiki/T_cell"&gt;T cells&lt;/a&gt; during their activation and differentiation as part
of the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Immune_response"&gt;immune response&lt;/a&gt;. The experiments involved in this study
generates a &lt;em&gt;large&lt;/em&gt; amount of &lt;em&gt;complex&lt;/em&gt; data:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Experimental conditions and procedures metadata.&lt;/li&gt;
&lt;li&gt;The spectra data and metadata obtained from the mass spectrometers
(usually in proprietary binary formats).&lt;/li&gt;
&lt;li&gt;Qualitative or identification data, consisting of search results of
those spectra against human protein sequence databases (using
multiple search engines, that use different output file-formats)
linking the spectra with possible peptides (small protein sequence
fragments) that may or may not contain phosphorylated amino-acids.&lt;/li&gt;
&lt;li&gt;Semi-quantitative data about the abundance of each possible
identified peptide.&lt;/li&gt;
&lt;li&gt;Missing values, identification scores, phosphorylation reassignments,
and a lot of relationships and inter-linked data…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And all this data has to be &lt;strong&gt;processed&lt;/strong&gt;, &lt;strong&gt;stored&lt;/strong&gt;, &lt;strong&gt;curated&lt;/strong&gt;, and
made &lt;strong&gt;easily accessible&lt;/strong&gt; to researchers in our lab and worldwide, so
they can study it to obtain biomedical knowledge about the
phosphorylation changes in peptides and proteins involved in the signal
transduction pathways of T cells after their activation during the
specific immune response.&lt;/p&gt;
&lt;p&gt;We have used different Python &lt;em&gt;packages&lt;/em&gt; to develop different tools and
applications to accomplish those objectives:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The &lt;a class="reference external" href="https://bitbucket.org/lp-csic-uab/easiermgf"&gt;EasierMgf&lt;/a&gt; front-end application, to extract plain text spectra
data from proprietary binary mass spectrometer files, was developed
using the &lt;a class="reference external" href="https://wxpython.org/"&gt;wxPython&lt;/a&gt; GUI toolkit.&lt;/li&gt;
&lt;li&gt;The &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Object-relational_mapping"&gt;ORM&lt;/a&gt; from the &lt;a class="reference external" href="https://www.djangoproject.com/"&gt;django&lt;/a&gt; framework was used to design and
interact with the LymPHOS2 &lt;a class="reference external" href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt; database, which stores all the
data and their inter-relationships.&lt;/li&gt;
&lt;li&gt;Also, different modules from the Python standard library (&lt;em&gt;json&lt;/em&gt;,
&lt;em&gt;xml.etree.ElementTree&lt;/em&gt;, &lt;em&gt;csv&lt;/em&gt;, &lt;em&gt;zipfile&lt;/em&gt;) were used to read and
&lt;a class="reference external" href="https://bitbucket.org/lp-csic-uab/lymphosweb/src/c127cd7a88a5dfb0a70b9e3b6243553c6adda6fb/src/LymPHOS_v1_5/lymphos/filters.py"&gt;import&lt;/a&gt; into the database the different files containing the data;
and to &lt;a class="reference external" href="https://bitbucket.org/lp-csic-uab/lymphostools"&gt;export&lt;/a&gt; database data into commonly used file formats for
the researches to work with.&lt;/li&gt;
&lt;li&gt;The &lt;a class="reference external" href="https://bitbucket.org/lp-csic-uab/lymphospquantifier"&gt;PQuantifier&lt;/a&gt; group of tools was developed to do the statistical
processing and analysis of the LymPHOS2 semi-quantitative data. It
uses the &lt;a class="reference external" href="https://www.sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt; ORM for fast database storage in local
&lt;a class="reference external" href="https://sqlite.org/"&gt;SQLite&lt;/a&gt; files, the &lt;a class="reference external" href="https://www.numpy.org/"&gt;NumPy&lt;/a&gt; N-dimensional array package, the
&lt;a class="reference external" href="https://scipy.org/scipylib"&gt;SciPy&lt;/a&gt; scientific computing library for the statistics, the
&lt;a class="reference external" href="https://pythonhosted.org/uncertainties/"&gt;uncertainties&lt;/a&gt; package for calculations with error propagation,
and the &lt;a class="reference external" href="https://matplotlib.org/"&gt;matplotlib&lt;/a&gt; 2D plotting library for some nice plots of
data distributions.&lt;/li&gt;
&lt;li&gt;And the full django framework itself was used to develop the
&lt;a class="reference external" href="http://www.lymphos.org"&gt;LymPHOS2&lt;/a&gt; web application. Which also uses the matplotlib library
to dynamically generate the mass spectra images.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final result is the &lt;a class="reference external" href="https://bitbucket.org/lp-csic-uab/lymphosweb/branch/Web_LymPHOS_2"&gt;LymPHOS2&lt;/a&gt; web-oriented database, that
nowadays (2017) contains 131.908 mass spectra, 15.566 phosphorylation
sites from 8.273 unique phospho-peptides and 4.937 proteins (which
represent a 45-fold increase over the original LymPHOS database of
2009); aside from the new quantitative data for 1.975 of the identified
phospho-peptides, which was not present in the previous version of
LymPHOS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Repositories and Presentation Slides:&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Bitbucket code repositories: &lt;a class="reference external" href="https://bitbucket.org/lp-csic-uab/"&gt;https://bitbucket.org/lp-csic-uab/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Presentation Slides (.ODP, .PDF and .PPTX):
&lt;a class="reference external" href="https://bitbucket.org/lp-csic-uab/lymphosdocs/downloads/"&gt;https://bitbucket.org/lp-csic-uab/lymphosdocs/downloads/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Credits:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The exposed work has been carried out at &lt;a class="reference external" href="http://proteomica.uab.cat"&gt;LP-CSIC/UAB&lt;/a&gt; from
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Catalonia"&gt;Catalonia&lt;/a&gt;, part of the &lt;a class="reference external" href="http://www.csic.es/"&gt;Spanish National Research Council (Consejo
Superior de Investigaciones Científicas - CSIC)&lt;/a&gt; and of &lt;a class="reference external" href="http://www.proteored.org/"&gt;ProteoRed
(Proteomics National NetWork Platform)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;people&lt;/em&gt; who have participated directly in the current work are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Data analysis, bioinformatics and informatics&lt;/strong&gt;: &lt;em&gt;Joaquin Abian&lt;/em&gt;
and &lt;em&gt;Óscar Gallardo&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mass Spectrometry, experimental design and implementation&lt;/strong&gt;:
&lt;em&gt;Montserrat Carrascal&lt;/em&gt;, &lt;em&gt;Nguyen Tien Dung&lt;/em&gt; and &lt;em&gt;Oriol Vidal-Cortes&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample preparation&lt;/strong&gt;: &lt;em&gt;Montserrat Carrascal&lt;/em&gt;, &lt;em&gt;Nguyen Tien Dung&lt;/em&gt;,
&lt;em&gt;Oriol Vidal-Cortes&lt;/em&gt; and &lt;em&gt;Vanessa Casas&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Past collaborators&lt;/strong&gt;: &lt;em&gt;David Ovelleiro&lt;/em&gt; and &lt;em&gt;Marina Gay&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Direction&lt;/strong&gt;: &lt;em&gt;Joaquin Abian&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</summary><category term="lymphos"></category></entry><entry><title>Prototyping Interactive Dashboards with Jupyter Notebooks</title><link href="https://pyvideo.org/pydata-barcelona-2017/prototyping-interactive-dashboards-with-jupyter-notebooks.html" rel="alternate"></link><published>2017-05-21T12:45:00+02:00</published><updated>2017-05-21T12:45:00+02:00</updated><author><name>Camilo Cardona</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/prototyping-interactive-dashboards-with-jupyter-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, I describe my experience in using jupyter-dashboards, ipywidgets and other libraries to build basic prototypes of data exploration dashboards for datasets related to Internet traffic and packet-switching networks. We’ll go over various examples on how to build and use the widgets to navigate the data, visualize analysis, and evaluate optimization changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As python programmers, we often use jupyter notebooks as a platform for reproducible data analysis and publication of results. The jupyter ecosystem is still under-utilized for projects that require interactions from non-coder users, such as explorative dashboards or Business intelligence applications. There are other specialised options for these UI/UX interfaces, but integrating them into the analysis pipeline (e.g. Jupyter) is often rather hard. Preferably, we would be able to prototype these interactive applications without leaving our environment.&lt;/p&gt;
&lt;p&gt;Jupyter-dashboards, ipywidgets, and other typical pydata libraries provide jupyter with the capabilities to create flexible, and rather powerful interactive elements. This toolset can be used in small projects to create the final facing interface for a limited number of end users. For bigger projects, a more robust UI/UX is normally required to support larger number of simultaneous users. However, the jupyter-dashboard/ipywidgets toolset can still be useful in these cases as an end-to-end prototyping platform, allowing us to get feedback from users at early stages of the project, thus helping us better steer our development.&lt;/p&gt;
&lt;p&gt;In this talk, I share experiences of using this toolset for different use cases related to analysis of Internet traffic and network engineering. I’ll go over examples of how to build widgets to explore the Internet traffic and other network data, while highlighting some of its characteristics. Furthermore, I’ll describe some optimisations performed by network engineers on their traffic, such as load balancing, and how the tools can be used to evaluate them.&lt;/p&gt;
</summary><category term="jupyter notebook"></category></entry><entry><title>Increasing the trading prediction by mining aggregated human texting messages</title><link href="https://pyvideo.org/pydata-barcelona-2017/increasing-the-trading-prediction-by-mining-aggregated-human-texting-messages.html" rel="alternate"></link><published>2017-05-21T12:00:00+02:00</published><updated>2017-05-21T12:00:00+02:00</updated><author><name>Yang Liu</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/increasing-the-trading-prediction-by-mining-aggregated-human-texting-messages.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A novel approach is proposed to predict intraday directional-movements of the stock in the trading market based on the text of breaking financial news or social media by event data. This work is an effort to put more emphasis on the text-mining methods and tackle some specific aspects thereof that are weak in previous works. The research is a specific market, namely, the trading and stock market,&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sentiment analysis This step is observed the time series of the company‘s event based on the financial news or the social media, which analyses problem of coreference in text mining that is sentiment classification. Fasttext is inspired method that focuses on the meaning of words, which solves to obscure by ambiguity and play on words. This way can classify polarity, subjectivity and intensity in the corpus. According to the results of the classification, then it changes to the word representation. Thus, prediction accuracy increases significantly at this layer that is attributed to appropriate noise-reduction from the feature-space. Model creation and prediction The first layer is termed Sentiment Integration Layer by the deep neural network, which integrates sentiment analysis capability into the algorithm by proposing a sentiment weight that reflects their sentiment. Additionally, this layer reduces the dimensions by eliminating value in terms of sentiment and thereby improves prediction accuracy. The second layer encompasses a deep neural network model creation algorithm. It updates the models with the most recent information available, the deep learning algorithm is extensively evaluated using real market data and news content across multiple years, which is a good fitting capability to evaluate and select the financial news. The third layer is that it uses the recursive neural network explore patterns and of financial news through the recursive and pooling operation.&lt;/p&gt;
</summary><category term="sentiment analysis"></category></entry><entry><title>The Neuroscience Lab</title><link href="https://pyvideo.org/pydata-barcelona-2017/the-neuroscience-lab.html" rel="alternate"></link><published>2017-05-21T12:00:00+02:00</published><updated>2017-05-21T12:00:00+02:00</updated><author><name>Nicholas A. Del Grosso</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/the-neuroscience-lab.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python is an excellent fit for scientists; it is readable, has an active scientific community, and, most importantly, is versatile across a wide variety of programming tasks. In this talk, we will celebrate this versatility of Python by sharing the research being done in our neuroscience laboratory, focusing on Python’s involvement in each step of our scientific research.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Biologists have always built their own hardware to perform unique experiments; today, new data collection and analysis techniques demand that they develop their own software as well. Often, this means that researchers without a formal education in programming must perform a wide variety of programming tasks in order to perform cutting-edge research. As a “real” programming language, Python is an excellent fit for scientists, not only for its oft-celebrated readability, active scientific community, and cross-platform support, but also for its versatility across a wide variety of domains. In this talk, we will celebrate this versatility of Python by sharing the research being done in our neuroscience laboratory, focusing how Python packages, applications, and plugins have become the tools of choice for everything from real-time machine vision and 3D graphics to data analysis and the presentation of scientific results.&lt;/p&gt;
</summary></entry><entry><title>Web Scraping with Python</title><link href="https://pyvideo.org/pydata-barcelona-2017/web-scraping-with-python.html" rel="alternate"></link><published>2017-05-21T12:00:00+02:00</published><updated>2017-05-21T12:00:00+02:00</updated><author><name>Carles Illa</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/web-scraping-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Web Scraping is widely used to get data from websites automatically. Whether it be contact information, news or selling prices, we rely on web scraping techniques as they allow us to collect large data with minimal effort.&lt;/p&gt;
&lt;p&gt;In the Python ecosystem we have a lot of different tools for extracting data from web pages, and in this talk we will present some of the most powerful ones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Web Scraping is the process of extracting useful information from the web automatically.&lt;/p&gt;
&lt;p&gt;It is widely used for various purposes like:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Lead generation&lt;/li&gt;
&lt;li&gt;Price monitoring on e-commerce sites&lt;/li&gt;
&lt;li&gt;Product reviews collection to do sentimental analysis&lt;/li&gt;
&lt;li&gt;Social profiles retrieving for recruiters&lt;/li&gt;
&lt;li&gt;Scraping blogs &amp;amp; forums to keep up with trends and news&lt;/li&gt;
&lt;li&gt;Maintaining up-to-date real estate databases&lt;/li&gt;
&lt;li&gt;News aggregators&lt;/li&gt;
&lt;li&gt;Reputation monitoring&lt;/li&gt;
&lt;li&gt;Events listings&lt;/li&gt;
&lt;li&gt;Websites changes detection&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this talk we will present some of the most powerful tools for extracting data from web pages using Python. We will learn how to use Python to request information from a web server, how to perform basic handling of the server’s response, and how to begin interacting with a website in an automated fashion.&lt;/p&gt;
</summary></entry><entry><title>Squeeze (gently) your data</title><link href="https://pyvideo.org/pydata-barcelona-2017/squeeze-gently-your-data.html" rel="alternate"></link><published>2017-05-21T10:30:00+02:00</published><updated>2017-05-21T10:30:00+02:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/squeeze-gently-your-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Coping with the growing rate of data sources is becoming a big challenge, not only in terms of efficiently storing it, but also (and most specially) in doing more general computations with them. Compressing your data may help in many (and sometimes unexpected) ways in this task. This talk will introduce several ways in which you can benefit from highly efficient compression libraries.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Nowadays CPUs are fast; they are coming with more and more cores and, in comparison, memory speed is not keeping this race in terms of speed. As a result, this opening gap is what is making of compression a valuable technique, not only for storing the same data by using less storage but also to accelerate data handling operations in an increasing number of cases.&lt;/p&gt;
&lt;p&gt;My talk will start by introducing the technological reasons behind the increasing benefit of using compression in data science, and then will show some practical cases where data compression can lead to much more efficient data pipelines. For this, I will be using well-proven compression libraries like &lt;a class="reference external" href="http://www.blosc.org/"&gt;Blosc&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/facebook/zstd"&gt;Zstandard&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/lz4/lz4"&gt;LZ4&lt;/a&gt; that, either in combination with data handling libraries (like &lt;a class="reference external" href="http://www.pytables.org/"&gt;PyTables&lt;/a&gt;, &lt;a class="reference external" href="http://bcolz.blosc.org/en/latest/"&gt;bcolz&lt;/a&gt; or &lt;a class="reference external" href="http://zarr.readthedocs.io/en/latest/"&gt;zarr&lt;/a&gt;), or used for handling high-speed data streams (transmitted e.g. via &lt;a class="reference external" href="http://www.grpc.io/"&gt;gRPC&lt;/a&gt;).&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>NASA Space APPS Challenge: Asteroid prediction impact</title><link href="https://pyvideo.org/pydata-barcelona-2017/nasa-space-apps-challenge-asteroid-prediction-impact.html" rel="alternate"></link><published>2017-05-20T17:00:00+02:00</published><updated>2017-05-20T17:00:00+02:00</updated><author><name>Gema Parreño</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/nasa-space-apps-challenge-asteroid-prediction-impact.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;TensorFlow and Prophet : Forecasting Asteroid impact with opensource software Talk about NASA Space APPS 2016 global finalist project , focused on neural net design&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The talk will have two separate parts ; the first of it focuses on explaining Tensorflow architecture and Prophet opensource tools and the second part will dive into the Neural Net design and forecast prediction .&lt;/p&gt;
</summary><category term="keynote"></category><category term="tensorflow"></category><category term="prophet"></category></entry><entry><title>DeepCare Chatbot - Generating answers to customers using Deep Learning and NLP</title><link href="https://pyvideo.org/pydata-barcelona-2017/deepcare-chatbot-generating-answers-to-customers-using-deep-learning-and-nlp.html" rel="alternate"></link><published>2017-05-20T15:45:00+02:00</published><updated>2017-05-20T15:45:00+02:00</updated><author><name>Pascal van Kooten</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/deepcare-chatbot-generating-answers-to-customers-using-deep-learning-and-nlp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The DeepCare chatbot is capable of learning to answer customer questions. Using a hybrid approach of NLP and Deep Learning, it tries to combat logical fallacies that occur in pure deep learning bots, while still coming up with unique answers.&lt;/p&gt;
&lt;p&gt;In particular, it uses a sequence-to-sequence (seq2seq) long-short-term-memory LSTM deep learning model to capture intricacies in questions. As organisations cannot afford a bot making logical mistakes, verification through NLP is used. This two-step model prevents the downside of &amp;quot;no control&amp;quot; on deep learning, as well as the too static nature of classical rule based NLP models, and thus enables potentially higher quality answers.&lt;/p&gt;
&lt;p&gt;A live demo will be available at &lt;a class="reference external" href="http://deepcare.online"&gt;http://deepcare.online&lt;/a&gt; on the day of the talk.&lt;/p&gt;
</summary><category term="chatbot"></category><category term="nlp"></category><category term="deep learning"></category></entry><entry><title>Extending Jupyter with Google Cloud Storage file system backend</title><link href="https://pyvideo.org/pydata-barcelona-2017/extending-jupyter-with-google-cloud-storage-file-system-backend.html" rel="alternate"></link><published>2017-05-20T15:45:00+02:00</published><updated>2017-05-20T15:45:00+02:00</updated><author><name>Egor Bulychev</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/extending-jupyter-with-google-cloud-storage-file-system-backend.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter has a modular architecture which allows extending it in numerous ways. This talk presents src-d/jgscm, Google Cloud Storage Jupyter file system backend which provides the ability to work with notebooks and other files directly in Google Cloud Storage. The described approach can be used for writing similar backends, e.g. for Amazon Cloud.&lt;/p&gt;
&lt;p&gt;See more here : &lt;a class="reference external" href="https://egorbu.github.io/pydata_2017_barcelona/index.html"&gt;https://egorbu.github.io/pydata_2017_barcelona/index.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;src-d/jgscm is a Jupyter file system backend for Google Cloud Storage. It allows to work with notebooks and other files directly in Google Cloud Storage. It's codebase is rather small and simple thanks to Jupyter's modular architecture. We start from the very basics, revise how Google Cloud Storage works and how Jupyter deals with the virtual file system and end up with the complete production-tested backend implementation. The described ideas are rather versatile and can be re-used to create backends for similar cloud storage systems, e.g. Amazon.&lt;/p&gt;
</summary><category term="jupyter notebook"></category></entry><entry><title>The Secret Life Of Rolling Pandas</title><link href="https://pyvideo.org/pydata-barcelona-2017/the-secret-life-of-rolling-pandas.html" rel="alternate"></link><published>2017-05-20T15:45:00+02:00</published><updated>2017-05-20T15:45:00+02:00</updated><author><name>Jaime Fernandez del Rio</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/the-secret-life-of-rolling-pandas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas provides a powerful set of functions to compute statistics on rolling windows. We will go beyond the convenient interface, and peek at the algorithmic gems that implement these operations efficiently: summed area tables, Welford's method, skip lists, ring buffers, deques... will all get their minute of fame, and attendees may learn a thing or two they can use in their everyday coding.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The talk will cover:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A quick overview of the Rolling and Expanding objects in pandas. Why straightforward implementations, even clever ones using advanced NumPy tricks, are inefficient.&lt;/li&gt;
&lt;li&gt;How is Series.rolling().sum() implemented. How the same idea can be extended to higher dimensions with the help of the inclusion-exclusion principle to yield summed area tables.&lt;/li&gt;
&lt;li&gt;How are Series.rolling().var() and friends implemented. The importance of numerically stable algorithms: why Welford's method is the better choice, even if it's a little slower. How can the same ideas be generalized to higher order moments, and used to parallelize computations.&lt;/li&gt;
&lt;li&gt;How are Series.rolling().max() and .min() implemented. The beauty of clever algorithms. The use of specialized data structures: a ring buffer as a deque.&lt;/li&gt;
&lt;li&gt;How are Series.rolling().median() and .quantile() implemented. More specialized data structures: the skip list, or why randomization can be your friend.&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="pandas"></category></entry><entry><title>Analyzing code contributions to the CPython project using NetworkX and Matplotlib</title><link href="https://pyvideo.org/pydata-barcelona-2017/analyzing-code-contributions-to-the-cpython-project-using-networkx-and-matplotlib.html" rel="alternate"></link><published>2017-05-20T15:00:00+02:00</published><updated>2017-05-20T15:00:00+02:00</updated><author><name>Jordi Torrents</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/analyzing-code-contributions-to-the-cpython-project-using-networkx-and-matplotlib.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;It's a well established fact that only a small fraction of developers account for most code contributions to FOSS projects. The CPython project is not an exception, but analyzing code contributions through time reveals that the people that contribute the most is not always the same. I model CPython's contribution dynamics as cooperation networks and analyze them using NetworkX and Matplotlib.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I analyze cooperation on the CPython project by analyzing the code contributions that each participant on the project has done through time. I model these contributions as a succession of bipartite networks where the bipartite node sets are contributors and source code files; each contributor is linked to the source files to which they have contributed, weighted by the number of lines of source code added. Analyzing the structure of these networks using NetworkX and Matplotlib I found an hierarchy of nested groups of developers that corresponds to the developers that do most of the code contributions in the CPython project. This hierarchy, on the one hand, reflects the empirically well established fact that in FOSS projects only a small fraction of the developers account for most of the contributions. And, on the other hand, refutes the naive views of early academic accounts that characterized FOSS projects as a flat hierarchy of peers in which every individual does more or less the same. I argue that the structure of CPython's cooperation network can be characterized as an open elite, where the top levels of this hierarchy are filled with new individuals at a high pace. This feature is key for understanding the mechanisms and dynamics that make FOSS communities able to develop long term projects, with high individual turnover, and yet achieve high impact and coherent results as a result of large scale cooperation.&lt;/p&gt;
&lt;p&gt;You can download the slides for this talk from &lt;a class="reference external" href="https://github.com/jtorrents/thesis/blob/master/presentations/pydata_bcn/cpython_code_contributions.pdf"&gt;https://github.com/jtorrents/thesis/blob/master/presentations/pydata_bcn/cpython_code_contributions.pdf&lt;/a&gt;&lt;/p&gt;
</summary><category term="cpython"></category><category term="networkx"></category><category term="matplotlib"></category></entry><entry><title>Running jupyter notebook remotely in a docker swarm cluster</title><link href="https://pyvideo.org/pydata-barcelona-2017/running-jupyter-notebook-remotely-in-a-docker-swarm-cluster.html" rel="alternate"></link><published>2017-05-20T15:00:00+02:00</published><updated>2017-05-20T15:00:00+02:00</updated><author><name>Jordi Deu-Pons</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/running-jupyter-notebook-remotely-in-a-docker-swarm-cluster.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The current version of Jupyter Notebook computes the document state at the browser side, this is a problem if you run long jobs in a remote notebook from a laptop. If you close the browser you lose all the output of the current running cell. I will explain how we solved this problem in our lab. This solution it also allows a &amp;quot;walkie-talkie&amp;quot; like real-time collaboration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The solution is based on running a docker container with a browser and a VNC server. All the remote access to the notebooks is done using Apache Guacamole a clientless remote desktop gateway. Everything is running on a dynamic docker swarm cluster of 20 nodes. As a lateral effect, this solution it also allows a real-time collaboration between users in a way that multiple users can access at the same time the same desktop (but they have to fight for the mouse and the keyboard).&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/jordeu/pytalks/tree/master/20170520_pydata_jupyter_in_a_docker_cluster"&gt;https://github.com/jordeu/pytalks/tree/master/20170520_pydata_jupyter_in_a_docker_cluster&lt;/a&gt;&lt;/p&gt;
</summary><category term="jupyter notebook"></category><category term="docker"></category><category term="swarm"></category></entry><entry><title>TensorFlow Wide &amp; Deep: Advanced Classification the easy way</title><link href="https://pyvideo.org/pydata-barcelona-2017/tensorflow-wide-deep-advanced-classification-the-easy-way.html" rel="alternate"></link><published>2017-05-20T15:00:00+02:00</published><updated>2017-05-20T15:00:00+02:00</updated><author><name>Yufeng Guo</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/tensorflow-wide-deep-advanced-classification-the-easy-way.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, we will go on an adventure to build a machine learning model that combines the benefits of linear regression models with deep neural networks. You will also gain some intuition about what is happening under the hood, and learn how you can use this model for your own datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Deep learning has already revolutionized machine learning research, but it remains out of reach for many developers. However, tools already exist today that enable leading-edge machine learning for many problem domains.
In this talk, we will go on an adventure to build a machine learning model that combines the benefits of linear models with deep neural networks. You will also gain some intuition about what is happening under the hood, and learn how to use this model for your own datasets! To accomplish this, we will use TensorFlow, an open-source machine learning library with a full Python interface. It has become the most popular machine learning library on GitHub, and the community around it is growing rapidly.&lt;/p&gt;
</summary><category term="tensorFlow"></category></entry><entry><title>Feature Importance and Ensemble Methods : a new perspective</title><link href="https://pyvideo.org/pydata-barcelona-2017/feature-importance-and-ensemble-methods-a-new-perspective.html" rel="alternate"></link><published>2017-05-20T12:45:00+02:00</published><updated>2017-05-20T12:45:00+02:00</updated><author><name>Constant Bridon</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/feature-importance-and-ensemble-methods-a-new-perspective.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Ensemble methods are extremely performant in terms of prediction, but lack easy interpretation. Feature importance is not only counting up how many times a feature has been used in a weak learner, but also by how much this feature contributes to the result. Detailed example and implementation are provided in a jupyter notebook in python for the library &amp;quot;xgboost&amp;quot; of extreme gradient boosting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I - Feature importance in ensemble algorithms - state of the art&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Feature importance in sklearn/xgboost: basically counts the occurrences of a feature in all the weak learners&lt;/li&gt;
&lt;li&gt;Construction of the trees in xgboost: if the trees are deep enough, every feature is going to be used&lt;/li&gt;
&lt;li&gt;Global feature importance is a misleading: a given feature might be critical for a given subpopulation but completely irrelevant for another (ex : multi-class classification)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;II - Xgboost real feature importance&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Prediction influence: first splits influence the prediction more than last splits, so the importance of a feature must be weighted by the discrimination it provides&lt;/li&gt;
&lt;li&gt;Point-to-point feature importance: following the path of a given prediction, it is possible to weigh the importance of every used feature&lt;/li&gt;
&lt;li&gt;A relevant assessment of feature importance: explanation of a given prediction, and aggregation on a set of data points&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;III - Implementation and examples&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Point-to-point feature importance illustration and implementation explanation&lt;/li&gt;
&lt;li&gt;Evolution of feature importance with respect to learning iterations&lt;/li&gt;
&lt;li&gt;Noisy variables cancellation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;IV - Limits and ways forward&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A word on correlated variables&lt;/li&gt;
&lt;li&gt;Is there a compromise performance/interpretation ?&lt;/li&gt;
&lt;/ol&gt;
</summary><category term="xgboost"></category></entry><entry><title>Happiness inside a job: a social network analysis</title><link href="https://pyvideo.org/pydata-barcelona-2017/happiness-inside-a-job-a-social-network-analysis.html" rel="alternate"></link><published>2017-05-20T12:45:00+02:00</published><updated>2017-05-20T12:45:00+02:00</updated><author><name>Guillem Duran Ballester</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/happiness-inside-a-job-a-social-network-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, we will show how to analyze social network data from a mobile phone application to predict employee turnover and employee happiness. We will look into the process of extracting new features from a dataset using social network analysis techniques. We will also show how these features can be visualized and used to boost machine learning models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk, we will show how to analyze social network data to predict employee turnover and employee happiness. This talk is a summary of different notebooks about social network analysis that will be available on &lt;a class="reference external" href="https://github.com/Guillem-db/PyData-Bcn-2017"&gt;Github&lt;/a&gt;. Please note that all the methodology and details we are using, are explained in depth in the notebooks.&lt;/p&gt;
&lt;p&gt;We will cover the following topics:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;From tables to graphs: How to use &lt;a class="reference external" href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt; and &lt;a class="reference external" href="https://networkx.github.io/"&gt;networkx&lt;/a&gt; to merge several DataFrames into a graph, and different criteria for creating a graph representation of a database.&lt;/li&gt;
&lt;li&gt;Graph features: How to extract new features using social network analysis techniques such as &lt;a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"&gt;Non-Negative Matrix Factorization&lt;/a&gt; and graph &lt;a class="reference external" href="https://networkx.github.io/documentation/development/reference/algorithms.centrality.html"&gt;centrality metrics&lt;/a&gt;. We will explain what these features represent, and how they relate to other types of features that can be extracted directly from the database.&lt;/li&gt;
&lt;li&gt;Improving ML models: How to use graph features to boost machine learning models. We will talk about different feature selection techniques that can be used to filter out the most significant set of graph-based features using &lt;a class="reference external" href="http://scikit-learn.org/"&gt;scikit-learn&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Visualization: How to visualize social interactions using &lt;a class="reference external" href="http://bokeh.pydata.org/en/latest/"&gt;bokeh&lt;/a&gt;, and how visualization techniques can be used to check data integrity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will also review common mistakes that can happen during the modeling process and, how to prevent invalid data from getting leaked into your machine learning models. To conclude we compare the merits of using a Python framework versus R Language in terms of development time and computational performance. Data set provided by &lt;a class="reference external" href="https://myhappyforce.com"&gt;https://myhappyforce.com&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Machine Learning to know if you R coming</title><link href="https://pyvideo.org/pydata-barcelona-2017/machine-learning-to-know-if-you-r-coming.html" rel="alternate"></link><published>2017-05-20T12:45:00+02:00</published><updated>2017-05-20T12:45:00+02:00</updated><author><name>Claudia Guirao</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/machine-learning-to-know-if-you-r-coming.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Predicting Hotel Reservation Cancellations with R language.&lt;/p&gt;
&lt;p&gt;Knowing in advance bookings that will be canceled before the arrival date is crucial in the hotel sector. We will examine the determinant characteristics of reservation cancellation, the constrains of this project and how the probability of cancellation was estimated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some users are more likely to cancel their bookings than others, depending on several factors/conditions. Knowing these factors and how many bookings will be canceled will help hotels to optimize their incomes and maximize their occupancy.&lt;/p&gt;
</summary><category term="R"></category><category term="machine learning"></category></entry><entry><title>A Beginners Guide to Weather &amp; Climate Data</title><link href="https://pyvideo.org/pydata-barcelona-2017/a-beginners-guide-to-weather-climate-data.html" rel="alternate"></link><published>2017-05-20T12:00:00+02:00</published><updated>2017-05-20T12:00:00+02:00</updated><author><name>Margriet Groenendijk</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/a-beginners-guide-to-weather-climate-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Weather is part of our every day lives. Who doesn’t check the weather forecast regularly? But where does the data come from, what is it made of? The answer is a mix of measurements and models. This session looks at observations, predictions and forecast models, and weather data as a variable to consider in machine learning models. Learn how it is done from several Python notebook examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Weather is part of our every day lives. Who doesn’t check the rain radar before heading out, or the weather forecast when planning a weekend away? But where does this data come from, what is it made of, and what can you do with it? The answer is a mix of measurements, models and statistics. This session will use Python notebooks with examples of how it is done:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Weather is quantified by the temperature, humidity, wind, rainfall and radiation. These variables are all measured at thousands of locations worldwide and all data is collected and checked to improve the weather forecasts. Learn how to interpolate this data to create for instance global temperature maps to estimate the global monthly and annual temperature.&lt;/li&gt;
&lt;li&gt;The observations are used to constrain the global weather forecast models. These models are complex and simulate the energy, water and carbon fluxes between and in the atmosphere, ocean and land. The output from the models is gridded data, which is generally stored in binary netcdf files. Learn how to load and process the gridded data into time series or maps for further analysis.&lt;/li&gt;
&lt;li&gt;The raw model data is processed for you when you request weather data through a REST API. Learn how to use a weather API to train a supervised model with historical weather time series and then use the weather forecast data to do predictions. This session provides you with a brief overview of the science behind weather and climate forecasts and gives you the tools to get started with weather data.&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="climate"></category></entry><entry><title>Despicable machines: how computers can be assholes</title><link href="https://pyvideo.org/pydata-barcelona-2017/despicable-machines-how-computers-can-be-assholes.html" rel="alternate"></link><published>2017-05-20T12:00:00+02:00</published><updated>2017-05-20T12:00:00+02:00</updated><author><name>Maciej Siwek</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/despicable-machines-how-computers-can-be-assholes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There's a widespread belief among machine learning practitioners that algorithms are objective and allow us to deal with the messy reality in a nice, objective way, without worrying about all the yucky human nature things. This talk will argue that this belief is wrong. Algorithms, just like the humans who create them, can be severely biased and despicable indeed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When working on a new ML solution to solve a given problem, do you think that you are simply using objective reality to infer a set of unbiased rules that will allow you to predict the future? Do you think that worrying about the morality of your work is something other people should do? If so, this talk is for you.&lt;/p&gt;
&lt;p&gt;In this brief time, I will try to convince you that you hold great power over how the future world will look like and that you should incorporate thinking about morality into the set of ML tools you use every day. We will take a short journey through several problems, which surfaced over the last few years, as ML and AI generally, became more widely used. We will look at bias present in training data, at some real-world consequences of not considering it (including one or two hair-raising stories) and cutting-edge research on how to counteract this.&lt;/p&gt;
&lt;p&gt;The outline of the talk is:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Intro the problem: ML algos can be biased!&lt;/li&gt;
&lt;li&gt;Two concrete examples.&lt;/li&gt;
&lt;li&gt;What's been done so far (i.e. techniques from recently-published papers).&lt;/li&gt;
&lt;li&gt;What to do next: unanswered questions.&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="ethics"></category><category term="machine learning"></category></entry><entry><title>How to manage complexity in distributed applications</title><link href="https://pyvideo.org/pydata-barcelona-2017/how-to-manage-complexity-in-distributed-applications.html" rel="alternate"></link><published>2017-05-20T12:00:00+02:00</published><updated>2017-05-20T12:00:00+02:00</updated><author><name>Guillem Borrell Nogueras</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/how-to-manage-complexity-in-distributed-applications.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Complexity is tricky. Some years ago we got how to scale the performance of distributed applications, and that's why everyone is talking about Big Data. But the challenge now is scaling the complexity within a fast-changing environment without penalizing the performance. These are the conclusions after one year developing a library trying to handle this issue, and using it in production.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The development of large-scale distributed applications is an engineering challenge by itself. Development has to be orthogonal to be scalable, as you may know if you have heard about the Mythical Man-Month and the Conway's law: trying to make your application faster may slow down your development. Managing complexity is a new technology trend, and NFQ and the Carlos III University of Madrid have developed a library to make large-scale distributed applications more sensible called pylm (&lt;a class="reference external" href="https://pylm.readthedocs.io"&gt;https://pylm.readthedocs.io&lt;/a&gt;). Since this library has been already used in production, it is time to summarize what are the challenges one faces when building something more intricate than a Spark cluster.&lt;/p&gt;
&lt;p&gt;This talk is about the value of developing in-house tools and obtaining deep technological insight opposed to the successive integration of trendy technologies. The latter is suitable to implement one-shot tools for isolated projects, but when facing a multi-year complex project, the former becomes a more solid ground for long-term maintenance. Complexity piles up nonlinearly, and the most popular tools nowadays cringe when they have to be tightly integrated, since in the long term it is impossible to isolate the technical and the human aspects of development.&lt;/p&gt;
&lt;p&gt;Complexity's weight is getting heavier in this scalability-obsessed world, and it's time to talk about it.&lt;/p&gt;
&lt;p&gt;This project has been funded by the Spanish Ministry of Economy and Competitivity under the grant IDI-20150936, cofinanced with FEDER funds&lt;/p&gt;
</summary><category term="pylm"></category></entry><entry><title>The Magic Behind PySpark, how it impacts perf &amp; the "future"</title><link href="https://pyvideo.org/pydata-barcelona-2017/the-magic-behind-pyspark-how-it-impacts-perf-the-future.html" rel="alternate"></link><published>2017-05-20T10:30:00+02:00</published><updated>2017-05-20T10:30:00+02:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/the-magic-behind-pyspark-how-it-impacts-perf-the-future.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A look at how PySpark &amp;quot;works&amp;quot; today and how we can make it better in the future + insert engine noises of a fast car +&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk will introduce PySpark along with the magic done to make it work and be friends with the JVM. We will discuss why lazy evaluation makes a huge difference in PySpark, both in terms of general optimizations it opens up as well as Python specific considerations. From there we will explore much of the future of Spark, DataFrames &amp;amp; Datasets and what this means for PySpark. Most Spark DataFrame examples limit them selves to things written in the relational style query language, but we will explore how to add more functionality through UDFS.&lt;/p&gt;
&lt;p&gt;We will wrap up with looking at the different pieces of work being done to make PySpark faster, from using better interchange formats like Apache Arrow to crazy hair brained schemes inspired by (but not the fault of) the Javascript on Spark project.&lt;/p&gt;
&lt;p&gt;Hopefully no one is scared away from using Spark once they see the 300 small gnome like creatures behind the curtain, but parental guidance is encouraged for those who still believe in magic, reliable distributed systems, and vendor marketing brochures.&lt;/p&gt;
</summary><category term="keynote"></category><category term="pyspark"></category></entry><entry><title>Chief Data Scientist and co-founder of Continuum Analytics</title><link href="https://pyvideo.org/pydata-barcelona-2017/chief-data-scientist-and-co-founder-of-continuum-analytics.html" rel="alternate"></link><published>2017-05-20T09:30:00+02:00</published><updated>2017-05-20T09:30:00+02:00</updated><author><name>Travis Oliphant</name></author><id>tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/chief-data-scientist-and-co-founder-of-continuum-analytics.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData Barcelona 2017&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R.&lt;/p&gt;
&lt;p&gt;PyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.&lt;/p&gt;
</summary><category term="keynote"></category></entry></feed>