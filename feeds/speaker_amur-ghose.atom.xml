<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Amur Ghose</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_amur-ghose.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Batch norm with entropic regularization turns deterministic autoencoders into generative models</title><link href="https://pyvideo.org/uai-2020/batch-norm-with-entropic-regularization-turns-deterministic-autoencoders-into-generative-models.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Amur Ghose</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/batch-norm-with-entropic-regularization-turns-deterministic-autoencoders-into-generative-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Batch norm with entropic regularization turns deterministic autoencoders into generative models&lt;/p&gt;
&lt;p&gt;Amur Ghose (UWaterloo)*; Abdullah Rashwan (University of Waterloo); Pascal Poupart (University of Waterloo)&lt;/p&gt;
&lt;p&gt;The variational autoencoder is a well defined deep generative model that utilizes an encoder-decoder framework where an encoding neural network outputs a non-deterministic code for â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Batch norm with entropic regularization turns deterministic autoencoders into generative models&lt;/p&gt;
&lt;p&gt;Amur Ghose (UWaterloo)*; Abdullah Rashwan (University of Waterloo); Pascal Poupart (University of Waterloo)&lt;/p&gt;
&lt;p&gt;The variational autoencoder is a well defined deep generative model that utilizes an encoder-decoder framework where an encoding neural network outputs a non-deterministic code for reconstructing an input. The encoder achieves this by sampling from a distribution for every input, instead of outputting a deterministic code per input. The great advantage of this process is that it allows the use of the network as a generative model for sampling from the data distribution beyond provided samples for training. We show in this work that utilizing batch normalization as a source for non-determinism suffices to turn deterministic autoencoders into generative models on par with variational ones, so long as we add a suitable entropic regularization to the training objective.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>