<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - ST John</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 03 Aug 2020 00:00:00 +0000</lastBuildDate><item><title>Amortized variance reduction for doubly stochastic objective</title><link>https://pyvideo.org/uai-2020/amortized-variance-reduction-for-doubly-stochastic-objective.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Amortized variance reduction for doubly stochastic objective&lt;/p&gt;
&lt;p&gt;Ayman Boustati (University of Warwick)*; Sattar Vakili (Prowler.io); James Hensman (PROWLER.io); ST John (PROWLER.io)&lt;/p&gt;
&lt;p&gt;Approximate inference in complex probabilistic models such as deep Gaussian processes requires the optimisation of doubly stochastic objective functions. These objectives incorporate randomness both from mini-batch subsampling of the data and from Monte Carlo estimation of expectations. If the gradient variance is high, the stochastic optimisation problem becomes difficult with a slow rate of convergence. Control variates can be used to reduce the variance, but past approaches do not take into account how mini-batch stochasticity affects sampling stochasticity, resulting in sub-optimal variance reduction. We propose a new approach in which we use a recognition network to cheaply approximate the optimal control variate for each mini-batch, with no additional model gradient computations. We illustrate the properties of this proposal and test its performance on logistic regression and deep Gaussian processes.&amp;quot;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ayman Boustati</dc:creator><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2020-08-03:/uai-2020/amortized-variance-reduction-for-doubly-stochastic-objective.html</guid><category>UAI 2020</category></item></channel></rss>