<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Fri, 12 Oct 2018 00:00:00 +0000</lastBuildDate><item><title>Fast random number generation in Python and NumPy</title><link>https://pyvideo.org/pycon-za-2018/fast-random-number-generation-in-python-and-numpy.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A fast Random Number Generator (RNG) is key to doing Monte Carlo
simulations, efficiently initialising machine learning models, shuffling
long sequences of numbers and many tasks in scientific computing.
CPython and NumPy use implementations of the Mersenne Twister RNG and
rejection sampling to generate random numbers in an interval. The NumPy
implementation trades more samples for cheaper division by a power of
two. The implementation is very efficient when the random interval is a
power of two, but on average generate many more samples compared to the
GNU C or Java algorithms. The Python RNG uses an algorithm similar to
GNU C.&lt;/p&gt;
&lt;p&gt;A recent paper by Daniel Lemire (&lt;a class="reference external" href="https://arxiv.org/abs/1805.10941"&gt;https://arxiv.org/abs/1805.10941&lt;/a&gt;)
discusses an efficient division light method to generate uniform random
numbers in an interval. This method is apparently used in the Go
language. On 64-bit platforms there are also fast alternative RNGs that
perform comparatively on statistical examinations passing tests like
BigCrush. The splitmix64 (part of the standard Java API) and lehmer64
RNGs, for example, require approximately 1.5 cycles to generate 32
random bits (without using SIMD) while the 32-bit Mersenne Twister
requires approximately 10 cycles per 32 bits.&lt;/p&gt;
&lt;p&gt;This talk will discuss the inclusion of Lemire's method in NumPy as an
alternative to the current rejection sampling implementation. My
implementation results in a 2x - 3x improvement in the performance of
generating a sequence of random numbers. Using splitmix64 or lehmer64
RNGs in NumPy instead of the Mersenne Twister results in a further 2x
performance improvement.&lt;/p&gt;
&lt;p&gt;The random module in Python does not do the rejection sampling in C like
NumPy does. Much of the time to get a random number is therefore spent
in the Python code. This talk will also discuss a fast random Python
module that implements Lemire's method instead of the current rejection
sampling, provides alternative RNGs and moves more of the code into C.&lt;/p&gt;
&lt;p&gt;I'm considering doing pull requests for both the NumPy modification and
the Python fast random module and will present a detailed analysis of
the proposed modifications.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bernardt Duvenhage</dc:creator><pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-10-12:pycon-za-2018/fast-random-number-generation-in-python-and-numpy.html</guid></item><item><title>Semantic Concept Embedding for a natural language FAQ system</title><link>https://pyvideo.org/pycon-za-2017/semantic-concept-embedding-for-a-natural-language-faq-system.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For a number of months now work has been proceeding in order to bring
perfection to the crudely conceived idea of a super-positioning of
word vectors that would not only capture the tenor of a sentence in a
vector of similar dimension, but that is based on the high
dimensional manifold hypothesis to optimally retain the various
semantic concepts. Such a super-positioning of word vectors is called
the semantic concept embedding.&lt;/p&gt;
&lt;p&gt;Now basically the only new principle involved is that instead of using
the mean of the word vectors of a sentence one rather retains the
dominant semantic concepts over all of the words. A modification
informed by the aforementioned manifold hypothesis.&lt;/p&gt;
&lt;p&gt;The original implementation retains the absolute maximum value over each
of the dimensions of an embedding such as the GloVe embedding developed
at Stanford University. The use of these semantic concept vectors then
allows effective matching of users' questions to an online FAQ system
which in turn allows a natural language adaptation of said system that
easily achieves an F-score of 0.922 on the Quora dataset given only
three examples of how any particular question may be asked.&lt;/p&gt;
&lt;p&gt;The semantic concept embedding has now reached a high level of
development. First, an analysis of the word embedding is applied to find
the prepotent semantic concepts. The associated direction vectors are
then used to transform the embeddings in just the right way to optimally
detangle the principal manifolds and further increase the performance of
the natural language FAQ system.&lt;/p&gt;
&lt;p&gt;This talk will give an overview of:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The problem of semantic sentence embedding.&lt;/li&gt;
&lt;li&gt;How NLTK, numpy, and Python machine learning frameworks are used to solve the problem.&lt;/li&gt;
&lt;li&gt;How semantic concept embedding is used for natural language FAQ systems in chatbots, etc.&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bernardt Duvenhage</dc:creator><pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-06:pycon-za-2017/semantic-concept-embedding-for-a-natural-language-faq-system.html</guid></item></channel></rss>