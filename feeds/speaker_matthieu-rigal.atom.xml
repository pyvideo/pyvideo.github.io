<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_matthieu-rigal.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2015-08-02T00:00:00+00:00</updated><entry><title>Bringing PostgreSQL towards zero downtime migration with Python</title><link href="https://pyvideo.org/europython-2015/bringing-postgresql-towards-zero-downtime-migration-with-python.html" rel="alternate"></link><published>2015-08-02T00:00:00+00:00</published><updated>2015-08-02T00:00:00+00:00</updated><author><name>Matthieu Rigal</name></author><id>tag:pyvideo.org,2015-08-02:europython-2015/bringing-postgresql-towards-zero-downtime-migration-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Matthieu Rigal - Bringing PostgreSQL towards zero downtime migration with Python
[EuroPython 2015]
[24 July 2015]
[Bilbao, Euskadi, Spain]&lt;/p&gt;
&lt;p&gt;Using an SQL database offers a bunch of advantages; first of all its
maturity and that it is understood by almost every software developer.
But it has at least one main disadvantage. As the data is structured,
if you want to modify the structure, for example on a long-running
project, you need a migration and therefore almost for sure, a
downtime.&lt;/p&gt;
&lt;p&gt;When you have to make a migration, to modify the structure of data for
a small amount of records, it is so fast that it never gets
problematic. But if you think to modify the structure of tables
containing millions or billions of records, the time required to
simply apply the structural change is problematic.&lt;/p&gt;
&lt;p&gt;Here are some changes we are working on at orderbird to go towards
zero downtime migrations using some of the latest improvements of
PostgreSQL 9.4, mainly logical replication and mixing in a little
magic of some python scripting with psycopg.&lt;/p&gt;
</summary></entry></feed>