<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Bayesian Optimization</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_bayesian-optimization.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2021-08-25T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Bayesian Optimization - Can you do better than randomly guessing parameters?</title><link href="https://pyvideo.org/euroscipy-2017/bayesian-optimization-can-you-do-better-than-randomly-guessing-parameters.html" rel="alternate"></link><published>2017-08-31T00:00:00+00:00</published><updated>2017-08-31T00:00:00+00:00</updated><author><name>Tim Head</name></author><id>tag:pyvideo.org,2017-08-31:/euroscipy-2017/bayesian-optimization-can-you-do-better-than-randomly-guessing-parameters.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Choosing the right hyper-parameters for a deep neural network, configuring a fluid dynamics simulation or finding the recipe of the next prize winning beer have three things in common: each trial is expensive, you don't have an analytic function you can minimise with &lt;cite&gt;scipy.minimize&lt;/cite&gt; and you only get …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Choosing the right hyper-parameters for a deep neural network, configuring a fluid dynamics simulation or finding the recipe of the next prize winning beer have three things in common: each trial is expensive, you don't have an analytic function you can minimise with &lt;cite&gt;scipy.minimize&lt;/cite&gt; and you only get noisy observations from each trial.&lt;/p&gt;
&lt;p&gt;Bayesian optimisation (BO) to the rescue! BO is a clever piece of math designed to solve exactly these kinds of problems. This talk is for people who have to find the best configuration for an &amp;quot;algorithm&amp;quot; that is expensive to run. Currently you might be performing a grid search or trying settings at random. Neither of these learn from observations they have already made. The fundamental idea of BO is to use previous observations to make a prediction about which settings to try next. By doing this you can reduce the number of evaluations needed to find the optimal settings.&lt;/p&gt;
&lt;p&gt;In this talk you will learn about bayesian optimisation, how to implement the basics yourself, some tricks of the trade, and I will introduce you to the scikit-optimize library: a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for BO and attempts to be accessible and easy to use in many different contexts.&lt;/p&gt;
&lt;p&gt;We will start by looking at some simple examples in depth, discuss when BO is the right tool and when not, and then use scikit-optimize to find the best hyper-parameters for a neural network.&lt;/p&gt;
</content><category term="EuroSciPy 2017"></category><category term="bayesian optimization"></category></entry><entry><title>Procesos Gaussianos y Optimización Bayesiana</title><link href="https://pyvideo.org/riiaa-2021/procesos-gaussianos-y-optimizacion-bayesiana.html" rel="alternate"></link><published>2021-08-25T00:00:00+00:00</published><updated>2021-08-25T00:00:00+00:00</updated><author><name>Antonio del Rio Chanona</name></author><id>tag:pyvideo.org,2021-08-25:/riiaa-2021/procesos-gaussianos-y-optimizacion-bayesiana.html</id><content type="html"></content><category term="RIIAA 2021"></category><category term="Artificial Intelligence"></category><category term="Bayesian Optimization"></category><category term="Gaussian Processes"></category><category term="Statistical Theory"></category></entry></feed>