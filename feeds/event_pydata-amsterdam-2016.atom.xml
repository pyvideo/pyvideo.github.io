<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-amsterdam-2016.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-03-26T00:00:00+00:00</updated><entry><title>Building a live face recognition system in the blink of a very slow eye</title><link href="https://pyvideo.org/pydata-amsterdam-2016/building-a-live-face-recognition-system-in-the-blink-of-a-very-slow-eye.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Rodrigo Agundez</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/building-a-live-face-recognition-system-in-the-blink-of-a-very-slow-eye.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;In this tutorial we will create a face recognition application from scratch, it will provide you hands-on experience on the basics of Face Recognition. We will use the OpenCV library which makes the tutorial accessible to beginners. Together, we'll go from building our face dataset to recognizing faces in a live video. If time permits we will use this face recognition system to classify banking da&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Building a live face recognition system in the blink of a very slow eye&lt;/p&gt;
&lt;p&gt;In this hands-on tutorial we will build a live face recognition system from scratch with the use of the OpenCV methods. Since face recognition is the main goal of this tutorial we will form teams of 2-3 people and recognize the faces in a live feed. We will make use of the OpenCV computer vision and machine learning library. OpenCV includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms&lt;/p&gt;
</summary><category term="tutorial"></category><category term="opencv"></category></entry><entry><title>CART: Not only Classification and Regression Trees</title><link href="https://pyvideo.org/pydata-amsterdam-2016/cart-not-only-classification-and-regression-trees.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Marc Garcia</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/cart-not-only-classification-and-regression-trees.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;Decision trees are very simple methods compared to Support Vector Machines, or Deep Learning. But they have some interesting properties that make them unique. For classification, for regression, or to extract probabilities, decision trees are easy to set up, and debug. And they are excellent to get a better understanding of your data.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;This talk will cover Decision Trees, from theory, to their implementation in Python.&lt;/p&gt;
&lt;p&gt;The talk will have a very practical approach, using examples and real cases to illustrate how to use decision trees, what we can expect from using them, and what kind of problems we will need to address.&lt;/p&gt;
&lt;p&gt;The main topics covered will include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What are decision trees?&lt;/li&gt;
&lt;li&gt;How decision trees are trained?&lt;/li&gt;
&lt;li&gt;Data preprocessing for decision trees&lt;/li&gt;
&lt;li&gt;Understanding your data better with decision tree visualization&lt;/li&gt;
&lt;li&gt;Debugging decision trees using common sense and prior domain knowledge&lt;/li&gt;
&lt;li&gt;Avoiding overfitting, without cross-validation&lt;/li&gt;
&lt;li&gt;Python implementation&lt;/li&gt;
&lt;li&gt;Performance&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Closing Notes and Lightning Talks</title><link href="https://pyvideo.org/pydata-amsterdam-2016/closing-notes-and-lightning-talks.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/closing-notes-and-lightning-talks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary><category term="lightning talks"></category></entry><entry><title>Data driven literary analysis</title><link href="https://pyvideo.org/pydata-amsterdam-2016/data-driven-literary-analysis.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Serena Peruzzo</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/data-driven-literary-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Can unsupervised learning mimic a literary critic? This talk will give an overview of unsupervised document classification techniques and apply them to the analysis and classification of Shakespeare’s plays.&lt;/p&gt;
&lt;p&gt;Unsupervised document classification addresses the problem of assigning categories to documents without the use of a training set or predefined categories. This is useful to enhance information retrieval, the basic assumption being that similar contents are also relevant to the same query. A similar assumption is made in literature to define literary genres and sub-genres, where works which share specific conventions in terms of form and content are described by the same genre.&lt;/p&gt;
&lt;p&gt;The talk gives an overview of document clustering and its challenges, with a focus on dimensionality reduction and how to address it with topic modelling techniques like LDA (Latent Dirichlet Allocation). Using Shakespeare’s body of work as a case study, the talk describes how to use nltk, sklearn and gensim to process and analyse theatrical works with the final goal of testing whether document clustering yields to the same classification given by literature experts.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/sereprz/data-driven-literary-analysis-an-unsupervised-approach-to-text-analysis-and-classification"&gt;https://speakerdeck.com/sereprz/data-driven-literary-analysis-an-unsupervised-approach-to-text-analysis-and-classification&lt;/a&gt;&lt;/p&gt;
</summary><category term="nltk"></category><category term="sklearn"></category><category term="gensim"></category></entry><entry><title>Do Angry People Have Poor Grammar?</title><link href="https://pyvideo.org/pydata-amsterdam-2016/do-angry-people-have-poor-grammar.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Ben Fields</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/do-angry-people-have-poor-grammar.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;This talk is about two things: natural language processing (NLP) and statistical dependance. We will walk through the ins and outs of sentiment analysis in Python (mostly using NLTK) and a swift introduction to the statistics of dependence. We'll put these techniques to use on a dataset of every reddit public comment, perhaps the best data source for exploring the behavior of shouty Web comments.&lt;/p&gt;
&lt;p&gt;This talk is about two things: natural language processing (NLP) and statistical dependence. We will embark on a data science workflow using various python scientific computing tools to better understand the behavior of commenters on Reddit. To do this we'll go through an introduction to sentiment analysis in Python (mostly using NLTK) and a swift explanation of the statistics of variable dependence.&lt;/p&gt;
&lt;p&gt;We'll couple these freshly learned methods with an excellent dataset for this domain: every public reddit comment. We'll talk a bit about handling and preprocessing data of this size and character. Then we'll compile scores for both sentiment and spelling/grammar. In the end we may just discover if angry comment are also grammatically poor comments. And the audience will walk away a few more tools in scientific computing toolbelt.&lt;/p&gt;
&lt;p&gt;Slides available here:  &lt;a class="reference external" href="https://speakerdeck.com/bfields/do-angry-people-have-poor-grammar-an-exploration-of-language-processing-and-statistics-in-python"&gt;https://speakerdeck.com/bfields/do-angry-people-have-poor-grammar-an-exploration-of-language-processing-and-statistics-in-python&lt;/a&gt;&lt;/p&gt;
</summary><category term="nltk"></category></entry><entry><title>Finding relations in documents with IEPY</title><link href="https://pyvideo.org/pydata-amsterdam-2016/finding-relations-in-documents-with-iepy.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Daniel Moisset</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/finding-relations-in-documents-with-iepy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;IEPY is an open source tool to identify relations between entities described in natural language documents. This talk will show you what can you use it for, what it can and can not do, and describe some real world examples of its use. You should come if you're dealing with information extraction problems on text, and after the talk you'll know if IEPY is the tool for you and what you need to do.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;IEPY is an open source tool to identify entities and relations between them, as described in natural language documents. In other words, it is a tool for extracting structured information from unstructured sources. It was developed as a joint project between Machinalis and the Natural Language Processing research group of the University at Cordoba, Argentina. It recently won the 2015 Sadosky Award for &amp;quot;Industry and Academy Collaboration Project&amp;quot;. IEPY is developed in python and can apply and mix rule based approaches, machine learning approaches, and manual tagging. It is actually designed to allow a hybrid approach (starting for rules, machine learning from that, using a human to clarify uncertain cases, and then integrate human answers in the machine learning model, etc). It includes the document store, learning engine, and a user interface to make it practical to provide manually tagged inputs by non-technical people. This talk will give an overview of what IEPY does (and general details on how it does it), but will be strongly focused on what kind of problems it is best applied to, what are the main situations where it can be challenging to implement it. I will support that description showcasing two of our main success cases: one analysis that was done over the techcrunch news articles to detect funding events, and one analysis done over military files from the last dictatorship in Argentina to help track people involved in human rights violations.&lt;/p&gt;
</summary><category term="iepy"></category></entry><entry><title>`from __past__ import print_statement`: a Dadaist Rejection of Python 2 vs 3</title><link href="https://pyvideo.org/pydata-amsterdam-2016/from-__past__-import-print_statement-a-dadaist-rejection-of-python-2-vs-3.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/from-__past__-import-print_statement-a-dadaist-rejection-of-python-2-vs-3.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;If the title doesn't make any sense, then there's no hope that the description will be any better. This talk will be a strange dive into interpreter hacks, the pointlessness of the Python 2 vs 3 debate, and the twisted artistic drive that pushes the speaker to come up with these perversions of the Python language. Prepared to be simultaneously repulsed, intrigued, and completely bored.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Honestly, does anyone read abstracts? Clearly you do! Maybe the talk is happening right now. You probably have no idea what the speaker is saying. You're looking at your phone: &amp;quot;what was this talk about again?&amp;quot;&lt;/p&gt;
&lt;p&gt;Would you be surprised to find out that even the speaker doesn't know what this talk is about? I hope so—I hope you would be surprised; how could the speaker not know what he's talking about? He's wearing a SUIT after all!&lt;/p&gt;
&lt;p&gt;(This talk constitutes he opinions of the presenter alone and do not reflect the opinions of &amp;quot;PyData&amp;quot; or &amp;quot;NumFOCUS.&amp;quot;)&lt;/p&gt;
</summary></entry><entry><title>From Data Science to Production - deploy, scale, enjoy!</title><link href="https://pyvideo.org/pydata-amsterdam-2016/from-data-science-to-production-deploy-scale-enjoy.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Sergii Khomenko</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/from-data-science-to-production-deploy-scale-enjoy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;Data cleaning is the first step of every Data Science project. Next one does Data Science. The talk covers a missing step of deployment and scaling Data Applications in production. We will go through all major steps of the process like Dockerizing application, Continuous Deployment with further AWS stack creation and rolling deploys although also covering new trends in Serverless architecture.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Data Science is quite a young field. One of the definitions of Data Scientist: Person who is better at statistics than any software engineer and better at software engineering than any statistician. Hence, it's quite important to talk not only about best practices of feature generation and not overfitting but also about more of software engineering topics.&lt;/p&gt;
&lt;p&gt;The talk is based on our experience of Data Science developments at Stylight, an international fashion e-commerce company, that operates in 15 countries worldwide. We refer to our Data Applications written in R and Python, Scala; but the content is not limited to mentioned languages and applicable others.&lt;/p&gt;
&lt;p&gt;The talk consists three main parts. A first part introduces best practices of development. How to structure your development, make deployment easy and reproducible, how to make Continuous Integration and commit triggered deployments. The second part covers production deployment to AWS stack, in particular focusing on concepts of immutable infrastructure and infrastructure as code. The last part about using serverless architecture for data applications. We introduce an example of our outlier detection system, that automatically scales based on such approach.&lt;/p&gt;
</summary></entry><entry><title>Gotta catch'em all: recognizing sloppy work in crowdsourcing tasks</title><link href="https://pyvideo.org/pydata-amsterdam-2016/gotta-catchem-all-recognizing-sloppy-work-in-crowdsourcing-tasks.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Maciej Gryka</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/gotta-catchem-all-recognizing-sloppy-work-in-crowdsourcing-tasks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;If you have ever used crowdsourcing, you know that dealing with sloppy workers is a major part of the effort. Come see this talk if you want to learn about how to solve this problem using machine learning and some elbow grease. As a bonus, you will also find out how to properly persist your ML models and use them to serve predictions through an HTTP API.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In 2016 nobody needs convincing that crowdsourced work is a solution to many problems from data labeling, to gathering subjective opinions, to producing transcripts etc. Turns out it can also work really well for functional software testing - but it's not easy to get right.&lt;/p&gt;
&lt;p&gt;One well-known problem with crowdsourcing is sloppy work - where people perform only the absolute minimum actions allowing them to get paid, without actually fulfilling the intended tasks. In many scenarios this can be counteracted by asking multiple workers to complete the same task, but that dramatically increases cost and can still be error-prone. Detecting lazy work is another way to increase quality of gathered data and we have found a way to do this reliably for quite a large variety of tasks.&lt;/p&gt;
&lt;p&gt;In this talk I will describe how we have trained a machine learning model to discriminate between good and sloppy work. The outline is as follows:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;describe the specific problem we had (5m)&lt;/li&gt;
&lt;li&gt;overview of the solution (2m)&lt;/li&gt;
&lt;li&gt;ML model details (10m)&lt;/li&gt;
&lt;li&gt;data capture&lt;/li&gt;
&lt;li&gt;labelling&lt;/li&gt;
&lt;li&gt;balancing the dataset&lt;/li&gt;
&lt;li&gt;feature engineering&lt;/li&gt;
&lt;li&gt;training, retraining&lt;/li&gt;
&lt;li&gt;model itself&lt;/li&gt;
&lt;li&gt;model persistence (10m)&lt;/li&gt;
&lt;li&gt;productizing the result by putting it behind an HTTP API (3m)&lt;/li&gt;
&lt;li&gt;limitations, trade-offs, what could we do better (3m)&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>How big are your banks? Because the ones in the U.S. are pretty big!</title><link href="https://pyvideo.org/pydata-amsterdam-2016/how-big-are-your-banks-because-the-ones-in-the-us-are-pretty-big.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>David R  Pugh</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/how-big-are-your-banks-because-the-ones-in-the-us-are-pretty-big.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;Using Python, Pandas, Scikit-learn, and Bokeh I will explore detailed quarterly balance sheet data for all deposit-taking U.S banks for the years 1992 through 2015. I will assess the statistical support for Zipf's Law as a model for the upper tail of the bank size distribution and study how the upper tail of this distribution has evolved over the last 25 years.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Using Python, Pandas, Scikit-learn and Bokeh I will explore a 5+ GB data set of publicly available balance sheet data covering all Federal Deposit Insurance Corporation (FDIC) regulated U.S. banks from Q1 1992 through Q4 2015. The objective will be to understand how to measure the size of a bank and to model the distribution of bank size.&lt;/p&gt;
&lt;p&gt;Specifically, I will develop a few different measures of bank size and then I will assess the statistical support for Zipf's Law (i.e., a the power law distribution with a scaling exponent of roughly α=2) as an appropriate model for the upper tail of the size distribution of U.S. banks. Although I will find statistically significant departures from Zipf's Law for most measures of bank size in most years, a power law distribution with α = 1.9 out performs other plausible heavy-tailed alternative distributions.&lt;/p&gt;
&lt;p&gt;If there is time, I may discuss some possible policy implications suggested by the data analysis that I have done so far.&lt;/p&gt;
</summary></entry><entry><title>Hybrid Recommender Systems in Python</title><link href="https://pyvideo.org/pydata-amsterdam-2016/hybrid-recommender-systems-in-python.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Maciej Kula</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/hybrid-recommender-systems-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Systems based on collaborative filtering are the workhorse of recommender systems. They yield great results when abundant data is available. Unfortunately, their performance suffers when encountering new items or new users.&lt;/p&gt;
&lt;p&gt;In this talk, I'm going to talk about hybrid approaches that alleviate this problem, and introduce a mature, high-performance Python recommender package called LightFM.&lt;/p&gt;
&lt;p&gt;Introduction to collaborative filtering.
Works well when data is abundant (MovieLens, Amazon), but poorly when new users and items are common.
Introduce hybrid approaches: metadata embeddings.
This is implemented in LightFM.
LightFM has a couple of tricks up its sleeve: multicore training, training with superior ranking losses.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/maciejkula/hybrid-recommender-systems-at-pydata-amsterdam-2016"&gt;https://speakerdeck.com/maciejkula/hybrid-recommender-systems-at-pydata-amsterdam-2016&lt;/a&gt;&lt;/p&gt;
</summary><category term="lightfm"></category></entry><entry><title>Improving PySpark Performance: Spark performance beyond the JVM</title><link href="https://pyvideo.org/pydata-amsterdam-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames/Datasets and traditional RDDs with Python. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</summary></entry><entry><title>Jupyter: Notebooks in Multiple Languages for Data Science</title><link href="https://pyvideo.org/pydata-amsterdam-2016/jupyter-notebooks-in-multiple-languages-for-data-science.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Thomas Kluyver</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/jupyter-notebooks-in-multiple-languages-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;We'll talk about how the Jupyter Notebook has evolved from a Python specific tool to a general data science tool that supports many different languages, and about our own experiences in supporting a wide variety of languages for data science. We'll also demonstrate some of the new features and ideas being developed in and around the project.&lt;/p&gt;
&lt;p&gt;Jupyter notebooks have become an invaluable tool for all kinds of data science. Originally developed as part of the IPython project, notebooks have evolved from a Python specific tool to support many programming languages; more than 50 different execution kernels have now been published. For all of these languages, notebooks are a way to record and describe a data science workflow, and then share it, publicly or privately, allowing the recipients to easily modify and execute the code.&lt;/p&gt;
&lt;p&gt;We’ll describe the architectural changes and decisions involved in the transition to supporting multiple languages, as well as our own experience in supporting data science languages ranging from C++ to R to Bash. You’ll also get a high-level understanding of how to create a new kernel, if a language you’re excited about is not yet supported.&lt;/p&gt;
&lt;p&gt;We’ll also highlight some of the current development work taking place in and around Jupyter, including redesigned UI, mechanisms for collaboration on notebooks, ways to share live, executable notebooks online, and projects that reuse the Jupyter machinery in different user interfaces.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1PHnnkKYgjq1lcSDaVyhZP0Fs7qC70iA07b2Jv0uisUE/edit?usp=sharing"&gt;https://docs.google.com/presentation/d/1PHnnkKYgjq1lcSDaVyhZP0Fs7qC70iA07b2Jv0uisUE/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;
</summary><category term="jupyter notebook"></category></entry><entry><title>Machine Learning with Scikit-Learn</title><link href="https://pyvideo.org/pydata-amsterdam-2016/machine-learning-with-scikit-learn.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Andreas Mueller</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/machine-learning-with-scikit-learn.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;Scikit-learn has emerged as one of the most popular open source machine learning toolkits, now widely used in academia and industry. scikit-learn provides easy-to-use interfaces to perform advances analysis and build powerful predictive models. The tutorial will cover basic concepts of machine learning, such as supervised and unsupervised learning, cross validation and model selection.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Scikit-learn has emerged as one of the most popular open source machine learning toolkits, now widely used in academia and industry. scikit-learn provides easy-to-use interfaces to perform advances analysis and build powerful predictive models. The tutorial will cover basic concepts of machine learning, such as supervised and unsupervised learning, cross validation and model selection. We will see how to prepare data for machine learning, and go from applying a single algorithm to building a machine learning pipeline. We will also cover how to build machine learning models on text data, and how to handle very large datasets.&lt;/p&gt;
&lt;p&gt;If you want to follow along, there will be material online that you can download beforehand: &lt;a class="reference external" href="https://github.com/amueller/pydata-amsterdam-2016"&gt;https://github.com/amueller/pydata-amsterdam-2016&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I usually recommend to install the anaconda python distribution before coming: &lt;a class="reference external" href="https://store.continuum.io/cshop/anaconda/"&gt;https://store.continuum.io/cshop/anaconda/&lt;/a&gt; and make sure that you are able to run the ipython notebook.&lt;/p&gt;
</summary><category term="scikit-learn"></category></entry><entry><title>Measuring Search Engine Quality using Spark and Python</title><link href="https://pyvideo.org/pydata-amsterdam-2016/measuring-search-engine-quality-using-spark-and-python.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Sujit Pal</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/measuring-search-engine-quality-using-spark-and-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;We describe a system built using Python and Apache Spark which measured the effectiveness of different query configurations of an Apache Solr search platform, using click logs for a reference query set of 80,000+ user queries. The system replays the click logs against the engine to compute the Average Click Rank (ACR) metric as a proxy for user satisfaction, providing a way to identify improvements in quality without having to do a production deployment, and ensuring that only improved configurations are submitted to a slow and expensive A/B testing process.&lt;/p&gt;
&lt;p&gt;For each search engine configuration, the ACR is recomputed by replaying the query logs against it and finding the position (or click rank) for the user's selected document. The ACR is computed by averaging those positions across all user queries in the query log. Lower click ranks are indicative of better engine configuration for that query, since it implies that the user found what they were looking for nearer the top of the results. Similarly, a low ACR across all queries is an indicator of good search engine configuration as a whole.&lt;/p&gt;
&lt;p&gt;This system has also been used to analyze user behavior, by partitioning the results across content types, response times, etc, and analyzing differences in click rank distribution. It has also been used to identify and investigate slow queries, resulting in improvements in which have also benefited the search application.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/sujitpal/measuring-search-engine-quality-using-spark-and-python"&gt;http://www.slideshare.net/sujitpal/measuring-search-engine-quality-using-spark-and-python&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>More psychologists need to learn python</title><link href="https://pyvideo.org/pydata-amsterdam-2016/more-psychologists-need-to-learn-python.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Titus von Köller</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/more-psychologists-need-to-learn-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;This will be a talk about my experience of supplementing psychology knowledge with programming. It will feature the case study of agent-based modeling in psychology. As an extrapolation from this, it will talk about the benefits of seeing the world through the perspective of theoretical and practical CS, while retaining the qualitative perspective I gained in Psychology.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;While writing the thesis for my psychology studies, I came to view programming as unique perspective onto the world. In the abstract, psychology, too, studies processes and interactions between processes and it seemed intuitive to me, that programming could be key in making the abstract behind descriptive wording tangible through the real-world implementation of functional models. Therefore, I got into programming, learned Python, and completed a from scratch agent-based model on simple models of cognition in Python and IPython Notebook as an experimental basis for the aforementioned thesis.&lt;/p&gt;
&lt;p&gt;Since then, I have been hooked on computing and am currently building a strong theoretical foundation in CS. Therefore, in this talk I will share the case study of using an agent-based model in Psychology and reflect upon cross-insemination between these two disciplines.&lt;/p&gt;
</summary></entry><entry><title>Networks meet Finance in Python</title><link href="https://pyvideo.org/pydata-amsterdam-2016/networks-meet-finance-in-python.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Miguel Vaz</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/networks-meet-finance-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;I will talk about network models in finance, and walk through real data and very visual examples using the pydata toolset - pandas, bokeh, pandas, networkx, ipywidgets. Special focus will be given to correlation networks, with applications to market characterization and portfolio risk management (as done in Pozzi 2013) using the latest available market data.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In the course of the 2008 Lehman and the subsequent European debt crisis, it became clear that both financial industry and regulators had underestimated the degree of interconnectedness and interdependency across financial assets and institutions. This type of information is especially well represented by network models, which had first gained popularity in computer science, biology and social sciences.&lt;/p&gt;
&lt;p&gt;The study of network models in finance is already providing insight into the structure of the financial world and the economy. Network models are proving to be useful tools for providing early-warning signals of systemic risk (e.g. Squartini 2013), measuring liquidity and concentration risk, identifying sectors from time-series correlations (e.g. Fenn 2011) as well as insights into finding diversified baskets of assets in the classical investment framework (e.g. Pozzi 2013).&lt;/p&gt;
&lt;p&gt;I will provide an overview of some of the aforementioned work, and walk through (real data) examples using the pydata toolset. Special focus will be given to the study of correlation networks, with applications to portfolio risk management as in (Pozzi 2013) using the latest available market data. The examples make heavy use of pandas, bokeh, pandas, networkx, ipywidgets.&lt;/p&gt;
</summary><category term="pandas"></category><category term="bokeh"></category><category term="networkx"></category><category term="ipywidgets"></category></entry><entry><title>NoSQL Python: making data frames work for you in a non-rectangular world</title><link href="https://pyvideo.org/pydata-amsterdam-2016/nosql-python-making-data-frames-work-for-you-in-a-non-rectangular-world.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Aileen Nielsen</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/nosql-python-making-data-frames-work-for-you-in-a-non-rectangular-world.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Anyone who's dealt with a CSV file that contains arrays or a JSON with nested fields knows the pain of shoehorning non-rectangular data into standard Python data tools, such as data frame. This presentation will show you Python best practices for managing such non-rectangular data and highlight new opportunities for using &amp;quot;NoSQL&amp;quot; Python for interesting and painless analyses of real world data.&lt;/p&gt;
&lt;p&gt;NoSQL Python sounds suspiciously trendy. Is this a real thing?&lt;/p&gt;
&lt;p&gt;Most commonly used data frameworks in Python rely on SQL-like thinking. They work great, but unfortunately they don't always match real world data. A server fails intermittently, and you find you're missing measurements in an unpredictable way. A patient drops in and out of a study . You ask survey respondents what their favorite color is but they give you five colors. Suddenly you don't know quite how many columns you need or what data types those columns should have.&lt;/p&gt;
&lt;p&gt;These are just a few examples of real-world, non-rectangular data. Most of this real-world data makes its way into nested JSON, irregularly formatted JSON, unreliable API results, and slightly quirky CSV files.&lt;/p&gt;
&lt;p&gt;The nitty-gritty: how do you 'do' NoSQL Python?&lt;/p&gt;
&lt;p&gt;We'll cover best-practices for dealing with a variety of situations, starting with plain-vanilla JSON and branching off to defensive practices for dealing with highly-nested JSON, unreliably formatted API results (JSON or otherwise), and CSVs with array and other kinds of problematic fields.&lt;/p&gt;
&lt;p&gt;We'll also talk about best practices for processing these in terms of speeding up analysis and storing data in an easy-to-access and easy-to-understand format. In this portion of the talk, we'll still focus on keeping to data frames, making the rectangular format work for our non-rectangular data.&lt;/p&gt;
&lt;p&gt;Finally we'll take a look at roll-your-own NoSQL Python, unabashedly NoSQL frameworks, and what you should look for as you architect your own data decisions. We'll conclude with general rules of thumb for knowing the best way to proceed before you go too far down the wrong road.&lt;/p&gt;
&lt;p&gt;Now you've got it, what to do with it?&lt;/p&gt;
&lt;p&gt;The most interesting data and data-driven decision-making is coming out of non-rectangular data sources. What people do, how and when they do it, and what our computers do in response all comes down to non-rectangular, NoSQL data and NoSQL data-driven decision making. I'll highlight some well-known and lesser-known examples of NoSQL data results and the growing need for more work of this kind.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/aileen-nielsen-nosql-python-making-data-frames-work-for-you-in-a-nonrectangular-world"&gt;http://www.slideshare.net/PyData/aileen-nielsen-nosql-python-making-data-frames-work-for-you-in-a-nonrectangular-world&lt;/a&gt;&lt;/p&gt;
</summary><category term="nosql"></category></entry><entry><title>Pandas: from bdate_range to wide_to_long</title><link href="https://pyvideo.org/pydata-amsterdam-2016/pandas-from-bdate_range-to-wide_to_long.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Giovanni Lanzani</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/pandas-from-bdate_range-to-wide_to_long.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;The notebook can be found at &lt;a class="reference external" href="http://s.lanzani.nl/pydataamsterdam"&gt;http://s.lanzani.nl/pydataamsterdam&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;In this tutorial we will walk through the most useful pandas features with examples and exercises. The tutorial will assume some basic Python knowledge.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In this tutorial we will walk through the most useful pandas features with examples and exercises. We will take a look at:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Series&lt;/li&gt;
&lt;li&gt;Dataframes&lt;/li&gt;
&lt;li&gt;(Re)Indexing&lt;/li&gt;
&lt;li&gt;Dropping data&lt;/li&gt;
&lt;li&gt;Adding data&lt;/li&gt;
&lt;li&gt;Filtering&lt;/li&gt;
&lt;li&gt;Apply functions to dataframes&lt;/li&gt;
&lt;li&gt;Missing data&lt;/li&gt;
&lt;li&gt;Merge and combine&lt;/li&gt;
&lt;li&gt;Stacking and unstacking&lt;/li&gt;
&lt;li&gt;Replacing values&lt;/li&gt;
&lt;li&gt;Binary decomposition&lt;/li&gt;
&lt;li&gt;Plotting&lt;/li&gt;
&lt;li&gt;Data aggregation&lt;/li&gt;
&lt;li&gt;Quantile bucket analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During the tutorial we will decide how many exercises to fit. The students will get the notebook if time won't be enough to cover everything.&lt;/p&gt;
</summary><category term="pandas"></category></entry><entry><title>PyData Amsterdam 2016</title><link href="https://pyvideo.org/pydata-amsterdam-2016/pydata-amsterdam-2016.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Unknown</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/pydata-amsterdam-2016.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary><category term="recap"></category></entry><entry><title>Python based predictive analytics with GraphLab Create</title><link href="https://pyvideo.org/pydata-amsterdam-2016/python-based-predictive-analytics-with-graphlab-create.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Danny Bickson</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/python-based-predictive-analytics-with-graphlab-create.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;One of the most exciting areas in data science is the development of new predictive applications; apps used to drive product recommendations, predict machine failures, forecast airfare etc. These applications output real-time predictions and recommendations in response to user and machine input to directly derive business value and create cool experience.&lt;/p&gt;
&lt;p&gt;The most interesting apps utilize multiple types of data (tables, graphs, text &amp;amp; images) in a creative way. In this talk, we will show how to quickly build and deploy a predictive app that exploits the power of combining different data types together using GraphLab Create, our open source based Python software.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/PyData/danny-bickson-python-based-predictive-analytics-with-graphlab-create"&gt;http://www.slideshare.net/PyData/danny-bickson-python-based-predictive-analytics-with-graphlab-create&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Realtime Bayesian A-B testing with Spark Streaming</title><link href="https://pyvideo.org/pydata-amsterdam-2016/realtime-bayesian-a-b-testing-with-spark-streaming.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Dennis Bohle</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/realtime-bayesian-a-b-testing-with-spark-streaming.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
</summary></entry><entry><title>Running (snippets of) Python in the browser</title><link href="https://pyvideo.org/pydata-amsterdam-2016/running-snippets-of-python-in-the-browser.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Almar Klein</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/running-snippets-of-python-in-the-browser.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;In this talk, we’ll start with an overview of solutions to run Python in the browser. Then we’ll explain how our solution (PyScript) works and what its advantages/disadvantages are. Next, we’ll demonstrate its use in a few real-world projects.&lt;/p&gt;
&lt;p&gt;It is evident that “the web” is becoming an increasingly important place to publish research findings. Apart from the obvious advantage of being able to reach a wide audience at negligible cost, browser technology allows for increasingly sophisticated means to present your data and findings in various interactive ways.&lt;/p&gt;
&lt;p&gt;Unfortunately, the language of the web (JavaScript) is a language that is notorious for its many flaws. It seems that Pythonistas are especially repelled by the language, since the number of projects that try to “run Python in the browser” is steadily growing.&lt;/p&gt;
&lt;p&gt;In our approach (PyScript), we do not aim to run the full Python language in the browser. It’s rather a way to write JavaScript using a Python syntax, making it as Pythonic as we can. There are a few pitfalls, but these have been reduced over time, which makes writing PyScript feel almost like writing Python.&lt;/p&gt;
&lt;p&gt;One advantage of our approach is that one can generate snippets of JavaScript code that can interact as-is with other JavaScript libraries. Combined with the fact that PyScript is valid Python, it allows for a natural way to define client-side reactivity inside common Python modules. Also, it makes PyScript faster than most other Python-in-the-browser solutions.&lt;/p&gt;
&lt;p&gt;One of its use-cases is Bokeh’s new capability to define client-side callbacks in Python. There are plans to use a similar approach to allow users to define custom Bokeh models. The Flexx project is a pure Python widget toolkit, that renders in the browser. All widgets are implemented with PyScript (wrapping PhosphorJS for some layouts). In a project with Clinical Graphics, PyScript is used in a system that allows visualization and user-annotations of medical images, with plans for 3D visualizations.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1X069udByOTw3d4-NVU14XwhTANCdECvjY0R7fDuXTpA/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000"&gt;https://docs.google.com/presentation/d/1X069udByOTw3d4-NVU14XwhTANCdECvjY0R7fDuXTpA/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000&lt;/a&gt;&lt;/p&gt;
</summary><category term="pyscript"></category></entry><entry><title>Scaling Up Genomics with Spark</title><link href="https://pyvideo.org/pydata-amsterdam-2016/scaling-up-genomics-with-spark.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Sean Owen</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/scaling-up-genomics-with-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;This talk will briefly introduce the problem of genomics and existing home-grown efforts to bring &amp;quot;big data&amp;quot; technology to solve genomics&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;It's amazing that our genome so completely and uniquely encodes each of us with a simple 4-protein code, like a file. More amazingly, we're so similar that we can build a reference map of human genomes and reason about commonalities. Genomics has taken off in the last two decades driven largely by advances in computing; the work of mapping the genome is incredibly data and compute intensive. This talk will briefly introduce the problem of genomics and existing home-grown efforts to bring &amp;quot;big data&amp;quot; technology to solve it. It will compare these with the separate rise of technologies like Apache Hadoop and Spark, and how these ideas are helping genomics scale up even further.&lt;/p&gt;
</summary><category term="genomics"></category></entry><entry><title>Store and manage data effortlessly with HDF5</title><link href="https://pyvideo.org/pydata-amsterdam-2016/store-and-manage-data-effortlessly-with-hdf5.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Margaret Mahan</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/store-and-manage-data-effortlessly-with-hdf5.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;Are you looking for accessible, compressed, organized data? HDF5 might be the solution you’re looking for. HDF5 works like a file system within a file, designed for flexible and efficient storage and I/O for high volume, complex data. Come learn from a Pyentist how to leverage HDF5, get started with h5py, and see a real-world example of a processing pipeline utilizing HDF5.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Are you&lt;/p&gt;
&lt;p&gt;a Pyentist1?
frequently ‘grep’-ing?
drowning in ASCII files?
extending filenames for each processing step?
looking for accessible, compressed, organized data?
If you answered yes to any of these questions, then HDF5 might be the solution you’re looking for. HDF5 is entirely open source and supported by a variety of programming languages and tools, including Python (h5py). HDF5 not only supports large, complex, heterogeneous data but is self-describing and supports data slicing. In this talk, you’ll learn about embracing HDF5 from a Pyentist.&lt;/p&gt;
&lt;p&gt;This talk is aimed at data scientists who have large, numerical datasets that need to be managed and stored but also accessed and processed efficiently. Basic knowledge of NumPy and UNIX will be useful for attendees but not required. Attendees will learn how to get started with h5py, as well as how to leverage HDF5 in order to attain accessible, compressed, and organized data.&lt;/p&gt;
&lt;p&gt;HDF5 stands for Hierarchical Data Format, version 5. It is a file format, library, and data model for storing and managing data. More simply, HDF5 can be described as a file system within a file. An HDF5 file contains two kinds of objects, namely, datasets and groups. Datasets work like NumPy arrays while groups work like dictionaries that hold datasets and other groups. In addition, objects can have attributes, or metadata. HDF5 is designed for flexible and efficient storage and I/O for high volume, complex data. Data scientists will find HDF5 to be invaluable for managing, manipulating, and storing their data.&lt;/p&gt;
&lt;p&gt;Part of this talk will demonstrate how to get started with HDF5. In this demo, attendees will learn how to: create and handle HDF5 files using h5py, manage and manipulate datasets, work with groups, and make use of attributes. A real-world example of a processing pipeline of brain recordings, utilizing HDF5 for storing and managing data at each processing step, will be presented. Attendees will have access to an IPython notebook to follow along during the demo and explore examples. After this talk, attendees will be able to begin using HDF5 to effortlessly store and manage their data.&lt;/p&gt;
</summary><category term="hdf5"></category><category term="h5py"></category><category term="jupyter notebook"></category></entry><entry><title>The Duct Tape of Heroes: Bayes Rule</title><link href="https://pyvideo.org/pydata-amsterdam-2016/the-duct-tape-of-heroes-bayes-rule.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Vincent Warmerdam</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/the-duct-tape-of-heroes-bayes-rule.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;In this talk I will give many examples of when Bayes rule will help you in your day to day work. I'll quickly show many examples of bayesian statistical thinking in action; the pleasure of inference, probabilistic graphs, model selection, feature generation, even operations research! I'll finish with a dataset from Heroes of the Storm and I'll show why Bayesian models can outperform randomforests.&lt;/p&gt;
&lt;p&gt;My talk is made up of the following examples;&lt;/p&gt;
&lt;p&gt;basic disease example: what is the value of adding an extra test to a patient
give an example of an inference task that is very hard to do properly without bayesian thinking
creating simple probibalistic models with pandas and showing how they are robust against missing data. I will also demo pomegranate, a new probabilistic programming tool for python.
show how you can use bayes rule to pick models
demo a bayesian probablistic approach to finding overpowered characters in the Heroes of the Storm video game.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://koaning.io/theme/notebooks/bayes.pdf"&gt;http://koaning.io/theme/notebooks/bayes.pdf&lt;/a&gt;.&lt;/p&gt;
</summary><category term="pomegranate"></category><category term="bayes"></category></entry><entry><title>the idea behind Automatic Relevance Determination and Bayesian Interpolation</title><link href="https://pyvideo.org/pydata-amsterdam-2016/the-idea-behind-automatic-relevance-determination-and-bayesian-interpolation.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Florian Wilhelm</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/the-idea-behind-automatic-relevance-determination-and-bayesian-interpolation.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Even in the era of Big Data there are many real-world problems where the number of input features has about the some order of magnitude than the number of samples. Often many of those input features are irrelevant and thus inferring the relevant ones is an important problem in order to prevent over-fitting. Automatic Relevance Determination solves this problem by applying Bayesian techniques.&lt;/p&gt;
&lt;p&gt;In order to motivate Automatic Relevance Determination (ARD) an intuition for the problem of choosing a complex model that fits the data well vs a simple model that generalizes well is established. Thereby the idea behind Occam's razor is presented as a way of balancing bias and variance. This leads us to the mathematical framework of Bayesian interpolation and model selection to choose between different models based on the data.&lt;/p&gt;
&lt;p&gt;To derive ARD as gently as possible the mathematical basics of a simple linear model are repeated as well as the idea of regularization to prevent over-fitting. Based on that, the Bayesian Ridge Regression (BayesianRidge in Scikit-Learn) is introduced. Generalizing the concept of Bayesian Ridge Regression even more gets us eventually to the the idea behind ARD (ARDRegression in Scikit-Learn).&lt;/p&gt;
&lt;p&gt;With the help of a practical example, we consolidate what has been learnt so far and compare ARD to an ordinary least square model. Now we dive deep into the mathematics of ARD and present the algorithm that solves the minimization problem of ARD. Finally, some details of Scikit-Learn's ARD implementation are discussed.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/FlorianWilhelm2/explaining-the-idea-behind-automatic-relevance-determination-and-bayesian-interpolation-59498957"&gt;http://www.slideshare.net/FlorianWilhelm2/explaining-the-idea-behind-automatic-relevance-determination-and-bayesian-interpolation-59498957&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>The PyData map: Presenting a map of the landscape of PyData tools</title><link href="https://pyvideo.org/pydata-amsterdam-2016/the-pydata-map-presenting-a-map-of-the-landscape-of-pydata-tools.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Peader Coyle</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/the-pydata-map-presenting-a-map-of-the-landscape-of-pydata-tools.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;The PyData ecosystem is growing rapidly, with existing tools maturing and exciting new tools appearing on a regular basis. This talk will examine the crowded PyData ecosystem and bring some clarity to which Python data tool is the right one to reach for on any given analysis. It will focus on use-cases for pure python, toolz, Numpy, Pandas, Blaze, xray, bcolz, Castra, Dask, and Spark.&lt;/p&gt;
&lt;p&gt;The PyData ecosystem can be a bit confusing for those new to Python, or even experienced programmers moving to Python for its excellent data analysis capabilities.&lt;/p&gt;
&lt;p&gt;We often get confused by the PyData Ecosystem. This will present a detailed look with examples of some of the cool tools out there.&lt;/p&gt;
&lt;p&gt;It will touch on pure python, toolz, Numpy, Pandas, Blaze, xray, bcolz, Dask, and Spark, with a focus on the use-cases for each one.&lt;/p&gt;
&lt;p&gt;What do you do when your data doesn't fit in-memory, when do you need to use a functional programming approach - when do you need a compression? Where does Dask fit into all of this? When do you need Spark?&lt;/p&gt;
&lt;p&gt;And discuss the differences in how data is stored and where you'd use different tools. Peadar will also provide a map of the landscape inspired by the famous Machine Learning flow chart from Andreas Mueller.&lt;/p&gt;
&lt;p&gt;GitHub available here: &lt;a class="reference external" href="https://github.com/springcoil/pydataamsterdamkeynote"&gt;https://github.com/springcoil/pydataamsterdamkeynote&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>The Role of Python in the Oil &amp; Gas Industry</title><link href="https://pyvideo.org/pydata-amsterdam-2016/the-role-of-python-in-the-oil-gas-industry.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Giuseppe Pagliuca</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/the-role-of-python-in-the-oil-gas-industry.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Imagine that you are sitting in the operator room of a medium-size oil &amp;amp; gas production facility, such as at an offshore platform or an onshore gas plant, refinery, or chemical plant. This can include units like pipelines, compressors, pumps, heat exchangers etc. For each unit of this network several measured quantities (like pressure, temperature, flow, liquid levels, etc.) might be available (&lt;/p&gt;
&lt;p&gt;This presentation is focused on the role of python for the design of new projects in the oil &amp;amp; gas world as well as for data processing for existing oil &amp;amp; gas production systems. During the design phase of a project, the key words are “uncertainty” and “risk management”. A reliable project must sustain difficult conditions (both internal and external) at an affordable cost. Sensitivities and statistics are the two most important arrows on the engineer’s quiver; these two tasks than can be massively automatized in python to increase the robustness and efficiency of the design.&lt;/p&gt;
&lt;p&gt;For existing assets the difficult role of the engineers is to translate a raw mix of measured and simulated information, often with errors and missing data points, into decisions in a short time. A Jupyter notebook can be an excellent solution for a quick evaluation of the status, data processing and sharing of information.&lt;/p&gt;
&lt;p&gt;This presentation gives an overview of the strategy in using python in our design and operations, as illustrated by a number of application examples.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://github.com/gpagliuca/pydata"&gt;https://github.com/gpagliuca/pydata&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Tools and Tricks from a Pragmatic Data Scientist</title><link href="https://pyvideo.org/pydata-amsterdam-2016/tools-and-tricks-from-a-pragmatic-data-scientist.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Lucas Bernardi</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/tools-and-tricks-from-a-pragmatic-data-scientist.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;In this talk I will share some of my favourite tools and tricks I use every day as Data Scientist. They help me to solve all kind of problems, from statistical modeling all the way to scalability issues. Expect machine learning, math, algortithms and of course python. All of them are necessary to be a Pragmatic Data Scientist.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;The Pragmatic Data Scientist Catalog:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;kNN: from slow to fast using math.&lt;/li&gt;
&lt;li&gt;Clustering: From k-Means to Spherical Clustering in a breeze.&lt;/li&gt;
&lt;li&gt;Missing values in classification: from bad to good using old truth.&lt;/li&gt;
&lt;li&gt;Power law: Bucketing the beast.&lt;/li&gt;
&lt;li&gt;The King of proportions Ranking.&lt;/li&gt;
&lt;li&gt;Hyperparameter search: one tool to rule them all.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Python code for all tricks and tools will be available in github for everyone to use change and challenge.&lt;/p&gt;
</summary></entry><entry><title>Using random search for efficient hyper-parameters optimization with H2O</title><link href="https://pyvideo.org/pydata-amsterdam-2016/using-random-search-for-efficient-hyper-parameters-optimization-with-h2o.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Jo fai Chow</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/using-random-search-for-efficient-hyper-parameters-optimization-with-h2o.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Optimizing hyper-parameters is a common yet time-consuming task for machine learning practitioners. Previous studies have shown that, when compared to traditional strategies like manual search and grid search, random search can achieve equal performance in a computationally efficient manner. In this talk I will demonstrate the random search feature in H2O with machine learning examples based on publicly available datasets.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/JofaiChow/20160312h2orandomgridsearch"&gt;http://www.slideshare.net/JofaiChow/20160312h2orandomgridsearch&lt;/a&gt;
Other materials available here: &lt;a class="reference external" href="https://github.com/h2oai/h2o-meetups/tree/master/2016_03_12_PyData_Amsterdam"&gt;https://github.com/h2oai/h2o-meetups/tree/master/2016_03_12_PyData_Amsterdam&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Winning Ways for Your Visualization Plays</title><link href="https://pyvideo.org/pydata-amsterdam-2016/winning-ways-for-your-visualization-plays.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Mark Grundland</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/winning-ways-for-your-visualization-plays.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;What enables an effective data visualization to deliver insight at a glance? This talk presents practical techniques for how information visualization design can take better account of the fundamental limitations of visual perception, exploring the design choices that determine whether a picture can meaningfully convey the data set it is meant to represent.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Data deserves to be seen. In an information economy, there is no shortage of information; only genuine understanding is in short supply. Knowledge workers are continually asked to make sense of more information than they could possibly have time to read and assimilate. Users have come to demand insight at a glance: the whole picture, not just an endless list of results. After all, as information becomes ever more abundant, attention remains as scarce as ever. Visualization, animation, and interaction can be gainfully employed to develop information systems that are both useful, enabling users to get the job done well, and usable, empowering users to do job with ease. Effective information visualization should be immediately appealing to the eye and directly relevant to the task, routinely enjoyable to the user and uniquely valuable to the business. By integrating the power of computational analysis with the expertise of human judgment, visualization serves to turn aggregated information into actionable insight, illustrating the way numbers can tell a story compelling enough for people to make decisions they can trust.&lt;/p&gt;
&lt;p&gt;This presentation explores practical techniques for information visualization design to take better account of the fundamental limitations of visual perception. It includes examples of innovative visualizations used in a variety of applications, including a research project for Grapeshot and IBM to create an online news analysis service that tracks the relative influence of different news sources on shaping how news coverage evolves over time.&lt;/p&gt;
</summary><category term="data visualization"></category></entry></feed>