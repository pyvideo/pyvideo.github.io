<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Yichong Xu</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 03 Aug 2020 00:00:00 +0000</lastBuildDate><item><title>Zeroth Order Non-convex optimization with Dueling-Choice Bandits</title><link>https://pyvideo.org/uai-2020/zeroth-order-non-convex-optimization-with-dueling-choice-bandits.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Zeroth Order Non-convex optimization with Dueling-Choice Bandits&lt;/p&gt;
&lt;p&gt;Yichong Xu (Carnegie Mellon University)*; Aparna Joshi (Carnegie Mellon University); Aarti Singh (Carnegie Mellon University); Artur Dubrawski (CMU)&lt;/p&gt;
&lt;p&gt;We consider a novel setting of zeroth order non-convex optimization, where in addition to querying the function value at a given point, we can also duel two points and get the point with the larger function value. We refer to this setting as optimization with dueling-choice bandits, since both direct queries and duels are available for optimization. We give the COMP-GP-UCB algorithm based on  GP-UCB (Srinivas et al., 2009),, where instead of directly querying the point with the maximum Upper Confidence Bound (UCB), we perform constrained optimization and use comparisons to filter out suboptimal points. COMP-GP-UCB comes with theoretical guarantee of $O(frac{Phi}{sqrt{T}})$ on simple regret where $T$ is the number of direct queries and $Phi$ is an improved information gain stemming from a comparison-based constraint set that restricts the space for optimum search. In contrast, in the plain direct query setting, $Phi$ depends on the entire domain. We discuss theoretical aspects and show experimental results to demonstrate efficacy of our algorithm.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yichong Xu</dc:creator><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2020-08-03:/uai-2020/zeroth-order-non-convex-optimization-with-dueling-choice-bandits.html</guid><category>UAI 2020</category></item></channel></rss>