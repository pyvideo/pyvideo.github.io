<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_jupyter.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-10T00:00:00+00:00</updated><entry><title>Building a Powerful Pet Detector in Notebooks</title><link href="https://pyvideo.org/europython-2019/building-a-powerful-pet-detector-in-notebooks.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Katherine Kampf</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/building-a-powerful-pet-detector-in-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Ever wondered what breed that dog or cat is? Let’s build a pet detector
service to recognize them in pictures! In this talk, we will walk
through the training, optimizing, and deploying of a deep learning model
using Azure Notebooks. We will use transfer learning to retrain a
MobileNet model using TensorFlow to recognize dog and cat breeds using
the Oxford IIIT Pet Dataset. Next, we’ll optimize the model and tune our
hyperparameters to improve the model accuracy. Finally, we will deploy
the model as a web service in. Come to learn how you can quickly create
accurate image recognition models with a few simple techniques!&lt;/p&gt;
</summary><category term="Data"></category><category term="Data Science"></category><category term="Deep Learning"></category><category term="Jupyter"></category><category term="Machine-Learning"></category></entry><entry><title>Using Python to Teach Computational Finance</title><link href="https://pyvideo.org/europython-2019/using-python-to-teach-computational-finance.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Tyler Brough</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/using-python-to-teach-computational-finance.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this demo-driven session, we will introduce the Probo package for
teaching Python programming and concepts from computational finance to
beginning programmers in the domain of finance. We'll show how Python is
the perfect tool for teaching computational thinking to develop deeper
quantitative reasoning. Jupyter notebooks, together with Python packages
such as NumPy and Pandas, provide the ideal learning environment.&lt;/p&gt;
&lt;p&gt;We will start by introducing the Probo package for derivative pricing
and hedging. We will demo the pricing of European and American options
via the famous Black-Scholes option pricing model. Other examples
include Monte Carlo simulation and binomial trees. Using Probo, the
answers to derivative pricing problems are right at the students'
fingertips. Students can operationalize their understanding by going
directly from the mathematics of derivative pricing theories to their
implementation in clean and simple code.&lt;/p&gt;
&lt;p&gt;We will end with a demonstration using Probo to teach the concept of
dynamic hedging. Dynamic hedging is perhaps &lt;em&gt;the&lt;/em&gt; crucial concept in
modern financial derivatives theory. It is also one of the most
difficult concepts to grasp. We'll show how developing deeper intuition
is possible with computational thinking via Monte Carlo simulation of
delta-hedging. By leveraging the power and simplicity of Python and
Jupyter notebooks, the Probo package provides the ideal learning
platform for students of computational finance.&lt;/p&gt;
</summary><category term="Beginners"></category><category term="Business"></category><category term="Business Cases"></category><category term="Education"></category><category term="Jupyter"></category></entry><entry><title>Visual debugger for Jupyter Notebooks: Myth or Reality?</title><link href="https://pyvideo.org/europython-2019/visual-debugger-for-jupyter-notebooks-myth-or-reality.html" rel="alternate"></link><published>2019-07-10T00:00:00+00:00</published><updated>2019-07-10T00:00:00+00:00</updated><author><name>Elizaveta Shashkova</name></author><id>tag:pyvideo.org,2019-07-10:europython-2019/visual-debugger-for-jupyter-notebooks-myth-or-reality.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many Python developers like Jupyter Notebooks for their flexibility:
they are very useful for interactive prototyping, scientific
experiments, visualizations and many other tasks. There are different
development tools which make working with Jupyter Notebooks easier and
smoother, but all of them lack very important feature: visual debugger.
Since Jupyter Kernel is a usual Python process, it looks reasonably to
use one of existing Python debuggers with it. But is it really possible?&lt;/p&gt;
&lt;p&gt;In this talk we’ll try to understand how Python debugger should be
changed to work with Jupyter cells and how these changes are already
implemented in the PyCharm IDE. After that we’ll look into the whole
Jupyter architecture and try to understand which bottlenecks in it
prevent creation of universal Jupyter debugger at the moment.&lt;/p&gt;
&lt;p&gt;This talk requires a basic knowledge of Jupyter Notebooks and
understanding of Python functions and objects. It will be interesting
for people who want to learn internals of the tools they use every day.
Also it might be an inspiration for people who want to implement a
visual debugger in their favourite IDE.&lt;/p&gt;
</summary><category term="Debugging"></category><category term="Jupyter"></category><category term="Python general"></category><category term="Tooling"></category></entry><entry><title>Geospatial analysis with Python</title><link href="https://pyvideo.org/pycon-italia-2019/geospatial-analysis-with-python.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Francesco Bruni</name></author><id>tag:pyvideo.org,2019-05-03:pycon-italia-2019/geospatial-analysis-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Due to its effectiveness and simplicity, Python is spreading as a choice
for handling geospatial data. From running algorithms capable of
extracting geo- referred insights or processing geo archives, Python
could represent a powerful tool to handle geo-related problems thanks to
an extensive set of libraries. The training will give an overview about
processing geospatial data with Python. An approximate agenda will
include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction&lt;ul&gt;
&lt;li&gt;setting up the the environment&lt;/li&gt;
&lt;li&gt;an introduction to geospatial world&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Working with geospatial data&lt;ul&gt;
&lt;li&gt;playing with geo archives (vector/rasters)&lt;/li&gt;
&lt;li&gt;extracting geo analytics&lt;/li&gt;
&lt;li&gt;Vector tiles big vector data&lt;/li&gt;
&lt;li&gt;Apache Spark for geospatial raster analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Python for Earth observation&lt;ul&gt;
&lt;li&gt;Classifying earth observation images&lt;/li&gt;
&lt;li&gt;Extracting insights from Copernicus products&lt;/li&gt;
&lt;li&gt;Use SNAP from Python&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No tools are required for attending this training. Bring your PC with
Docker installed. Further instructions will be provided.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1674"&gt;https://python.it/feedback-1674&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 10:30 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt; in __on &lt;strong&gt;Saturday 4
May&lt;/strong&gt; at 18:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="Jupyter"></category><category term="pyspark"></category><category term="geospatial"></category><category term="geopynotebook"></category><category term="spark"></category><category term="docker"></category></entry><entry><title>Meet dask and distributed: the unsung heroes of Python scientific data ecosystem.</title><link href="https://pyvideo.org/pycon-italia-2019/meet-dask-and-distributed-the-unsung-heroes-of-python-scientific-data-ecosystem.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Alessandro Amici</name></author><id>tag:pyvideo.org,2019-05-03:pycon-italia-2019/meet-dask-and-distributed-the-unsung-heroes-of-python-scientific-data-ecosystem.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Thanks to its world-class data tools and libraries, like Numpy, Pandas,
Jupyter, Matplotlib and xarray, Python is becoming the language of
choice in many scientific communities from Physics to Climate Science,
from Earth Observation to Economy.&lt;/p&gt;
&lt;p&gt;A turn-key but less-know component of the scientific ecosystem is the
dask library that enable seamless parallel, distributed and GPU
computing in most cases without code changes.&lt;/p&gt;
&lt;p&gt;We will use climate science as an typical example of a discipline where
simple tasks become easily big data problems and where mastering xarray,
dask and dask.distributed is the key to turn them back into simple
tasks, possibly on a large cluster of VMs (that you can easily provision
from your preferred cloud provider).&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://gitpitch.com/alexamici/talks/master?p=PyConX-2019"&gt;https://gitpitch.com/alexamici/talks/master?p=PyConX-2019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1704"&gt;https://python.it/feedback-1704&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 17:15 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="Jupyter"></category><category term="dask.distributed"></category><category term="Big-Data"></category><category term="xarray"></category><category term="dask"></category><category term="climate-change"></category><category term="earth-obeservation"></category><category term="pandas"></category></entry><entry><title>Best tools for Jupyter notebooks</title><link href="https://pyvideo.org/pycon-philippines-2019/best-tools-for-jupyter-notebooks.html" rel="alternate"></link><published>2019-02-23T00:00:00+00:00</published><updated>2019-02-23T00:00:00+00:00</updated><author><name>Dmitry Trofimov</name></author><id>tag:pyvideo.org,2019-02-23:pycon-philippines-2019/best-tools-for-jupyter-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The jupyter notebook is now a default format for scripting in many areas of computer engineering and education like data science, machine learning, and artificial intelligence. But what are the most efficient practices to work with Jupyter notebooks to get the most out of this technology? In this talk Dmitry Trofimov, team lead of PyCharm IDE, will show how different tools help you solve your daily tasks using Jupyter notebooks: JupyterLab, Hydrogen plugin for Atom, VS Code, and PyCharm (all tools are open-sourced). We will discuss their pros and cons and look at them in action.&lt;/p&gt;
</summary><category term="Jupyter"></category><category term="jupyter lab"></category><category term="jupyterlab"></category></entry><entry><title>Beyond Jupyter Notebooks - Building your own Data Science platform with Python &amp; Docker</title><link href="https://pyvideo.org/pycon-de-2018/beyond-jupyter-notebooks-building-your-own-data-science-platform-with-python-docker.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Joshua Görner</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/beyond-jupyter-notebooks-building-your-own-data-science-platform-with-python-docker.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Interactive notebooks like Jupyter have become more and more popular in
the recent past and build the core of many data scientist's workplace.
Being accessed via web browser they allow scientists to easily structure
their work by combining code and documentation.&lt;/p&gt;
&lt;p&gt;Yet notebooks often lead to isolated and disposable analysis artefacts.
Keeping the computation inside those notebooks does not allow for
convenient concurrent model training, model exposure or scheduled model
retraining.&lt;/p&gt;
&lt;p&gt;Those issues can be addressed by taking advantage of recent developments
in the discipline of software engineering. Over the past years
containerization became the technology of choice for crafting and
deploying applications. Building a data science platform that allows for
easy access (via notebooks), flexibility and reproducibility (via
containerization) combines the best of both worlds and addresses Data
Scientist's hidden needs.&lt;/p&gt;
</summary><category term="Artificial Intelligence"></category><category term="Algorithms"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Infrastructure"></category><category term="Jupyter"></category><category term="Machine Learning"></category><category term="Programming"></category><category term="Python"></category></entry><entry><title>Driving simulation and data analysis of magnetic nanostructures through Jupyter Notebook</title><link href="https://pyvideo.org/pycon-de-2018/driving-simulation-and-data-analysis-of-magnetic-nanostructures-through-jupyter-notebook.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Hans Fangohr</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/driving-simulation-and-data-analysis-of-magnetic-nanostructures-through-jupyter-notebook.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We present ongoing work from a project that makes a particular computer
simulation (implemented in C++ and Tk/Tcl) accessible through a Python
interface, and through the Jupyter Notebook. The talk describes the
motivation and current status of the project.&lt;/p&gt;
&lt;p&gt;In more detail, the computer simulation in question is the Object
Oriented Micromagnetic Modelling Framework
(&lt;a class="reference external" href="http://math.nist.gov/oommf/"&gt;OOMMF&lt;/a&gt;) which is likely the most
widely used micromagnetic simulation package. It can be driven through a
graphical (Tk) user interface or through a configuration file that
defines a simulation run.&lt;/p&gt;
&lt;p&gt;In this talk, we first show a Python interface to OOMMF that allows the
driving of OOMMF simulations from a Python program or interpreter
prompt. This way we embed a widely used scientific code from 1990s in a
general purpose programming language
[&lt;a class="reference external" href="https://doi.org/10.1063/1.4977225"&gt;1&lt;/a&gt;] and enable the full use of
the ecosystem of scientific libraries available for Python. For example,
design optimisation, specialised post-processing, and the creation of
figures can all be carried out using a single script; making the work
more easily reproducible.&lt;/p&gt;
&lt;p&gt;Second, we integrate the Python interface to OOMMF into a Jupyter
notebook, so that all existing benefits of using Jupyter are inherited
for the use in computational micromagnetics, which is the reason we
named our code Jupyter- OOMMF (&lt;a class="reference external" href="http://joommf.github.io/"&gt;JOOMMF&lt;/a&gt;). A
&lt;a class="reference external" href="https://tryjoommf.soton.ac.uk/"&gt;JupyterHub installation&lt;/a&gt; of the tool
reduces barriers in uptake, and all the code is &lt;a class="reference external" href="https://github.com/joommf"&gt;on
github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We discuss the benefits of driving computer simulation and data analysis
through Jupyter Notebooks.&lt;/p&gt;
&lt;p&gt;This project is a part of the Jupyter-OOMMF (JOOMMF) activity in the
&lt;a class="reference external" href="http://opendreamkit.org/"&gt;OpenDreamKit&lt;/a&gt; project and we acknowledge
financial support from Horizon 2020 European Research Infrastructures
project (676541). The work is also supported by the EPSRC CDT in Next
Generation Computational Modelling EP/L015382/1, and the EPSRC grants
EP/M022668/1 and EP/N032128/1.&lt;/p&gt;
&lt;p&gt;For additional context: micromagnetic modelling is a key research method
in academia and industry to support development of high-capacity
magnetic storage devices that are cheap, fast, and reliable, and to
enable research into future alternative storage and processing
technologies such as spintronics. The OOMMF modelling package has been
used in &lt;a class="reference external" href="https://math.nist.gov/oommf/oommf_cites.html"&gt;over 2500
publications&lt;/a&gt; since
1999.&lt;/p&gt;
&lt;p&gt;[1] Beg, M., Pepper, R. A., and Fangohr, H. User interfaces for
computational science: A domain specific language for OOMMF embedded in
Python. AIP Advances 7, 056025 (2017), &lt;a class="reference external" href="https://doi.org/10.1063/1.4977225"&gt;https://doi.org/10.1063/1.4977225&lt;/a&gt;&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Jupyter"></category><category term="Programming"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>How to teach space invaders to your computer</title><link href="https://pyvideo.org/pycon-de-2018/how-to-teach-space-invaders-to-your-computer.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>David Wölfle</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/how-to-teach-space-invaders-to-your-computer.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;First things first: playing good old Atari games might be cool but why
should I write a program for doing it? Well teaching a computer to play
a game means teaching it to develop strategies and use foresight
planning to solve a certain problem. The tools you gather while solving
i.e. space invaders are the same you may use to solve any problem which
requires a sequential set of decisions in order to find an optimal
solution to some problem, like i.e. controlling a robot that collects
garbage. Furthermore, there is a lot of scientific research on
reinforcement learning that focuses on solving Atari games which makes
it a good starting point, as large amounts of publications and open
source code already exists.&lt;/p&gt;
&lt;p&gt;What to expect from this talk? At first there will be a very short
introduction to reinforcement learning theory, just the very basics,
common applications and some references for further reading. Next points
are, how to run Atari games from inside python for a learning task (with
OpenAI's gym), and where to find an algorithm for the actual learning
problem. Finally it will be shown how to build it all together in a
jupyter notebook and let the algorithm play the game. Et voilà that's
your computer beating you in space invaders.&lt;/p&gt;
</summary><category term="Deep Learning &amp; Artificial Intelligence"></category><category term="Jupyter"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>Introduction to Docker for Pythonistas</title><link href="https://pyvideo.org/pycon-de-2018/introduction-to-docker-for-pythonistas.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Jan Wagner</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/introduction-to-docker-for-pythonistas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="line-block"&gt;
&lt;div class="line"&gt;My Talk aims to introduce you to Docker and how it works, how you can
use prebuild Images from the Docker-Hub and how you can make your own
Images.&lt;/div&gt;
&lt;div class="line"&gt;In more Detail, the following Points will be covered:&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Jupyter"></category><category term="Machine Learning"></category></entry><entry><title>Microservices from the trenches: how we delivery fancy sofas across Europe</title><link href="https://pyvideo.org/pycon-de-2018/microservices-from-the-trenches-how-we-delivery-fancy-sofas-across-europe.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Christian Barra</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/microservices-from-the-trenches-how-we-delivery-fancy-sofas-across-europe.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At made.com we use micro-services written in Python to power our entire
backend system and deliver an incredible amount of orders each week.
During this presentation I will explain what are micro-services and
compare them to monolith applications. By analysing what are the
differences between micro- services and monolith applications I will
show why you need them and for which situations they are extremely
useful. I will also talk about what you need on different levels
regarding infrastructure, knowledge and experience to get the most out
of a micro-services architecture. The last part of the presentation is
dedicated to the drawbacks of running a micro-services architecture and
sharing some solutions. I will conclude sharing some useful resources
about micro services and taking some questions.&amp;quot;&lt;/p&gt;
</summary><category term="Data Science"></category><category term="Jupyter"></category></entry><entry><title>Processing Geodata using Python</title><link href="https://pyvideo.org/pycon-de-2018/processing-geodata-using-python.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Martin Christen</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/processing-geodata-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a large amount of Python modules available suitable for spatial
data processing. In this talk, it is shown how to analyze, manipulate
and visualize geospatial data by using open source modules. The
following modules will be introduced:&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="Jupyter"></category><category term="Python"></category><category term="Visualisation"></category></entry><entry><title>Solving Data Science Problems using a Jupyter Notebook and SAP HANA's in-database Machine Learning Libraries</title><link href="https://pyvideo.org/pycon-de-2018/solving-data-science-problems-using-a-jupyter-notebook-and-sap-hanas-in-database-machine-learning-libraries.html" rel="alternate"></link><published>2018-10-25T00:00:00+00:00</published><updated>2018-10-25T00:00:00+00:00</updated><author><name>Dr Frank Gottfried</name></author><id>tag:pyvideo.org,2018-10-25:pycon-de-2018/solving-data-science-problems-using-a-jupyter-notebook-and-sap-hanas-in-database-machine-learning-libraries.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Companies store their data in databases with highly restricted access
regulations. The latest regulatorily changes enforces the need to work
on the datasets in this controlled environment without created
additional external copies. However Data Scientists prefer to work with
tools they are most familiar like Python, R and Jupyter Notebooks using
to a large amount of open- source packages (numpy, matplotlib, pandas,
..). SAP HANA provides highly optimized in-database machine learning
libraries. In this talk we will present how a Data Scientist can work in
an environment he/she is most familiar with and access the data stored
in SAP HANA using SAP HANA machine learning libraries with a
scikit-learn type interface. Data will remain in the database and will
be exposed as dataframes (similar to Pandas dataframes). We will explain
the software architecture and present a complete end-to-end use case by
using a Jupyter Notebook.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Data Science"></category><category term="Jupyter"></category><category term="Machine Learning"></category><category term="Visualisation"></category></entry><entry><title>Binder - lowering the bar to sharing interactive software</title><link href="https://pyvideo.org/pycon-de-2018/binder-lowering-the-bar-to-sharing-interactive-software.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Tim Head</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/binder-lowering-the-bar-to-sharing-interactive-software.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Binder project drastically lowers the bar to sharing and re-using
software. As a user wanting to try out someone else’s work I only have
to click a single link. As the author preparing a binder-ready project
is much easier than having to support many different platforms and for
many projects involves little additional work.&lt;/p&gt;
&lt;p&gt;In this talk I will introduce the audience to the concepts and ideas
behind the Binder project. I will showcase examples from the community
to illustrate use-cases and show off the power of Binder.&lt;/p&gt;
&lt;p&gt;Three pieces of software power Binder:
&lt;a class="reference external" href="http://repo2docker.readthedocs.io/en/latest/"&gt;repo2docker&lt;/a&gt;,
&lt;a class="reference external" href="https://binderhub.readthedocs.io/en/latest/"&gt;BinderHub&lt;/a&gt; and
&lt;a class="reference external" href="http://jupyterhub.readthedocs.io/en/stable/"&gt;JupyterHub&lt;/a&gt;. Using an
example repository I will go through the steps required to make a
repository binder- ready and what happens when a user launches it. At
each step I will illustrate the role that each of the three software
components play and how they interact.&lt;/p&gt;
&lt;p&gt;Binder is a project created by its community. I will present pathways
for getting involved with the community.&lt;/p&gt;
&lt;p&gt;To wrap up I will highlight plans for future developments and features
of Binder.&lt;/p&gt;
</summary><category term="Community"></category><category term="Data Science"></category><category term="DevOps"></category><category term="Jupyter"></category><category term="Science"></category><category term="Web"></category></entry><entry><title>Cython to speed up your Python code</title><link href="https://pyvideo.org/pycon-de-2018/cython-to-speed-up-your-python-code.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Stefan Behnel</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/cython-to-speed-up-your-python-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="http://cython.org"&gt;Cython&lt;/a&gt; is not only a very fast and comfortable
way to talk to native code and libraries, it is also a widely used tool
for speeding up Python code. The Cython compiler translates Python code
to C or C++ code, and applies many static optimisations that make Python
code run visibly faster than in the interpreter. But even better, it
supports static type annotations that allow direct use of C/C++ data
types and functions, which the compiler uses to convert and optimise the
code into fast, native C. The tight integration of all three languages,
Python, C and C++, makes it possible to freely mix Python features like
generators and comprehensions with C/C++ features like native data
types, pointer arithmetic or manually tuned memory management in the
same code.&lt;/p&gt;
&lt;p&gt;This talk by a core developer introduces the Cython compiler by
interactive code examples, and shows how you can use it to speed up your
real-world Python code. You will learn how you can profile a Python
module and use Cython to compile and optimise it into a fast binary
extension module. All of that, without losing the ability to run it
through common development tools like code checkers or coverage test
tools.&lt;/p&gt;
</summary><category term="Big Data"></category><category term="Infrastructure"></category><category term="Jupyter"></category><category term="Parallel Programming"></category></entry><entry><title>Introduction and practical experience about Quantum Computing using the Python libraries from IBM and Google</title><link href="https://pyvideo.org/pycon-de-2018/introduction-and-practical-experience-about-quantum-computing-using-the-python-libraries-from-ibm-and-google.html" rel="alternate"></link><published>2018-10-24T00:00:00+00:00</published><updated>2018-10-24T00:00:00+00:00</updated><author><name>Dr. Andreas Riegg</name></author><id>tag:pyvideo.org,2018-10-24:pycon-de-2018/introduction-and-practical-experience-about-quantum-computing-using-the-python-libraries-from-ibm-and-google.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As publicly announced in early 2018, Daimler AG has started cooperations
with IBM and Google on Quantum Computing. When doing concrete
experiments with the Quantum Computing cloud based offerings, two
different Python libraries provided by IBM and Google are used. They are
named QISKIT in the case of IBM and CIRQ in the case of Google. The
experiments with both libraries are handled using appropriate Jupyter
Notebooks. This talk gives a brief introduction on Quantum Computing,
specifically on Quantum Computers based on transmon-based QBits. This is
followed by an introduction of the both Python libraries that are used.
Then some details about the Jupyter notebooks that are used are given.
The talk will finish with some demos and an overview about the most
important practical experiences with both Quantum Computing offerings.&lt;/p&gt;
</summary><category term="Jupyter"></category><category term="Programming"></category><category term="Python"></category><category term="Science"></category></entry><entry><title>Bokeh: Using python for interactive data visualization</title><link href="https://pyvideo.org/pycon-italia-2018/bokeh-using-python-for-interactive-data-visualization.html" rel="alternate"></link><published>2018-04-21T00:00:00+00:00</published><updated>2018-04-21T00:00:00+00:00</updated><author><name>Ernesto Arbitrio</name></author><id>tag:pyvideo.org,2018-04-21:pycon-italia-2018/bokeh-using-python-for-interactive-data-visualization.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Bokeh is an interactive visualization library that targets modern web
browsers for presentation. Its goal is to provide elegant, concise
construction of novel graphics in the style of D3.js, and to extend this
capability with high- performance interactivity over very large or
streaming datasets. In this talk, you will learn to use Bokeh to:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;create simple interactive plots&lt;/li&gt;
&lt;li&gt;annotate and styling the plots&lt;/li&gt;
&lt;li&gt;using bokeh plots in web pages&lt;/li&gt;
&lt;li&gt;link interactive visualizations to a running python instance&lt;/li&gt;
&lt;li&gt;stream data to plots&lt;/li&gt;
&lt;li&gt;using Datashader to view and interact with large datasets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data visualization is key to understanding the information contained in
data. The interactivity provide a valuable means for engineers, data
journalist, and scientist to explore their data. This talk will
introduce ( &lt;em&gt;using also some code example&lt;/em&gt; ) the audience to the basics
of using Bokeh, demonstrate different aspects of the library, and teach
how to deal with data formats ( &lt;em&gt;pandas&lt;/em&gt; , &lt;em&gt;numpy&lt;/em&gt; , &lt;em&gt;bokeh Column Data
Source&lt;/em&gt; )&lt;/p&gt;
&lt;p&gt;Knowledge required: &lt;tt class="docutils literal"&gt;python3&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;pandas basics&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;numpy bascis&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;data mining&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;jupyter notebook&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;For some visualization examples refers
&lt;a class="reference external" href="https://bokeh.pydata.org/en/latest/docs/gallery.html#gallery"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 12:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="plot"></category><category term="Jupyter"></category><category term="pydata"></category><category term="bokeh"></category><category term="DataVisualization"></category><category term="dataminig"></category></entry><entry><title>Scaling your Data infrastructure</title><link href="https://pyvideo.org/pycon-italia-2018/scaling-your-data-infrastructure.html" rel="alternate"></link><published>2018-04-20T00:00:00+00:00</published><updated>2018-04-20T00:00:00+00:00</updated><author><name>Christian Barra</name></author><id>tag:pyvideo.org,2018-04-20:pycon-italia-2018/scaling-your-data-infrastructure.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;This talk aims to answer a few questions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What do you do when you need to move your model from your laptop to
production?&lt;/li&gt;
&lt;li&gt;Is &lt;tt class="docutils literal"&gt;big data == I need to use JVM&lt;/tt&gt; the right assumption?&lt;/li&gt;
&lt;li&gt;How can I put my jupyter notebook in production?&lt;/li&gt;
&lt;li&gt;How do you apply the best software engineering practices (testing and
ci for example) inside your data science process?&lt;/li&gt;
&lt;li&gt;How do you “decouple” your data scientists, developers and devops
teams?&lt;/li&gt;
&lt;li&gt;How do you guarantee the reproducibility of your models?&lt;/li&gt;
&lt;li&gt;How do you scale your training process when does not fit in memory
anymore?&lt;/li&gt;
&lt;li&gt;How do you serve your models and provide an easy rollback system?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Agenda:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The Data Science workflow&lt;/li&gt;
&lt;li&gt;Scaling is not just a matter of the size of your Data&lt;/li&gt;
&lt;li&gt;Scaling when the size of your Data matters&lt;/li&gt;
&lt;li&gt;DDS, Dockerized Data Science&lt;/li&gt;
&lt;li&gt;Cassiny&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll share my experience highlighting some of the challenges I faced and
the solutions I came up to answer these questions.&lt;/p&gt;
&lt;p&gt;During this presentation I will mention libraries like jupyter, atom,
scikit- learn, dask, ray, parquet, arrow and many others.&lt;/p&gt;
&lt;p&gt;The principles and best practices I will share are something that you
can apply, more or less easily, if you are running or in the process to
run a production system based on the Python stack.&lt;/p&gt;
&lt;p&gt;This talk will focus on (my) best practices to run the Python Data stack
together and I will also talk about Cassiny, an open source project I
started, that aims to simplify your life if you want to use a completely
Python based solution in your data science workflow.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 20 April&lt;/strong&gt; at 11:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="Jupyter"></category><category term="CloudComputing"></category><category term="pydata"></category><category term="#lessonslearned"></category><category term="Big-Data"></category><category term="S3"></category><category term="Data-Scientist"></category><category term="#amicodialessia"></category><category term="java"></category><category term="docker"></category><category term="cloud"></category></entry><entry><title>Launch Jupyter to the Cloud with Docker and Terraform</title><link href="https://pyvideo.org/pycon-israel-2018/launch-jupyter-to-the-cloud-with-docker-and-terraform.html" rel="alternate"></link><published>2018-06-05T00:00:00+00:00</published><updated>2018-06-05T00:00:00+00:00</updated><author><name>Cheuk Ting Ho</name></author><id>tag:pyvideo.org,2018-06-05:pycon-israel-2018/launch-jupyter-to-the-cloud-with-docker-and-terraform.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Launch Jupyter to the Cloud: an example of using Docker and Terraform&lt;/p&gt;
&lt;p&gt;In this talk, we will use a task: hiring a GPU on Google Cloud Platform to train neural network, as an example to show how an application can be deployed on a cloud platform with Docker and Terraform. The goal is to have Jupyter Notebook running in an environment with Tensorflow (GPU version) and other libraries installed on a Google Compute Engine.&lt;/p&gt;
&lt;p&gt;This talk is for people with no experience in application deployment on cloud service but would benefit form computational reproducibility and cloud service, potentially data scientists/ analysts or tech practitioners who didn't have a software developing background. We will use an example that is simple but useful in data science to demonstrate basic usage of Docker and Terraform which would be beneficial to beginners who would like to simplify their work flow with those tools.&lt;/p&gt;
</summary><category term="jupyter"></category><category term="docker"></category><category term="terraform"></category></entry><entry><title>Docker for Data Science</title><link href="https://pyvideo.org/pycon-us-2018/docker-for-data-science.html" rel="alternate"></link><published>2018-05-10T00:00:00+00:00</published><updated>2018-05-10T00:00:00+00:00</updated><author><name>Aly Sivji</name></author><id>tag:pyvideo.org,2018-05-10:pycon-us-2018/docker-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter notebooks simplify the process of developing and sharing Data Science projects across groups and organizations. However, when we want to deploy our work into production, we need to extract the model from the notebook and package it up with the required artifacts (data, dependencies, configurations, etc) to ensure it works in other environments. Containerization technologies such as Docker can be used to streamline this workflow.&lt;/p&gt;
&lt;p&gt;This hands-on tutorial presents Docker in the context of Reproducible Data Science - from idea to application deployment. You will get a thorough introduction to the world of containers; learn how to incorporate Docker into various Data Science projects; and walk through the process of building a Machine Learning model in Jupyter and deploying it as a containerized Flask REST API.&lt;/p&gt;
</summary><category term="jupyter"></category><category term="docker"></category><category term="data science"></category></entry><entry><title>Jupyter Tools for Teaching and Learning</title><link href="https://pyvideo.org/pycon-us-2018/jupyter-tools-for-teaching-and-learning.html" rel="alternate"></link><published>2018-05-10T00:00:00+00:00</published><updated>2018-05-10T00:00:00+00:00</updated><author><name>Douglas Blank</name></author><id>tag:pyvideo.org,2018-05-10:pycon-us-2018/jupyter-tools-for-teaching-and-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Project Jupyter is the center of a set of technologies that grew out of simple tools to make Python easier to use. Today, Jupyter is composed of powerful client-server applications and protocols for computing in many programming languages. This talk focuses on using these technologies for pedagogical purposes.&lt;/p&gt;
&lt;p&gt;Every course I have taught since the Fall semester of 2014 has been over the web via our JupyterHub server. These courses have included firstyear writing seminars, as well as courses in Programming Languages, Assembly Language, Introduction to Biology (in Python), and in Processing (Java-based). In this talk I hope to help identify best-practices for using Jupyter in the classroom. I will discuss and demonstrate tools and techniques, and explore the challenges of using Jupyter for teaching and learning.&lt;/p&gt;
</summary><category term="jupyter"></category></entry><entry><title>Integrating Jupyter Notebooks into your Infrastructure</title><link href="https://pyvideo.org/pycon-de-2017/integrating-jupyter-notebooks-into-your-infrastructure.html" rel="alternate"></link><published>2017-10-25T00:00:00+00:00</published><updated>2017-10-25T00:00:00+00:00</updated><author><name>Florian Rhiem</name></author><id>tag:pyvideo.org,2017-10-25:pycon-de-2017/integrating-jupyter-notebooks-into-your-infrastructure.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Florian Rhiem&lt;/strong&gt; (&amp;#64;FlorianRhiem)&lt;/p&gt;
&lt;p&gt;Florian Rhiem is a scientific software developer at Forschungszentrum Jülich in the Scientific IT-Systems department of Peter Grünberg Institute / Jülich Centre for Neutron Science. In 2014, he finished his master's thesis on visualization of multidimensional functions and graduated as Master of Science in Technomathematics. He works with Python and C, mostly focusing on 3D visualization software.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Jupyter Notebooks combine executable code and rich text elements in a web application. In this talk you will learn how a custom JupyterHub installation can be used to integrate Jupyter Notebooks into your infrastructure, including existing authentication methods and custom software distributions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Jupyter Notebooks allow you to create and share interactive documents containing both rich text and executable code. As researchers, you can write Notebooks that allow your readers to reproduce your results on their own. As data scientists, you can use interactive visualizations to explore and analyze data sets directly in your browser. With official support for Python, Julia and R and community support for programming languages ranging from Fortran to JavaScript, Jupyter can be used in a wide variety of workflows.&lt;/p&gt;
&lt;p&gt;In this talk, you will learn how a custom JupyterHub installation has been used to seamlessly integrate Jupyter Notebooks into the existing infrastructure at the Jülich Centre for Neutron Science and the Peter Grünberg Institute. After introducing JupyterHub and its components, the presentation will show how each can be customized to use already existing resources and services, such as LDAP authentication, NFS-based storage and custom software distributions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</summary><category term="jupyterhub"></category><category term="jupyter"></category><category term="use-case"></category></entry><entry><title>Jupyter: if you don't use it yet you're doing wrong</title><link href="https://pyvideo.org/pycon-italia-2017/jupyter-if-you-dont-use-it-yet-youre-doing-wrong.html" rel="alternate"></link><published>2017-04-08T00:00:00+00:00</published><updated>2017-04-08T00:00:00+00:00</updated><author><name>Christian Barra</name></author><id>tag:pyvideo.org,2017-04-08:pycon-italia-2017/jupyter-if-you-dont-use-it-yet-youre-doing-wrong.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever heard about Jupyter Notebook ? It’s like Python REPL on
steroids !&lt;/p&gt;
&lt;p&gt;In this talk I will introduce the Jupyter project, the libraries behind
this project and how you can use Jupyter Notebook for a lot of
activities like fast prototyping, data science, education and so on !&lt;/p&gt;
&lt;p&gt;The talk will be an introduction, Beginners are welcome !&lt;/p&gt;
</summary><category term="iPython"></category><category term="Data-Scientist"></category><category term="Jupyter"></category><category term="pydata"></category></entry><entry><title>Sparking Pandas: an experiment</title><link href="https://pyvideo.org/pycon-italia-2017/sparking-pandas-an-experiment.html" rel="alternate"></link><published>2017-04-07T00:00:00+00:00</published><updated>2017-04-07T00:00:00+00:00</updated><author><name>Francesco Bruni</name></author><id>tag:pyvideo.org,2017-04-07:pycon-italia-2017/sparking-pandas-an-experiment.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a good library to deal with tabular data. What if you need to
manage an amount of data that doesn’t fit into memory? What if you want
to “distribute” your computations among multiple machines?&lt;/p&gt;
&lt;p&gt;Starting from a real scenario, Apache Spark will be presented as the
main tool to read and process collected data. It will be shown how a
Pandas-like syntax will come in handy to run aggregations, filtering and
grouping using a Spark Dataframe.&lt;/p&gt;
&lt;p&gt;A previous knowledge of Docker and Docker Compose will be very useful
while knowing MongoDB (where data will be fetched from) is not
mandatory. Basics of functional programming will help to understand
Spark inner logic.&lt;/p&gt;
</summary><category term="microservices"></category><category term="Jupyter"></category><category term="mongodb"></category><category term="data-visualization"></category><category term="data-analysis"></category><category term="spark"></category><category term="docker"></category></entry><entry><title>tqdm - A Fast, Extensible Progress Bar for Python and CLI</title><link href="https://pyvideo.org/pydata-meetups/tqdm-a-fast-extensible-progress-bar-for-python-and-cli.html" rel="alternate"></link><published>2019-05-07T00:00:00+00:00</published><updated>2019-05-07T00:00:00+00:00</updated><author><name>Casper da Costa-Luis</name></author><id>tag:pyvideo.org,2019-05-07:pydata-meetups/tqdm-a-fast-extensible-progress-bar-for-python-and-cli.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A demonstration of the world's most-used progress bar by its primary developer&lt;/p&gt;
</summary><category term="lightning-talks"></category><category term="tutorial"></category><category term="performance"></category><category term="jupyter"></category><category term="notebook"></category><category term="tqdm"></category><category term="progressbar"></category><category term="progressmeter"></category><category term="progress-bar"></category><category term="meter"></category><category term="rate"></category><category term="eta"></category><category term="console"></category><category term="terminal"></category><category term="time"></category><category term="progress"></category><category term="bar"></category><category term="gui"></category><category term="python"></category><category term="parallel"></category><category term="cli"></category><category term="utilities"></category></entry><entry><title>Taming Big Data with Jupyter and Friends</title><link href="https://pyvideo.org/pydata-meetups/taming-big-data-with-jupyter-and-friends.html" rel="alternate"></link><published>2017-03-02T00:00:00+00:00</published><updated>2017-03-02T00:00:00+00:00</updated><author><name>Carol Willing</name></author><id>tag:pyvideo.org,2017-03-02:pydata-meetups/taming-big-data-with-jupyter-and-friends.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This month, we are excited to host Carol Willing who will be discussing the Jupyter eco-sytem.  Carol develops software, electronics, educational tutorials, and is passionate about outreach.  She is a core developer on the Jupyter Project and is a former director at the Python Software foundation.  She continues to contribute her time to OpenHatch, Systers, PyLadies San Diego, and San Diego Python.&lt;/p&gt;
</summary><category term="jupyter"></category></entry><entry><title>Data Science &amp; Data Visualization in Python. How to harness power of Python for social good?</title><link href="https://pyvideo.org/pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Radovan Kavicky</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python as an Open Data Science tool offers many libraries for data visualization and I will show you how to use and combine the best. I strongly believe that power of data is not only in the information &amp;amp; insight that data can provide us, Data is and can be really beautiful and can not only transform our perception but also the world that we all live in.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my talk I will primarily focus on answering/offer the answer to these questions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Why we need data science and why more and more people should be really interested in analyzing data and data visualization? (motivation)&lt;/li&gt;
&lt;li&gt;What is data science and how to start doing it in Python? (introduction of procedures, tools, most popular IDE-s for Python, etc.)&lt;/li&gt;
&lt;li&gt;What tools for data analysis and data visualization Python offers? (in each stage of analysis the best libraries will be shown for the specific purpose; as for data visualization we will focus particularly on Bokeh, Seaborn, Plotly and use of Jupyter Notebook and Plotly)&lt;/li&gt;
&lt;li&gt;How to 'unlock' the insight hidden in data through Python and how to use it to transform not only public administration or business, but ultimately the transformation of the whole society and economy towards the insight &amp;amp; knowledge based? (potential of data science)&lt;/li&gt;
&lt;li&gt;Open Data, Open Government Partnership, Open Public Administration &amp;amp; all the advantages of Open Data Science &amp;amp; Python. Data-Driven Approach. Everywhere. Now. (the end of talk +vision)&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="python"></category><category term="data-science"></category><category term="data-visualization"></category><category term="analytics"></category><category term="PyData"></category><category term="PyDataBLN"></category><category term="PyDataBerlin"></category><category term="PyDataBA"></category><category term="PyDataBratislava"></category><category term="talk"></category><category term="Data"></category><category term="Bokeh"></category><category term="Social Good"></category><category term="datascience"></category><category term="jupyter"></category><category term="open science"></category><category term="open data science"></category><category term="DataVisualization"></category><category term="data-analysis"></category><category term="analysis"></category><category term="matplotlib"></category><category term="numpy"></category><category term="data wrangling"></category><category term="jupyter notebook"></category><category term="pandas"></category><category term="machine learning"></category><category term="deep learning"></category><category term="Open Data"></category><category term="Citizen Data Science"></category></entry><entry><title>Vizic - A Jupyter Based Interactive Visualization Tool for Astronomical Catalogs</title><link href="https://pyvideo.org/scipy-2017/vizic-a-jupyter-based-interactive-visualization-tool-for-astronomical-catalogs.html" rel="alternate"></link><published>2017-07-16T00:00:00+00:00</published><updated>2017-07-16T00:00:00+00:00</updated><author><name>Weixiang Yu</name></author><id>tag:pyvideo.org,2017-07-16:scipy-2017/vizic-a-jupyter-based-interactive-visualization-tool-for-astronomical-catalogs.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In the face of ever-growing datasets in astronomical sky surveys, we present Vizic, a Python/JavaScript library designed for Jupyter Notebook, which visualizes astronomical catalogs and presents them in vectorized form inside widgets in Jupyter notebooks. Visualized catalogs are fully interactive under a tiled web map approach. Vizic provides a unique and efficient way to visualize and explore multiple catalogs through interactive object filtering, color mapping, zooming and data selection using a lasso-like tool. In addition, custom overlays such as Voronoi layer can assist astronomers to visualize and interact with cosmic structures. At the end of this talk, we will give a brief demo to illustrate the potentials of Vizic in scientific research.&lt;/p&gt;
</summary><category term="vizic"></category><category term="jupyter"></category></entry><entry><title>Making your code faster: Cython and parallel processing in the Jupyter Notebook</title><link href="https://pyvideo.org/pydata-dc-2016/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Gustavo Patino</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/gapatino/Making-Your-Code-Faster-Cython-and-parallel-processing-in-the-Jupyter-Notebook"&gt;https://github.com/gapatino/Making-Your-Code-Faster-Cython-and-parallel-processing-in-the-Jupyter-Notebook&lt;/a&gt;
Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook"&gt;http://www.slideshare.net/PyData/making-your-code-faster-cython-and-parallel-processing-in-the-jupyter-notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As the complexity and scope of applications grow, it is very common to run into slow performance issues. In Python, it is possible to improve the speed of execution with the use of parallel processing and the Cython compiler. The Jupyter Notebook makes the implementation of both of them a relatively simple task, which will be the focus of this session.&lt;/p&gt;
</summary><category term="code"></category><category term="Cython"></category><category term="jupyter"></category><category term="jupyter notebook"></category><category term="notebook"></category><category term="parallel"></category><category term="processing"></category></entry><entry><title>Popping Kernels: An Exploration of Kernel Development for Jupyter Notebooks</title><link href="https://pyvideo.org/pydata-chicago-2016/popping-kernels-an-exploration-of-kernel-development-for-jupyter-notebooks.html" rel="alternate"></link><published>2016-08-26T00:00:00+00:00</published><updated>2016-08-26T00:00:00+00:00</updated><author><name>Safia Abdalla</name></author><id>tag:pyvideo.org,2016-08-26:pydata-chicago-2016/popping-kernels-an-exploration-of-kernel-development-for-jupyter-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;This talk will give individuals with no kernel experience and some Python experience, a brief introduction to the concepts they need to understand in order to develop kernels. This talk will also be useful to individuals who are looking for fun projects that will allow them to strengthen their skills in a particular programming language.&lt;/p&gt;
</summary><category term="development"></category><category term="jupyter"></category><category term="jupyter notebook"></category></entry><entry><title>Diffing and Merging Jupyter Notebooks with nbdime</title><link href="https://pyvideo.org/scipy-2016/diffing-and-merging-jupyter-notebooks-with-nbdime-scipy-2016-min-ragan-kelley.html" rel="alternate"></link><published>2016-07-15T00:00:00+00:00</published><updated>2016-07-15T00:00:00+00:00</updated><author><name>Min Ragan Kelley</name></author><id>tag:pyvideo.org,2016-07-15:scipy-2016/diffing-and-merging-jupyter-notebooks-with-nbdime-scipy-2016-min-ragan-kelley.html</id><summary type="html"></summary><category term="SciPy 2016"></category><category term="jupyter"></category><category term="jupyter notebook"></category></entry><entry><title>JupyterLab: Building Blocks for Interactive Computing</title><link href="https://pyvideo.org/scipy-2016/jupyterlab-building-blocks-for-interactive-computing-scipy-2016-brian-granger.html" rel="alternate"></link><published>2016-07-15T00:00:00+00:00</published><updated>2016-07-15T00:00:00+00:00</updated><author><name>Brian Granger</name></author><id>tag:pyvideo.org,2016-07-15:scipy-2016/jupyterlab-building-blocks-for-interactive-computing-scipy-2016-brian-granger.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Project Jupyter provides building blocks for interactive and exploratory computing. These building blocks make science and data science reproducible across over 40 programming language (Python, Julia, R, etc.). Central to the project is the Jupyter Notebook, a web-based interactive computing platform that allows users to author data- and code-driven narratives - computational narratives - that combine live code, equations, narrative text, visualizations, interactive dashboards and other media.&lt;/p&gt;
&lt;p&gt;While the Jupyter Notebook has proved to be an incredibly productive way of working with code and data interactively, it is helpful to decompose notebooks into more primitive building blocks: kernels for code execution, input areas for typing code, markdown cells for composing narrative content, output areas for showing results, terminals, etc. The fundamental idea of JupyterLab is to offer a user interface that allows users to assemble these building blocks in different ways to support interactive workflows that include, but go far beyond, Jupyter Notebooks.&lt;/p&gt;
&lt;p&gt;JupyterLab accomplishes this by providing a modular and extensible user interface that exposes these building blocks in the context of a powerful work space. Users can arrange multiple notebooks, text editors, terminals, output areas, etc. on a single page with multiple panels, tabs, splitters, and collapsible sidebars with a file browser, command palette and integrated help system. The codebase and UI of JupyterLab is based on a flexible plugin system that makes it easy to extend with new components.&lt;/p&gt;
&lt;p&gt;In this talk, we will demonstrate the JupyterLab interface, its codebase, and describe how it fits within the overall roadmap of the project.&lt;/p&gt;
</summary><category term="SciPy 2016"></category><category term="jupyter"></category><category term="jupyterlab"></category><category term="jupyter notebook"></category></entry><entry><title>Keynote: Project Jupyter</title><link href="https://pyvideo.org/scipy-2016/keynote-project-jupyter-scipy-2016-brian-granger.html" rel="alternate"></link><published>2016-07-15T00:00:00+00:00</published><updated>2016-07-15T00:00:00+00:00</updated><author><name>Brian Granger</name></author><id>tag:pyvideo.org,2016-07-15:scipy-2016/keynote-project-jupyter-scipy-2016-brian-granger.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Brian Granger is an Associate Professor of Physics at Cal Poly State University in San Luis Obispo, CA. He has a background in theoretical physics, with a Ph.D from the University of Colorado. His current research interests include quantum computing, parallel and distributed computing and interactive computing environments for scientific computing and data science. He is a leader of the IPython project, co-founder of Project Jupyter and is an active contributor to a number of other open source projects focused on data science in Python. He is a board member of the NumFocus Foundation and a fellow at Cal Poly’s Center for Innovation and Entrepreneurship. He is &amp;#64;ellisonbg on Twitter and GitHub.&lt;/p&gt;
&lt;p&gt;Announcement of Altair, Altair is a declarative statistical visualization library for Python. Altair is developed by Brian Granger and Jake Vanderplas in close collaboration with the UW Interactive Data Lab.&lt;/p&gt;
&lt;p&gt;With Altair, you can spend more time understanding your data and its meaning. Altair's API is simple, friendly and consistent and built on top of the powerful Vega-Lite JSON specification. This elegant simplicity produces beautiful and effective visualizations with a minimal amount of code.&lt;/p&gt;
</summary><category term="SciPy 2016"></category><category term="altair"></category><category term="jupyter"></category><category term="jupyter notebook"></category></entry><entry><title>Proselint: The Linting of Science Prose, and the Science of Prose Linting</title><link href="https://pyvideo.org/scipy-2016/proselint-the-linting-of-science-prose-and-the-science-of-prose-linting-scipy-2016-michael-pac.html" rel="alternate"></link><published>2016-07-15T00:00:00+00:00</published><updated>2016-07-15T00:00:00+00:00</updated><author><name>Michael Pacer</name></author><id>tag:pyvideo.org,2016-07-15:scipy-2016/proselint-the-linting-of-science-prose-and-the-science-of-prose-linting-scipy-2016-michael-pac.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Writing is notoriously hard, even for the best writers, and it's not for lack of good advice — a tremendous amount of knowledge is strewn across usage guides, dictionaries, technical manuals, essays, pamphlets, websites, and the hearts and minds of great authors and editors. But this knowledge is trapped, waiting to be extracted and transformed.&lt;/p&gt;
&lt;p&gt;We built Proselint, a Python-based linter for prose. Proselint identifies violations of expert style and usage guidelines. Proselint is open-source software released under the BSD license and works with Python 2 and 3. It runs as a command-line utility or editor plugin (e.g., Sublime Text, Atom, Vim, Emacs) and outputs advice in standard formats (e.g., JSON). Though in its infancy – perhaps 2% of what it could be – Proselint already includes modules addressing: redundancy, jargon, illogic, clichés, sexism, misspelling, inconsistency, misuse of symbols, malapropisms, oxymorons, security gaffes, hedging, apologizing, pretension.     Proselint can be seen as both a language tool for scientists and a tool for language science. On the one hand, it includes modules that promote clear and consistent prose in science writing. On the other, it measures language usage and explores the factors relevant to creating a useful linter.&lt;/p&gt;
</summary><category term="SciPy 2016"></category><category term="lint"></category><category term="prose"></category><category term="jupyter"></category></entry><entry><title>JupyterHub as an Interactive Supercomputing Gateway</title><link href="https://pyvideo.org/scipy-2016/jupyterhub-as-an-interactive-supercomputing-gateway-scipy-2016-michael-milligan.html" rel="alternate"></link><published>2016-07-14T00:00:00+00:00</published><updated>2016-07-14T00:00:00+00:00</updated><author><name>Michael Milligan</name></author><id>tag:pyvideo.org,2016-07-14:scipy-2016/jupyterhub-as-an-interactive-supercomputing-gateway-scipy-2016-michael-milligan.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;At the Minnesota Supercomputing Institute we are exploring ways to provide the immediacy and flexibility of interactive computing within the batch-scheduled, tightly controlled world of traditional cluster supercomputing. As Jupyter Notebook has gained in popularity, the steps needed to use it within such an environment have proven to be a barrier to entry even as increasingly powerful Python tools have developed to take advantage of large computational resources. JupyterHub to the rescue! Except out of the box, it doesn't know anything about resource types, job submission, and so on. We developed BatchSpawner and friends as a general JupyterHub backend for batch-scheduled environments. In this talk I will walk through how we have deployed JupyterHub to provide a user-friendly gateway to interactive supercomputing.&lt;/p&gt;
</summary><category term="SciPy 2016"></category><category term="hpc"></category><category term="jupyterhub"></category><category term="jupyter"></category><category term="jupyter notebook"></category><category term="supercomputing"></category></entry><entry><title>Reproducible, One Button Workflows with the Jupyter Notebook &amp; Scons</title><link href="https://pyvideo.org/scipy-2016/reproducible-one-button-workflows-with-the-jupyter-notebook-scons-scipy-2016-jessica-hamrick.html" rel="alternate"></link><published>2016-07-14T00:00:00+00:00</published><updated>2016-07-14T00:00:00+00:00</updated><author><name>Jessica Hamrick</name></author><id>tag:pyvideo.org,2016-07-14:scipy-2016/reproducible-one-button-workflows-with-the-jupyter-notebook-scons-scipy-2016-jessica-hamrick.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What is the best way to develop analysis code in the Jupyter notebook, while managing complex dependencies between analyses? In this talk, I will introduce nbflow, which is a project that integrates a Python-based build system (SCons) with the Jupyter notebook, enabling researchers to easily build sophisticated,
complex analysis pipelines entirely within notebooks while still maintaining a &amp;quot;one-button workflow&amp;quot; in which all analyses can be executed, in the correct order, from a single command. I will show how nbflow can be applied to existing analyses and how it can be used to construct an analysis pipeline stretching the entire way from data cleaning, to computing statistics, to generating figures,
and even to automatically generating LaTeX commands that can be used in publications to format results without the risk of copy-and-paste error.&lt;/p&gt;
</summary><category term="SciPy 2016"></category><category term="jupyter"></category><category term="jupyter notebook"></category><category term="workflow"></category><category term="nbflow"></category></entry></feed>