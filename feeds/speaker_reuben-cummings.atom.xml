<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_reuben-cummings.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-05-17T00:00:00+00:00</updated><entry><title>Using Functional Programming for efficient Data Processing and Analysis</title><link href="https://pyvideo.org/pycon-us-2017/using-functional-programming-for-efficient-data-processing-and-analysis.html" rel="alternate"></link><published>2017-05-17T00:00:00+00:00</published><updated>2017-05-17T00:00:00+00:00</updated><author><name>Reuben Cummings</name></author><id>tag:pyvideo.org,2017-05-17:pycon-us-2017/using-functional-programming-for-efficient-data-processing-and-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As a multi paradigm language, Python has great support for functional
programming. For better or for worse, leading data libraries such as
Pandas eschew the this style for object-oriented programming. This
tutorial will explain how to take advantage of Python's excellent
functional programming capabilities to efficiently obtain, clean,
transform, and store data from disparate sources.&lt;/p&gt;
</summary></entry><entry><title>Data Mining and Processing for fun and profit</title><link href="https://pyvideo.org/pycon-za-2016/data-mining-and-processing-for-fun-and-profit.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Reuben Cummings</name></author><id>tag:pyvideo.org,2016-10-07:pycon-za-2016/data-mining-and-processing-for-fun-and-profit.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="section" id="audience"&gt;
&lt;h4&gt;AUDIENCE&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;data scientists (current and aspiring)&lt;/li&gt;
&lt;li&gt;those who want to know more about data mining, analysis, and
processing&lt;/li&gt;
&lt;li&gt;those interested in functional programming&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="description"&gt;
&lt;h4&gt;DESCRIPTION&lt;/h4&gt;
&lt;p&gt;Data mining is a key skill that involves transforming data found online
and elsewhere from a hodgepodge of numbers into actionable information.
Using examples ranging from RSS feeds, open data portals, and web
scraping, this tutorial will show you how to efficiently obtain and
transform data from disparate sources.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="abstract"&gt;
&lt;h4&gt;ABSTRACT&lt;/h4&gt;
&lt;p&gt;Data mining is a key skill that any self proclaimed data scientist
should possess. It involves transforming data from disparate sources and
a hodgepodge of numbers into actionable information. Tabular data, e.g.,
csv/excel files, is very common in data mining and greatly benefits from
python's functional programming idioms. For better or for worse, the
leading python data libraries, Numpy and Pandas, eschew the functional
programming style for object-oriented programming.&lt;/p&gt;
&lt;p&gt;Using examples ranging from RSS feeds, the South Africa Data Portal API,
raw excel files, and basic web scraping, this tutorial will show how to
efficiently locate, obtain, transform, and remix data from the web.
These examples will prove that you can do a lot with functional
programming and without the need for Numpy or Pandas.&lt;/p&gt;
&lt;p&gt;Finally, it will introduce meza: a pure python, functional, data
analysis library and alternative to Pandas.&lt;/p&gt;
&lt;p&gt;IPython notebooks and sample data files will be distributed beforehand
on Github to facilitate code distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="objectives"&gt;
&lt;h4&gt;OBJECTIVES&lt;/h4&gt;
&lt;p&gt;Attendees will learn what data and data mining are, why they are
important. They will learn some basic functional programming idioms and
see how it is ideally suited to data mining. They will also see in what
areas the 20lb gorilla (Pandas) shines and when a lightweight
alternative (meza) is more practical.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="additional-info"&gt;
&lt;h4&gt;ADDITIONAL INFO&lt;/h4&gt;
&lt;div class="section" id="level"&gt;
&lt;h5&gt;Level&lt;/h5&gt;
&lt;p&gt;Intermediate&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="prerequisites"&gt;
&lt;h5&gt;Prerequisites&lt;/h5&gt;
&lt;p&gt;Students should have at least basic knowledge of python itertools and
functional programming paradigms, e.g., map, filter, reduce, and list
comprehensions.&lt;/p&gt;
&lt;p&gt;Laptops should have python3 and the following pypi libs installed: bs4,
requests, and meza.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="format"&gt;
&lt;h5&gt;Format&lt;/h5&gt;
&lt;p&gt;Students will be instructed in the completion of a series of exercises
that will explore using python for data mining. It will involve lessons
to introduce concepts; demos which implement the concepts using meza,
beautiful soup, and requests; and exercises for students to apply the
concepts.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="outline"&gt;
&lt;h4&gt;OUTLINE&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;[10 min] Part I&lt;ul&gt;
&lt;li&gt;[2 min] Intro (lecture)&lt;ul&gt;
&lt;li&gt;Who am I?&lt;/li&gt;
&lt;li&gt;Topics to cover&lt;/li&gt;
&lt;li&gt;format&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;[8 min] Definitions (lecture)&lt;ul&gt;
&lt;li&gt;What is data?&lt;/li&gt;
&lt;li&gt;What is data mining?&lt;/li&gt;
&lt;li&gt;Why is it data mining important?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;[35 min] Part II&lt;ul&gt;
&lt;li&gt;[15 min] You might not need pandas (demo)&lt;ul&gt;
&lt;li&gt;Obtaining data&lt;/li&gt;
&lt;li&gt;Analyzing and Transforming data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;[20 min] interactive data gathering (exercise)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;[45 min] Part III&lt;ul&gt;
&lt;li&gt;[10 min] Introducing meza (demo)&lt;/li&gt;
&lt;li&gt;[20 min] interactive data processing (exercise)&lt;/li&gt;
&lt;li&gt;[15 min] Q&amp;amp;A&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</summary></entry><entry><title>Stream processing made easy with riko</title><link href="https://pyvideo.org/pycon-za-2016/stream-processing-made-easy-with-riko.html" rel="alternate"></link><published>2016-10-06T00:00:00+00:00</published><updated>2016-10-06T00:00:00+00:00</updated><author><name>Reuben Cummings</name></author><id>tag:pyvideo.org,2016-10-06:pycon-za-2016/stream-processing-made-easy-with-riko.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;div class="section" id="audience"&gt;
&lt;h4&gt;AUDIENCE&lt;/h4&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;data scientists (current and aspiring)&lt;/li&gt;
&lt;li&gt;those who want to know more about data processing&lt;/li&gt;
&lt;li&gt;those who are intimidate by &amp;quot;big data&amp;quot; (java) frameworks and are
interested in a simpler, pure python alternative&lt;/li&gt;
&lt;li&gt;those interested in async and/or parallel programming&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="description"&gt;
&lt;h4&gt;DESCRIPTION&lt;/h4&gt;
&lt;p&gt;Big data processing is all the rage these days. Heavyweight frameworks
such as Spark, Storm, Kafka, Samza, and Flink have taken the spotlight
despite their complex setup, java dependency, and intense computer
resource usage.&lt;/p&gt;
&lt;p&gt;Those interested in simple, pure python solutions have limited options.
Most alternative software is synchronous, doesn't perform well on large
data sets, or is poorly documented.&lt;/p&gt;
&lt;p&gt;This talk aims to explain stream processing and its uses, and introduce
riko: a pure python stream processing library built with simplicity in
mind. Complete with various examples, youâ€™ll get to see how riko lazily
processes streams via its synchronous, asynchronous, and parallel
processing APIs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="objectives"&gt;
&lt;h4&gt;OBJECTIVES&lt;/h4&gt;
&lt;p&gt;Attendees will learn what streams are, how to process them, and the
benefits of stream processing. They will also see that most data isn't
&amp;quot;big data&amp;quot; and therefore doesn't require complex (java) systems
(**cough** spark and storm *&lt;em&gt;cough*&lt;/em&gt;) to process it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="detailed-abstract"&gt;
&lt;h4&gt;DETAILED ABSTRACT&lt;/h4&gt;
&lt;div class="section" id="stream-processing"&gt;
&lt;h5&gt;Stream processing?&lt;/h5&gt;
&lt;div class="section" id="what-are-streams"&gt;
&lt;h6&gt;What are streams?&lt;/h6&gt;
&lt;p&gt;A stream is a sequence of data. The sequence can be as simple as a list
of integers or as complex as a generator of dictionaries.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-do-you-process-streams"&gt;
&lt;h6&gt;How do you process streams?&lt;/h6&gt;
&lt;p&gt;Stream processing is the act of taking a data stream through a series of
operations that apply a (usually pure) function to each element in the
stream. These operations are pipelined so that the output of one
function is the input of the next one. By using pure functions, the
processing becomes embarrassingly parallel: you can split the items of
the stream into separate processes (or threads) which then perform the
operations simultaneously (without the need for communicating between
processes/threads). [1-4]&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-can-stream-processing-do"&gt;
&lt;h6&gt;What can stream processing do?&lt;/h6&gt;
&lt;p&gt;Stream processing allows you to efficiently manipulate large data sets.
Through the use of lazy evaluation, you can process data stream too
large to fit into memory all at once.&lt;/p&gt;
&lt;p&gt;Additionally, stream processing has several real world applications
including:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;parsing rss feeds (rss readers, think
&lt;a class="reference external" href="http://feedly.com/"&gt;feedly&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;combining different types data from multiple sources in innovative
ways (mashups, think &lt;a class="reference external" href="http://trendsmap.com/"&gt;trendsmap&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;taking data from multiple sources, manipulating the data into a
homogeneous structure, and storing the result in a database
(extracting, transforming, and loading data; aka ETL, data
wrangling...)&lt;/li&gt;
&lt;li&gt;aggregating similarly structured data from siloed sources and
presenting it via a unified interface (aggregators, think
&lt;a class="reference external" href="kayak.com"&gt;kayak&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[5, 6]&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="stream-processing-frameworks"&gt;
&lt;h5&gt;Stream processing frameworks&lt;/h5&gt;
&lt;p&gt;If you've heard anything about stream processing, chances are you've
also heard about frameworks such as Spark, Storm, Kafka, Samza, and
Flink. While popular, these frameworks have a complex setup and
installation process, and are usually overkill for the amount of data
typical python users deal with. Using a few examples, I will show basic
Storm usage and how it stacks up against BASH.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="introducing-riko"&gt;
&lt;h5&gt;Introducing riko&lt;/h5&gt;
&lt;p&gt;Supporting both Python 2 and 3, riko is the first pure python stream
processing library to support synchronous, asynchronous, and parallel
processing. It's built using functional programming methodology and lazy
evaluation by default.&lt;/p&gt;
&lt;div class="section" id="basic-riko-usage"&gt;
&lt;h6&gt;Basic riko usage&lt;/h6&gt;
&lt;p&gt;Using a series of examples, I will show basic riko usage. Examples will
include counting words, fetching streams, and rss feed manipulation. I
will highlight the key features which make riko a better stream
processing alternative to Storm and the like.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="riko-s-many-paradigms"&gt;
&lt;h6&gt;riko's many paradigms&lt;/h6&gt;
&lt;p&gt;Depending on the type of data being processed; a synchronous,
asynchronous, or parallel processing method may be ideal. Fetching data
from multiple sources is suited for asynchronous or thread based
parallel processing. Computational intensive tasks are suited for
processor based parallel processing. And asynchronous processing is best
suited for debugging or low latency environments.&lt;/p&gt;
&lt;p&gt;riko is designed to support all of these paradigms using the same api.
This means switching between paradigms requires trivial code changes
such as adding a yield statement or changing a keyword argument.&lt;/p&gt;
&lt;p&gt;Using a series of examples, I will show each of these paradigms in
action.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary></entry></feed>