<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 18 Aug 2018 00:00:00 +0000</lastBuildDate><item><title>High Performance Python Microservice Communication</title><link>https://pyvideo.org/pybay-2018/high-performance-python-microservice-communication.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joe Cabrera</dc:creator><pubDate>Sat, 18 Aug 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-08-18:pybay-2018/high-performance-python-microservice-communication.html</guid></item><item><title>Indexing all the things: Building your search engine in Python</title><link>https://pyvideo.org/pygotham-2017/indexing-all-the-things-building-your-search-engine-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Since the emergence of Elasticsearch, common Information Retrieval tasks such as indexing, scoring and retrieval of documents into a search engine have never been easier. However unique challenges still exist for indexing large sets of data from databases. At Jopwell, we need to insure that data in our database is kept in constant sync with data in our search index.&lt;/p&gt;
&lt;p&gt;Initially you need to take data from a traditional SQL database and flatten it for indexing in Elasticsearch. Since indexing this data can be a memory intensive task, Celery is useful for ensuring you can index large sets of data in both a distributed and memory-conservative manner. Once all your documents are in your Elasticsearch index, you need to retrieve data from your database related to a user’s search results.&lt;/p&gt;
&lt;p&gt;In this talk, I’ll show the basics of creating a search engine in Python, keeping these it synced with another data store and how you can keep your index running smoothly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Talk Outline&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to the problem (2 min)&lt;/li&gt;
&lt;li&gt;Building your document indexer (7 min)&lt;ul&gt;
&lt;li&gt;Flattening database data into a search document&lt;/li&gt;
&lt;li&gt;Using Celery to index documents efficiently&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scoring and search results retrieval (7 min)&lt;ul&gt;
&lt;li&gt;Scoring algorithms&lt;/li&gt;
&lt;li&gt;Retrieving matching results from the database&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Strategies for syncing data from (7 min)&lt;ul&gt;
&lt;li&gt;Traditional SQL database&lt;/li&gt;
&lt;li&gt;Elasticsearch index&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Future work (2 min)&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joe Cabrera</dc:creator><pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-06:pygotham-2017/indexing-all-the-things-building-your-search-engine-in-python.html</guid></item><item><title>Building a data processing pipeline in Python</title><link>https://pyvideo.org/pygotham-2015/building-a-data-processing-pipeline-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recently, the growth of publicly available data has been enormous.
Python has a number of libraries and tool to aid you in building your
data processing pipeline. These tools include Celery, Requests,
BeautifulSoup and SQL-Alchemy. When combined together you can build an
efficient and scalable data processing pipeline.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joe Cabrera</dc:creator><pubDate>Sat, 15 Aug 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-08-15:pygotham-2015/building-a-data-processing-pipeline-in-python.html</guid></item></channel></rss>