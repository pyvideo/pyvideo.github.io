<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Frank Schlimbach</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_frank-schlimbach.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-07-23T00:00:00+00:00</updated><subtitle></subtitle><entry><title>The Painless Route in Python to Fast and Scalable Machine Learning</title><link href="https://pyvideo.org/europython-2020/the-painless-route-in-python-to-fast-and-scalable-machine-learning.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>David Liu</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/the-painless-route-in-python-to-fast-and-scalable-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python is the lingua franca for data analytics and machine learning. Its superior productivity makes it the preferred tool for prototyping. However, traditional Python packages are not necessarily designed to provide high performance and scalability for large datasets.
We start our tutorial with a short introduction on how to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python is the lingua franca for data analytics and machine learning. Its superior productivity makes it the preferred tool for prototyping. However, traditional Python packages are not necessarily designed to provide high performance and scalability for large datasets.
We start our tutorial with a short introduction on how to get close-to-native performance with Intel-optimized packages, such as numpy, scipy, and scikit-learn. The next part of the tutorial is focused on getting high performance and scalability from multi-cores on a single machine to large clusters of workstations. We will demonstrate that with Python it is possible to achieve the same performance and scalability as with hand-tuned C++/MPI code:
-       Scalable Dataframe Compiler (SDC) is used to compile analytics code using pandas/Python and scale it to bare-metal cluster performance. It compiles a subset of Python code into efficient parallel binaries that use message passing to perform collective communications.
-       A convenient Python API to data analytics and machine learning primitives (daal4py). While its interface is scikit-learn-like, its MPI-based engine allows to scale machine learning algorithms to bare-metal cluster performance.
-       In the tutorial, we will use SDC and daal4py together to build an end-to-end analytics pipeline that scales to clusters, requiring only minimal code changes.&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Analytics"></category><category term="Big Data"></category><category term="Distributed Systems"></category><category term="Machine-Learning"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>High Performance and Scalability Made Easy For Data-Analytics/Machine-Learning Codes</title><link href="https://pyvideo.org/pycon-italia-2019/high-performance-and-scalability-made-easy-for-data-analyticsmachine-learning-codes.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Frank Schlimbach</name></author><id>tag:pyvideo.org,2019-05-03:/pycon-italia-2019/high-performance-and-scalability-made-easy-for-data-analyticsmachine-learning-codes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Is your data-analysis code running for too long with Scikit-Learn,
Numpy, Scipy and/or Pandas? Does your data-set blow your memory? Are
spark/dask/… not giving you the scalability and/or ease-of-use your
need?&lt;/p&gt;
&lt;p&gt;Intel implemented optimizations in Numpy, Scikit-Learn, Scipy and Pandas
which achieve up to orders of …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Is your data-analysis code running for too long with Scikit-Learn,
Numpy, Scipy and/or Pandas? Does your data-set blow your memory? Are
spark/dask/… not giving you the scalability and/or ease-of-use your
need?&lt;/p&gt;
&lt;p&gt;Intel implemented optimizations in Numpy, Scikit-Learn, Scipy and Pandas
which achieve up to orders of magnitude better performance for many
functionalities compared to standard implementations. The optimized
packages are drop-in replacements which do not require any code changes
and allow processing more data in less time.&lt;/p&gt;
&lt;p&gt;Moreover, two new tools allow you to easily bring your full data
analytics pipeline to unprecedented scales: daal4py and HPAT. Daal4py is
a convenient Python API to Intel® DAAL (Intel® Data Analytics
Acceleration Library). While its interface is scikit-learn-like, its
MPI-based engine under the hood allows scaling machine learning
algorithms to bare-metal cluster performance with only little code
changes. HPAT (High Performance Analytics Toolkit) scales analytics
codes using Pandas/Python to bare-metal cluster performance. It
automatically compiles a subset of Python (Pandas/Numpy/Daal4py) to
efficient parallel binaries with MPI, also requiring only minimal code
changes. With these tools your code can be orders of magnitude faster
than alternatives like Apache Spark - without the pain of dealing
directly with lower-level languages and/or tools like C and/or message
passing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1805"&gt;https://python.it/feedback-1805&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 15:45 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon Italia 2019"></category></entry></feed>