<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Jiewen Tan</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Mon, 16 Oct 2023 00:00:00 +0000</lastBuildDate><item><title>Lightning Talk: Large-Scale Distributed Training with Dynamo and Triton</title><link>https://pyvideo.org/pytorch-conference-2023/lightning-talk-large-scale-distributed-training-with-dynamo-and-triton.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we cover PyTorch/XLA distributed API in relation with Torch.Dynamo. Specifically, we discuss the new PyTorch/XLA SPMD API for automatic parallelization and our latest LLaMA2 training results. PyTorch/XLA SPMD makes it simple for PyTorch developers to distribute their ML workloads (e.g., training &amp;amp; inference with Dynamo) with easy-to-use API, and uses XLA GSPMD, high-performance automatic parallelization system. Under the hood, it transforms the user single-device program into a partitioned one. We will share how we enabled advanced 2D sharding strategies for LLaMA2 using PyTorch/XLA SPMD.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Yeounoh Chung</dc:creator><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-large-scale-distributed-training-with-dynamo-and-triton.html</guid><category>PyTorch Conference 2023</category><category>Lightning Talk</category></item></channel></rss>