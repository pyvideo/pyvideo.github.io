<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 03 Aug 2019 00:00:00 +0000</lastBuildDate><item><title>Fundamental Results in ML and How to Use Them</title><link>https://pyvideo.org/pydata-delhi-2019/fundamental-results-in-ml-and-how-to-use-them.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We’ve all heard terms like Bayes error, perceptron learning theorem, the
fundamental theorem of statistical learning, VC dimension, etc. This
talk is about using the math-heavy fundamentals of machine learning to
understand the very solvability of classification problems. By the end
of the talk, you will get a clear picture of how these ideas can be
practically applied to classification problems.&lt;/p&gt;
&lt;p&gt;Why does a classifier not fit? This can only happen for two reasons:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Because the model is not smart enough, or&lt;/li&gt;
&lt;li&gt;Because the training data itself is not “classifiable”.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unfortunately, the only obvious way to determine the &lt;em&gt;classifiability&lt;/em&gt;
or &lt;em&gt;separability&lt;/em&gt; of a training dataset is to use a variety of
classification models with a variety of hyperparameters. In other words,
separability of classes in a dataset is usually expressed only in terms
of which model worked on that dataset.&lt;/p&gt;
&lt;p&gt;Unfortunately, this does not answer the fundamental question of whether
a dataset is classifiable or not. If we keep on increasing the
complexity of models and trying them out on a dataset without success,
all we can infer from this is that the set of models we have tried out
&lt;em&gt;so far&lt;/em&gt; are incapable of learning the classification problem. It does
not necessarily mean that the problem is unsolvable.&lt;/p&gt;
&lt;p&gt;Fortunately, many shallow learning models have been widely studied and
are well understood. As such, it is quite possible to place theoretical
bounds on their performance in the context of a dataset. There are a
variety of statistics that we can use &lt;em&gt;a priori&lt;/em&gt; to determine the
likelihood of a model fitting a dataset.&lt;/p&gt;
&lt;p&gt;This talk is about how we can use these results towards developing a
strategy, a structured approach for carrying out machine learning
experiments, instead of blindly running models and hoping that one of
them works. Starting from elementary results like Bayes theorem and the
perceptron learning rule all the way up to complex ideas like kernel
methods and VC dimension, this talk develops a framework for the
analysis of data in the context of separability of classes.&lt;/p&gt;
&lt;p&gt;While the talk might sound theoretical, major focus will be on how to
make practical, hands-on use of these concepts to better understand your
data and your models. By the end of the talk, you will have learnt how
to &lt;em&gt;prioritize&lt;/em&gt; which models to use on which dataset, and how to compute
the likelihood of them fitting on the data. This rigorous analysis of
models and data saves a lot of effort and money, as the talk will
demonstrate with real-world examples.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jaidev Deshpande</dc:creator><pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-08-03:pydata-delhi-2019/fundamental-results-in-ml-and-how-to-use-them.html</guid></item><item><title>Continuous Integration for data scientists by Jaidev Deshpande 36:39</title><link>https://pyvideo.org/pycon-india-2016/continuous-integration-for-data-scientists-by-jaidev-deshpande-3639.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jaidev Deshpande</dc:creator><pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-09-23:pycon-india-2016/continuous-integration-for-data-scientists-by-jaidev-deshpande-3639.html</guid></item><item><title>Automatic Data Validation and Cleaning with PySemantic</title><link>https://pyvideo.org/pydelhiconf-2016/automatic-data-validation-and-cleaning-with-pysemantic.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Title Talk by  Jaidev Deshpande ( &amp;#64;jaidevd) at #PyDelhiConf on 05th Mar 2016. Slides and content available at &lt;a class="reference external" href="https://cfp.pydelhi.org/pydelhi-conference-2016/proposals/automatic-data-validation-and-cleaning-with-pysemantic/"&gt;https://cfp.pydelhi.org/pydelhi-conference-2016/proposals/automatic-data-validation-and-cleaning-with-pysemantic/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Visit &lt;a class="reference external" href="https://pydelhi.org/talks"&gt;https://pydelhi.org/talks&lt;/a&gt; for more.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jaidev Deshpande</dc:creator><pubDate>Sun, 03 Apr 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-04-03:pydelhiconf-2016/automatic-data-validation-and-cleaning-with-pysemantic.html</guid></item></channel></rss>