<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-madrid-2016.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-05-16T00:00:00+00:00</updated><entry><title>Basic Python Packages for Science</title><link href="https://pyvideo.org/pydata-madrid-2016/basic-python-packages-for-science.html" rel="alternate"></link><published>2016-05-16T00:00:00+00:00</published><updated>2016-05-16T00:00:00+00:00</updated><author><name>Alejandro Sáez Mollejo</name></author><id>tag:pyvideo.org,2016-05-16:pydata-madrid-2016/basic-python-packages-for-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Aeropython’s guide to the Python Galaxy!
This workshop will be the perfect introduction to the powerful tools and packages that Python can offer to the Data Scientist. We will start with the Jupyter Notebook, and explore the basics of NumPy, matplotlib, SciPy, and many more, through hot topics of today like pollution trends or gravitational waves.&lt;/p&gt;
&lt;p&gt;This workshop is an adaptation of the Aeropython introductory course to Python for engineers, which has been taught about fifteen times at five different universities. We will guide the attendees through the main packages used in science, specifically in data science:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Jupyter Notebook&lt;/li&gt;
&lt;li&gt;Interactive Widgets&lt;/li&gt;
&lt;li&gt;NumPy&lt;/li&gt;
&lt;li&gt;Matplotlib&lt;/li&gt;
&lt;li&gt;SciPy&lt;/li&gt;
&lt;li&gt;Scikit-Learn&lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;SymPy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will follow interesting actual themes in order to add flavour, discovering the packages while working on actual data. For example, we will use temperature and pollution data, trying to find underlying patterns, and will try to replicate some of the work made on the Gravitational Waves experiment, which was as well made in Python.&lt;/p&gt;
</summary></entry><entry><title>Lightning Talks and Closing Address - Sunday.</title><link href="https://pyvideo.org/pydata-madrid-2016/lightning-talks-and-closing-address-sunday.html" rel="alternate"></link><published>2016-05-06T00:00:00+00:00</published><updated>2016-05-06T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2016-05-06:pydata-madrid-2016/lightning-talks-and-closing-address-sunday.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary><category term="lightning talks"></category></entry><entry><title>Modelling a text corpus using Deep Boltzmann Machines</title><link href="https://pyvideo.org/pydata-madrid-2016/modelling-a-text-corpus-using-deep-boltzmann-machines.html" rel="alternate"></link><published>2016-05-05T00:00:00+00:00</published><updated>2016-05-05T00:00:00+00:00</updated><author><name>Ricardo Pio Monti</name></author><id>tag:pyvideo.org,2016-05-05:pydata-madrid-2016/modelling-a-text-corpus-using-deep-boltzmann-machines.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Deep Boltzmann machines (DBMs) are exciting for a variety of reasons, principal among which is the fact that they are able to learn probabilistic representations of data in an entirely unsupervised manner. This allows DBMs to leverage large quantities of unlabelled data which are often available. The resulting representations can then be fine-tuned using limited labelled data or studied to obtain a more comprehensive understanding of the data at hand. This talk will begin by providing a high level description of DBMs and the training algorithms involved in learning such models. A topic modelling example will be used as a motivating example to discuss practical aspects of fitting DBMs and potential pitfalls. The entire code for this project is written in python using only standard libraries (e.g., numpy).&lt;/p&gt;
</summary></entry><entry><title>Python for developing an automated trading platform</title><link href="https://pyvideo.org/pydata-madrid-2016/python-for-developing-an-automated-trading-platform.html" rel="alternate"></link><published>2016-05-03T00:00:00+00:00</published><updated>2016-05-03T00:00:00+00:00</updated><author><name>Miguel Sánchez de León Peque</name></author><id>tag:pyvideo.org,2016-05-03:pydata-madrid-2016/python-for-developing-an-automated-trading-platform.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Nowadays Python is the perfect environment for developing a real-time automated trading tool. In this talk we will discuss the development of: a general-purpose multiagent-system module using Pyro and ZeroMQ; a platform, based on it, for developing automated trading strategies using Numpy, Numba, Theano, etc.; and a GUI for visualizing real-time market data using PyQtGraph and Qt.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://peque.github.io/PyData-Madrid-2016/"&gt;https://peque.github.io/PyData-Madrid-2016/&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Woosh: a fast pure-Python search engine library</title><link href="https://pyvideo.org/pydata-madrid-2016/woosh-a-fast-pure-python-search-engine-library.html" rel="alternate"></link><published>2016-05-01T00:00:00+00:00</published><updated>2016-05-01T00:00:00+00:00</updated><author><name>Claudia Guirao</name></author><id>tag:pyvideo.org,2016-05-01:pydata-madrid-2016/woosh-a-fast-pure-python-search-engine-library.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Whoosh lets you index free-form or structured text and then quickly find matching documents based on simple or complex search criteria. Whoosh is a fast, pure Python search engine library.You can use Whoosh anywhere you can use Python, no compiler or Java required.&lt;/p&gt;
&lt;p&gt;What is Whoosh?&lt;/p&gt;
&lt;p&gt;Whoosh is a fast, pure Python search engine library.&lt;/p&gt;
&lt;p&gt;The primary design impetus of Whoosh is that it is pure Python. You should be able to use Whoosh anywhere you can use Python, no compiler or Java required. Like one of its ancestors, Lucene, Whoosh is not really a search engine, it’s a programmer library for creating a search engine. Practically no important behavior of Whoosh is hard-coded. Indexing of text, the level of information stored for each term in each field, parsing of search queries, the types of queries allowed, scoring algorithms, etc. are all customizable, replaceable, and extensible.&lt;/p&gt;
</summary></entry><entry><title>New Computer Trends and How This Affects Us</title><link href="https://pyvideo.org/pydata-madrid-2016/new-computer-trends-and-how-this-affects-us.html" rel="alternate"></link><published>2016-04-29T00:00:00+00:00</published><updated>2016-04-29T00:00:00+00:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2016-04-29:pydata-madrid-2016/new-computer-trends-and-how-this-affects-us.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Nowadays computers are being designed quite differently as they were made more than a decade ago; however, very little in software architecture has changed in order to accommodate for the changes in the hardware architecture. During my talk I am going to describe which those fundamental changes are and how to deal with them from the point of view of a long-time developer.&lt;/p&gt;
&lt;p&gt;During the last decade the evolution of the computers has been much different than before. Instead of seeing acceleration in CPU clock speeds we are seeing more cores in CPUs, and instead of having plain simple architectures with a CPU, memory and hard disk, we are seeing computer facilities with several CPUs, several levels of caches and several persistent layers of storage.&lt;/p&gt;
&lt;p&gt;It is unfortunate that not many libraries are being designed nowadays with this shift in mind. My intention is to explain how to tackle with this efficiently during the making of libraries for handling big datasets. We will see that the election of a good language is important too, and how Python, complemented with others (like Cyhton, C or Julia), is a good match for this.&lt;/p&gt;
</summary></entry><entry><title>A Hitchhiker's Guide to Data Science</title><link href="https://pyvideo.org/pydata-madrid-2016/a-hitchhikers-guide-to-data-science.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>Christine Doig</name></author><id>tag:pyvideo.org,2016-04-28:pydata-madrid-2016/a-hitchhikers-guide-to-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The popularity of Python for data science has rocketed in recent years. Its ease of use, a great developer and user community and a solid core of scientific libraries, has attracted many users that were previously using other languages, commercial solutions, or had never wrote a line of code. In this talk, we will explore data science, the state of the Python ecosystem and how to get started. Data science is an interdisciplinary field concerned with extracting valuable insights from data. But, even before the term data science existed, many fields, such as statistics, science, and business intelligence, had already been dealing with data, and using it to discover interesting patterns. Data science is about bringing those communities together and Python has made the conversation easier by providing a common language. Our open source community has created awesome libraries that make the life of anyone searching for answers in data much easier. They have democratized knowledge extraction from data. The interest in the Python community for data science libraries has also caused the proliferation of many open source projects, making it hard for the novice to get their head around all the names and functionality. This talk is a “getting started” guide to the entire ecosystem, to help those struggling with navigating the waters. After the talk, attendees will be comfortable exploring on their own all the possibilities the community has to offer them. Each data science journey is different, the possibilities are infinite, figuring out which one is yours is on you. Follow your path, but get a map. This talk will be your start.&lt;/p&gt;
</summary></entry><entry><title>A primer on recommendation systems.</title><link href="https://pyvideo.org/pydata-madrid-2016/a-primer-on-recommendation-systems.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>Manuel Garrido Peña</name></author><id>tag:pyvideo.org,2016-04-28:pydata-madrid-2016/a-primer-on-recommendation-systems.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;Recommendation systems are one topic that most Data Scientists are familiar with. However, there is a lack of entry level, general view tutorials on the python ecosystem. This workshop will start with the basics, and implement recommendation engines with different degrees of complexity, and talk about Similarity Index, Content filtering and Collaborative filtering.&lt;/p&gt;
</summary></entry><entry><title>City attractiveness seen through Twitter</title><link href="https://pyvideo.org/pydata-madrid-2016/city-attractiveness-seen-through-twitter.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>Antònia Tugores</name></author><id>tag:pyvideo.org,2016-04-28:pydata-madrid-2016/city-attractiveness-seen-through-twitter.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;How long would information originating from a given city require to reach any other city if were to pass from person to person only through face to face conversations? Or, what is the likelihood that information reaches a certain distance away after a given time period. We introduce a method to assess the attractiveness of cities using geolocated tweets as a proxy for human mobility.&lt;/p&gt;
&lt;p&gt;Introduction&lt;/p&gt;
&lt;p&gt;Research on mobility has traditionally relied on surveys and economic datasets, generally composed of small samples with a low spatio-temporal resolution. However, with the increasing availability of large databases generated by the use of geolocated information and communication technologies (ICT) devices such as mobile phones, credit or transport cards, the situation is now changing. This is the reason why we focus here on geolocated tweets, which have already proven to be an useful tool to analyse world wide mobility and provide the ideal framework for our analysis. We use these geolocated data in two different contexts, relevant in both scientific and economic worlds.&lt;/p&gt;
&lt;p&gt;Human diffusion and city influence&lt;/p&gt;
&lt;p&gt;The study of competition and interactions between cities has a long history in fields such as Geography, Spatial Economics and Urbanism. This research has traditionally taken as basis information on finance exchanges, sharing of firm headquarters, number of passengers transported by air or tons of cargo dispatched from one city to another.&lt;/p&gt;
&lt;p&gt;We have used Twitter data to track users and classify cities according to the mobility patterns of their visitors. Top cities as mobility sources or attraction points are identified as central places at a global scale for cultural and information interchanges. More specifically and assuming data reliability, we consider both the users’ displacements after visiting each city and the origins of each user within each city. Differences in the mobility for local residents and external visitors are taken into account, in such a way that cities can be ranked according to the extension covered by the diffusion of visitors and residents, taken both together and separately, and by the attractiveness they exhibit towards visitors.&lt;/p&gt;
&lt;p&gt;Finally, we also consider the interaction between cities, forming a network that provide a framework to study urban communities and the role cities play within their own community (regional) versus a global perspective.&lt;/p&gt;
&lt;p&gt;Touristic site attractiveness&lt;/p&gt;
&lt;p&gt;Travelling is getting more accessible in the present era of progressive globalization. Over the last fifty years, geographers and economists have attempted to understand the contribution of tourism to global and regional economy and to assess the impact of tourism on local people.&lt;/p&gt;
&lt;p&gt;We propose a ranking of touristic sites worldwide based on their attractiveness measured with geolocated data as a proxy for human mobility. Instead of studying the most visited places, the focus of the analysis was set on the sites attracting visitors from different parts of the world.&lt;/p&gt;
&lt;p&gt;In particular, we propose three rankings of the touristic sites’ attractiveness based on the spatial distribution of the visitors’ place of residence. Then, we study the touristic site’s visiting figures by country of residence. And finally, we focus on users detected in more than one site and explore the relationships between the 20 touristic sites by building a network of undirected trips between them.&lt;/p&gt;
&lt;p&gt;We have introduced a new method to measure the influence of cities based on the Twitter user displacements as proxies for the mobility flows. The method, despite some possible biases due to the population using online social media, allows for a direct measurement of a place influence in the world. We proposed three types of rankings capturing different perspectives: rankings based on “city-to-world” and “world-to-city” interactions and rankings based on “city-to-city” interaction.&lt;/p&gt;
&lt;p&gt;Using a similar approach, we have analysed the influence and the attractiveness of a touristic site. Many different rankings of the most visited touristic sites exist, they are often based on the number of visitors, which does not tell us much about their attractiveness at a global scale.&lt;/p&gt;
&lt;p&gt;In summary, we have illustrated the power of geolocated data to provide world wide information regarding mobility.&lt;/p&gt;
</summary></entry><entry><title>Embrace conda packages.</title><link href="https://pyvideo.org/pydata-madrid-2016/embrace-conda-packages.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>Juan Luís Cano Rodríguez</name></author><id>tag:pyvideo.org,2016-04-28:pydata-madrid-2016/embrace-conda-packages.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Installation problems represent half of your mailing list traffic? Nobody in your team knows how to properly configure the Visual Studio compilers? Forcing your users to download a script full of &amp;quot;sudo&amp;quot; commands? Providing Docker containers and virtual machines as the only sane way to run your software? No more suffering or pain: create conda packages with conda-build and stop worrying today.&lt;/p&gt;
&lt;p&gt;From my perspective, scientific software seems to have the more complex build processes ever imagined. The amount of compiled dependencies is overwhelming, even though many packages share a fair amount of them. Still, the landscape of building systems is simply terrifying, every team rolls its own alternative and in the end the only sane approach is to create an Ubuntu-like virtual machine just to run a particular software. The wonderful dream of reproducibility didn't involve debugging arcane CMake scripts at night, compiling PETSc in a million ways or installing everything in root locations just because search paths are hardcoded everywhere.&lt;/p&gt;
&lt;p&gt;The idea of this talk is to make some fun at the current situation and try to put it to an end once and for all. conda-build looks like the solution we've always desired to build Python packages with compiled extensions, with its declarative nature and its clear separation of build and testing phases. Yet it is much more than that, since it provides a way to build packages which have nothing to do with Python, which I consider is the closest thing we have to a true cross platform package manager. Combined with cloud services like AppVeyor and Travis CI, we can create a build pipeline that creates cross-platform, relocatable conda packages for Windows, OS X and Linux.&lt;/p&gt;
&lt;p&gt;Following my experience with FEniCS, Firedrake and Pyomo, we will also explore how to overcome some conda-build limitations and provide some ideas on how could the developers work them so it becomes the perfect build tool we have always wanted, but never deserved.&lt;/p&gt;
</summary></entry><entry><title>Lightning Talks - Saturday session.</title><link href="https://pyvideo.org/pydata-madrid-2016/lightning-talks-saturday-session.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>Gabriel de Maeztu</name></author><id>tag:pyvideo.org,2016-04-28:pydata-madrid-2016/lightning-talks-saturday-session.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="15%" /&gt;
&lt;col width="10%" /&gt;
&lt;col width="38%" /&gt;
&lt;col width="37%" /&gt;
&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;&lt;a class="reference external" href="https://youtu.be/IKPQrn-rS-Q?t=120"&gt;02:20&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Gabriel de Maeztu&lt;/td&gt;
&lt;td&gt;Jupyter themes&lt;/td&gt;
&lt;td&gt;&lt;a class="reference external" href="https://github.com/merqurio/jupyter_themes"&gt;https://github.com/merqurio/jupyter_themes&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a class="reference external" href="https://youtu.be/IKPQrn-rS-Q?t=350"&gt;05:50&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Juan Luis Cano Rodríguez&lt;/td&gt;
&lt;td&gt;A marte con python usando poliastro&lt;/td&gt;
&lt;td&gt;&lt;a class="reference external" href="http://poliastro.github.io"&gt;http://poliastro.github.io&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a class="reference external" href="https://youtu.be/IKPQrn-rS-Q?t=683"&gt;11:23&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Israel Saeta Pérez&lt;/td&gt;
&lt;td&gt;&lt;a class="reference external" href="http://slides.com/israelsaetaperez/sklearn-pandas/"&gt;Sklearn-Pandas&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a class="reference external" href="https://pypi.python.org/pypi/sklearn-pandas/"&gt;https://pypi.python.org/pypi/sklearn-pandas/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a class="reference external" href="https://youtu.be/IKPQrn-rS-Q?t=999"&gt;16:39&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;PyBikes&lt;/td&gt;
&lt;td&gt;&lt;a class="reference external" href="https://github.com/eskerda/PyBikes"&gt;https://github.com/eskerda/PyBikes&lt;/a&gt; &lt;a class="reference external" href="https://citybik.es/"&gt;https://citybik.es/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a class="reference external" href="https://youtu.be/IKPQrn-rS-Q?t=1288"&gt;21:28&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Siro Moreno&lt;/td&gt;
&lt;td&gt;&lt;a class="reference external" href="https://github.com/AunSiro/Algoritmos-Geneticos-R-Python-Meetup"&gt;Introduction to genetic algorithms. AeroPython.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a class="reference external" href="https://github.com/AeroPython/Taller-PyConEs-2015/"&gt;https://github.com/AeroPython/Taller-PyConEs-2015/&lt;/a&gt; &lt;a class="reference external" href="https://github.com/AeroPython/aeropy/tree/Xfoil_interaction"&gt;https://github.com/AeroPython/aeropy/tree/Xfoil_interaction&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a class="reference external" href="https://youtu.be/IKPQrn-rS-Q?t=1655"&gt;27:35&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Javier Martin&lt;/td&gt;
&lt;td&gt;Simple example about how to generate input data for a map-based dashboard&lt;/td&gt;
&lt;td&gt;&lt;a class="reference external" href="https://github.com/jmartinter/geo-dashboard"&gt;https://github.com/jmartinter/geo-dashboard&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a class="reference external" href="https://youtu.be/IKPQrn-rS-Q?t=2048"&gt;34:08&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Bokeh lightning talk&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</summary><category term="lightning talks"></category></entry><entry><title>Python tools for webscraping</title><link href="https://pyvideo.org/pydata-madrid-2016/python-tools-for-webscraping.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>José Manuel Ortega</name></author><id>tag:pyvideo.org,2016-04-28:pydata-madrid-2016/python-tools-for-webscraping.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If we want to extract the contents of a website automating information extraction, often we find that the website does not offer any API to get the data you need and It is necessary use scraping techniques to recover data from a Web automatically. Some of the most powerful tools for extracting the data in web pages can be found in the python ecosystem.&lt;/p&gt;
&lt;p&gt;Introduction to webscraping&lt;/p&gt;
&lt;p&gt;WebScraping is the process of collecting or extracting data from web pages automatically. Nowdays is a very active field and developing shared goals with the semantic web field, natural language processing,artificial intelligence and human computer interaction.&lt;/p&gt;
&lt;p&gt;Python tools for webscraping&lt;/p&gt;
&lt;p&gt;Some of the most powerful tools to extract data can be found in the python ecosystem, among which we highlight Beautiful soup, Webscraping, PyQuery and Scrapy.&lt;/p&gt;
&lt;p&gt;Comparison between webscraping tools&lt;/p&gt;
&lt;p&gt;A comparison of the mentioned tools will be made, showing advantages and disadvantages of each one,highlighting the elements of each one to perform data extraction as regular expressions,css selectors and xpath expressions.&lt;/p&gt;
&lt;p&gt;Project example with scrapy&lt;/p&gt;
&lt;p&gt;Scrapy is a framework written in python for extraction automated data that can be used for a wide range of applications such as data mining processing. When using Scrapy we have to create a project, and each project consists of:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Items: We define the elements to be extracted.&lt;/li&gt;
&lt;li&gt;Spiders: The heart of the project, here we define the extract data procedure.&lt;/li&gt;
&lt;li&gt;Pipelines: Are the proceeds to analyze elements: data validation, cleansing html code outline&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Introduction to webscraping(5 min)
I will mention the main scraping techniques&lt;/p&gt;
&lt;p&gt;1.1. WebScraping&lt;/p&gt;
&lt;p&gt;1.2. Screen scraping&lt;/p&gt;
&lt;p&gt;1.3. Report mining&lt;/p&gt;
&lt;p&gt;1.4. Spiders&lt;/p&gt;
&lt;p&gt;Python tools for webscraping(10 min)
For each library I will make and introduction with a basic example. In some examples I will use requests library for sending HTTP requests&lt;/p&gt;
&lt;p&gt;2.1. BeautifulSoup&lt;/p&gt;
&lt;p&gt;2.2. Webscraping&lt;/p&gt;
&lt;p&gt;2.2. PyQuery&lt;/p&gt;
&lt;p&gt;Comparing scraping tools(5 min)&lt;/p&gt;
&lt;p&gt;3.1.Introduction to techniques for obtain data from web pages like regular expressions,css selectors, xpath expressions&lt;/p&gt;
&lt;p&gt;3.2.Comparative table comparing main features of each tool&lt;/p&gt;
&lt;p&gt;Project example with scrapy(10 min)&lt;/p&gt;
&lt;p&gt;4.1.Project structure with scrapy&lt;/p&gt;
&lt;p&gt;4.2.Components(Scheduler,Spider,Pipeline,Middlewares)&lt;/p&gt;
&lt;p&gt;4.3.Generating reports in json,csv and xml formats&lt;/p&gt;
</summary></entry><entry><title>The solution of inverse problems</title><link href="https://pyvideo.org/pydata-madrid-2016/the-solution-of-inverse-problems.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>Tomás Gómez Álvarez-Arenas</name></author><id>tag:pyvideo.org,2016-04-28:pydata-madrid-2016/the-solution-of-inverse-problems.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The concept of inverse problem (IP) in science and engineering (as opposed to the direct problem) is introduced and put into context within the broader field of data analysis by comparing it with complementary approaches like regression analysis, machine learning or bayesian inference. The two main applications of the solution of IP is explained: model verification and parameters extraction.&lt;/p&gt;
&lt;p&gt;Different techniques to solve the IP are briefly presented including brute force and Gradient Descent (GD) approaches. Then we focus on GD techniques including stochastic GD, variable step GD and the application to constrained problems. The critical elements in the application of GD approaches is discussed in detail: the selection of the initial guess, the definition of the cost function, the selection of the step, the problem of local minima, and the evaluation of the error of the final estimation by using stochastic approaches. In addition, some more advanced techniques to deal with large-dimensions problems and to learn from the path followed by different algorithms to find the solution of the IP are introduced.&lt;/p&gt;
&lt;p&gt;These elements are presented along with some examples using time and frequency domain data: chirps, gravitational waves and resonant ultrasonic spectroscopy (RUS), and the results obtained using a simple implementation of the GD method in Python 2.7. Gravitational wave data are obtained from the LIGO project (&lt;a class="reference external" href="https://losc.ligo.org/s/events/GW150914/GW150914_tutorial.html"&gt;https://losc.ligo.org/s/events/GW150914/GW150914_tutorial.html&lt;/a&gt;), while RUS data are obtained from &lt;a class="reference external" href="https://us-biomat.com/"&gt;https://us-biomat.com/&lt;/a&gt;. For these examples the possibility to use the IP solution to both parameter extraction and model confirmation is discussed&lt;/p&gt;
</summary></entry><entry><title>Towards a full stack python monitoring+analysis framework</title><link href="https://pyvideo.org/pydata-madrid-2016/towards-a-full-stack-python-monitoringanalysis-framework.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>Pablo Manuel García Corzo</name></author><id>tag:pyvideo.org,2016-04-28:pydata-madrid-2016/towards-a-full-stack-python-monitoringanalysis-framework.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Are traditional monitoring solution ready for the software defined world (SDN/NFV) and the IoT? Analytics can’t be considered anymore as an extra but a must for diving in an ocean of data where threshold-based KPIs are not enough. We will attempt to identify lacks in current tools and design a full stack python monitoring+analytics framework capable to face the new challenges of today.&lt;/p&gt;
&lt;p&gt;It’s quite probable that you already have a standard monitoring tool (such as Nagios) deployed in your production environment reading thousands of KPIs from your systems every minute.&lt;/p&gt;
&lt;p&gt;As you may already suspect, there’s much more interesting information hidden in all that data than just plain threshold based alarms per kpi and you wish to exploit it in much more depth.&lt;/p&gt;
&lt;p&gt;Furthermore, traditional monitoring solutions are not ready for the increasing flow of heterogeneous information coming from the software defined world (SDN/NFV) and the IoT, where isolated threshold based alarms are not enough and analytics are a must for redefining your KPIs.&lt;/p&gt;
&lt;p&gt;In this talk, we will attempt to design a full stack python monitoring+analytics framework capable to face those new challenges. For such a purpose we will study several state of the art libraries and tools covering all the pieces involved in the framework:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Agents and brokers for data acquisition (ncpa, dweepy, phant, paho, hbmqtt...)&lt;/li&gt;
&lt;li&gt;Data storage (gnocchi, Arctic, HDF5, RRD...)&lt;/li&gt;
&lt;li&gt;Analytics (numpy, pandas, scikit-learn...)&lt;/li&gt;
&lt;li&gt;Dashboard generation (Bokeh, plotly, matplotlib...)&lt;/li&gt;
&lt;li&gt;Alarm raising (mqttwarn, telegram bots...)&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Understanding Random Forests</title><link href="https://pyvideo.org/pydata-madrid-2016/understanding-random-forests.html" rel="alternate"></link><published>2016-04-28T00:00:00+00:00</published><updated>2016-04-28T00:00:00+00:00</updated><author><name>Marc García</name></author><id>tag:pyvideo.org,2016-04-28:pydata-madrid-2016/understanding-random-forests.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;No machine learning algorithm dominates in every domain, but random forests are usually tough to beat by much. And they have some advantages compared to other models. No much input preparation needed, implicit feature selection, fast to train, and ability to visualize the model. While it is easy to get started with random forests, a good understanding of the model is key to get the most of them.&lt;/p&gt;
&lt;p&gt;This talk will cover decision trees from theory, to their implementation in scikit-learn. An overview of ensemble methods and bagging will follow, to end up explaining and implementing random forests and see how they compare to other state-of-the-art models.&lt;/p&gt;
&lt;p&gt;The talk will have a very practical approach, using examples and real cases to illustrate how to use both decision trees and random forests.&lt;/p&gt;
&lt;p&gt;We will see how the simplicity of decision trees, is a key advantage compared to other methods. Unlike black-box methods, or methods tough to represent in multivariate cases, decision trees can easily be visualized, analyzed, and debugged, until we see that our model is behaving as expected. This exercise can increase our understanding of the data and the problem, while making our model perform in the best possible way.&lt;/p&gt;
&lt;p&gt;Random Forests can randomize and ensemble decision trees to increase its predictive power, while keeping most of their properties.&lt;/p&gt;
&lt;p&gt;The main topics covered will include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What are decision trees?&lt;/li&gt;
&lt;li&gt;How decision trees are trained?&lt;/li&gt;
&lt;li&gt;Understanding and debugging decision trees&lt;/li&gt;
&lt;li&gt;Ensemble methods&lt;/li&gt;
&lt;li&gt;Bagging&lt;/li&gt;
&lt;li&gt;Random Forests&lt;/li&gt;
&lt;li&gt;When decision trees and random forests should be used?&lt;/li&gt;
&lt;li&gt;Python implementation with scikit-learn&lt;/li&gt;
&lt;li&gt;Analysis of performance&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>An Architecture to Tweet Them All</title><link href="https://pyvideo.org/pydata-madrid-2016/an-architecture-to-tweet-them-all.html" rel="alternate"></link><published>2016-04-10T00:00:00+00:00</published><updated>2016-04-10T00:00:00+00:00</updated><author><name>Jesús Sánchez</name></author><id>tag:pyvideo.org,2016-04-10:pydata-madrid-2016/an-architecture-to-tweet-them-all.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Twitter has a lot of information that can be very useful if we know how to extract the relevant pieces. The main topic of the talk is to show an architecture (well tested in production). The architecture uses technologies like RabbitMQ, CouchDB, ElasticSearch, Kibana, a lot of Python and Spark Streaming with Scala. We will focus on the motivations to choose those components and how we extract the information and how we take the decisions about the obtained datasets.&lt;/p&gt;
</summary></entry><entry><title>Remove before flight: Analysing flight safety data.</title><link href="https://pyvideo.org/pydata-madrid-2016/remove-before-flight-analysing-flight-safety-data.html" rel="alternate"></link><published>2016-04-10T00:00:00+00:00</published><updated>2016-04-10T00:00:00+00:00</updated><author><name>Jesús Martos</name></author><id>tag:pyvideo.org,2016-04-10:pydata-madrid-2016/remove-before-flight-analysing-flight-safety-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The pursuit of safety in aviation is a task that requires our constant vigilance and effort. Throughout the use of a database from the NTSB the motivation of this talk is the use of different Python packages (Pandas, Scikit-learn) in order to answer multiple questions: Is commercial air transport safer now than 30 years ago? Which flight phase is safer? Which are the main accident causes?&lt;/p&gt;
&lt;p&gt;Python has become a very useful tool from a data science point of view. This talk is aimed at anyone who is interested in data analysis, statistics or machine learning in Python having the added incentive of dealing with real data from the aviation authorities.&lt;/p&gt;
&lt;p&gt;A typical data analysis workflow will be followed: starting by the data inspection and cleaning. Information from the National Transportation Safety Board (NTSB) will be analyzed using the capabilities of Pandas library to read, clean and manipulate data. This example is particularly suitable to highlight the main characteristics of the DataFrame object and show how data can be accessed, filtered, classified and plotted. Some different graphs ranging from classical plots and scatters to pie charts and representation on maps will be showed. Moreover, we will try to make use of some of the information to derive useful models for accident trends using regression and clustering algorithms from scikit-learn.&lt;/p&gt;
</summary></entry><entry><title>The Future of NumPy Indexing.</title><link href="https://pyvideo.org/pydata-madrid-2016/the-future-of-numpy-indexing.html" rel="alternate"></link><published>2016-04-10T00:00:00+00:00</published><updated>2016-04-10T00:00:00+00:00</updated><author><name>Jaime Fernández</name></author><id>tag:pyvideo.org,2016-04-10:pydata-madrid-2016/the-future-of-numpy-indexing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Advanced (a.k.a. fancy) indexing is one of NumPy's greatest features. Once past the rather steep learning curve, it enables a very expressive and powerful syntax, and makes coding a wide range of complex operations a breeze. But this versatility comes with a dark side of surprising results for some seemingly simple cases, and conflicts with the design choices of more recent data analysis packages. This has led to a viewpoint with growing support among the community that fancy indexing may be too fancy for its own good. This talk will review the workings of advanced indexing, highlighting where it excels, and where it falls short, and give some context on the logic behind some design decisions. It will also cover the existing NumPy Enhancement Proposal (NEP) to &amp;quot;implement an intuitive and fully featured advanced indexing.&amp;quot;&lt;/p&gt;
</summary></entry><entry><title>Pandas for Beginners</title><link href="https://pyvideo.org/pydata-madrid-2016/pandas-for-beginners.html" rel="alternate"></link><published>2016-04-08T00:00:00+00:00</published><updated>2016-04-08T00:00:00+00:00</updated><author><name>Francisco Correoso</name></author><id>tag:pyvideo.org,2016-04-08:pydata-madrid-2016/pandas-for-beginners.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;During the workshop the main features and capabilities of Pandas library will be reviewed.&lt;/p&gt;
&lt;p&gt;pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.&lt;/p&gt;
&lt;p&gt;Main Workshop topics:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Main Data Structures.&lt;/li&gt;
&lt;li&gt;I/O.&lt;/li&gt;
&lt;li&gt;Basic statistical tools.&lt;/li&gt;
&lt;li&gt;Data selection.&lt;/li&gt;
&lt;li&gt;Grouping, aggregation, merge, join,...&lt;/li&gt;
&lt;li&gt;Plotting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Materials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Python for Distributed Systems</title><link href="https://pyvideo.org/pydata-madrid-2016/python-for-distributed-systems.html" rel="alternate"></link><published>2016-04-08T00:00:00+00:00</published><updated>2016-04-08T00:00:00+00:00</updated><author><name>Guillem Borrell</name></author><id>tag:pyvideo.org,2016-04-08:pydata-madrid-2016/python-for-distributed-systems.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;From big data to supercomputing, most modern high-performance tools are concurrent and parallel. This tutorial introduces some of the tools that are available in the Python ecosystem to develop, deploy and maintain modern and efficient distributed applications.&lt;/p&gt;
&lt;p&gt;This workshop will not cover trendy applications or bundled frameworks like Hadoop or Spark. It won't build recipies that you can reuse for any particular purpose. The goal is to buid a general comprehension about how to program distributed applications in a general way.&lt;/p&gt;
&lt;p&gt;The workshop will walk through the following topics.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Distributed hardware. A short introduction to clouds and supercomputers.&lt;/li&gt;
&lt;li&gt;Distributed software. Large distributed applications usually exploit task-based parallelism. Messaging is the way to make those tasks talk to each other. There are many different messaging strategies, protocols, transports, layers... Each one is suitable for a different case.&lt;/li&gt;
&lt;li&gt;Parallel algorithms. A short introduction about some algorithms that incorporate messaging.&lt;/li&gt;
&lt;li&gt;Threading and concurrency. If a task communicates and computes, it is doing two things at the same time, but it is not, since Python has a GIL...&lt;/li&gt;
&lt;li&gt;Management, service discovery, logging and availability. Managing tens, hundreds or thousands of tasks can be tricky. But Python has tools that may simplify the management of parallel applications.&lt;/li&gt;
&lt;/ol&gt;
</summary></entry><entry><title>Using Containers for Big Data</title><link href="https://pyvideo.org/pydata-madrid-2016/using-containers-for-big-data.html" rel="alternate"></link><published>2016-04-08T00:00:00+00:00</published><updated>2016-04-08T00:00:00+00:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2016-04-08:pydata-madrid-2016/using-containers-for-big-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Madrid 2016&lt;/p&gt;
&lt;p&gt;Most of the talks and workshop tutorials can be found here: &lt;a class="reference external" href="https://github.com/PyDataMadrid2016/Conference-Info"&gt;https://github.com/PyDataMadrid2016/Conference-Info&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;En nuestro trabajo de análisis normalmente nos centramos en usar algoritmos que nos permitan ejecutar nuestros objetivos de la manera más eficiente posible. Sin embargo, cuando estamos usando grandes cantidades de datos, los contenedores de esos datos resultan tan importantes o más que los propios algoritmos. En este taller veremos algunos de los más contenedores para Big Data más importantes.&lt;/p&gt;
&lt;p&gt;En la actualidad existe una variedad bastante grande de contenedores de datos para almacenar grandes cantidades de datos en Python, tanto en memoria como en disco. En mi taller pasaremos revista a unos cuantos de los más útiles, empezando por los más básicos y generales (listas, diccionarios, NumPy/ndarray, pandas/DataFrames) a los más especializados (RDBMS, PyTables/Table/HDF5, bcolz/carray/ctable). Durante el camino se darán pistas de cuando usar unos u otros dependiendo del caso de uso.&lt;/p&gt;
&lt;p&gt;Los asistentes deben asistir con un portatil y con los requisitos listados en &lt;a class="reference external" href="https://github.com/FrancescAlted/PyConES2015"&gt;https://github.com/FrancescAlted/PyConES2015&lt;/a&gt; debidamente instalados.&lt;/p&gt;
</summary></entry></feed>