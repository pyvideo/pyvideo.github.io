<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 21 Apr 2018 00:00:00 +0000</lastBuildDate><item><title>Recent advancements in NLP and Deep Learning: A Quant's Perspective</title><link>https://pyvideo.org/pycon-italia-2018/recent-advancements-in-nlp-and-deep-learning-a-quants-perspective.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a gold-rush among hedge-funds for text mining algorithms to
quantify textual data and generate trading signals. Harnessing the power
of alternative data sources became crucial to find novel ways of
enhancing trading strategies.&lt;/p&gt;
&lt;p&gt;With the proliferation of new data sources, natural language data became
one of the most important data sources which could represent the public
sentiment and opinion about market events, which then can be used to
predict financial markets.&lt;/p&gt;
&lt;p&gt;Talk is split into 5 parts;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Who is a quant and how do they use NLP?&lt;/li&gt;
&lt;li&gt;How deep learning has changed NLP?&lt;/li&gt;
&lt;li&gt;Let’s get dirty with word embeddings&lt;/li&gt;
&lt;li&gt;Performant deep learning layer for NLP: The Recurrent Layer&lt;/li&gt;
&lt;li&gt;Using all that to make money&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="who-is-a-quant-and-how-do-they-use-nlp"&gt;
&lt;h4&gt;1. Who is a quant and how do they use NLP?&lt;/h4&gt;
&lt;p&gt;Quants use mathematical and statistical methods to create algorithmic
trading strategies.&lt;/p&gt;
&lt;p&gt;Due to recent advances in available deep learning frameworks and
datasets (time series, text, video etc) together with decreasing cost of
parallelisable hardware, quants are experimenting with various NLP
methods which are applicable to quantitative trading.&lt;/p&gt;
&lt;p&gt;In this section, we will get familiar with the brief history of text
mining work that quants have done so far and recent advancements.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-deep-learning-has-changed-nlp"&gt;
&lt;h4&gt;2. How deep learning has changed NLP?&lt;/h4&gt;
&lt;p&gt;In recent years, data representation and modeling methods are vastly
improved. For example when it comes to textual data, rather than using
high dimensional sparse matrices and suffering from curse of
dimensionality, distributional vectors are more efficient to work with.&lt;/p&gt;
&lt;p&gt;In this section, I will talk about distributional vectors a.k.a. word
embeddings and recent neural network architectures used when building
NLP models.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lets-get-dirty-with-word-embeddings"&gt;
&lt;h4&gt;3. Let’s get dirty with word embeddings&lt;/h4&gt;
&lt;p&gt;Models such as Word2vec or GloVe helps us create word embeddings from
large unlabeled corpus which represent the relation between words, their
contextual relationships in numerical vector spaces and these
representations not only work for words but also could be used for
phrases and sentences.&lt;/p&gt;
&lt;p&gt;In this section, I will talk about inner workings of these models and
important points when creating domain-specific embeddings (e.g. for
sentiment analysis in financial domain).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="performant-deep-learning-layer-for-nlp-the-recurrent-layer"&gt;
&lt;h4&gt;4. Performant deep learning layer for NLP: The Recurrent Layer&lt;/h4&gt;
&lt;p&gt;Recurrent Neural Networks (RNNs) can capture and hold the information
which was seen before (context), which is important for dealing with
unbounded context in NLP tasks.&lt;/p&gt;
&lt;p&gt;Long Short Term Memory (LSTM) networks, which is a special type of RNN,
can understand the context even if words have long term dependencies,
words which are far back in their sequence.&lt;/p&gt;
&lt;p&gt;In this talk, I will compare LSTMs with other deep learning
architectures and will look at LSTM unit from a technical point of view.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-all-that-to-make-money"&gt;
&lt;h4&gt;5. Using all that to make money&lt;/h4&gt;
&lt;p&gt;Financial news, especially if it’s major, can change the sentiment among
investors and affect the related asset price with immediate price
corrections.&lt;/p&gt;
&lt;p&gt;For example, what’s been communicated in quarterly earnings calls might
indicate whether the price of share will drop or increase based on the
language used. If the message of the company is not direct and featuring
complex sounding language, it usually indicates that there’s some shady
stuff going on and if this information extracted right, it’s a valuable
trading signal. For similar reasons, scanning announcements and
financial disclosures for trading signals became a common NLP practice
in investment industry.&lt;/p&gt;
&lt;p&gt;In this section, I will talk about the various data sources that
researchers can use and also explain common NLP workflows and deep
learning practices for quantifying textual data for generating trading
signals.&lt;/p&gt;
&lt;p&gt;I will end with summary with application architecture in case anyone
would like to implement similar systems for their own use.&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;sabato 21 aprile&lt;/strong&gt; at 14:45 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Umit Mert Cakmak</dc:creator><pubDate>Sat, 21 Apr 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-04-21:pycon-italia-2018/recent-advancements-in-nlp-and-deep-learning-a-quants-perspective.html</guid><category>nlp</category><category>data-science</category><category>Keras</category><category>Python</category><category>Deep-Learning</category><category>machine-learning</category><category>spaCy</category><category>nltk</category></item><item><title>Data driven literary analysis</title><link>https://pyvideo.org/pydata-amsterdam-2016/data-driven-literary-analysis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Can unsupervised learning mimic a literary critic? This talk will give an overview of unsupervised document classification techniques and apply them to the analysis and classification of Shakespeare’s plays.&lt;/p&gt;
&lt;p&gt;Unsupervised document classification addresses the problem of assigning categories to documents without the use of a training set or predefined categories. This is useful to enhance information retrieval, the basic assumption being that similar contents are also relevant to the same query. A similar assumption is made in literature to define literary genres and sub-genres, where works which share specific conventions in terms of form and content are described by the same genre.&lt;/p&gt;
&lt;p&gt;The talk gives an overview of document clustering and its challenges, with a focus on dimensionality reduction and how to address it with topic modelling techniques like LDA (Latent Dirichlet Allocation). Using Shakespeare’s body of work as a case study, the talk describes how to use nltk, sklearn and gensim to process and analyse theatrical works with the final goal of testing whether document clustering yields to the same classification given by literature experts.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="https://speakerdeck.com/sereprz/data-driven-literary-analysis-an-unsupervised-approach-to-text-analysis-and-classification"&gt;https://speakerdeck.com/sereprz/data-driven-literary-analysis-an-unsupervised-approach-to-text-analysis-and-classification&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Serena Peruzzo</dc:creator><pubDate>Sat, 26 Mar 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/data-driven-literary-analysis.html</guid><category>nltk</category><category>sklearn</category><category>gensim</category></item><item><title>Do Angry People Have Poor Grammar?</title><link>https://pyvideo.org/pydata-amsterdam-2016/do-angry-people-have-poor-grammar.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;This talk is about two things: natural language processing (NLP) and statistical dependance. We will walk through the ins and outs of sentiment analysis in Python (mostly using NLTK) and a swift introduction to the statistics of dependence. We'll put these techniques to use on a dataset of every reddit public comment, perhaps the best data source for exploring the behavior of shouty Web comments.&lt;/p&gt;
&lt;p&gt;This talk is about two things: natural language processing (NLP) and statistical dependence. We will embark on a data science workflow using various python scientific computing tools to better understand the behavior of commenters on Reddit. To do this we'll go through an introduction to sentiment analysis in Python (mostly using NLTK) and a swift explanation of the statistics of variable dependence.&lt;/p&gt;
&lt;p&gt;We'll couple these freshly learned methods with an excellent dataset for this domain: every public reddit comment. We'll talk a bit about handling and preprocessing data of this size and character. Then we'll compile scores for both sentiment and spelling/grammar. In the end we may just discover if angry comment are also grammatically poor comments. And the audience will walk away a few more tools in scientific computing toolbelt.&lt;/p&gt;
&lt;p&gt;Slides available here:  &lt;a class="reference external" href="https://speakerdeck.com/bfields/do-angry-people-have-poor-grammar-an-exploration-of-language-processing-and-statistics-in-python"&gt;https://speakerdeck.com/bfields/do-angry-people-have-poor-grammar-an-exploration-of-language-processing-and-statistics-in-python&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ben Fields</dc:creator><pubDate>Sat, 26 Mar 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/do-angry-people-have-poor-grammar.html</guid><category>nltk</category></item><item><title>Procesamiento del lenguaje natural en python</title><link>https://pyvideo.org/pycon-es-2014/procesamiento-del-lenguaje-natural-en-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Cubre algunos aspectos del procesamiento de lenguaje natural con NLTK (Natural Language ToolKit), explicando por encima en qué consiste, pasos para poder procesar un lenguaje, identificar patrones en un lenguaje y casos de uso útiles para aplicar.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Iván Compañy</dc:creator><pubDate>Mon, 06 Apr 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-04-06:pycon-es-2014/procesamiento-del-lenguaje-natural-en-python.html</guid><category>nltk</category><category>natural language processing</category></item><item><title>Beginner's Guide to Machine Learning Competitions</title><link>https://pyvideo.org/pytexas-2015/beginners-guide-to-machine-learning-competitions.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial will offer a hands-on introduction to machine learning and
the process of applying these concepts in a Kaggle competition. We will
introduce attendees to machine learning concepts, examples and flows,
while building up their skills to solve an actual problem. At the end of
the tutorial attendees will be familiar with a real data science flow:
feature preparation, modeling, optimization and validation.&lt;/p&gt;
&lt;p&gt;Packages used in the tutorial will include: IPython notebook,
scikit-learn, pandas and NLTK. We’ll use IPython notebook for
interactive exploration and visualization, in order to gain a basic
understanding of what’s in the data. From there, we’ll extract features
and train a model using scikit-learn. This will bring us to our first
submission. We’ll then learn how to structure the problem for offline
evaluation and use scikit-learn’s clean model API to train many models
simultaneously and perform feature selection and hyperparameter
optimization.&lt;/p&gt;
&lt;p&gt;At the end of session, attendees will have time to work on their own to
improve their models and make multiple submissions to get to the top of
the leaderboard, just like in a real competition. Hopefully attendees
will not only leave the tutorial having learned the core data science
concepts and flow, but also having had a great time doing it.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Christine Doig</dc:creator><pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-10-09:pytexas-2015/beginners-guide-to-machine-learning-competitions.html</guid><category>tutorial</category><category>machine learning</category><category>nltk</category><category>pandas</category><category>scikit-learn</category><category>ipython</category></item><item><title>PyConAU 2010: Using Python for Natural Language Generation and Analysis</title><link>https://pyvideo.org/pycon-au-2010/pyconau-2010--using-python-for-natural-language-g.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Using Python for Natural Language Generation and Analysis&lt;/p&gt;
&lt;p&gt;Presented by Tennessee J Leeuwenburg (Australian Government Bureau of
Meteorology)&lt;/p&gt;
&lt;p&gt;Python is used within the Bureau of Meteorology to automatically
generate weather forecast text based on numerical data. In addition, the
development team has also used Python to introspect the forecast
language and statistics used in the past. NTLK is an open-source
language processing toolkit which can be used for visualising language
patterns. This presentation will talk about some of the techniques used
for automatically describing datasets and also how NTLK can be used to
discover information about language uses and requirements in an
organisation.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tennessee J Leeuwenburg</dc:creator><pubDate>Sat, 26 Jun 2010 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2010-06-26:pycon-au-2010/pyconau-2010--using-python-for-natural-language-g.html</guid><category>language</category><category>nltk</category><category>parsing</category><category>pyconau</category><category>pyconau2010</category></item><item><title>Human as a Second Language: Succeeding with the Natural Language Toolkit</title><link>https://pyvideo.org/pycon-au-2012/human-as-a-second-language-succeeding-with-the-n.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The Natural Language Toolkit (NLTK) suite offers powerful tools for
natural language processing and analysis. Like many other code
libraries, it enables programmers to achieve results when working with
data they may not be an expert&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Natural Language Toolkit (NLTK) suite offers powerful tools for
natural language processing and analysis. Like many other code
libraries, it enables programmers to achieve results when working with
data they may not be an expert in the handling of - in this case, human
language. The NLTK is particularly valuable as human language skills are
in general something programmers can get along without, and therefore
they are likely to be ill- equipped with the tools to most effectively
work with language data. However, while NLTK provides programmers with a
way to work with all the relevant parts of language without needing to
rely on their own grammar skills, there are many concepts in the field
of natural language processing that require basic comprehension of
natural language operation, which may make knowing where to start
working with the NLTK difficult for the average programmer.&lt;/p&gt;
&lt;p&gt;This presentation will demonstrate some of the NLTK's powerful and
impressive features, while covering the concepts that will enable any
programmer to work cool tricks on natural language. The application of
the NLTK to a very basic artificial intelligence will be shown.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Elyse Maria Glina</dc:creator><pubDate>Tue, 21 Aug 2012 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2012-08-21:pycon-au-2012/human-as-a-second-language-succeeding-with-the-n.html</guid><category>nltk</category></item><item><title>The Python and the Elephant: Large Scale Natural Language Processing with NLTK and Dumbo (#120)</title><link>https://pyvideo.org/pycon-us-2010/pycon-2010--the-python-and-the-elephant--large-sc.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Python and the Elephant: Large Scale Natural Language Processing
with NLTK and Dumbo&lt;/p&gt;
&lt;p&gt;Presented by Nitin Madnani (University of Maryland, College Park); Dr.
Jimmy J Lin (University of Maryland)&lt;/p&gt;
&lt;p&gt;A practical look at NLTK and Dumbo, python-powered and open-source
toolkits and APIs for processing natural language on a large scale.&lt;/p&gt;
&lt;p&gt;For people like us who make a living trying to make a computer
&amp;quot;understand&amp;quot; human language, Python is a very powerful language, given
its rapid prototyping abilities, native unicode support and a stellar
standard library. This relationship has been strengthened further by an
open-source, python- based Natural Language ToolKit
(&lt;a class="reference external" href="http://www.nltk.org/"&gt;www.nltk.org&lt;/a&gt;) which is being widely used in
the community for both teaching and research purposes and gaining
traction in the general Python community as well
(&lt;a class="reference external" href="http://www.nltk.org/book"&gt;www.nltk.org/book&lt;/a&gt;). Recently, the Python
community has seen the release of Dumbo
(&lt;a class="reference external" href="http://wiki.github.com/klbostee/dumbo"&gt;http://wiki.github.com/klbostee/dumb
o&lt;/a&gt;), an open-source,
python-based cloud-computing API (based on Hadoop) via the hands of
Klaas Bosteels.&lt;/p&gt;
&lt;p&gt;In this talk, we show how the amalgamation of Python, NLTK and Dumbo can
allow for very large-scale natural language processing efficiently and
elegantly.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr. Jimmy J Lin</dc:creator><pubDate>Fri, 19 Feb 2010 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2010-02-19:pycon-us-2010/pycon-2010--the-python-and-the-elephant--large-sc.html</guid><category>dumbo</category><category>nltk</category><category>pycon</category><category>pycon2010</category></item><item><title>Linguistics of Twitter</title><link>https://pyvideo.org/pycon-us-2011/pycon-2011--linguistics-of-twitter.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Dialectical changes in America are influencing expression online. This
talk will discuss a current project which is using the Natural Language
Toolkit to develop up to date reference materials to measure and monitor
online natural language.&lt;/p&gt;
&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Contrary to expectations, the prevalence of television did not cause
every American to speak in a common standard dialect. Rather, smaller
sub-regional dialects are merging into stronger regional dialects with
the largest change in spoken English since the 1750's taking place in
the Northern Cities Vowel Shift.&lt;/p&gt;
&lt;p&gt;Social Media is widely considered a conversational media, users often
leaning on their dialect which to express themselves.&lt;/p&gt;
&lt;p&gt;Taking a recent tweet for example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
'_andBeautyKills: – after tonight, don’t leave your boy roun’ me, umma #true playa fareal.'
&lt;/pre&gt;
&lt;p&gt;This tweet presents a problem for traditional natural language
processing paradigm:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Do they build out an extensive reg ex to solve this?&lt;/li&gt;
&lt;li&gt;Even Worse, do they reject it because of non-Standard English?&lt;/li&gt;
&lt;li&gt;How do they respond such that communication is effective?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Currently under development with Python using the Natural Language
Toolkit are the tools and methodologies to process, understand and
respond to communication that falls outside Standard American English.
This talk will focus on the status of existing tools, where development
stands, challenges for traditional tools and potential opportunities for
exploration.&lt;/p&gt;
&lt;p&gt;While limited to American English, any participant who is studying
natural language processing of any language is welcome and sure to
learn. The techniques could be applied to languages around the world for
which the motivated programmer is knowledgeable about.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Michael D. Healy</dc:creator><pubDate>Fri, 11 Mar 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-03-11:pycon-us-2011/pycon-2011--linguistics-of-twitter.html</guid><category>nltk</category><category>pycon</category><category>pycon2011</category><category>twitter</category></item><item><title>Statistical machine learning for text classification with scikit-learn</title><link>https://pyvideo.org/pycon-us-2011/pycon-2011--statistical-machine-learning-for-text.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Statistical machine learning for text classification with scikit-learn&lt;/p&gt;
&lt;p&gt;Presented by Olivier Grisel&lt;/p&gt;
&lt;p&gt;The goal of this talk is to give a state-of-the-art overview of machine
learning algorithms applied to text classification tasks ranging from
language and topic detection in tweets and web pages to sentiment
analysis in consumer products reviews.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;Unstructured or semi-structured text data is ubiquitous thanks to the
read- write nature of the web. However human authors are often lazy and
don't fill- in structured metadata forms in web applications. It is
however possible to automate some structured knowledge extraction with
simple and scalable statistical learning tools implemented in python.
For instance:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;guessing the language and topic of tweets and web pages&lt;/li&gt;
&lt;li&gt;analyze the sentiment (positive or negative) in consumer products
reviews in blogs or customer emails&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This talk will introduce the main operational steps of supervised
learning:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;extracting the relevant features from text documents&lt;/li&gt;
&lt;li&gt;selecting the right machine learning algorithm to train a model for
the task at hand&lt;/li&gt;
&lt;li&gt;using the trained model on previously unseen documents&lt;/li&gt;
&lt;li&gt;evaluating the predictive accuracy of the trained model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will also demonstrate the results obtained for above tasks using the
&lt;a class="reference external" href="http://scikit-learn.sourceforge.net/"&gt;scikit-learn&lt;/a&gt; package and
compare it to other implementations such as &lt;a class="reference external" href="http://nltk.org/"&gt;nltk&lt;/a&gt;
and the &lt;a class="reference external" href="http://code.google.com/apis/predict/"&gt;Google Prediction
API&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Olivier Grisel</dc:creator><pubDate>Fri, 11 Mar 2011 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2011-03-11:pycon-us-2011/pycon-2011--statistical-machine-learning-for-text.html</guid><category>googlepredictionapi</category><category>machine learning</category><category>nltk</category><category>pycon</category><category>pycon2011</category><category>scikit-learn</category></item></channel></rss>