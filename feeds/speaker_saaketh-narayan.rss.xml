<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Saaketh Narayan</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 18 Sep 2024 00:00:00 +0000</lastBuildDate><item><title>Fast, Scalable Distributed Training with StreamingDataset</title><link>https://pyvideo.org/pytorch-conference-2024/fast-scalable-distributed-training-with-streamingdataset.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;StreamingDataset makes training on large datasets from cloud storage as fast, cheap, and scalable as possible. It’s specially designed for multi-node, distributed training for large models — maximizing correctness guarantees, performance, and ease of use. Key features include elastically deterministic training, instant mid-epoch resumption, effective shuffling, high training throughput, and flexible data mixing, among other features. When training with StreamingDataset, the data shards are written to cloud storage in MDS, our file format that allows for low-latency random access to samples. By being as efficient as possible with shard downloads and shuffling, StreamingDataset minimizes egress costs while ensuring that dataloading never bottlenecks model training. StreamingDataset powers training for LLMs with over 100 billion parameters like DBRX, to advanced diffusion models, to two-tower recommendation models, and more, scaling to training jobs on thousands of GPUs with ease. Join us to learn how StreamingDataset can elevate your distributed model training experience.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Saaketh Narayan</dc:creator><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/fast-scalable-distributed-training-with-streamingdataset.html</guid><category>PyTorch Conference 2024</category><category>Lightning Talk</category></item></channel></rss>