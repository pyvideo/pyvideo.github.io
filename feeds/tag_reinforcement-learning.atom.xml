<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - reinforcement learning</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_reinforcement-learning.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-04-17T00:00:00+00:00</updated><subtitle></subtitle><entry><title>(Alpha) Zero to Elo</title><link href="https://pyvideo.org/pycon-italia-2018/alpha-zero-to-elo.html" rel="alternate"></link><published>2018-04-22T00:00:00+00:00</published><updated>2018-04-22T00:00:00+00:00</updated><author><name>Simone Totaro</name></author><id>tag:pyvideo.org,2018-04-22:/pycon-italia-2018/alpha-zero-to-elo.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Reinforcement learning is a sequential decision making framework that
has received a lot of attention given incredible results from DeepMind,
DQN and AlphaGo. More recently DeepMind delivered a more general AI game
player call AlphaZero, which learns by self-play.&lt;/p&gt;
&lt;p&gt;During this session we will introduce you the key ideas …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Reinforcement learning is a sequential decision making framework that
has received a lot of attention given incredible results from DeepMind,
DQN and AlphaGo. More recently DeepMind delivered a more general AI game
player call AlphaZero, which learns by self-play.&lt;/p&gt;
&lt;p&gt;During this session we will introduce you the key ideas behind
AlphaZero:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Reinforcement Learning and Value function&lt;/li&gt;
&lt;li&gt;MonteCarlo Tree Search&lt;/li&gt;
&lt;li&gt;Self-play&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will explore how they contribute to beat greatest Go player of our
time, in Python.&lt;/p&gt;
&lt;p&gt;Agenda:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Introduction to AI in games&lt;/li&gt;
&lt;li&gt;What AlphaZero is made of? (Boring theory)&lt;/li&gt;
&lt;li&gt;How to build your own AlphaZero? (Cool code)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;in __on &lt;strong&gt;domenica 22 aprile&lt;/strong&gt; at 15:30 &lt;a class="reference external" href="/p3/schedule/pycon9/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</content><category term="PyCon Italia 2018"></category><category term="Deep-Learning"></category><category term="reinforcement-learning"></category><category term="datascience"></category></entry><entry><title>Improving Machine Learning from Human Feedback</title><link href="https://pyvideo.org/pydata-berlin-2023/improving-machine-learning-from-human-feedback.html" rel="alternate"></link><published>2023-04-17T00:00:00+00:00</published><updated>2023-04-17T00:00:00+00:00</updated><author><name>Erin Mikail Staples</name></author><id>tag:pyvideo.org,2023-04-17:/pydata-berlin-2023/improving-machine-learning-from-human-feedback.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large generative models rely upon massive data sets that are collected automatically. For example, GPT-3 was trained with data from &amp;quot;Common Crawl&amp;quot; and &amp;quot;Web Text&amp;quot;, among other sources. As the saying goes — bigger isn't always better. While powerful, these data sets (and the models that they create) often come …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large generative models rely upon massive data sets that are collected automatically. For example, GPT-3 was trained with data from &amp;quot;Common Crawl&amp;quot; and &amp;quot;Web Text&amp;quot;, among other sources. As the saying goes — bigger isn't always better. While powerful, these data sets (and the models that they create) often come at a cost, bringing their &amp;quot;internet-scale biases&amp;quot; along with their &amp;quot;internet-trained models.&amp;quot; While powerful, these models beg the question — is unsupervised learning the best future for machine learning?&lt;/p&gt;
&lt;p&gt;ML researchers have developed new model-tuning techniques to address the known biases within existing models and improve their performance (as measured by response preference, truthfulness, toxicity, and result generalization). All of this at a fraction of the initial training cost. In this talk, we will explore these techniques, known as Reinforcement Learning from Human Feedback (RLHF), and how open-source machine learning tools like PyTorch and Label Studio can be used to tune off-the-shelf models using direct human feedback.&lt;/p&gt;
</content><category term="PyData Berlin 2023"></category><category term="reinforcement learning"></category><category term="human feedback"></category><category term="pytorch"></category><category term="label studio"></category></entry><entry><title>Aprendizaje por refuerzo con OpenAI Gym</title><link href="https://pyvideo.org/pydata-cordoba-2019/aprendizaje-por-refuerzo-con-openai-gym.html" rel="alternate"></link><published>2019-09-27T00:00:00+00:00</published><updated>2019-09-27T00:00:00+00:00</updated><author><name>Ana Laura Diedrichs</name></author><id>tag:pyvideo.org,2019-09-27:/pydata-cordoba-2019/aprendizaje-por-refuerzo-con-openai-gym.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Esta charla ofrece una introducción a aprendizaje por refuerzo y cómo podemos experimentar con estos algoritmos usando la librería OpenAI Gym.&lt;/p&gt;
</content><category term="PyData Córdoba 2019"></category><category term="artificial intelligence"></category><category term="openai gym"></category><category term="reinforcement learning"></category></entry><entry><title>Inteligencia Artificial para Principiantes</title><link href="https://pyvideo.org/riiaa-2021/inteligencia-artificial-para-principiantes.html" rel="alternate"></link><published>2021-08-25T00:00:00+00:00</published><updated>2021-08-25T00:00:00+00:00</updated><author><name>Benjamín Sánchez Léngelin</name></author><id>tag:pyvideo.org,2021-08-25:/riiaa-2021/inteligencia-artificial-para-principiantes.html</id><content type="html"></content><category term="RIIAA 2021"></category><category term="Artificial Intelligence"></category><category term="Data Science"></category><category term="Machine Learning"></category><category term="Reinforcement Learning"></category></entry><entry><title>Relaciones de equivalencia y métricas de estados para procesos de decisión Markovianos.</title><link href="https://pyvideo.org/riiaa-2021/relaciones-de-equivalencia-y-metricas-de-estados-para-procesos-de-decision-markovianos.html" rel="alternate"></link><published>2021-08-25T00:00:00+00:00</published><updated>2021-08-25T00:00:00+00:00</updated><author><name>Pablo Castro</name></author><id>tag:pyvideo.org,2021-08-25:/riiaa-2021/relaciones-de-equivalencia-y-metricas-de-estados-para-procesos-de-decision-markovianos.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Este taller presenta la teoría de relaciones de equivalencia y métricas de estado para procesos de decisión Markovianos (MDPs). En años recientes se ha popularizado como una técnica teórica y empírica para el aprendizaje por refuerzo. El enfoque principal será matemático, pero utilizaremos collaboratory notebooks para poder tener un …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Este taller presenta la teoría de relaciones de equivalencia y métricas de estado para procesos de decisión Markovianos (MDPs). En años recientes se ha popularizado como una técnica teórica y empírica para el aprendizaje por refuerzo. El enfoque principal será matemático, pero utilizaremos collaboratory notebooks para poder tener un mejor entendimiento de las diferentes teorías y sus aplicaciones al aprendizaje por refuerzo.&lt;/p&gt;
</content><category term="RIIAA 2021"></category><category term="Artificial Intelligence"></category><category term="Reinforcement Learning"></category></entry></feed>