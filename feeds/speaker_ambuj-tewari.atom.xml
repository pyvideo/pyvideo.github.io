<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Ambuj Tewari</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_ambuj-tewari.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>No-regret Exploration in Contextual Reinforcement Learning</title><link href="https://pyvideo.org/uai-2020/no-regret-exploration-in-contextual-reinforcement-learning.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Aditya Modi</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/no-regret-exploration-in-contextual-reinforcement-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;No-regret Exploration in Contextual Reinforcement Learning&lt;/p&gt;
&lt;p&gt;Aditya Modi (Univ. of Michigan Ann Arbor)*; Ambuj Tewari (University of Michigan)&lt;/p&gt;
&lt;p&gt;We consider the recently proposed reinforcement learning (RL) framework of Contextual Markov Decision Processes (CMDP), where the agent interacts with a (potentially adversarial) sequence of episodic tabular MDPs. In addition, a …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;No-regret Exploration in Contextual Reinforcement Learning&lt;/p&gt;
&lt;p&gt;Aditya Modi (Univ. of Michigan Ann Arbor)*; Ambuj Tewari (University of Michigan)&lt;/p&gt;
&lt;p&gt;We consider the recently proposed reinforcement learning (RL) framework of Contextual Markov Decision Processes (CMDP), where the agent interacts with a (potentially adversarial) sequence of episodic tabular MDPs. In addition, a context vector determining the MDP parameters is available to the agent at the start of each episode, thereby allowing it to learn a context-dependent near-optimal policy. In this paper, we propose a no-regret online RL algorithm in the setting where the MDP parameters are obtained from the context using generalized linear mappings (GLMs). We propose and analyze optimistic and randomized exploration methods which make (time and space) efficient online updates. The GLM based model subsumes previous work in this area and also improves previous known bounds in the special case where the contextual mapping is linear. In addition, we demonstrate a generic template to derive confidence sets using an online learning oracle and give a lower bound for the setting.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry><entry><title>Randomized Exploration for Non-Stationary Stochastic Linear Bandits</title><link href="https://pyvideo.org/uai-2020/randomized-exploration-for-non-stationary-stochastic-linear-bandits.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Baekjin Kim</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/randomized-exploration-for-non-stationary-stochastic-linear-bandits.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Randomized Exploration for Non-Stationary Stochastic Linear Bandits&lt;/p&gt;
&lt;p&gt;Baekjin Kim (University of Michigan)*; Ambuj Tewari (University of Michigan)&lt;/p&gt;
&lt;p&gt;We investigate two perturbation approaches to overcome conservatism that optimism based algorithms chronically suffer from in practice. The first approach replaces optimism with a simple randomization when using confidence sets. The second …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Randomized Exploration for Non-Stationary Stochastic Linear Bandits&lt;/p&gt;
&lt;p&gt;Baekjin Kim (University of Michigan)*; Ambuj Tewari (University of Michigan)&lt;/p&gt;
&lt;p&gt;We investigate two perturbation approaches to overcome conservatism that optimism based algorithms chronically suffer from in practice. The first approach replaces optimism with a simple randomization when using confidence sets. The second one adds random perturbations to its current estimate before maximizing the expected reward. For non-stationary linear bandits, where each action is associated with a $d$-dimensional feature and the unknown parameter is time-varying with total variation $B_T$, we propose two randomized algorithms, Discounted Randomized LinUCB (D-RandLinUCB) and Discounted Linear Thompson Sampling (D-LinTS) via the two perturbation approaches. We highlight the statistical optimality versus computational efficiency trade-off between them in that the former asymptotically achieves the optimal dynamic regret $tilde{cO}(d ^{2/3}B_T^{1/3} T^{2/3})$, but the latter is oracle-efficient with an extra logarithmic factor in the number of arms compared to minimax-optimal dynamic regret. In a simulation study, both algorithms show the outstanding performance in tackling conservatism issue that Discounted LinUCB struggles with.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry><entry><title>Regret Analysis of Bandit Problems with Causal Background Knowledge</title><link href="https://pyvideo.org/uai-2020/regret-analysis-of-bandit-problems-with-causal-background-knowledge.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Yangyi Lu</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/regret-analysis-of-bandit-problems-with-causal-background-knowledge.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Regret Analysis of Bandit Problems with Causal Background Knowledge&lt;/p&gt;
&lt;p&gt;Yangyi Lu (University of Michigan)*; Amirhossein Meisami (Adobe); Ambuj Tewari (University of Michigan); William Yan (Adobe Systems Incorporated)&lt;/p&gt;
&lt;p&gt;We study how to learn optimal interventions sequentially given causal information represented as a causal graph along with associated conditional distributions. Causal …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Regret Analysis of Bandit Problems with Causal Background Knowledge&lt;/p&gt;
&lt;p&gt;Yangyi Lu (University of Michigan)*; Amirhossein Meisami (Adobe); Ambuj Tewari (University of Michigan); William Yan (Adobe Systems Incorporated)&lt;/p&gt;
&lt;p&gt;We study how to learn optimal interventions sequentially given causal information represented as a causal graph along with associated conditional distributions. Causal modeling is useful in real world problems like online advertisement where complex causal mechanisms underlie the relationship between interventions and outcomes. We propose two algorithms, causal upper confidence bound (C-UCB) and causal Thompson Sampling (C-TS), that enjoy improved cumulative regret bounds compared with algorithms that do not use causal information. We thus resolve an open problem posed by Lattimore et al. (2016). Further, we extend C-UCB and C-TS to the linear bandit setting and propose causal linear UCB (CL-UCB) and causal linear TS (CL-TS) algorithms. These algorithms enjoy a cumulative regret bound that only scales with the feature dimension. Our experiments show the benefit of using causal information. For example, we observe that even with a few hundreds of iterations, the regret of causal algorithms is less than that of standard algorithms by a factor of three. We also show that under certain causal structures, our algorithms scale better than the standard bandit algorithms as the number of interventions increases.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry><entry><title>What You See May Not Be What You Get: UCB Bandit Algorithms Robust to ε-Contamination</title><link href="https://pyvideo.org/uai-2020/what-you-see-may-not-be-what-you-get-ucb-bandit-algorithms-robust-to-e-contamination.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Laura Niss</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/what-you-see-may-not-be-what-you-get-ucb-bandit-algorithms-robust-to-e-contamination.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What You See May Not Be What You Get: UCB Bandit Algorithms Robust to ε-Contamination&lt;/p&gt;
&lt;p&gt;Laura Niss (University of Michigan)*; Ambuj Tewari (University of Michigan)&lt;/p&gt;
&lt;p&gt;Motivated by applications of bandit algorithms in education, we consider a stochastic multi-armed bandit problem with ε-contaminated rewards. We allow an adversary to arbitrarily …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What You See May Not Be What You Get: UCB Bandit Algorithms Robust to ε-Contamination&lt;/p&gt;
&lt;p&gt;Laura Niss (University of Michigan)*; Ambuj Tewari (University of Michigan)&lt;/p&gt;
&lt;p&gt;Motivated by applications of bandit algorithms in education, we consider a stochastic multi-armed bandit problem with ε-contaminated rewards. We allow an adversary to arbitrarily give unbounded contaminated rewards with full knowledge of the past and future. We impose only the constraint that at any time t the proportion of contaminated rewards for any actions is less than or equal to ε. We derive concentration inequalities for two robust mean estimators for sub-Gaussian distributions in the ε-contamination context. We define the ε-contaminated stochastic bandit problem and use our robust mean estimators to give two variants of a robust Upper Confidence Bound (UCB) algorithm, crUCB. Using regret derived from only the underlying stochastic rewards, both variants of crUCB achieve O(√KTlogT). Our simulations are designed to reflect reasonable settings a teacher would experience when implementing a bandit algorithm. We show that in certain adversarial regimes crUCB not only outperforms algorithms designed for stochastic (UCB1) and adversarial bandits (EXP3) but also those that have “best of both worlds” guarantees (EXP3++ and Tsallis-Inf) even when our constraint
on the proportion of contaminated rewards is broken.&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>