<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_nicholas-del-grosso.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-09-04T00:00:00+00:00</updated><entry><title>Scientific DevOps: Designing Reproducible Data Analysis Pipelines with Containerized Workflow Managers</title><link href="https://pyvideo.org/euroscipy-2019/scientific-devops-designing-reproducible-data-analysis-pipelines-with-containerized-workflow-managers.html" rel="alternate"></link><published>2019-09-04T00:00:00+00:00</published><updated>2019-09-04T00:00:00+00:00</updated><author><name>Nicholas Del Grosso</name></author><id>tag:pyvideo.org,2019-09-04:euroscipy-2019/scientific-devops-designing-reproducible-data-analysis-pipelines-with-containerized-workflow-managers.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Open source and open science come together when the software is
accessible, transparent, and owned by all. For data analysis pipelines
that grow in complexity beyond a single Jupyter notebook, this can
become a challenge as the number of steps and software dependencies
increase. In this talk, Nicholas Del Grosso will review a variety of
tools for packaging and managing a data analysis pipeline, showing how
they fit together and benefit the development, testing, deployment, and
publication processes and the scientific community. In particular, this
talk will cover:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Workflow managers&lt;/strong&gt; (e.g. Snakemake, PyDoit, Luigi) to combine
complex pipelines into single applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container Solutions&lt;/strong&gt; (e.g. Docker and Singularity) to package and
deploy the software on others' computers, including high-performance
computing clusters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Scientific Filesystem&lt;/strong&gt; to build explorable and multi-purpose
applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Testing Frameworks&lt;/strong&gt; (e.g. PyTest, Hypothesis) to declare and
confirm the assumptions and functionality of the analysis pipeline.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ease-of-Use Utilities&lt;/strong&gt; to share the pipeline online and make it
accessible to non-programmers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By writing software that stays manageable, reproducible, and deployable
continuously throughout the development cycle, we can better fulfill the
goals of open science and good scientific practice in a digital era.&lt;/p&gt;
&lt;p&gt;A review of DevOps tools as applied to data analysis pipelines,
including workflow managers, software containers, testing frameworks,
and online repositories for performing reproducible science that scales.&lt;/p&gt;
</summary></entry></feed>