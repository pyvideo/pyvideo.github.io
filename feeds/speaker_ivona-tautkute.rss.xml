<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Thu, 05 Dec 2019 00:00:00 +0000</lastBuildDate><item><title>AI meets Fashion for product retrieval with multi-modally generated data</title><link>https://pyvideo.org/pydata-la-2019/ai-meets-fashion-for-product-retrieval-with-multi-modally-generated-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The talk will cover generative modeling for multimodal input (image and
text) in the context of product retrieval in fashion/e-commerce.&lt;/p&gt;
&lt;p&gt;The presentation will include examples of applying generative (GAN)
architectures for image generation with multimodal query using models
derived from Conditional GAN, StackGAN, AttnGAN and others.&lt;/p&gt;
&lt;p&gt;Retrieving products from large databases and finding items of particular
interest for the user is a topic of ongoing research. Moving further
from text search, tag based search and image search, there is still a
lot of ambiguity when visual and textual features need to be merged.
Text query might compliment an image (&amp;quot;I want sport shoes like these in
the image, produced by XXX, wide fit and comfortable&amp;quot;) or might
represent a difference from image query (&amp;quot;I want a dress like that in
the picture, only with shorter sleeves&amp;quot;).&lt;/p&gt;
&lt;p&gt;Talk outline:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Use cases in e-commerce and fashion&lt;/li&gt;
&lt;li&gt;Current methods for learning multimodal embedding (VSE, Multimodal
Siamese Networks)&lt;/li&gt;
&lt;li&gt;Intro to GAN architectures that take latent representation as an
input (we can influence what we generate, yeah!)&lt;/li&gt;
&lt;li&gt;How do you feed multimodal input into GAN&lt;/li&gt;
&lt;li&gt;Results and comparison&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ivona Tautkute</dc:creator><pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-12-05:pydata-la-2019/ai-meets-fashion-for-product-retrieval-with-multi-modally-generated-data.html</guid></item><item><title>What looks good with my sofa ?</title><link>https://pyvideo.org/pydata-new-york-city-2017/what-looks-good-with-my-sofa.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The goal of our engine is to retrieve interior objects, e.g. furniture or wall clocks, that share visual and aesthetic similarities with the query. Our search engine allows the user to take a photo of a room and retrieve with a high recall a list of items identical or visually similar to those present in the photo. Additionally, it allows to return other items that aesthetically and stylistically fit well together. To achieve this goal, our system blends the results obtained using textual and visual modalities. Thanks to this blending strategy, we increase the average style similarity score of the retrieved items by 11%.&lt;/p&gt;
&lt;p&gt;Our work is implemented as a Web-based application and it is planned to be opened to the public on Tooploox website.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ivona Tautkute</dc:creator><pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-11-27:pydata-new-york-city-2017/what-looks-good-with-my-sofa.html</guid></item></channel></rss>