<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_astrid-prinz.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-07-13T00:00:00+00:00</updated><entry><title>Building &amp; Replicating Models of Visual Search Behavior w/ Tensorflow, Nengo, &amp; Scientific Python Stack</title><link href="https://pyvideo.org/scipy-2019/building-replicating-models-of-visual-search-behavior-w-tensorflow-nengo-scientific-python-stack.html" rel="alternate"></link><published>2019-07-13T00:00:00+00:00</published><updated>2019-07-13T00:00:00+00:00</updated><author><name>David Nicholson</name></author><id>tag:pyvideo.org,2019-07-13:scipy-2019/building-replicating-models-of-visual-search-behavior-w-tensorflow-nengo-scientific-python-stack.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Animals constantly use their eyes to search their environment. What can neural networks and other cognitive models tell us about this behavior? We present two related studies that leverage scientific Python libraries to address this question. The first uses Tensorflow to replicate and extend a previous study of how convolutional neural networks perform a classic visual search task. The second study compares visual search behavior of two types of models: a recurrent neural network model from Google DeepMind, and spiking cognitive models built with the Nengo neural simulator. We discuss what our results suggest about such models.&lt;/p&gt;
</summary></entry></feed>