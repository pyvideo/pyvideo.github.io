<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_tony-ojeda.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-05-20T00:00:00+00:00</updated><entry><title>Human-Machine Collaboration for Improved Analytical Processes</title><link href="https://pyvideo.org/pycon-us-2017/human-machine-collaboration-for-improved-analytical-processes.html" rel="alternate"></link><published>2017-05-20T00:00:00+00:00</published><updated>2017-05-20T00:00:00+00:00</updated><author><name>Tony Ojeda</name></author><id>tag:pyvideo.org,2017-05-20:pycon-us-2017/human-machine-collaboration-for-improved-analytical-processes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Over the last several years, Python developers interested in data
science and analytics have acquired a variety of tools and libraries
that aim to facilitate analytical processes. Libraries such as Pandas,
Statsmodels, Scikit-learn, Matplotlib, Seaborn, and Yellowbrick have
made tasks such as data wrangling, statistical modeling, machine
learning, and data visualization much quicker and easier. They have
accomplished this by automating and abstracting away some of the more
tedious, repetitive processes involved with analyzing and modeling data.&lt;/p&gt;
&lt;p&gt;Over the next few years, we are sure to witness the introduction of new
tools that are increasingly intelligent and have the ability to automate
more complex analytical processes. However, as we begin using these
tools (and developing new ones), we should strongly consider the level
of automation that is most appropriate for each case. Some analytical
processes are technically difficult to automate, and therefore require
large degrees of human steering. Others are relatively easy to automate
but perhaps should not be due to the unpredictability of results or
outputs requiring a level of compassionate decision-making that machines
simply don’t possess. Such processes would benefit greatly from the
collaboration between automated machine tasks and uniquely human ones.
After all, it is often systems that utilize a combination of both human
and machine intelligence that achieve better results than either could
on their own.&lt;/p&gt;
&lt;p&gt;In this talk, we will discuss human-machine collaboration as it applies
to analyzing data with Python. We will review a framework for
exploratory data analysis with the goal of identifying which tasks
should be automated, which tasks should not, and which tasks would
benefit from a more interactive, symbiotic, and collaborative process
between the human and the machine. We will explore Python libraries that
we can use to build tools that allow us to perform different types of
analysis. We’ll also introduce the Cultivar project, an example of a
hybrid analytics tool that combines a Django framework with Javascript
visualizations and Celery for task management to facilitate more
efficient and effective human-machine systems for data analysis.&lt;/p&gt;
</summary></entry><entry><title>Fantastic Data and Where To Find Them: An introduction to APIs, RSS, and Scraping</title><link href="https://pyvideo.org/pycon-us-2017/fantastic-data-and-where-to-find-them-an-introduction-to-apis-rss-and-scraping.html" rel="alternate"></link><published>2017-05-17T00:00:00+00:00</published><updated>2017-05-17T00:00:00+00:00</updated><author><name>Nicole Donnelly</name></author><id>tag:pyvideo.org,2017-05-17:pycon-us-2017/fantastic-data-and-where-to-find-them-an-introduction-to-apis-rss-and-scraping.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Whether you’re building a custom web application, getting started in
machine learning, or just want to try something new, everyone needs
data. And while the web offers a seemingly boundless source for custom
data sets, the collection of that data can present a whole host of
obstacles. From ever-changing APIs to rate-limiting woes, from
nightmarishly nested XML to convoluted DOM trees, working with APIs and
web scraping are challenging but critically useful skills for
application developers and data scientists alike. In this tutorial,
we’ll introduce RESTful APIs, RSS feeds, and web scraping in order to
see how different ingestion techniques impact application development.
We’ll explore how and when to use Python libraries such as
&lt;tt class="docutils literal"&gt;feedparser&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;requests&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;beautifulsoup&lt;/tt&gt;, and &lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt;. And
finally we will present common data collection problems and how to
overcome them.&lt;/p&gt;
&lt;p&gt;We’ll take a hands-on, directed exercise approach combined with short
presentations to engage a range of different APIs (with and without
authentication), explore examples of how and why you might web scrape,
and learn the ethical and legal considerations for both. To prepare
attendees to create their own data ingestion scripts, the tutorial will
walk through a set of examples for robust and responsible data
collection and ingestion. This tutorial will conclude with a case study
of &lt;a class="reference external" href="https://pypi.python.org/pypi/baleen/0.3.3"&gt;Baleen&lt;/a&gt;, an automated
RSS ingestion service designed to construct a production-grade text
corpus for NLP research and machine learning applications. Exercises
will be presented both as Jupyter Notebooks and Python scripts.&lt;/p&gt;
</summary></entry><entry><title>Transforming Data to Unlock Its Latent Value</title><link href="https://pyvideo.org/pydata-carolinas-2016/transforming-data-to-unlock-its-latent-value.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Tony Ojeda</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/transforming-data-to-unlock-its-latent-value.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will be about gaining an understanding of the real world
entities represented by our data, creatively conceptualizing different
perspectives from which our data can be analyzed, and then bringing
those conceptualizations to life with the help of Python libraries
such as Pandas, Scikit-Learn, Seaborn, and Yellowbrick so that we can
unlock the latent value and insights hidden in our data.&lt;/p&gt;
&lt;p&gt;At the heart of data analysis, there lies a need to understand the
real world entities being represented in the data. Every data set we
encounter is an attempt to capture a slice of our complex world and
communicate some information about it in a way that has potential to
be informative to humans, machines, or both. Moving from basic
analyses to advanced analytics requires the ability to imagine
multiple ways of conceptualizing the composition of entities and the
relationships present in our data. It also requires the realization
that different levels of aggregation, disaggregation, and
transformation can open up new pathways to understanding our data and
identifying the valuable insights it contains.&lt;/p&gt;
&lt;p&gt;In this talk, we’ll discuss several ways to think about the
composition and representation of our data. We’ll also demonstrate a
series of methods that leverage tools like networks, hierarchical
aggregations, and unsupervised clustering to visually explore our
data, transform it to discover new insights, help frame analytical
problems and questions, and even improve machine learning model
performance. In exploring these approaches, and with the help of
Python libraries such as Pandas, Scikit-Learn, Seaborn, and
Yellowbrick, we will provide a practical framework for thinking
creatively and visually about your data and unlocking latent value and
insights hidden deep beneath its surface.&lt;/p&gt;
</summary></entry><entry><title>Data Transformation: A Framework for Exploratory Data Analysis</title><link href="https://pyvideo.org/pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Tony Ojeda</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Exploratory data analysis plays a critical role in the job of every data scientist, but very few have a structured process or framework for exploring data quickly and efficiently. This talk will introduce the exploratory framework I use in my day-to-day work and will walk attendees through a practical example of how to use the framework to unlock hidden insights with the help of Python libraries.&lt;/p&gt;
&lt;p&gt;At the heart of data analysis, there lies a need to understand the real world entities being represented in the data. Every data set we encounter is an attempt to capture a slice of our complex world and communicate some information about it in a way that has potential to be informative to humans, machines, or both. Moving from basic analyses to advanced analytics requires the ability to imagine multiple ways of conceptualizing the composition of entities and the relationships present in our data. It also requires the realization that different levels of aggregation, disaggregation, and transformation can open up new pathways to understanding our data and identifying the valuable insights it contains.&lt;/p&gt;
&lt;p&gt;In this talk, we’ll discuss several ways to think about the composition and representation of our data. We’ll also demonstrate a series of methods that leverage tools like networks, hierarchical aggregations, and unsupervised clustering to visually explore our data, transform it to discover new insights, help frame analytical problems and questions, and even improve machine learning model performance. In exploring these approaches, and with the help of Python libraries such as Pandas, Scikit-Learn, Seaborn, and NetworkX, we will provide a practical framework for thinking creatively and visually about your data and unlocking latent value and insights hidden deep beneath its surface.&lt;/p&gt;
</summary><category term="analysis"></category><category term="Data"></category><category term="Data Analysis"></category><category term="framework"></category></entry><entry><title>Natural Language Processing with NLTK and Gensim</title><link href="https://pyvideo.org/pycon-us-2016/tony-ojeda-benjamin-bengfort-laura-lorenz-natural-language-processing-with-nltk-and-gensim.html" rel="alternate"></link><published>2016-05-30T00:00:00+00:00</published><updated>2016-05-30T00:00:00+00:00</updated><author><name>Tony Ojeda</name></author><id>tag:pyvideo.org,2016-05-30:pycon-us-2016/tony-ojeda-benjamin-bengfort-laura-lorenz-natural-language-processing-with-nltk-and-gensim.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Speakers: Tony Ojeda, Benjamin Bengfort, Laura Lorenz&lt;/p&gt;
&lt;p&gt;In this tutorial, we will begin by exploring the features of the NLTK library. We will then focus on building a language-aware data product - a topic identification and document clustering algorithm from a web crawl of blog sites. The clustering algorithm will use a simple Lesk K-Means clustering to start, and then will improve with an LDA analysis using the popular Gensim library.&lt;/p&gt;
&lt;p&gt;Slides can be found at: &lt;a class="reference external" href="https://speakerdeck.com/pycon2016"&gt;https://speakerdeck.com/pycon2016&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/PyCon/2016-slides"&gt;https://github.com/PyCon/2016-slides&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>