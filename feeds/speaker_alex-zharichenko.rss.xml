<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Alex Zharichenko</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sat, 30 Jul 2022 00:00:00 +0000</lastBuildDate><item><title>Scraping Your Way to a Dataset</title><link>https://pyvideo.org/pyohio-2019/scraping-your-way-to-a-dataset.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large datasets are vital for the majority of analytic and machine
learning tasks. But what happens when the data you need isn't available
in some convenient and easily obtainable form? This talk will go through
the process of data scraping to create a dataset that can be then used
for various analytical or machine learning tasks.&lt;/p&gt;
&lt;p&gt;It is essential to have a very large and high-quality dataset in order
to perform significant analytics or to use in various machine learning
tasks. For some tasks, there exists simple APIs or repositories of data
to collect from. But for many other tasks like tracking prices of
products, predicting stock prices, and predicting outcomes of sports
games there isn't a convenient way to retrieve this information besides
a webpage. Because of these circumstances, learning to scrape data from
webpages and other sources allows us to create our own dataset.
Additionally, scraping grants us the ability to ask better questions
about data in the world.&lt;/p&gt;
&lt;p&gt;This talk is geared towards beginner-to-intermediate Python developers
that want to be able to ask and answer better questions through data.
This talk will provide a guide for web scraping through two examples,
and it will explain how to get the scraped data into a usable form.
Throughout the talk, I will highlight some tips for improving scraper
performance, minimizing the risk that a web server will stop you, and
different ways to store the collected data. The first of the two
examples will examine a simple case of scraping data about the lottery
and the second will explore a more challenging case of scraping course
information from a University.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Zharichenko</dc:creator><pubDate>Sat, 27 Jul 2019 14:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-07-27:/pyohio-2019/scraping-your-way-to-a-dataset.html</guid><category>PyOhio 2019</category></item><item><title>The Gems of the Python Standard Library</title><link>https://pyvideo.org/pyohio-2020/the-gems-of-the-python-standard-library.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You may know about a lot of the amazing standard libraries such as
&lt;tt class="docutils literal"&gt;collections&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;json&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;sqlite3&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;argparse&lt;/tt&gt;, etc.&lt;/p&gt;
&lt;p&gt;But what about the more unknown libraries, such as &lt;tt class="docutils literal"&gt;sched&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;struct&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;enum&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;bisect&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;heapq&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;decimal&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;statistics&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;pathlib&lt;/tt&gt;, and many more!&lt;/p&gt;
&lt;p&gt;This talk will go through, at a high level, the usage of some of these
libraries and how they can allow you to performs some tasks more
computationally efficient or hack together solutions faster without
needing additionally third party libraries.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Zharichenko</dc:creator><pubDate>Sat, 25 Jul 2020 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2020-07-25:/pyohio-2020/the-gems-of-the-python-standard-library.html</guid><category>PyOhio 2020</category></item><item><title>Redlining Your CPU for Fun and Profit: Threading and Multiprocessing in Python</title><link>https://pyvideo.org/pyohio-2021/redlining-your-cpu-for-fun-and-profit-threading-and-multiprocessing-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Is your Python script taking too long, and your CPU utilization too low?
Enter the concurrent execution standard libraries (&lt;tt class="docutils literal"&gt;threading&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;multiprocessing&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;concurrent.futures&lt;/tt&gt;, etc.). Throughout the talk,
we will look through various toy problems and learn how we can quickly
speed it up for IO bound and CPU bound processes using the Python
standard library, so you can get more done and raise that CPU usage to
100%.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Zharichenko</dc:creator><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2021-07-31:/pyohio-2021/redlining-your-cpu-for-fun-and-profit-threading-and-multiprocessing-in-python.html</guid><category>PyOhio 2021</category></item><item><title>Doing Everything Data Without Leaving the Notebook: Programmatic Jupyter Notebooks</title><link>https://pyvideo.org/pyohio-2022/doing-everything-data-without-leaving-the-notebook-programmatic-jupyter-notebooks.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter notebooks is one of the most powerful tools for any data
scientist. It makes doing tasks like data wrangling, modeling,
visualizing really quick and easy for even people with not a lot of
experience in software engineering.&lt;/p&gt;
&lt;p&gt;But, a problem arises that to actually put that code into production
involves a lot of copying, pasting, and refactoring into order to be
used in a full fledged system. But what if we didn't have to leave the
notebook? What if the notebook could be the production ready code?&lt;/p&gt;
&lt;p&gt;This talk will giving an introduction to using the papermill library,
how it works, why it's powerful, and an actual use case of how I use
papermill in a pipeline that transforms raw data into clean tidy data,
and then runs multiple many notebooks to generate various visualizations
and statistics to be later used in a study.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Zharichenko</dc:creator><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2022-07-30:/pyohio-2022/doing-everything-data-without-leaving-the-notebook-programmatic-jupyter-notebooks.html</guid><category>PyOhio 2022</category></item></channel></rss>