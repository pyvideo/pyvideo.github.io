<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Hanna Hajishirzi</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_hanna-hajishirzi.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2024-09-18T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Keynote: Open Language Models (OLMo): Accelerating the Science of Language Modeling</title><link href="https://pyvideo.org/pytorch-conference-2024/keynote-open-language-models-olmo-accelerating-the-science-of-language-modeling.html" rel="alternate"></link><published>2024-09-18T00:00:00+00:00</published><updated>2024-09-18T00:00:00+00:00</updated><author><name>Hanna Hajishirzi</name></author><id>tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/keynote-open-language-models-olmo-accelerating-the-science-of-language-modeling.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Keynote: Open Language Models (OLMo): Accelerating the Science of Language Modeling - Hanna Hajishirzi, Senior Director NLP Research, Allen Institute for AI&lt;/p&gt;
&lt;p&gt;Over the past few years, and especially since the deployment of ChatGPT in November 2022,  neural language models with billions of parameters and trained on trillions of words â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Keynote: Open Language Models (OLMo): Accelerating the Science of Language Modeling - Hanna Hajishirzi, Senior Director NLP Research, Allen Institute for AI&lt;/p&gt;
&lt;p&gt;Over the past few years, and especially since the deployment of ChatGPT in November 2022,  neural language models with billions of parameters and trained on trillions of words are powering the fastest-growing computing applications in history and generating discussion and debate across society. However, AI scientists cannot study or improve those state-of-the-art models because the models' parameters, training data, code, and even documentation are not openly available. In this talk, I present our OLMo project toward building strong language models and making them fully open to researchers along with open-source code for data management, training, inference, and interaction. In particular, I describe DOLMa, a 3T token open dataset curated for training language models, Tulu, our instruction-tuned language model, and OLMo v1, a fully-open 7B parameter language model trained from scratch.&lt;/p&gt;
</content><category term="PyTorch Conference 2024"></category></entry></feed>