<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Dimitrios Stamos</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_dimitrios-stamos.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Online Parameter-Free Learning of Multiple Low Variance Tasks</title><link href="https://pyvideo.org/uai-2020/online-parameter-free-learning-of-multiple-low-variance-tasks.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Giulia Denevi</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/online-parameter-free-learning-of-multiple-low-variance-tasks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Online Parameter-Free Learning of Multiple Low Variance Tasks&lt;/p&gt;
&lt;p&gt;Giulia Denevi (IIT/UNIGE); Massimiliano Pontil (IIT)*; Dimitrios Stamos (University College London)&lt;/p&gt;
&lt;p&gt;We propose a method to learn a common bias vector for a growing sequence of low-variance tasks. Unlike state-of-the-art approaches, our method does not require tuning any hyper-parameter. Our …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Online Parameter-Free Learning of Multiple Low Variance Tasks&lt;/p&gt;
&lt;p&gt;Giulia Denevi (IIT/UNIGE); Massimiliano Pontil (IIT)*; Dimitrios Stamos (University College London)&lt;/p&gt;
&lt;p&gt;We propose a method to learn a common bias vector for a growing sequence of low-variance tasks. Unlike state-of-the-art approaches, our method does not require tuning any hyper-parameter. Our approach is presented in the non-statistical setting and can be of two variants. The “aggressive” one updates the bias after each datapoint, the “lazy” one updates the bias only at the end of each task. We derive an across-tasks regret bound for the method. When compared to state-of-the-art approaches, the aggressive variant returns faster rates, the lazy one recovers standard rates, but with no need of tuning hyper-parameters. We then adapt the methods to the statistical setting: the aggressive variant becomes a multi-task learning method, the lazy one a meta-learning method. Experiments confirm the effectiveness of our methods in practice.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>