<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Thu, 31 Oct 2019 00:00:00 +0000</lastBuildDate><item><title>Big Data Pipeline Design and Tuning in PySpark</title><link>https://pyvideo.org/pycon-se-2019/big-data-pipeline-design-and-tuning-in-pyspark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PySpark is a great tool for doing big data ETL pipeline. While designing a big data pipeline, which is easy to maintain with a holistic view, simple to spot bottleneck is difficult. Not to say enable analytics on ETL pipelines. Rockie Yang will share his experiences on build effective ETL pipeline with PySpark.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rockie Yang</dc:creator><pubDate>Thu, 31 Oct 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-10-31:pycon-se-2019/big-data-pipeline-design-and-tuning-in-pyspark.html</guid><category>pyspark</category><category>data pipeline</category><category>etl</category></item><item><title>Geospatial analysis with Python</title><link>https://pyvideo.org/pycon-italia-2019/geospatial-analysis-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Due to its effectiveness and simplicity, Python is spreading as a choice
for handling geospatial data. From running algorithms capable of
extracting geo- referred insights or processing geo archives, Python
could represent a powerful tool to handle geo-related problems thanks to
an extensive set of libraries. The training will give an overview about
processing geospatial data with Python. An approximate agenda will
include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction&lt;ul&gt;
&lt;li&gt;setting up the the environment&lt;/li&gt;
&lt;li&gt;an introduction to geospatial world&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Working with geospatial data&lt;ul&gt;
&lt;li&gt;playing with geo archives (vector/rasters)&lt;/li&gt;
&lt;li&gt;extracting geo analytics&lt;/li&gt;
&lt;li&gt;Vector tiles big vector data&lt;/li&gt;
&lt;li&gt;Apache Spark for geospatial raster analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Python for Earth observation&lt;ul&gt;
&lt;li&gt;Classifying earth observation images&lt;/li&gt;
&lt;li&gt;Extracting insights from Copernicus products&lt;/li&gt;
&lt;li&gt;Use SNAP from Python&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No tools are required for attending this training. Bring your PC with
Docker installed. Further instructions will be provided.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1674"&gt;https://python.it/feedback-1674&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 10:30 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt; in __on &lt;strong&gt;Saturday 4
May&lt;/strong&gt; at 18:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See schedule**&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesco Bruni</dc:creator><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2019-05-03:pycon-italia-2019/geospatial-analysis-with-python.html</guid><category>Jupyter</category><category>pyspark</category><category>geospatial</category><category>geopynotebook</category><category>spark</category><category>docker</category></item><item><title>Sparkflow: Utilizing Pyspark for Training Tensorflow Models on Large Datasets</title><link>https://pyvideo.org/pydata-indy-2018/sparkflow-utilizing-pyspark-for-training-tensorflow-models-on-large-datasets.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As more public, large datasets are becoming available, distributed data processing tools such as Apache Spark are vital for data scientists. While SparkML provides many machine learning algorithms, standard pipelines, and a basic linear algebra library, it does not support training deep learning models. Due to the rise of Tensorflow in the last two years, Lifeomic built the Sparkflow library to combine the power of the Pipeline api from Spark with training Deep Learning models in Tensorflow. Sparkflow uses the Hogwild algorithm to train deep learning models in a distributed manor, which underneath leverages the driver/executor architecture in Spark to manage copied networks and gradients. In this session, we describe some of the lessons learned in building Sparkflow, the pros and cons of asynchronous distributed deep learning, how to use Spark Pipelines with Tensorflow with very few lines of code, and where we are headed with the library in the near future.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Derek Miller</dc:creator><pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-10-12:pydata-indy-2018/sparkflow-utilizing-pyspark-for-training-tensorflow-models-on-large-datasets.html</guid><category>Pyspark</category><category>Tensorflow</category></item><item><title>Adapting from Spark to Dask: what to expect</title><link>https://pyvideo.org/pycon-us-2018/adapting-from-spark-to-dask-what-to-expect.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Until very recently, Apache Spark has been a de facto standard choice of a framework for batch data processing. For Python developers, diving into Spark is challenging, because it requires learning the Java infrastructure, memory management, configuration management. The multiple layers of indirection also make it harder to debug things, especially when throwing the Pyspark wrapper into the equation.&lt;/p&gt;
&lt;p&gt;With Dask emerging as a pure Python framework for parallel computing, Python developers might be looking at it with new hope, wondering if it might work for them in place of Spark. In this talk, I’m using a data aggregation example to highlight the important differences between the two frameworks, and make it clear how involved the switch may be.&lt;/p&gt;
&lt;p&gt;Note: Just in case it's unclear, there's no Java of any kind in this talk. All the code / examples use Python (PySpark).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Irina Truong</dc:creator><pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-11:pycon-us-2018/adapting-from-spark-to-dask-what-to-expect.html</guid><category>dsak</category><category>pyspark</category></item><item><title>Debugging PySpark -- Or trying to make sense of a JVM stack trace when you were minding your own bus</title><link>https://pyvideo.org/pycon-us-2018/debugging-pyspark-or-trying-to-make-sense-of-a-jvm-stack-trace-when-you-were-minding-your-own-bus.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common errors and how to detect them.&lt;/p&gt;
&lt;p&gt;Spark’s own internal logging can often be quite verbose, and this talk will examine how to effectively search logs from Apache Spark to spot common problems. In addition to the internal logging, this talk will look at options for logging from within our program itself.&lt;/p&gt;
&lt;p&gt;Spark’s accumulators have gotten a bad rap because of how they interact in the event of cache misses or partial recomputes, but this talk will look at how to effectively use Spark’s current accumulators for debugging as well as a look to future for data property type accumulators which may be coming to Spark in future version.&lt;/p&gt;
&lt;p&gt;In addition to reading logs, and instrumenting our program with accumulators, Spark’s UI can be of great help for quickly detecting certain types of problems.&lt;/p&gt;
&lt;p&gt;Debuggers are a wonderful tool, however when you have 100 computers the “wonder” can be a bit more like “pain”. This talk will look at how to connect remote debuggers, but also remind you that it’s probably not the easiest path forward.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Holden Karau</dc:creator><pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-11:pycon-us-2018/debugging-pyspark-or-trying-to-make-sense-of-a-jvm-stack-trace-when-you-were-minding-your-own-bus.html</guid><category>debug</category><category>pySpark</category></item><item><title>The Magic Behind PySpark, how it impacts perf &amp; the "future"</title><link>https://pyvideo.org/pydata-barcelona-2017/the-magic-behind-pyspark-how-it-impacts-perf-the-future.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A look at how PySpark &amp;quot;works&amp;quot; today and how we can make it better in the future + insert engine noises of a fast car +&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk will introduce PySpark along with the magic done to make it work and be friends with the JVM. We will discuss why lazy evaluation makes a huge difference in PySpark, both in terms of general optimizations it opens up as well as Python specific considerations. From there we will explore much of the future of Spark, DataFrames &amp;amp; Datasets and what this means for PySpark. Most Spark DataFrame examples limit them selves to things written in the relational style query language, but we will explore how to add more functionality through UDFS.&lt;/p&gt;
&lt;p&gt;We will wrap up with looking at the different pieces of work being done to make PySpark faster, from using better interchange formats like Apache Arrow to crazy hair brained schemes inspired by (but not the fault of) the Javascript on Spark project.&lt;/p&gt;
&lt;p&gt;Hopefully no one is scared away from using Spark once they see the 300 small gnome like creatures behind the curtain, but parental guidance is encouraged for those who still believe in magic, reliable distributed systems, and vendor marketing brochures.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Holden Karau</dc:creator><pubDate>Sat, 20 May 2017 10:30:00 +0200</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/the-magic-behind-pyspark-how-it-impacts-perf-the-future.html</guid><category>keynote</category><category>pyspark</category></item><item><title>Improving PySpark Performance Spark performance beyond the JVM</title><link>https://pyvideo.org/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Holden Karau</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</guid><category>jvm</category><category>performance</category><category>pyspark</category><category>spark</category></item><item><title>PySpark in Practice</title><link>https://pyvideo.org/pydata-berlin-2016/pyspark-in-practice.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;In this talk we will share our best practices of using PySpark in numerous customer facing data science engagements. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Unit testing with PySpark&lt;/li&gt;
&lt;li&gt;Integration with SQL on Hadoop engines&lt;/li&gt;
&lt;li&gt;Data pipeline management and workflows&lt;/li&gt;
&lt;li&gt;Data Structures (RDDs vs. Data Frames vs. Data Sets)&lt;/li&gt;
&lt;li&gt;When to use MLlib vs scikit-learn&lt;/li&gt;
&lt;li&gt;Operationalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At Pivotal Labs we have many data science engagements on big data. Typical problems involve real-time data from sensors collected by telecom operators to GPS data produced by vehicle tracking systems. One widespread framework to solve those inherently difficult problems is Apache Spark. In this talk, we want to share our best practices with PySpark, Spark’s Python API, highlighting our experience as well as dos and dont's. In particular, we will focus on the whole data science pipeline from data ingestion, data munging and wrangling to the actual model building.&lt;/p&gt;
&lt;p&gt;Finally, many businesses have started to realise that there is no return on investment from data science if the models do not go into production. At Pivotal Labs, one our core principle is API first. Therefore, we will also talk how we put our models into production, sharing our hands-on knowledge in this field and also how this fits into test-driven development.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ronert Obst</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/pyspark-in-practice.html</guid><category>pyspark</category></item><item><title>Spotting trends and tailoring recommendations: PySpark on Big Data in fashion</title><link>https://pyvideo.org/pydata-berlin-2016/spotting-trends-and-tailoring-recommendations-pyspark-on-big-data-in-fashion.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Predicting what people like when they choose what to wear is a non-trivial task involving several ingredients. At Mallzee, the data is variegated, large and has to be processed quickly to produce recommendations on products, tailored to each user. We use PySpark to crunch large sets of different data and create models in order to generate robust and meaningful suggestions.&lt;/p&gt;
&lt;p&gt;Mallzee is a fashion app where people can see products (clothes, shoes and accessories) and decide whether they like them or not. They can also buy products and create feeds of preferred brands and categories of items. We have large amounts of data generated by the users when they scroll and search through products and we use it to understand the user. We want to give everyone meaningful recommendations on the items they might like, hence tailoring the experience to who they are. We build pseudo-intelligent algorithms capable of extracting the style profile of a user and we crunch products data to match items to the user based on a classification model. The model is validated and statistical analyses are performed to determine the tipping point when recommendations are valuable to the user. The talk will go through the steps we implement by showing how the full data stack of Python is used in achieving this goal and how it is interfaced with a Spark cluster through PySpark in order to run Machine Learning algorithms on big data.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martina Pugliese</dc:creator><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-01:pydata-berlin-2016/spotting-trends-and-tailoring-recommendations-pyspark-on-big-data-in-fashion.html</guid><category>pyspark</category></item><item><title>Machine Learning at Scale</title><link>https://pyvideo.org/pydata-berlin-2016/machine-learning-at-scale.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Python machine learning libraries like scikit-learn are a fantastic resource but not always well suited to large datasets. How can we use Python for machine learning in such cases? This talk will introduce PySpark and MLlib as tools for distributed machine learning. We will discuss what these tools are, how they work, and cover some basic code examples of machine learning on a cluster.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;Intro&lt;/dt&gt;
&lt;dd&gt;&lt;ol class="first last loweralpha"&gt;
&lt;li&gt;Why is scikit-learn not enough?&lt;/li&gt;
&lt;li&gt;What is Spark?&lt;/li&gt;
&lt;li&gt;What is MLlib?&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;Spark&lt;/dt&gt;
&lt;dd&gt;&lt;ol class="first last loweralpha"&gt;
&lt;li&gt;Overview of Spark&lt;/li&gt;
&lt;li&gt;Overview of PySpark&lt;/li&gt;
&lt;li&gt;PySpark code sample&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;MLlib&lt;/dt&gt;
&lt;dd&gt;&lt;ol class="first last loweralpha"&gt;
&lt;li&gt;Overview of MLlib&lt;/li&gt;
&lt;li&gt;MLlib code samples&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nathan Epstein</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:pydata-berlin-2016/machine-learning-at-scale.html</guid><category>scikit-learn</category><category>pyspark</category><category>mllib</category></item><item><title>PySpark - Data processing in Python on top of Apache Spark.</title><link>https://pyvideo.org/europython-2015/pyspark-data-processing-in-python-on-top-of-apache-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Peter Hoffmann - PySpark - Data processing in Python on top of Apache Spark.
[EuroPython 2015]
[22 July 2015]
[Bilbao, Euskadi, Spain]&lt;/p&gt;
&lt;p&gt;[Apache Spark][1] is a computational engine for large-scale data processing. It
is responsible for scheduling, distribution and monitoring applications which
consist of many computational task across many worker machines on a computing
cluster.&lt;/p&gt;
&lt;p&gt;This Talk will give an overview of PySpark with a focus on Resilient
Distributed Datasets and the DataFrame API. While Spark Core itself is written
in Scala and runs on the JVM, PySpark exposes the Spark programming model to
Python. It defines an API for Resilient Distributed Datasets (RDDs). RDDs are a
distributed memory abstraction that lets programmers perform in-memory
computations on large clusters in a fault-tolerant manner. RDDs are immutable,
partitioned collections of objects. Transformations construct a new RDD from a
previous one. Actions compute a result based on an RDD. Multiple
computation steps
are expressed as directed acyclic graph (DAG). The DAG execution model is
a generalization of the Hadoop MapReduce computation model.&lt;/p&gt;
&lt;p&gt;The Spark DataFrame API was introduced in Spark 1.3. DataFrames envolve Spark's
RDD model and are inspired by Pandas and R data frames. The API provides
simplified operators for filtering, aggregating, and projecting over large
datasets. The DataFrame API supports diffferent data sources like JSON
datasources, Parquet files, Hive tables and JDBC database connections.&lt;/p&gt;
&lt;p&gt;Resources:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;[An Architecture for Fast and General Data Processing on Large Clusters][2] Matei Zaharia&lt;/li&gt;
&lt;li&gt;[Spark][6] Cluster Computing with Working Sets - Matei Zaharia et al.&lt;/li&gt;
&lt;li&gt;[Resilient Distributed Datasets][5] A Fault-Tolerant Abstraction for In-Memory Cluster Computing -Matei Zaharia et al.&lt;/li&gt;
&lt;li&gt;[Learning Spark][3] Lightning Fast Big Data Analysis - Oreilly&lt;/li&gt;
&lt;li&gt;[Advanced Analytics with Spark][4] Patterns for Learning from Data at Scale - Oreilly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[1]: &lt;a class="reference external" href="https://spark.apache.org"&gt;https://spark.apache.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2]: &lt;a class="reference external" href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf"&gt;http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3]: &lt;a class="reference external" href="http://shop.oreilly.com/product/0636920028512.do"&gt;http://shop.oreilly.com/product/0636920028512.do&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4]: &lt;a class="reference external" href="http://shop.oreilly.com/product/0636920035091.do"&gt;http://shop.oreilly.com/product/0636920035091.do&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5]: &lt;a class="reference external" href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf"&gt;https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6]: &lt;a class="reference external" href="http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf"&gt;http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Peter Hoffmann</dc:creator><pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-08-03:europython-2015/pyspark-data-processing-in-python-on-top-of-apache-spark.html</guid><category>pyspark</category></item></channel></rss>