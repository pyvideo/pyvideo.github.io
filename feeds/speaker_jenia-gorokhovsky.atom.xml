<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_jenia-gorokhovsky.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-06-05T00:00:00+00:00</updated><entry><title>Deep model serving - scale and ergonomics</title><link href="https://pyvideo.org/pycon-israel-2018/deep-model-serving-scale-and-ergonomics.html" rel="alternate"></link><published>2018-06-05T00:00:00+00:00</published><updated>2018-06-05T00:00:00+00:00</updated><author><name>Jenia Gorokhovsky</name></author><id>tag:pyvideo.org,2018-06-05:pycon-israel-2018/deep-model-serving-scale-and-ergonomics.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A serving system for Deep Learning models is a tricky design problem. It's a large scale production system, so we want it to scale well, adapt to changing traffic patterns, and have low latency. It’s also part of the Data Scientist’s core loop - so it should be very flexible, and running an experiment on live traffic should be easy. In this talk, I’ll discuss key design considerations for such a system covering both perspectives. I’ll also describe a system we built at Taboola for serving TensorFlow models. It serves billions of requests per day, spread over dozens of models, and still has pretty good ergonomics,&lt;/p&gt;
</summary></entry></feed>