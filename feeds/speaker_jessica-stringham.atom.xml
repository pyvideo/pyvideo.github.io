<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_jessica-stringham.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-05-19T00:00:00+00:00</updated><entry><title>Experiment Assignment on the Web</title><link href="https://pyvideo.org/pycon-us-2017/experiment-assignment-on-the-web.html" rel="alternate"></link><published>2017-05-19T00:00:00+00:00</published><updated>2017-05-19T00:00:00+00:00</updated><author><name>Jessica Stringham</name></author><id>tag:pyvideo.org,2017-05-19:pycon-us-2017/experiment-assignment-on-the-web.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A popular way of improving websites is to run experiments on it. We
split users into groups, show two or more variations of the site,
measure how well each one does, and then show the best version to
everyone. In this talk, I'll walk through a toy Python program that does
the first step: splits users into groups. A few interesting problems
arise: grouping users, whitelists, and scaling. I'll share different
ways to address them. I'll also give examples of things that can go
terribly wrong when designing experiment assignment code.&lt;/p&gt;
</summary></entry></feed>