<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Anmol Krishan Sachdeva</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_anmol-krishan-sachdeva.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-07-23T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Understanding and Implementing Recurrent Neural Networks using Python</title><link href="https://pyvideo.org/europython-2018/understanding-and-implementing-recurrent-neural-networks-using-python.html" rel="alternate"></link><published>2018-07-25T00:00:00+00:00</published><updated>2018-07-25T00:00:00+00:00</updated><author><name>Anmol Krishan Sachdeva</name></author><id>tag:pyvideo.org,2018-07-25:/europython-2018/understanding-and-implementing-recurrent-neural-networks-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recurrent Neural Networks (RNNs) have become famous over time due to
their property of retaining internal memory. These neural nets are
widely used in recognizing patterns in sequences of data, like numerical
timer series data, images, handwritten text, spoken words, genome
sequences, and much more. Since these nets possess …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recurrent Neural Networks (RNNs) have become famous over time due to
their property of retaining internal memory. These neural nets are
widely used in recognizing patterns in sequences of data, like numerical
timer series data, images, handwritten text, spoken words, genome
sequences, and much more. Since these nets possess memory, there is a
certain analogy that we can make to the human brain in order to learn
how RNNs work. RNNs can be thought of as a network of neurons with
feedback connections, unlike feedforward connections which exist in
other types of Artificial Neural Networks.&lt;/p&gt;
&lt;p&gt;The flow of talk will be as follows: - Self Introduction - Introduction
to Deep Learning - Artificial Neural Networks (ANNs) - Diving DEEP into
Recurrent Neural Networks (RNNs) - Comparing Feedforward Networks with
Feedback Networks - Quick walkthrough: Implementing RNNs using Python
(Keras) - Understanding Backpropagation Through Time (BPTT) and
Vanishing Gradient Problem - Towards more sophisticated RNNs: Gated
Recurrent Units (GRUs)/Long Short-Term Memory (LSTMs) - End of talk -
Questions and Answers Session&lt;/p&gt;
</content><category term="EuroPython 2018"></category></entry><entry><title>Painting with GANs: Challenges and Technicalities of Neural Style Transfer</title><link href="https://pyvideo.org/europython-2020/painting-with-gans-challenges-and-technicalities-of-neural-style-transfer.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>Anmol Krishan Sachdeva</name></author><id>tag:pyvideo.org,2020-07-23:/europython-2020/painting-with-gans-challenges-and-technicalities-of-neural-style-transfer.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Building Artistic Artefacts using Generative Networks&lt;/p&gt;
&lt;p&gt;A lot of advancements are happening in the field of Deep Learning and Generative Adversarial Networks are one of them. We have seen GANs being applied for photo editing and in-painting, generating new image datasets and realistic photographs, increasing resolution of images (Super …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Building Artistic Artefacts using Generative Networks&lt;/p&gt;
&lt;p&gt;A lot of advancements are happening in the field of Deep Learning and Generative Adversarial Networks are one of them. We have seen GANs being applied for photo editing and in-painting, generating new image datasets and realistic photographs, increasing resolution of images (Super Resolution), and many more things. Some people have also exploited GANs for generating fake content. All the above-mentioned examples are result of a technique where the focus is to generate uncommon yet original samples from scratch. However, these examples have very less commercial applications and GANs are capable of doing much more. The focus of this talk is a technique called &amp;quot;Neural Style Transfer (NST)&amp;quot; which has numerous commercial applications in the gaming world, fashion/design industry, mobile applications, and many more fields. Challenges and technicalities of NSTs will be covered in great detail. We will teach the machines on how to paint images and utilize Style Transfer networks to generate artistic artefacts.&lt;/p&gt;
&lt;p&gt;The flow of the talk will be as follows:
~ Self Introduction [1 minute]
~ A Succinct Prelude to GANs [10 minutes]
~ Understanding Style Transfer [5 minutes]
~ Learning about Neural Style Transfer Networks [5 minutes]
~ Loss Functions: Content, Style, Total Variantion [10 minutes]
~ Code Walkthrough and Result Analysis [5 minutes]
~ Challenges and Applications [5 minutes]
~ Questions and Answers Session [3-4 minutes]&lt;/p&gt;
</content><category term="EuroPython 2020"></category><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Computer Vision"></category><category term="Deep Learning"></category><category term="Generative Adversarial Networks"></category><category term="Image Processing"></category><category term="Machine-Learning"></category></entry></feed>