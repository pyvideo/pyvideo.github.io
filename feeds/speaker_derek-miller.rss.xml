<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Fri, 12 Oct 2018 00:00:00 +0000</lastBuildDate><item><title>Sparkflow: Utilizing Pyspark for Training Tensorflow Models on Large Datasets</title><link>https://pyvideo.org/pydata-indy-2018/sparkflow-utilizing-pyspark-for-training-tensorflow-models-on-large-datasets.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As more public, large datasets are becoming available, distributed data processing tools such as Apache Spark are vital for data scientists. While SparkML provides many machine learning algorithms, standard pipelines, and a basic linear algebra library, it does not support training deep learning models. Due to the rise of Tensorflow in the last two years, Lifeomic built the Sparkflow library to combine the power of the Pipeline api from Spark with training Deep Learning models in Tensorflow. Sparkflow uses the Hogwild algorithm to train deep learning models in a distributed manor, which underneath leverages the driver/executor architecture in Spark to manage copied networks and gradients. In this session, we describe some of the lessons learned in building Sparkflow, the pros and cons of asynchronous distributed deep learning, how to use Spark Pipelines with Tensorflow with very few lines of code, and where we are headed with the library in the near future.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Derek Miller</dc:creator><pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-10-12:pydata-indy-2018/sparkflow-utilizing-pyspark-for-training-tensorflow-models-on-large-datasets.html</guid><category>Pyspark</category><category>Tensorflow</category></item></channel></rss>