<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Anna P. Meyer</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_anna-p-meyer.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-07-31T00:00:00+00:00</updated><subtitle></subtitle><entry><title>On Minimizing the Impact of Dataset Shifts on Actionable Explanations</title><link href="https://pyvideo.org/uai-2023/on-minimizing-the-impact-of-dataset-shifts-on-actionable-explanations.html" rel="alternate"></link><published>2023-07-31T00:00:00+00:00</published><updated>2023-07-31T00:00:00+00:00</updated><author><name>Anna P. Meyer</name></author><id>tag:pyvideo.org,2023-07-31:/uai-2023/on-minimizing-the-impact-of-dataset-shifts-on-actionable-explanations.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;On Minimizing the Impact of Dataset Shifts on Actionable Explanations &amp;quot;
Anna P. Meyer, Dan Ley, Suraj Srinivas, Himabindu Lakkaraju
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/meyer23a.html"&gt;https://proceedings.mlr.press/v216/meyer23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
The Right to Explanation is an important regulatory principle that allows individuals to request actionable explanations for algorithmic decisions. However, several technical â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;On Minimizing the Impact of Dataset Shifts on Actionable Explanations &amp;quot;
Anna P. Meyer, Dan Ley, Suraj Srinivas, Himabindu Lakkaraju
(&lt;a class="reference external" href="https://proceedings.mlr.press/v216/meyer23a.html"&gt;https://proceedings.mlr.press/v216/meyer23a.html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Abstract
The Right to Explanation is an important regulatory principle that allows individuals to request actionable explanations for algorithmic decisions. However, several technical challenges arise when providing such actionable explanations in practice. For instance, models are periodically retrained to handle dataset shifts. This process may invalidate some of the previously prescribed explanations, thus rendering them unactionable. But, it is unclear if and when such invalidations occur, and what factors determine explanation stability i.e., if an explanation remains unchanged amidst model retraining due to dataset shifts. In this paper, we address the aforementioned gaps and provide one of the first theoretical and empirical characterizations of the factors influencing explanation stability. To this end, we conduct rigorous theoretical analysis to demonstrate that model curvature, weight decay parameters while training, and the magnitude of the dataset shift are key factors that determine the extent of explanation (in)stability. Extensive experimentation with real-world datasets not only validates our theoretical results, but also demonstrates that the aforementioned factors dramatically impact the stability of explanations produced by various state-of-the-art methods.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://www.auai.org/uai2023/oral_slides/517-oral-slides.pdf"&gt;https://www.auai.org/uai2023/oral_slides/517-oral-slides.pdf&lt;/a&gt;&lt;/p&gt;
</content><category term="UAI 2023"></category></entry></feed>