<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Samuel Rochette</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_samuel-rochette.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-11-04T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Quantifying uncertainty in machine learning models</title><link href="https://pyvideo.org/pydata-new-york-city-2019/quantifying-uncertainty-in-machine-learning-models.html" rel="alternate"></link><published>2019-11-04T00:00:00+00:00</published><updated>2019-11-04T00:00:00+00:00</updated><author><name>Samuel Rochette</name></author><id>tag:pyvideo.org,2019-11-04:/pydata-new-york-city-2019/quantifying-uncertainty-in-machine-learning-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many models give a lot more information during the inference process that we usually know. We will begin with an intrinsic estimation of all the distribution with random forest algorithm. Then we will extend those 'prediction intervals' to almost every regression models thanks to the quantile loss. Eventually we â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many models give a lot more information during the inference process that we usually know. We will begin with an intrinsic estimation of all the distribution with random forest algorithm. Then we will extend those 'prediction intervals' to almost every regression models thanks to the quantile loss. Eventually we will discuss about probability calibration to measure uncertainty in classification.&lt;/p&gt;
</content><category term="PyData New York City 2019"></category></entry></feed>