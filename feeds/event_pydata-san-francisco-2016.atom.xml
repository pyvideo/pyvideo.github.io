<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-san-francisco-2016.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-08-24T00:00:00+00:00</updated><entry><title>A hybrid approach to model randomness and fuzziness</title><link href="https://pyvideo.org/pydata-san-francisco-2016/a-hybrid-approach-to-model-randomness-and-fuzziness.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Taposh Roy</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/a-hybrid-approach-to-model-randomness-and-fuzziness.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Taposh Roy, Austin Powell | A hybrid approach to model randomness and fuzziness using Dempster Shaffer method&lt;/p&gt;
&lt;p&gt;Current main stream data science work is mainly focused on prediction, segmentation and data analysis. This mainly involves supervised learning where we learn from historic data. The predicted observations usually gives a probability score which measures randomness. We believe there are situations where along with randomness we need some fuzziness to give some confidence to the observation.&lt;/p&gt;
&lt;p&gt;Current main stream data science work is mainly focused on prediction, segmentation and data analysis. This mainly involves supervised learning where we learn from historic data. The predicted observations usually gives a probability score which measures randomness. We believe there are situations where along with randomness we need some fuzziness to give some confidence to the observation. Fuzziness and randomness are two very different concepts. We say that fuzziness occurs when there is no boundary between outcomes. And we commonly refer to randomness as the uncertainty associated with effective variability from alternative outputs. Although different, it is simple to provide real world scenarios where randomness and fuzziness work together. One example is in the stock market where there are no historical similarities to use in modeling such as new high patterns. Here, we say that there are no defined boundaries that forecast event probabilities in the next few days. Another example is in the emergency room where the early detection algorithm gives a risk score, but the environmental factors impacting the patient are not predictable. In both of these examples, we can fairly easily model the randomness, but there is a fuzziness that needs to be dealt with also. We will discuss a model we are researching that improves our predictability of that fuzziness and randomness. The heart of this improvement lies in how we approach variability. Variability of alternative outputs can describe randomness and is easily modeled using regression or other predictive analytic methods. Using fuzziness, a second source of variability can be individualized (per observation) in the vagueness with which the attributes are selected. This type of uncertainty is due to variety of environmental factors depending on the use case. For the stock market, uncertainty is due to a set of economic conditions and policy reforms that have not been looked at before, while for an emergency room scenario it might be due to variety of scenarios before the patient was hospitalized. Research recommends several different approaches to simulate the choice behavior in different choice contexts. Our focus is on designing choice models that can take into account one of two sources of uncertainty.&lt;/p&gt;
</summary></entry><entry><title>A Pratctical Introduction to Airflow</title><link href="https://pyvideo.org/pydata-san-francisco-2016/a-pratctical-introduction-to-airflow.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Matt Davis</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/a-pratctical-introduction-to-airflow.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Airflow is a pipeline orchestration tool for Python that allows users to configure multi-system workflows that are executed in parallel across workers. I’ll cover the basics of Airflow so you can start your Airflow journey on the right foot. This talk aims to answer questions such as: What is Airflow useful for? How do I get started? What do I need to know that’s not in the docs?&lt;/p&gt;
&lt;p&gt;Airflow is a popular pipeline orchestration tool for Python that allows users to configure complex (or simple!) multi-system workflows that are executed in parallel across any number of workers. A single pipeline might contain bash, Python, and SQL operations. With dependencies specified between tasks, Airflow knows which ones it can run in parallel and which ones must run after others. Airflow is written in Python and users can add their own operators with custom functionality, doing anything Python can do.&lt;/p&gt;
&lt;p&gt;Moving data through transformations and from one place to another is a big part of data science/engineering, but there are only two widely-used orchestration systems for doing so that are written in Python: Luigi and Airflow. We’ve been using Airflow (&lt;a class="reference external" href="http://pythonhosted.org/airflow/"&gt;http://pythonhosted.org/airflow/&lt;/a&gt;) for several months at Clover Health and have learned a lot about its strengths and weaknesses. We use it to run several pipelines multiple times per day. One includes over 450 heavily linked tasks!&lt;/p&gt;
</summary><category term="airflow"></category></entry><entry><title>Altair Declarative, Statistical Visualization for Python</title><link href="https://pyvideo.org/pydata-san-francisco-2016/altair-declarative-statistical-visualization-for-python.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Brian Granger</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/altair-declarative-statistical-visualization-for-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Altair is a declarative statistical visualization library for Python. Altair provides a user-centric Python API on top of the declarative visualization stack of Vega-Lite, Vega and D3.js. A wide range of statistical visualizations can be created with only a few carefully design abstractions.
Altair provides a Python API for building statistical visualizations in a declarative manner. By statistical visualization we mean:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The data source is a DataFrame that consists of columns of different data types (quantitative, ordinal, nominal and date/time).&lt;/li&gt;
&lt;li&gt;The DataFrame is in a tidy format where the rows correspond to samples and the columns correspond the observed variables.&lt;/li&gt;
&lt;li&gt;The data is mapped to the visual properties (position, color, size, shape, faceting, etc.) using the group-by operation of Pandas and SQL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Altair API contains no actual visualization rendering code but instead emits JSON data structures following the Vega-Lite specification. For convenience, Altair can optionally use ipyvega to display client-side renderings seamlessly in the Jupyter notebook.
Altair has the following features:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Carefully-designed, declarative Python API based on traitlets.&lt;/li&gt;
&lt;li&gt;Auto-generated internal Python API that guarantees visualizations are type-checked and in full conformance with the Vega-Lite specification.&lt;/li&gt;
&lt;li&gt;Auto-generate Altair Python code from a Vega-Lite JSON spec.&lt;/li&gt;
&lt;li&gt;Display visualizations in the live Jupyter Notebook, on GitHub and nbviewer.&lt;/li&gt;
&lt;li&gt;Export visualizations to PNG images, stand-alone HTML pages and the Online Vega-Lite Editor.&lt;/li&gt;
&lt;li&gt;Serialize visualizations as JSON files.&lt;/li&gt;
&lt;li&gt;Explore Altair with 40 example datasets and over 70 examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Altair is developed by Brian Granger and Jake Vanderplas in close collaboration with the UW Interactive Data Lab.&lt;/p&gt;
</summary></entry><entry><title>Anaconda Ecosystem for Open Data Science</title><link href="https://pyvideo.org/pydata-san-francisco-2016/anaconda-ecosystem-for-open-data-science.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Ian Stokes Rees</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/anaconda-ecosystem-for-open-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;The Anaconda Distribution has been pivotal for the adoption of Open Data Science since its creation in 2012. Today Anaconda has grown far beyond just a cross-platform scientific Python distribution into a rich ecosystem of users, services, software, and libraries. This talk will demonstrate some of the exciting features of the Anaconda ecosystem.&lt;/p&gt;
</summary></entry><entry><title>Applied Time Series Econometrics in Python and R</title><link href="https://pyvideo.org/pydata-san-francisco-2016/applied-time-series-econometrics-in-python-and-r.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Jeffrey Yau</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/applied-time-series-econometrics-in-python-and-r.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Time series data is ubitious, and time series statistical models should be included in any data scientists’ toolkit. This tutorial covers the mathematical formulation, statistical foundation, and practical considerations of one of the most important classes of time series models: the AutoRegression Integrated Moving Average with Explanatory Variables model and its seasonal counterpart.&lt;/p&gt;
&lt;p&gt;Time series data is ubitious, both within and out of the field of data science: weekly initial unemployment claim, tick level stock prices, weekly company sales, daily number of steps taken recorded by a wearable, just to name a few. Some of the most important and commonly used data science techniques to analyze time series data are those in developed in the field of statistics. For this reason, time series statistical models should be included in any data scientists’ toolkit.&lt;/p&gt;
&lt;p&gt;This 120-minute tutorial covers the mathematical formulation, statistical foundation, and practical considerations of one of the most important classes of time series models, AutoRegression Integrated Moving Average with Explanatory Variables (ARIMAX) models, and its Seasonal counterpart (SARIMAX).&lt;/p&gt;
</summary><category term="tutorial"></category></entry><entry><title>Applying machine learning to software development to reduce bugs</title><link href="https://pyvideo.org/pydata-san-francisco-2016/applying-machine-learning-to-software-development-to-reduce-bugs.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Nitin Borwankar</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/applying-machine-learning-to-software-development-to-reduce-bugs.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Nitin Borwankar | Applying machine learning to software development to reduce bugs&lt;/p&gt;
&lt;p&gt;This talk shows how we can reduce risk of failure in software development by using machine learning. Using example code and artifacts from Apache projects, and scikit-learn with Jupyter notebook, we show how to identify areas of risk in large codelines. Some surprising statistical results on distribution of risk in code are also shown., which suggest we may be &amp;quot;doing it wrong&amp;quot;.&lt;/p&gt;
&lt;p&gt;Software development is in a historical transition from a stage of organized craftsmanship to a stage of industrial production of software. As a part of this transition machines are doing more and more of the repetitive work freeing humans to do the creative work and decision making. However we are behind in the ability to extract risk signals from development artifacts of large projects. We are not getting better at reducing project failures. We can use machine learning applied to the development process to improve outcomes.&lt;/p&gt;
&lt;p&gt;This talk shows how change history and issue tracking data can be correlated to identify areas of risk concentration in the project. Simultaneously some statistical results show us areas where conventional methods may be wasting effort and areas where these may be ignoring risk.&lt;/p&gt;
&lt;p&gt;Demo uses example code and artifacts from Apache projects, and scikit-learn with Jupyter notebook.&lt;/p&gt;
</summary></entry><entry><title>Architectures for Exploring Your Personal Data</title><link href="https://pyvideo.org/pydata-san-francisco-2016/architectures-for-exploring-your-personal-data.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Candida Haynes</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/architectures-for-exploring-your-personal-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Candida Haynes | Architectures for Exploring Your Personal Data&lt;/p&gt;
&lt;p&gt;As you emerge from three days of PyData learning experiences, fortified by exposure to life-changing data science tools and inspiring stories, this interactive talk will allow you to reflect on personal data project architectures of data past, present, and yet to come. Adventurous? Look into the future and test your chops by going through a beginner-friendly, hands-on process in which you analyze your own data to inform how that might influence your objective data science practice.&lt;/p&gt;
</summary></entry><entry><title>Build Data Apps by Deploying ML Models as API Services</title><link href="https://pyvideo.org/pydata-san-francisco-2016/build-data-apps-by-deploying-ml-models-as-api-services.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Ramesh Sampath</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/build-data-apps-by-deploying-ml-models-as-api-services.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Ramesh Sampath | Build Data Apps by Deploying ML Models as API Services&lt;/p&gt;
&lt;p&gt;As data scientists, we love building models using IPython Notebooks / Scikit-Learn / Pandas eco-system. But integrating these models with an web app can be a challenge. In this tutorial, we will take our machine learning models and make them available as APIs for use by Web and Mobile Apps. We will also build a simple webapp that uses our prediction service.&lt;/p&gt;
&lt;p&gt;Deploy your ML Models as a Service&lt;/p&gt;
&lt;p&gt;In this talk, we will learn one way to take our Machine Learning models and make them available as a Prediction Service. We will work through the following steps.&lt;/p&gt;
&lt;p&gt;Create a Simple Machine learning Model using Scikit-Learn / Pandas
Pickle the model
Using Tornado Web App, Make this model available as an API Service
Build an Web App that uses this deployed Model
Add Authentication to our Prediction API
Optionally, add Redis to Cache Prediction Results
Deploy the model in the Cloud (AWS)
Please have Anaconda or Miniconda installed on your local machine. I will mostly be using Python 3.5, but Python 2.7 should be fine as well.&lt;/p&gt;
</summary><category term="tutorial"></category><category term="machine learning"></category><category term="scikit-learn"></category><category term="pandas"></category><category term="tornado"></category></entry><entry><title>Building domain specific databases using Python for prototypes during take off</title><link href="https://pyvideo.org/pydata-san-francisco-2016/building-domain-specific-databases-using-python-for-prototypes-during-take-off.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Josh Yudaken</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/building-domain-specific-databases-using-python-for-prototypes-during-take-off.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Smyte is blocking fraudsters, spammers, scammers and harassers through analysis of web and mobile event data. There are hundreds of new database options cropping up, and while most of them are great they all make a range of different design choices which may/may not align with what you need.&lt;/p&gt;
&lt;p&gt;At Smyte we're solving infrastructure-heavy problems with a tiny engineering team. We've had to build custom databases in order to efficiently solve problems, but don't have the luxury of a ton of development time. In this talk I'll go through how we used Kafka &amp;amp; Python to implement a prototype Sliding HyperLogLog server. It held up for the six months we needed to, and some early decisions made it trivial to port the server to our new efficient C++ &amp;amp; RocksDB stack.&lt;/p&gt;
</summary></entry><entry><title>Building Recommender Systems Using Python</title><link href="https://pyvideo.org/pydata-san-francisco-2016/building-recommender-systems-using-python.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Divya Sardana</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/building-recommender-systems-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;This tutorial is about learning to build a recommender system in Python. The audience will learn the intuition behind different types of recommender systems and specifically implement three of them in python. They will get to learn how to evaluate recommender systems using precision and recall curves on a song dataset.&lt;/p&gt;
&lt;p&gt;The tutorial will start with an emphasis on learning the concepts behind recommender systems. Then, we will build three variants of recommender systems in Python.&lt;/p&gt;
</summary></entry><entry><title>Carousel A Python Model Simulation Framework</title><link href="https://pyvideo.org/pydata-san-francisco-2016/carousel-a-python-model-simulation-framework.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Mark Mikofski</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/carousel-a-python-model-simulation-framework.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Carousel is an extensible framework for mathematical models that handles generic routines such as loading and saving data, generating reports, converting units, propagating uncertainty and running simulations so developers can focus on creating complex algorithms that are easy to share and maintain.&lt;/p&gt;
&lt;p&gt;Introduction&lt;/p&gt;
&lt;p&gt;Mathematical models consist of algorithms glued together with generic routines. While the algorithms may sometimes be unique and complex, the rest of the code is often simple and routine. Sometimes mathematical models developed by teams of developers over time become difficult to update because there is no framework for how new data, calculations and outputs are integrated into the existing models. Carousel allows developers to focus on creating complex mathematical models that are robust and easy to maintain by abstracting generic routines and establishing a simple but extensible framework.&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://github.com/SunPower/Carousel/wiki/Carousel-PyData-SFO-2016.pdf"&gt;https://github.com/SunPower/Carousel/wiki/Carousel-PyData-SFO-2016.pdf&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Closing the Machine Learning Loop using Python</title><link href="https://pyvideo.org/pydata-san-francisco-2016/closing-the-machine-learning-loop-using-python.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Randall Shane</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/closing-the-machine-learning-loop-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;This session provides insight into developing machine learning systems that improve through closing the feedback loop, re-evaluate accuracy and generating more intelligent output given each iteration of data.&lt;/p&gt;
</summary></entry><entry><title>Clustering Data Science Interviews Seven Related but Distinct Categories</title><link href="https://pyvideo.org/pydata-san-francisco-2016/clustering-data-science-interviews-seven-related-but-distinct-categories.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Samantha Zeitlin</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/clustering-data-science-interviews-seven-related-but-distinct-categories.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Matching candidates with openings: defining features across several sets of Data Scientist selection criteria, using both qualitative and quantitative methods.&lt;/p&gt;
&lt;p&gt;For both candidates and interviewers, the interview process doesn't lend itself to the classic scientific method. We can't iterate over an interaction, and needs change. Deciding what to ask and who to interview can be a guessing game. As an interviewer, what kind of candidate do you really need? As a candidate, what kind of role do you really want? Having done more than 30 phone screens and technical challenges, I can define seven distinct types of Data Scientist interviews. I'll give examples of questions specifically relevant to assessing each aspect of a Data Scientist's skills, as well as discussing what's not being measured by the typical interview process.&lt;/p&gt;
</summary></entry><entry><title>Community sustainability in Wikipedia a review of research and initiatives</title><link href="https://pyvideo.org/pydata-san-francisco-2016/community-sustainability-in-wikipedia-a-review-of-research-and-initiatives.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Stuart Geiger</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/community-sustainability-in-wikipedia-a-review-of-research-and-initiatives.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Wikipedia relies on one of the world’s largest open collaboration communities. Since 2001, the community has grown substantially and faced many challenges. This presentation reviews research and initiatives around community sustainability in Wikipedia that are relevant for many open source projects, including issues of newcomer retention, governance, automated moderation, and marginalized groups.&lt;/p&gt;
</summary></entry><entry><title>Creating Knowledgebases from unstructured text</title><link href="https://pyvideo.org/pydata-san-francisco-2016/creating-knowledgebases-from-unstructured-text.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Sanghamitra Deb</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/creating-knowledgebases-from-unstructured-text.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;NLP and Machine Learning without training data.&lt;/p&gt;
&lt;p&gt;A major part of Big Data collected in most industries is in the form of unstructured text. Some examples are log files in IT sector, analysts reports in the finance sector, patents, laboratory notes and papers, etc. Some of the challenges of gaining insights from unstructred text is converting it into structured information and generating training sets for machine learning. Typically training sets for supervised learning are generated through the process of human annotation. In case of text this involves reading several thousands to million lines of texts by subject matter experts. This is very expensive and may not always be available, hence it is important to solve the problem of generating training sets before attempting to build machine learning models. Our approach is to combine rule based techniques with small amounts of SME time to by pass time consuming manual creation of training data. Once we have a good set of rules mimicking the training data we will use them to create knowledgebases out of the structured data. This knowledgebase can be further queried to gain insight on the domain. I have applied this technique to several domains, such as data from drug labels and medical journals, log data generated through customer interaction, generation of market research reports, etc. I will talk about the results in some of these domains and the advantage of using this approach.&lt;/p&gt;
</summary></entry><entry><title>Data Applications with Bokeh</title><link href="https://pyvideo.org/pydata-san-francisco-2016/data-applications-with-bokeh.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Bryan Van De Ven</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/data-applications-with-bokeh.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Data Applications with Bokeh&lt;/p&gt;
</summary></entry><entry><title>Derivation &amp; Presentation: How to Effectively Tell A Data Science Story</title><link href="https://pyvideo.org/pydata-san-francisco-2016/derivation-presentation-how-to-effectively-tell-a-data-science-story.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Gregory Kamradt</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/derivation-presentation-how-to-effectively-tell-a-data-science-story.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Gregory Kamradt | Derivation &amp;amp; Presentation: How to Effectively Tell A Data Science Story In A Business Environment |&lt;/p&gt;
&lt;p&gt;Data Science is essential for driving business value in the 21st century. This talk will focus on:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Doing data science in the work place through a business lens and&lt;/li&gt;
&lt;li&gt;Communicating that data message to your external stakeholders to drive actionable change.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The audience will leave with an alternative understanding of take home tests, interviews, data communication and ultimately ...story-telling.&lt;/p&gt;
</summary></entry><entry><title>Fighting Against Chaotically Separated Values with Embulk</title><link href="https://pyvideo.org/pydata-san-francisco-2016/fighting-against-chaotically-separated-values-with-embulk.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Sadayuki Furuhashi</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/fighting-against-chaotically-separated-values-with-embulk.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Python is a great tool for performing data analysis, but often time the hardest part is getting access to your data that’s located in a variety of business systems - files, database, and SaaS applications. Productionizing this process is even harder: scripts frequently fail and require precious to to fix and re-test. In this talk, I will review some open source tools I authored and show you how&lt;/p&gt;
&lt;p&gt;In this talk we will cover:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;How we created a data collection tool that can read any chaotically formatted files called &amp;quot;CSV&amp;quot; by guessing its structure automatically&lt;/li&gt;
&lt;li&gt;Explore the plugin-based-architecture that makes it easy to load data from external sources and publish to production systems. From files to business systems such as Salesforce &amp;amp; Mixpanel.&lt;/li&gt;
&lt;li&gt;Review current plugins (over 100 released by the OSS community) and use cases&lt;/li&gt;
&lt;li&gt;Explain how distributed execution enhances stability and scalability&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="embulk"></category></entry><entry><title>How Soon is Now: extracting publication dates with machine learning</title><link href="https://pyvideo.org/pydata-san-francisco-2016/how-soon-is-now-extracting-publication-dates-with-machine-learning.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Julie Lavoie</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/how-soon-is-now-extracting-publication-dates-with-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Scraping New York Times articles for publication dates is easy, scraping 10 000 different sites is hard. Beyond page-specific scraping, how do you build a parser than can extract the publication date of (almost) any news article online, no matter what the site is? We implemented a research paper in machine learning to solve this problem, and talk about the challenges we faced.&lt;/p&gt;
&lt;p&gt;We’ll cover when to use machine learning vs. humans or heuristics for data extraction, the different steps of how to phrase the problem in terms of machine learning, including feature selection on HTML documents, and issues that arise when turning research into production code. Data scientists and developers will leave knowing how to extract information from the web using new and more sophisticated techniques than simply writing a scraper.&lt;/p&gt;
</summary><category term="scraping"></category></entry><entry><title>How you really get your data science models into production the cool way!</title><link href="https://pyvideo.org/pydata-san-francisco-2016/how-you-really-get-your-data-science-models-into-production-the-cool-way.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Dat Tran</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/how-you-really-get-your-data-science-models-into-production-the-cool-way.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;This talk discusses one of Pivotal Labs’ core principles, API first, and how this can help to overcome the common language problem between data scientists and software engineers. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Tools&lt;/li&gt;
&lt;li&gt;Software engineering methodologies like continuous deployment and TDD&lt;/li&gt;
&lt;li&gt;Microservices&lt;/li&gt;
&lt;li&gt;PaaS and Cloud Native Data Science&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Over the years we have seen many non-tech companies starting to build their own data science teams because they realize that data will eat the world. In an effort to understand the space, these teams have begun to play with their data and create early prototypes. Unfortunately, those prototypes primarily end up in powerpoint and die. Of the ones that move forward, there is a gap in knowledge of their development team in how to release to production.&lt;/p&gt;
&lt;p&gt;From our experience, we’ve seen their data science and development team do not speak a common language. At Pivotal Labs, we found a good way to overcome this language problem. Our solution is to follow an API first approach which is one of our core principles.&lt;/p&gt;
&lt;p&gt;In this talk, I want to to share my experiences of how to put these models into production. I will focus on the tools that we use for this and what data science has to do with microservices. My presentation will contain an end-to-end data science example.&lt;/p&gt;
</summary></entry><entry><title>improv BOF How to Scrape Data from any Website</title><link href="https://pyvideo.org/pydata-san-francisco-2016/improv-bof-how-to-scrape-data-from-any-website.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Greg Dingle</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/improv-bof-how-to-scrape-data-from-any-website.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;BOF - How to Scrape Data from any Website&lt;/p&gt;
&lt;p&gt;Scraping gets hard--fast--when you need deal with dynamic content (read: javascript), pagination, and changing page structures. In other words, most popular websites built in the last 10 years.&lt;/p&gt;
&lt;p&gt;Learn how we built ParseHub to cope with these problems. Try the same techniques in python with the open-source Scrapy framework.&lt;/p&gt;
</summary></entry><entry><title>Introduction to Julia</title><link href="https://pyvideo.org/pydata-san-francisco-2016/introduction-to-julia.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Tony Kelman</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/introduction-to-julia.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Introduction to Julia&lt;/p&gt;
</summary></entry><entry><title>JupyterLab</title><link href="https://pyvideo.org/pydata-san-francisco-2016/jupyterlab.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Jamie Whitacre</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/jupyterlab.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;This talk provides an early view of JupyterLab, an evolution of the Jupyter Notebook that provides a modular and extensible user interface within the context of a powerful workspace.&lt;/p&gt;
&lt;p&gt;Project Jupyter provides building blocks for interactive and exploratory computing. These building blocks make science and data science reproducible across over 40 programming language (Python, Julia, R, etc.). Central to the project is the Jupyter Notebook, a web-based interactive computing platform that allows users to author data- and code-driven narratives - computational narratives - that combine live code, equations, narrative text, visualizations, interactive dashboards and other media.&lt;/p&gt;
&lt;p&gt;The fundamental idea of JupyterLab is to offer a user interface that supports interactive workflows that include, but go far beyond, Jupyter Notebooks. In JupyterLab, users can arrange multiple notebooks, text editors, terminals, output areas, etc. on a single page with multiple panels, tabs, splitters, and collapsible sidebars with a file browser, command palette and integrated help system. The codebase and UI of JupyterLab is based on a flexible plugin system that makes it easy to extend with new components.&lt;/p&gt;
</summary></entry><entry><title>Keynote: Empowering people by democratizing data skills</title><link href="https://pyvideo.org/pydata-san-francisco-2016/keynote-empowering-people-by-democratizing-data-skills.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Tracy Teal</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/keynote-empowering-people-by-democratizing-data-skills.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Empowering people by democratizing data skills - Tracy Teal&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>Keynote: Python for Pythonistas</title><link href="https://pyvideo.org/pydata-san-francisco-2016/keynote-python-for-pythonistas.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Peter Wang</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/keynote-python-for-pythonistas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Python for Pythonistas - Peter Wang&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>Keynote: Using the Python Data Science Stack to Determine</title><link href="https://pyvideo.org/pydata-san-francisco-2016/keynote-using-the-python-data-science-stack-to-determine.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Megan Price</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/keynote-using-the-python-data-science-stack-to-determine.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Megan Price | Keynote: Using the Python Data Science Stack to Determine How Many People Have Been Killed in Syria&lt;/p&gt;
&lt;p&gt;The Human Rights Data Analysis Group (HRDAG) uses methods from statistics and computer science to quantify mass violence. As part of that work, we rely on open source tools, including python and R, for data processing, management, analysis, and visualization. This talk will highlight how we use those tools to estimate how many people have been killed in the ongoing conflict in Syria.&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>Keynote: Working Efficiently with Big Data in Text Formats</title><link href="https://pyvideo.org/pydata-san-francisco-2016/keynote-working-efficiently-with-big-data-in-text-formats.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>David Mertz</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/keynote-working-efficiently-with-big-data-in-text-formats.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;In an ideal world, all our large datasets would live in well optimized storage formats, such as RDBMS's, key-value NoSQL stores, HDF5 hierarchical datasets, or other formats that are well typed and fast to access. In our actual world, a great deal of our data lives in CSV, flat-file, or JSON formats, roughly stored on file systems, with little typing of data values. Moreover, data in these formats often have variably sized records making seeking data a linear scan operation.&lt;/p&gt;
&lt;p&gt;Continuum Analytics has produced a custom optimized library called IOPro that includes a component called TextAdapter. TextAdapter provides abstractions to data access into these textual formats that adds much better data typing, minimizes memory use, uses indexing for seeking, and other facilities for better, faster data access without requiring conversion of exploratory datasets into permanent optimized formats. We will be releasing this code as an Open Source project, and plan on enhancing the library to allow further performance optimizations and integration with the Dask project.&lt;/p&gt;
&lt;p&gt;As well as looking at technical and performance details of TextAdapter, this talk will discuss the economic and social concerns of company developed and supported Open Source projects. Continuum continues to explore some of these issues through our release of TextAdapter, following on company trajectory of moving projects from proprietary to open source status whenever reasonable.&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>Large Scale CTR Prediction Lessons Learned</title><link href="https://pyvideo.org/pydata-san-francisco-2016/large-scale-ctr-prediction-lessons-learned.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Florian Hartl</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/large-scale-ctr-prediction-lessons-learned.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Starting with a basic setup for click-through rate (CTR) prediction, we will step by step improve on it by incorporating the lessons we've learned from operating and scaling such a mission-critical system. The presented lessons will be related to infrastructure, model comprehension, and specifics like how to deal with thresholds. They should be applicable to most ML models used in production.&lt;/p&gt;
&lt;p&gt;After briefly introducing Yelp and more specifically click-through rate (CTR) prediction at Yelp, we will start out with a basic setup for model-based predictions in a production system. From there we will point out deficiencies of said setup in various areas, some of which arise especially in large scale environments or when predicting CTRs.&lt;/p&gt;
</summary></entry><entry><title>LIghtning Talks</title><link href="https://pyvideo.org/pydata-san-francisco-2016/lightning-talks.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/lightning-talks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Lightning Talks&lt;/p&gt;
</summary><category term="lightning talks"></category></entry><entry><title>Machine Learning vs The Flu</title><link href="https://pyvideo.org/pydata-san-francisco-2016/machine-learning-vs-the-flu.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Rohan Koodli</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/machine-learning-vs-the-flu.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Rohan Koodli | Machine Learning vs The Flu, Creating better Flu Vaccines with Python and Machine Learning&lt;/p&gt;
&lt;p&gt;Millions of people are affected by the flu each year. I wanted to create a better way for scientists to make flu vaccines. My solution was to predict future flu genetic sequences so that scientists could easily analyze flu sequences and create vaccines. The main topics will include the use of Biopython and scikit-learn in a scientific environment, as well as data preprocessing and model selection.&lt;/p&gt;
&lt;p&gt;Every year, millions of people are affected by the flu. Every year, we take a vaccine to possibly gain immunity from that year’s flu. However, the vaccines may not always work. In recent years, people have made attempts at predicting how the influenza virus has changed. For example, every year, world health officials try to predict what drugs to include in the coming year’s vaccine. However, many of the methods used have been called “questionable” by the National Institutes of Medicine. So, my goal in this project was to come up with a better way for scientists to make flu vaccines.&lt;/p&gt;
&lt;p&gt;In my talk, I will show how I used Python to implement my flu prediction algorithm, as well as how I used libraries to make my algorithm more efficient and simpler in every step.&lt;/p&gt;
</summary></entry><entry><title>Mental Models to Use and Avoid as a Data Scientist</title><link href="https://pyvideo.org/pydata-san-francisco-2016/mental-models-to-use-and-avoid-as-a-data-scientist.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Jonathan Whitmore</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/mental-models-to-use-and-avoid-as-a-data-scientist.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Using Jupyter Notebooks and Python code, we will present several data-driven examples of some simple, powerful, yet relatively uncommon, ways of thinking as a good Data Scientist. We will also warn about a few dangerous ways of thinking to avoid. Our Jupyter Notebooks and slides will be made freely available after the talk.&lt;/p&gt;
</summary></entry><entry><title>Moving Forward Through The Darkness</title><link href="https://pyvideo.org/pydata-san-francisco-2016/moving-forward-through-the-darkness.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Chia Chi Chang</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/moving-forward-through-the-darkness.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;The blindness of modeling and how to break through. Training models from data is just like painting or capturing pictures from the world. From different angles, you will see the different pictures. The pictures are just the approximations of the world! So do the models! There must be some blindnesses behind the approximations. In this talk, we will point out several types of blindnesses in modeling procedures and introduce the solution of breaking through them.&lt;/p&gt;
</summary></entry><entry><title>Nipype</title><link href="https://pyvideo.org/pydata-san-francisco-2016/nipype.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Chris Gorgolewski</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/nipype.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;The ​Nipype ​project ​unifies the way hundreds of brain imaging tools can be run​ and​ also provides ​​means to quickly prototype image analysis pipelines as well as deploy them at scale both at high performance clusters as well as the cloud. Nipype is being used in labs, hospitals and SaaS companies around the world and has a strong open source community of developers.&lt;/p&gt;
&lt;p&gt;Human brain imaging has seen an enormous growth in terms of data analysis methods in the past 25 years. However, many of those methods are implemented in a plethora of different labs using heterogenous tools, languages, and data management standards. ​The Neuroimaging in Python (Nipy) community provides an ecosystem of scientific tools that build on the PyData stack. The ​Nipype ​project ​unifies the way hundreds of brain imaging tools can be run​ and​ also provides ​​means to quickly prototype image analysis pipelines as well as deploy them at scale both at high performance clusters as well as the cloud. Nipype is being used in labs, hospitals and SaaS companies around the world and has a strong open source community of developers.&lt;/p&gt;
</summary></entry><entry><title>Overview of the NumFOCUS Data Science Stack</title><link href="https://pyvideo.org/pydata-san-francisco-2016/overview-of-the-numfocus-data-science-stack.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/overview-of-the-numfocus-data-science-stack.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Overview of the sponsored tools in NumFOCUS' Data Science Stack&lt;/p&gt;
</summary></entry><entry><title>Pandas, Data Wrangling &amp; Data Science</title><link href="https://pyvideo.org/pydata-san-francisco-2016/pandas-data-wrangling-data-science.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Krishna Sankar</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/pandas-data-wrangling-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Krishna Sankar | Pandas, Data Wrangling &amp;amp; Data Science&lt;/p&gt;
&lt;p&gt;Let us explore Pandas from a Data Science perspective, mainly data exploration &amp;amp; feature extraction. In the process we will also ponder Data Science pragmas. We start with Pandas fundamentals and then move on to analyzing datasets. If you want to follow along, have a working iPython, download the notebooks at &lt;a class="reference external" href="https://github.com/xsankar/cautious-octo-waffle"&gt;https://github.com/xsankar/cautious-octo-waffle&lt;/a&gt; and the data. Run PreFlightCheck.ipynb.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Data Wrangling &amp;amp; Data Science Pipeline&lt;/li&gt;
&lt;li&gt;Pandas – APIs &amp;amp; Namespaces&lt;/li&gt;
&lt;li&gt;Pandas – Basic Maneuvers&lt;/li&gt;
&lt;li&gt;Hands-on : Titanic Dataset&lt;/li&gt;
&lt;li&gt;Pandas – Data Wrangling – Transformations, Aggregations &amp;amp; Join&lt;/li&gt;
&lt;li&gt;Hands-on : NW Dataset, State Of The Union Speeches &amp;amp; Debates, Recsys-2015 Data&lt;/li&gt;
&lt;li&gt;Q &amp;amp; A&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="pandas"></category></entry><entry><title>Python Visualization for Exploration of Data</title><link href="https://pyvideo.org/pydata-san-francisco-2016/python-visualization-for-exploration-of-data.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Stephen F Elston</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/python-visualization-for-exploration-of-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Visualization is an essential method in any data scientist’s toolbox. Visualization is a key first step in the exploration of most data sets. Visualization is also a powerful tool for presentation of results and for determining sources of problems with analytics. This tutorial introduces attendees to the most commonly used Python visualization packages, matplotlib, pandas plotting and seaborn.&lt;/p&gt;
&lt;p&gt;Visualization of complex real-world datasets presents a number of challenges to data scientists. By developing skills in data visualization, data scientist can confidently explore and understand the relationships in complex data sets while undertaking analyses&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/Quantia-Analytics/DyDataSF2016Visualization"&gt;https://github.com/Quantia-Analytics/DyDataSF2016Visualization&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Resampling techniques and other strategies</title><link href="https://pyvideo.org/pydata-san-francisco-2016/resampling-techniques-and-other-strategies.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Ajinkya More</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/resampling-techniques-and-other-strategies.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Ajinkya More | Resampling techniques and other strategies for handling highly unbalanced datasets in classification&lt;/p&gt;
&lt;p&gt;Many real world machine learning problems need to deal with imbalanced class distribution i.e. when one class has significantly higher/lower representation than the other classes. Often performance on the minority class is more crucial, e.g. fraud detection, product classification, medical diagnosis, etc. In this talk I will discuss several techniques to handle class imbalance in classification.&lt;/p&gt;
</summary></entry><entry><title>Searchable datasets in Python</title><link href="https://pyvideo.org/pydata-san-francisco-2016/searchable-datasets-in-python.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Dani Ushizima</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/searchable-datasets-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Dani Ushizima, Flavio Araujo, Romuere Silva | Searchable datasets in Python: images across domains, experiments, algorithms and learning&lt;/p&gt;
&lt;p&gt;pyCBIR is a new python tool for content-based image retrieval (CBIR) capable of searching relevant items in large databases, given unseen samples. While much work in CBIR has targeted ads and recommendation systems, our pyCBIR allows general purpose investigation across image domains. Also, pyCBIR contains ten distance metrics, and six bags of features, including a Convolutional Neural Network.&lt;/p&gt;
&lt;p&gt;Image capture turned into an ubiquitous activity in our daily lives, but mechanisms to organize and retrieve images based on their content are available only to a few people or to very specific problems. With the significant improvement in image processing speeds and availability of large storage systems, the development of methods to query and retrieve images is fundamental to simple human activities like cataloguing and complex research such as synthesizing materials. Content-Based Image Retrieval (CBIR) systems use computer vision techniques to describe images in terms of its properties in order to search similar samples given an image itself as a query, instead of keywords. For this reason, the system works independently of annotations, which can be time consuming and impossible in some scenarios, e.g. high-throughput imaging instruments.
While much work in CBIR has targeted ads and recommendation systems, our pyCBIR allows general purpose investigation across image domains and experiments. Also, pyCBIR contains different distance metrics, and several feature extraction techniques, including a Convolutional Neural Network (CNN).&lt;/p&gt;
</summary></entry><entry><title>Similarity Search in Patents via modern Artificial Intelligence</title><link href="https://pyvideo.org/pydata-san-francisco-2016/similarity-search-in-patents-via-modern-artificial-intelligence.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Sumeet Sandhu</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/similarity-search-in-patents-via-modern-artificial-intelligence.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;We will describe our new Patent Management Tool built using modern Artificial Intelligence (Deep Learning) in Python. The AI generates data-based vocabularies used in ‘similarity’ search - allowing a one-click lookup of semantically related words that would otherwise be missed by the user. Preliminary results show that ‘similarity’ search outperforms the traditional keyword-Boolean patent search.&lt;/p&gt;
</summary></entry><entry><title>The "NumPy" Approach</title><link href="https://pyvideo.org/pydata-san-francisco-2016/the-numpy-approach.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/the-numpy-approach.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;The &amp;quot;NumPy&amp;quot; Approach&lt;/p&gt;
</summary></entry><entry><title>Time Series for Python with PyFlux</title><link href="https://pyvideo.org/pydata-san-francisco-2016/time-series-for-python-with-pyflux.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Ross Taylor</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/time-series-for-python-with-pyflux.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;PyFlux is a new library for time series analysis for Python. It brings together a vast array of time series models, including recent models such as score-driven models and variational state space models, as well a flexible choice of inference options, including black box variational inference. In this talk I will introduce some of the features, with some fun applications to sports modelling.&lt;/p&gt;
&lt;p&gt;This talk will introduce the PyFlux library for time series analysis in Python. I will walk through the modelling and inference options in an accessible, high-level way. I will focus on score-driven (GAS) models, which are a new flexible alternative to traditional time series models. This Python library represents the first comprehensive implementation of a GAS library, a model type that has the potential to be as widespread as ARIMA models. I will demonstrate the usefulness of the models through some fun examples in PyFlux, such as a power rating model for NFL football teams, as well as some finance examples, which I shall attempt to make fun (challenge accepted).&lt;/p&gt;
</summary></entry><entry><title>TrailDB tutorial Store and process billions of events efficiently</title><link href="https://pyvideo.org/pydata-san-francisco-2016/traildb-tutorial-store-and-process-billions-of-events-efficiently.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Ville Tuulos</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/traildb-tutorial-store-and-process-billions-of-events-efficiently.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Ville Tuulos | TrailDB tutorial Store and process billions of events efficiently&lt;/p&gt;
&lt;p&gt;TrailDB is an efficient library for storing and querying series of events. It shines at compressing a large number of discrete events in a small space - often small enough to allow processing on a single server. TrailDB is implemented in C and it comes with Python bindings.&lt;/p&gt;
&lt;p&gt;This is a hands-on tutorial that gets you started with TrailDB. Bring your own data or play with a public data set.&lt;/p&gt;
&lt;p&gt;What is TrailDB?&lt;/p&gt;
&lt;p&gt;TrailDB is designed to be a core building block for systems that need to store and process a large number of discrete events, organized by a primary key. It is complementary to existing relational and time-series databases and key-value stores.&lt;/p&gt;
&lt;p&gt;What makes TrailDB different is immutability: Immutable data enables deeper compression, scalability, and architectural decisions, which would not be feasible with existing databases. This is especially true for cloud environments with object stores like Amazon S3 that are a perfect match for compressed, immutable files.&lt;/p&gt;
&lt;p&gt;Developer productivity is another main motivation of TrailDB. Individual files are easy to manipulate using standard filesystem tools. The easily portable C library has only a few easily available dependencies, making it easily deployable. The API is clean and minimal by design. Language bindings are provided for Python, Go, Haskell, R, and D.&lt;/p&gt;
&lt;p&gt;TrailDB is a perfect match for use cases that involve detecting patterns over time, such as web/mobile analytics, anomaly detection, and various machine learning models. Since 2014, AdRoll has used TrailDB to store and query over 20 trillion events that power a number of products at AdRoll. TrailDB was open-sourced in May 2016.&lt;/p&gt;
</summary></entry><entry><title>Transfer Learning and Finetuning Deep Convolution Neural Network</title><link href="https://pyvideo.org/pydata-san-francisco-2016/transfer-learning-and-finetuning-deep-convolution-neural-network.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Anusua Trivedi</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/transfer-learning-and-finetuning-deep-convolution-neural-network.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Anusua Trivedi | Transfer Learning and Finetuning Deep Convolution Neural Network on Different Domain Specific Images&lt;/p&gt;
&lt;p&gt;We propose a method to apply a pre-trained deep convolution neural network (DCNN) on images to improve prediction accuracy. We use a pre-trained DCNN on two very different domain specific datasets, and apply fine-tuning to transfer the learned features to the prediction. Our approach improves prediction accuracy on both domain-specific datasets, compared to state-of-the-art approaches.&lt;/p&gt;
&lt;p&gt;In this talk, we propose prediction techniques using deep learning on different types of images datasets – medical images and fashion images.&lt;/p&gt;
&lt;p&gt;We show how to build a generic deep learning model, which could be used with:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A fluorescein angiographic eye image to predict Diabetic Retinopathy&lt;/li&gt;
&lt;li&gt;A fashion image to predict the clothing type in that image&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We propose a method to apply a pre-trained deep convolution neural network (DCNN) on images to improve prediction accuracy. We use an ImageNet pre-trained DCNN and apply fine-tuning to transfer the learned features to the prediction.&lt;/p&gt;
&lt;p&gt;We use this fine-tuned model on two very different domain specific datasets. Our approach improves prediction accuracy on both domain-specific datasets, compared to state-of-the-art Machine Learning approaches.&lt;/p&gt;
</summary></entry><entry><title>Virtual Laboratories Vision for an Earth Sciences</title><link href="https://pyvideo.org/pydata-san-francisco-2016/virtual-laboratories-vision-for-an-earth-sciences.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Charles Doutriaux</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/virtual-laboratories-vision-for-an-earth-sciences.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016
Charles Doutriaux | Virtual Laboratories Vision for an Earth Sciences Collaborative And Distributed Environment&lt;/p&gt;
&lt;p&gt;Earth Sciences and Climate Sciences in particular have become invaluable for policy makers. With advances in technologies the amount of data available is ever increasing making in situ analysis impossible. We envision to merge the Eart System Grid Federation and Climate Data Analysis Tools projects into Virtual Laboratories offering a remote distributed environment for Earth Sciences.&lt;/p&gt;
&lt;p&gt;With the advent of IPCC reports, Earth Sciences in general, and Climate Sciences in particular, have become an invaluable tool for policy makers. With advances in technologies, both in computation power and in storage capabilities, the amount of data available to scientists and policy makers is ever increasing. As a result, it has become nearly impossible for a scientific group (institution), to hold all the necessary data for a study at a single computer (facility). On one hand the Earth Science Grid Federation (ESGF) project solves the data distribution, on another hand the Climate Data Analysis Tools software offers advanced tools for earth science data analysis. “Virtual Laboratories” aim at combining both projects (non-exclusively) to allow scientists and policy makers to work in a remote and distributed environment.&lt;/p&gt;
</summary></entry><entry><title>Visualizing Geographic Data With Python</title><link href="https://pyvideo.org/pydata-san-francisco-2016/visualizing-geographic-data-with-python.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Christopher Roach</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/visualizing-geographic-data-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;The statistician George Box once wrote that “all models are wrong, but some are useful”; the same could be said for maps. In this talk, we’ll discuss the problems that arise when creating 2-dimensional representations of our world. We'll then see how to create data-rich maps using Python, matplotlib, and the basemap toolkit. we'll also see how to create maps for the web using the Folium library.&lt;/p&gt;
&lt;p&gt;Maps have been such a mainstay of our lives for so long now that it's hard to imagine just how complex it is to create one. Keep in mind though, the earth is a 3-dimensional spherical object, so we're stuck with the problem of &amp;quot;projecting&amp;quot; the world onto a 2-dimensional surface. What this means is that every map you've ever looked at was wrong in some way.&lt;/p&gt;
&lt;p&gt;In this talk, we’ll discuss what a map projection is, and why the Mercator projection, the map you use everyday, is both incorrect and unfair, but useful nonetheless. We’ll also see some ways that we can create maps using Python. We’ll first see how to create data-rich maps using the matplotlib library with the basemap toolkit. We’ll then see how to create maps for the web using libraries like Folium, the python interface to Leaflet.js.&lt;/p&gt;
&lt;p&gt;By the end of this talk, you should have a general understanding of the problems surrounding the creation of effective maps. You should also feel comfortable picking out a proper map projection and plotting data on it using a multitude of techniques and the Python language.&lt;/p&gt;
</summary></entry><entry><title>What's New in Python 3</title><link href="https://pyvideo.org/pydata-san-francisco-2016/whats-new-in-python-3.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/whats-new-in-python-3.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;What's New in Python 3 - James Powell&lt;/p&gt;
</summary></entry><entry><title>Why is Windows so broken? and what to do about it</title><link href="https://pyvideo.org/pydata-san-francisco-2016/why-is-windows-so-broken-and-what-to-do-about-it.html" rel="alternate"></link><published>2016-08-24T00:00:00+00:00</published><updated>2016-08-24T00:00:00+00:00</updated><author><name>Steve Dower</name></author><id>tag:pyvideo.org,2016-08-24:pydata-san-francisco-2016/why-is-windows-so-broken-and-what-to-do-about-it.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData SF 2016&lt;/p&gt;
&lt;p&gt;Python on Windows has a reputation for being, well, pretty broken. But the good news is that things have been getting better. Join Microsoft employee and CPython core developer Steve Dower as he walks through the recent improvements to the Python ecosystem on Windows, and how things will get even better in the future.&lt;/p&gt;
</summary></entry></feed>