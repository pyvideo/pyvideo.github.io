<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_dask.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-12-07T00:00:00+00:00</updated><entry><title>Introducting Dask-Gateway: Dask clusters as a service</title><link href="https://pyvideo.org/pydata-austin-2019/introducting-dask-gateway-dask-clusters-as-a-service.html" rel="alternate"></link><published>2019-12-07T00:00:00+00:00</published><updated>2019-12-07T00:00:00+00:00</updated><author><name>Jim Crist</name></author><id>tag:pyvideo.org,2019-12-07:pydata-austin-2019/introducting-dask-gateway-dask-clusters-as-a-service.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask-Gateway provides a secure, multi-tenant server for managing Dask clusters. It allows users to launch and use Dask clusters in a shared, centrally managed environment, and supports a wide variety of backends (e.g. Kubernetes, Hadoop, HPC systems, etc…). In this talk we'll discuss the use and design of Dask-Gateway, as well as some of the issues we encountered while developing this tool.&lt;/p&gt;
</summary><category term="dask"></category><category term="dask-gateway"></category></entry><entry><title>Meet dask and distributed: the unsung heroes of Python scientific data ecosystem.</title><link href="https://pyvideo.org/pycon-italia-2019/meet-dask-and-distributed-the-unsung-heroes-of-python-scientific-data-ecosystem.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Alessandro Amici</name></author><id>tag:pyvideo.org,2019-05-03:pycon-italia-2019/meet-dask-and-distributed-the-unsung-heroes-of-python-scientific-data-ecosystem.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Thanks to its world-class data tools and libraries, like Numpy, Pandas,
Jupyter, Matplotlib and xarray, Python is becoming the language of
choice in many scientific communities from Physics to Climate Science,
from Earth Observation to Economy.&lt;/p&gt;
&lt;p&gt;A turn-key but less-know component of the scientific ecosystem is the
dask library that enable seamless parallel, distributed and GPU
computing in most cases without code changes.&lt;/p&gt;
&lt;p&gt;We will use climate science as an typical example of a discipline where
simple tasks become easily big data problems and where mastering xarray,
dask and dask.distributed is the key to turn them back into simple
tasks, possibly on a large cluster of VMs (that you can easily provision
from your preferred cloud provider).&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://gitpitch.com/alexamici/talks/master?p=PyConX-2019"&gt;https://gitpitch.com/alexamici/talks/master?p=PyConX-2019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1704"&gt;https://python.it/feedback-1704&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 17:15 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="Jupyter"></category><category term="dask.distributed"></category><category term="Big-Data"></category><category term="xarray"></category><category term="dask"></category><category term="climate-change"></category><category term="earth-obeservation"></category><category term="pandas"></category></entry><entry><title>Democratizing Distributed Computing with Dask and JupyterHub</title><link href="https://pyvideo.org/pycon-us-2018/democratizing-distributed-computing-with-dask-and-jupyterhub.html" rel="alternate"></link><published>2018-05-12T00:00:00+00:00</published><updated>2018-05-12T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2018-05-12:pycon-us-2018/democratizing-distributed-computing-with-dask-and-jupyterhub.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We use JupyterHub, XArray, Dask, and Kubernetes to build a cloud-based system to enable scientists to analyze and manage large datasets.  We use this in practice to serve a broad community of atmospheric and climate scientists.&lt;/p&gt;
&lt;p&gt;Atmospheric and climate scientists analyze large volumes of observational and simulated data to better understand our planet.  They have historically used tools like NumPy and SciPy along with Jupyter notebooks to combine efficient computation with accessibility.  However, as datasets increase in size and collaboration extends to new populations of scientists these tools begin to feel their age.  In this talk we use more recent libraries to build a modern deployment for academic scientists.  In particular we use the following tools:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Dask:&lt;/strong&gt; to parallelize and scale NumPy computations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;XArray&lt;/strong&gt;: as a self-discribing data model and tool kit for labeled and index arrays&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JupyterLab:&lt;/strong&gt; to enable more APIs for users beyond the classic notebook&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JupyterHub:&lt;/strong&gt; to manage users and maintain environments for a new population of cloud-friendly users&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubernetes:&lt;/strong&gt; to manage everything and deploy easily on cloud hardware&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This talk will focus less on how these libraries work and will instead be a case study of using them together in an operational setting.  During the talk we will build up and deploy a running system that the audience can then use to access distributed computing resources.&lt;/p&gt;
</summary><category term="jupyterhub"></category><category term="xarray"></category><category term="dask"></category><category term="kubernetes"></category></entry><entry><title>Parallel Data Analysis with Dask</title><link href="https://pyvideo.org/pycon-us-2018/parallel-data-analysis-with-dask.html" rel="alternate"></link><published>2018-05-09T00:00:00+00:00</published><updated>2018-05-09T00:00:00+00:00</updated><author><name>Tom Augspurger</name></author><id>tag:pyvideo.org,2018-05-09:pycon-us-2018/parallel-data-analysis-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The libraries that power data analysis in Python are essentially limited to a single CPU core and to datasets that fit in RAM. Attendees will see how dask can parallelize their workflows, while still writing what looks like normal python, NumPy, or pandas code.&lt;/p&gt;
&lt;p&gt;Dask is a parallel computing framework, with a focus on analytical computing. We'll start with &lt;cite&gt;dask.delayed&lt;/cite&gt;, which helps parallelize your existing Python code. We’ll demonstrate &lt;cite&gt;dask.delayed&lt;/cite&gt; on a small example, introducing the concepts at the heart of dask like the &lt;em&gt;task graph&lt;/em&gt; and the &lt;em&gt;schedulers&lt;/em&gt; that execute tasks. We’ll compare this approach to the simpler, but less flexible, parallelization methods available in the standard library like &lt;cite&gt;concurrent.futures&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Attendees will see the high-level collections dask provides for writing regular Python, NumPy, or Pandas code that is then executed in parallel on datasets that may be larger than memory. These high level collections provide a familiar API, but the execution model is very different. We'll discuss concepts like the GIL, serialization, and other headaches that come up with parallel programming. We’ll use dask’s various schedulers to illustrate the differences between multi-threaded, multi-processes, and distributed computing.&lt;/p&gt;
&lt;p&gt;Dask includes a distributed scheduler for executing task graphs on a cluster of machines. We’ll provide each person access to their own cluster.&lt;/p&gt;
</summary><category term="dask"></category></entry><entry><title>Data Science at Scale Using dask and Numba</title><link href="https://pyvideo.org/pycon-israel-2017/data-science-at-scale-using-dask-and-numba.html" rel="alternate"></link><published>2017-06-13T00:00:00+00:00</published><updated>2017-06-13T00:00:00+00:00</updated><author><name>Yigal Weinberger</name></author><id>tag:pyvideo.org,2017-06-13:pycon-israel-2017/data-science-at-scale-using-dask-and-numba.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Today there is a great need for improved performance and scalability even in the early stages of the data science process, in my talk I will discuss the limitations of pandas in terms of performance and show a few hands on examples for common data processing procedure using two frameworks: Dask and Numba&lt;/p&gt;
</summary><category term="dask"></category><category term="numba"></category></entry><entry><title>Make it Work, Make it Right, Make it Fast Debugging and Profiling in Dask</title><link href="https://pyvideo.org/scipy-2017/make-it-work-make-it-right-make-it-fast-debugging-and-profiling-in-dask.html" rel="alternate"></link><published>2017-07-17T00:00:00+00:00</published><updated>2017-07-17T00:00:00+00:00</updated><author><name>James Crist</name></author><id>tag:pyvideo.org,2017-07-17:scipy-2017/make-it-work-make-it-right-make-it-fast-debugging-and-profiling-in-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a pure Python library for parallel and distributed computing. It's designed with simplicity and flexibility in mind, making it easy to parallelize the kind of messy custom workflows that often show up in science. However, once you get something working, how do you debug or profile it? Debugging and profiling parallel code is notoriously hard! In this talk we'll cover the various tools Dask provides for diagnosing bugs and performance bottlenecks, as well as tips and techniques for resolving these issues.&lt;/p&gt;
</summary><category term="dask"></category></entry><entry><title>Parallelizing Scientific Python with Dask</title><link href="https://pyvideo.org/scipy-2017/parallelizing-scientific-python-with-dask.html" rel="alternate"></link><published>2017-07-13T00:00:00+00:00</published><updated>2017-07-13T00:00:00+00:00</updated><author><name>James Crist</name></author><id>tag:pyvideo.org,2017-07-13:scipy-2017/parallelizing-scientific-python-with-dask.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Tutorial materials found here: &lt;a class="reference external" href="https://scipy2017.scipy.org/ehome/220975/493423/"&gt;https://scipy2017.scipy.org/ehome/220975/493423/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dask is a flexible tool for parallelizing Python code on a single machine or across a cluster. We can think of dask at a high and a low level:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;High level collections: Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and Pandas but can operate in parallel on datasets that don't fit into main memory. Dask's high-level collections are alternatives to NumPy and Pandas for large datasets.&lt;/li&gt;
&lt;li&gt;Low Level schedulers: Dask provides dynamic task schedulers that execute task graphs in parallel. These execution engines power the high-level collections mentioned above but can also power custom, user-defined workloads. These schedulers are low-latency (around 200 us) and work hard to run computations in a small memory footprint. Dask's schedulers are an alternative to direct use of threading or multiprocessing libraries in complex cases or other task scheduling systems like Luigi or IPython parallel.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Different users operate at different levels but it is useful to understand both. This tutorial will cover both the high-level use of dask.array and dask.dataframe and low-level use of dask graphs and schedulers.&lt;/p&gt;
</summary><category term="tutorial"></category><category term="dask"></category></entry><entry><title>Dask for ad hoc distributed computing</title><link href="https://pyvideo.org/pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/dask-for-ad-hoc-distributed-computing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk discusses parallel and distributed computing in Python, particularly for ad-hoc and custom algorithms. It focuses on Dask, a Python solution for flexible distributed computing.&lt;/p&gt;
&lt;p&gt;The Python data science stack contains efficient algorithms with intuitive interfaces for sophisticated and friendly analysis. As the data science community tackles larger problems with larger hardware we naturally ask how best to parallelize this software stack both across many cores in a single computer and across computers in a cluster. This turns out to be harder than it looks, even with traditional Big Data tools like MapReduce, Storm, and Spark. Both the complexity of the algorithms and the high expectations for interactivity raise challenges for these systems. This talk lays out the benefits and challenges of parallelizing a numeric analytic stack, and then describes Dask, a parallel framework gaining traction within the Python community for interactive performant parallel computing, and finally goes through a few domains where this work is enabling novel science today.&lt;/p&gt;
</summary><category term="dask"></category><category term="distributed"></category></entry><entry><title>Using Dask for Parallel Computing in Python</title><link href="https://pyvideo.org/pydata-dc-2016/using-dask-for-parallel-computing-in-python.html" rel="alternate"></link><published>2016-10-07T00:00:00+00:00</published><updated>2016-10-07T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2016-10-07:pydata-dc-2016/using-dask-for-parallel-computing-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Dask is a relatively new library for parallel computing in Python. It builds around familiar data structures to users of the PyData stack and enables them to scale up their work on one or many machines. This tutorial will introduce users to the core concepts of dask by working through some example problems. The tutorial will be distributed via Jupyter Notebooks.&lt;/p&gt;
</summary><category term="dask"></category><category term="parallel"></category><category term="parallel computing"></category></entry><entry><title>Using Dask for Parallel Computing in Python</title><link href="https://pyvideo.org/pydata-chicago-2016/using-dask-for-parallel-computing-in-python.html" rel="alternate"></link><published>2016-08-26T00:00:00+00:00</published><updated>2016-08-26T00:00:00+00:00</updated><author><name>Skipper Seabold</name></author><id>tag:pyvideo.org,2016-08-26:pydata-chicago-2016/using-dask-for-parallel-computing-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Dask is a relatively new library for parallel computing in Python. It builds around familiar data structures to users of the PyData stack and enables them to scale up their work on one or many machines. This tutorial will introduce users to the core concepts of dask by working through some example problems. The tutorial will be distributed via Jupyter Notebooks.&lt;/p&gt;
</summary><category term="dask"></category><category term="parallel"></category><category term="parallel computing"></category></entry><entry><title>Dask Parallel and Distributed Computing</title><link href="https://pyvideo.org/scipy-2016/dask-parallel-and-distributed-computing-scipy-2016-matthew-rocklin.html" rel="alternate"></link><published>2016-07-14T00:00:00+00:00</published><updated>2016-07-14T00:00:00+00:00</updated><author><name>Matthew Rocklin</name></author><id>tag:pyvideo.org,2016-07-14:scipy-2016/dask-parallel-and-distributed-computing-scipy-2016-matthew-rocklin.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Dask is a pure Python library for parallel and distributed computing. Last year Dask parallelized NumPy and Pandas computations on multi-core workstations. This year we discuss using Dask to design custom algorithms and execute those algorithms efficiently on a cluster. This talk discusses Pythonic APIs for parallel algorithm development as well as strategies for intuitive and efficient distributed computing. We discuss recent results in machine learning and novel scientific applications.&lt;/p&gt;
</summary><category term="SciPy 2016"></category><category term="dask"></category><category term="parallel computing"></category></entry></feed>