<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sun, 26 Jul 2015 00:00:00 +0000</lastBuildDate><item><title>Counterfactual evaluation of machine learning models</title><link>https://pyvideo.org/pydata-seattle-2015/counterfactual-evaluation-of-machine-learning-models.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Machine learning models often result in actions: search results are reordered, fraudulent transactions are blocked, etc. But how do you evaluate model performance when you are altering the distribution of outcomes? I'll describe how injecting randomness in production allows you to evaluate current models correctly and generate unbiased training data for new models.&lt;/p&gt;
&lt;p&gt;Stripe processes billions of dollars in payments a year and uses machine learning to detect and stop fraudulent transactions. Like models used for ad and search ranking, Stripe's models don't just score---they dictate actions that directly change outcomes. High-scoring transactions are blocked before they can ever get refunded or disputed by the card holder. Deploying an initial model that successfully blocks a substantial amount of fraud is a great first step, but since your model is altering outcomes, subsequent parts of the modeling process become more difficult:&lt;/p&gt;
&lt;p&gt;How do you evaluate the model? You can't observe the eventual outcomes of the transactions you block (would they have been refunded or disputed?) or the ads you didn't show (would they have been clicked?) In general, how do you quantify the difference between the world with the model and the world without it?&lt;/p&gt;
&lt;p&gt;How do you train new models? If your current model is blocking a lot of transactions, you have substantially fewer samples of fraud for your new training set. Furthermore, if your current model detects and blocks some types of fraud more than others, any new model you train will be biased towards detecting that residual fraud. Ideally, new models would be trained on the &amp;quot;unconditional&amp;quot; distribution that exists in the absence of the original model.&lt;/p&gt;
&lt;p&gt;In this talk, I'll describe how injecting a small amount of randomness in the production scoring environment allows you to answer these questions. We'll see how to obtain estimates of precision and recall (standard measures of model performance) from production data and how to approximate the distribution of samples that would exist in a world without the original model so that new models can be trained soundly.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;a class="reference external" href="http://www.slideshare.net/MichaelManapat/counterfactual-evaluation-of-machine-learning-models"&gt;http://www.slideshare.net/MichaelManapat/counterfactual-evaluation-of-machine-learning-models&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Michael Manapat</dc:creator><pubDate>Sun, 26 Jul 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-07-26:pydata-seattle-2015/counterfactual-evaluation-of-machine-learning-models.html</guid></item></channel></rss>