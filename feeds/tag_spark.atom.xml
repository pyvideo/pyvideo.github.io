<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/tag_spark.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2019-05-03T00:00:00+00:00</updated><entry><title>Geospatial analysis with Python</title><link href="https://pyvideo.org/pycon-italia-2019/geospatial-analysis-with-python.html" rel="alternate"></link><published>2019-05-03T00:00:00+00:00</published><updated>2019-05-03T00:00:00+00:00</updated><author><name>Francesco Bruni</name></author><id>tag:pyvideo.org,2019-05-03:pycon-italia-2019/geospatial-analysis-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Due to its effectiveness and simplicity, Python is spreading as a choice
for handling geospatial data. From running algorithms capable of
extracting geo- referred insights or processing geo archives, Python
could represent a powerful tool to handle geo-related problems thanks to
an extensive set of libraries. The training will give an overview about
processing geospatial data with Python. An approximate agenda will
include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction&lt;ul&gt;
&lt;li&gt;setting up the the environment&lt;/li&gt;
&lt;li&gt;an introduction to geospatial world&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Working with geospatial data&lt;ul&gt;
&lt;li&gt;playing with geo archives (vector/rasters)&lt;/li&gt;
&lt;li&gt;extracting geo analytics&lt;/li&gt;
&lt;li&gt;Vector tiles big vector data&lt;/li&gt;
&lt;li&gt;Apache Spark for geospatial raster analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Python for Earth observation&lt;ul&gt;
&lt;li&gt;Classifying earth observation images&lt;/li&gt;
&lt;li&gt;Extracting insights from Copernicus products&lt;/li&gt;
&lt;li&gt;Use SNAP from Python&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No tools are required for attending this training. Bring your PC with
Docker installed. Further instructions will be provided.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feedback form:&lt;/strong&gt; &lt;a class="reference external" href="https://python.it/feedback-1674"&gt;https://python.it/feedback-1674&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;in __on &lt;strong&gt;Friday 3 May&lt;/strong&gt; at 10:30 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See
schedule**&lt;/a&gt; in __on &lt;strong&gt;Saturday 4
May&lt;/strong&gt; at 18:00 &lt;a class="reference external" href="/en/sprints/schedule/pycon10/"&gt;**See schedule**&lt;/a&gt;&lt;/p&gt;
</summary><category term="Jupyter"></category><category term="pyspark"></category><category term="geospatial"></category><category term="geopynotebook"></category><category term="spark"></category><category term="docker"></category></entry><entry><title>Building Analytics Workflow using Airflow and Spark</title><link href="https://pyvideo.org/pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html" rel="alternate"></link><published>2019-02-23T00:00:00+00:00</published><updated>2019-02-23T00:00:00+00:00</updated><author><name>Yohei Onishi</name></author><id>tag:pyvideo.org,2019-02-23:pycon-philippines-2019/building-analytics-workflow-using-airflow-and-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Yohei had built and operates a data analytics system for global retail logistics operations using Airflow and Spark since the end of last year. In this session, He will talk about how you can build a scalable analytics workflow system based on Airflow (Python) and write extensible job using Python. GCP has provided fully managed Airflow service called Cloud Composer. So he will explain how you can easily build Airflow cluster compared to building your own Airflow cluster on the on-premise server or AWS EC2.&lt;/p&gt;
</summary><category term="airflow"></category><category term="spark"></category><category term="analytics"></category></entry><entry><title>Sparking Pandas: an experiment</title><link href="https://pyvideo.org/pycon-italia-2017/sparking-pandas-an-experiment.html" rel="alternate"></link><published>2017-04-07T00:00:00+00:00</published><updated>2017-04-07T00:00:00+00:00</updated><author><name>Francesco Bruni</name></author><id>tag:pyvideo.org,2017-04-07:pycon-italia-2017/sparking-pandas-an-experiment.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a good library to deal with tabular data. What if you need to
manage an amount of data that doesn’t fit into memory? What if you want
to “distribute” your computations among multiple machines?&lt;/p&gt;
&lt;p&gt;Starting from a real scenario, Apache Spark will be presented as the
main tool to read and process collected data. It will be shown how a
Pandas-like syntax will come in handy to run aggregations, filtering and
grouping using a Spark Dataframe.&lt;/p&gt;
&lt;p&gt;A previous knowledge of Docker and Docker Compose will be very useful
while knowing MongoDB (where data will be fetched from) is not
mandatory. Basics of functional programming will help to understand
Spark inner logic.&lt;/p&gt;
</summary><category term="microservices"></category><category term="Jupyter"></category><category term="mongodb"></category><category term="data-visualization"></category><category term="data-analysis"></category><category term="spark"></category><category term="docker"></category></entry><entry><title>Improving PySpark Performance Spark performance beyond the JVM</title><link href="https://pyvideo.org/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</summary><category term="jvm"></category><category term="performance"></category><category term="pyspark"></category><category term="spark"></category></entry><entry><title>Hassle Free ETL with PySpark</title><link href="https://pyvideo.org/pygotham-2016/hassle-free-etl-with-pyspark.html" rel="alternate"></link><published>2016-07-16T00:00:00+00:00</published><updated>2016-07-16T00:00:00+00:00</updated><author><name>Rob Howley</name></author><id>tag:pyvideo.org,2016-07-16:pygotham-2016/hassle-free-etl-with-pyspark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While the models of data science get all the press, the real work is in the maze of data preprocessing and pipelines. The goal of this talk is to get a glimpse into how you can use Python and the distributed power of Spark to simplify your (data) life, ditch the ETL boilerplate and get to the insights. We’ll intro PySpark and considerations in ETL jobs with respect to code structure and performance.&lt;/p&gt;
</summary><category term="spark"></category></entry></feed>