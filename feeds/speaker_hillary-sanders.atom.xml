<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Hillary Sanders</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_hillary-sanders.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2025-05-15T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Building Scalable AI Tool Servers with Model Context Protocol (MCP) and Heroku (Sponsor: Heroku)</title><link href="https://pyvideo.org/pycon-us-2025/building-scalable-ai-tool-servers-with-model-context-protocol-mcp-and-heroku-sponsor-heroku.html" rel="alternate"></link><published>2025-05-15T00:00:00+00:00</published><updated>2025-05-15T00:00:00+00:00</updated><author><name>Hillary Sanders</name></author><id>tag:pyvideo.org,2025-05-15:/pycon-us-2025/building-scalable-ai-tool-servers-with-model-context-protocol-mcp-and-heroku-sponsor-heroku.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large Language Models (LLMs) become vastly more powerful when given the ability to call external tools to gather information and take real-world actions - such as querying APIs, modifying databases, or kicking off workflows. This talk explores how to build and scale these tool servers using Python and the Model …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large Language Models (LLMs) become vastly more powerful when given the ability to call external tools to gather information and take real-world actions - such as querying APIs, modifying databases, or kicking off workflows. This talk explores how to build and scale these tool servers using Python and the Model Context Protocol (MCP).&lt;/p&gt;
&lt;p&gt;MCP is an open-source protocol that provides a standardized request format and response structure for LLMs interacting with external tools, ensuring consistency while allowing flexible execution under the hood. We’ll walk through implementing an MCP-compliant tool server in Python, covering topics like transport types, the connection lifecycle, and best practices.&lt;/p&gt;
&lt;p&gt;LLM tool servers must handle increasing traffic efficiently, requiring strategies for load balancing, container orchestration, and cloud deployment. We’ll discuss scaling Python services horizontally using load balancing and container orchestration. This includes practical considerations for deploying on 12-Factor App platforms like Heroku.&lt;/p&gt;
&lt;p&gt;Through live coding, we’ll implement, deploy, and scale an MCP-compliant tool server using Python. We’ll also demonstrate how an LLM can interact with MCP tool servers, enabling you to build powerful, API-driven AI agents.&lt;/p&gt;
&lt;p&gt;Attendees will gain hands-on knowledge of building, deploying, and scaling Python-based MCP tool servers and AI agents, made simple with Heroku’s streamlined deployment process.&lt;/p&gt;
</content><category term="PyCon US 2025"></category></entry></feed>