<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_jetze-schuurmans.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2018-05-26T00:00:00+00:00</updated><entry><title>(Re)training word embeddings for a specific domain</title><link href="https://pyvideo.org/pydata-amsterdam-2018/retraining-word-embeddings-for-a-specific-domain.html" rel="alternate"></link><published>2018-05-26T00:00:00+00:00</published><updated>2018-05-26T00:00:00+00:00</updated><author><name>Jetze Schuurmans</name></author><id>tag:pyvideo.org,2018-05-26:pydata-amsterdam-2018/retraining-word-embeddings-for-a-specific-domain.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Word embeddings (like GloVe, fastText and word2vec) are very powerful for capturing general word semantics. What if your use case is domain specific? Will your embeddings still work? If they donâ€™t, how do you retrain them?&lt;/p&gt;
</summary></entry></feed>