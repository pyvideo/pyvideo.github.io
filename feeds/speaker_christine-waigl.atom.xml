<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_christine-waigl.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-05-21T00:00:00+00:00</updated><entry><title>The Next Step: Finding Model Parameters With Random Walks</title><link href="https://pyvideo.org/pycon-us-2017/the-next-step-finding-model-parameters-with-random-walks.html" rel="alternate"></link><published>2017-05-21T00:00:00+00:00</published><updated>2017-05-21T00:00:00+00:00</updated><author><name>Christine Waigl</name></author><id>tag:pyvideo.org,2017-05-21:pycon-us-2017/the-next-step-finding-model-parameters-with-random-walks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The statistician John Tukey -- who designed the box plot and coined the
term &amp;quot;bit&amp;quot; -- wrote: &amp;quot;An approximate answer to the right problem is
worth a good deal more than an exact answer to an approximate problem&amp;quot;.
Python has become one of the major languages for statistical data
analysis, not least because of the expressiveness of the language itself
and the availability of tools like Jupyter Notebooks, which enable
iterative reasoning about a problem and its solutions.&lt;/p&gt;
&lt;p&gt;This talks takes one step beyond an introduction to statistics with
Python and aims to familiarize the audience with two concepts: a class
of problems (so-called inverse problems), and a powerful statistical
tool (the random walk, or more formally Markov-Chain Monte Carlo (MCMC)
sampling with the Metropolis algorithm).&lt;/p&gt;
&lt;p&gt;In inverse problems, model parameters are estimated from observational
data. Both model and data are expected to be affected by error. The
objective is not only to find parameters that best describe the
observations, but also to figure out how good, or how possibly bad, a
solution might be. Inverse problems are extremely common in many fields
and crop up each time we attempt to reconstruct a reality from sensor,
radar, scattering or imaging data.&lt;/p&gt;
&lt;p&gt;The Metropololis-Hastings algorithm offers a solution via random
sampling of a Bayesian posterior distribution. Even though listed as one
of the 20th century's top 10 algorithms by the journal &lt;em&gt;Computing in
Science &amp;amp; Engineering&lt;/em&gt;, the Metropolis algorithm is easy to understand
and implement, and a fun and instructive way to explore even complicated
multi-variate probability distributions.&lt;/p&gt;
</summary></entry></feed>