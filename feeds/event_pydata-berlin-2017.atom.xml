<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-berlin-2017.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2017-06-30T00:00:00+00:00</updated><entry><title>A word is worth a thousand pictures: Convolutional methods for text</title><link href="https://pyvideo.org/pydata-berlin-2017/a-word-is-worth-a-thousand-pictures-convolutional-methods-for-text.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Tal Perry</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/a-word-is-worth-a-thousand-pictures-convolutional-methods-for-text.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Link to slides: &lt;a class="reference external" href="https://www.slideshare.net/secret/2a5Xz9Sgc3D5GU"&gt;https://www.slideshare.net/secret/2a5Xz9Sgc3D5GU&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Description
Those folks in computer vision keep publishing amazing ideas about you to apply convolutions to images. What about those of us who work with text? Can't we enjoy convolutions as well? In this talk I'll review some convolutional architectures that worked great for images and were adapted to text and confront the hardest parts of getting them to work in Tensorflow .&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The go to architecture for deep learning on sequences such as text is the RNN and particularly LSTM variants. While remarkably effective, RNNs are painfully slow due their sequential nature. Convolutions allow us to process a whole sequence in parallel greatly reducing the time required to train and infer. One of the most important advances in convolutional architectures has been the use of gating to concur the vanishing gradient problem thus allowing arbitrarily deep networks to be trained efficiently.&lt;/p&gt;
&lt;p&gt;In this talk we'll review the key innovations in the DenseNet architecture and show how to adapt it to text. We'll go over &amp;quot;deconvolution&amp;quot; operators and dilated convolutions as means of handling long range dependencies. Finally we'll look at convolutions applied to [translation] (&lt;a class="reference external" href="https://arxiv.org/abs/1610.10099"&gt;https://arxiv.org/abs/1610.10099&lt;/a&gt;) at the character level.&lt;/p&gt;
&lt;p&gt;The goal of this talk is to demonstrate the practical advantages and relative ease with which these methods can be applied, as such we will focus on the ideas and implementations (in tensorflow) more than on the math.&lt;/p&gt;
</summary></entry><entry><title>Advanced Metaphors in Coding with Python</title><link href="https://pyvideo.org/pydata-berlin-2017/advanced-metaphors-in-coding-with-python.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/advanced-metaphors-in-coding-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R.&lt;/p&gt;
&lt;p&gt;PyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.&lt;/p&gt;
</summary><category term="tutorial"></category></entry><entry><title>AI assisted creativity</title><link href="https://pyvideo.org/pydata-berlin-2017/ai-assisted-creativity.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Roelof Pieters</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/ai-assisted-creativity.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A new wave of creative applications of AI has arrived, making science fiction authors struggle to keep up with reality. Recent advances in Deep Learning, especially generative models, make it possible to generate text, audio, speech, and images. There's a wonderfully trippy world of neural nets &amp;quot;going wild&amp;quot; out there, which you, the python enthusiastic, can be part of...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A new wave of creative applications of AI has arrived, making science fiction authors struggle to keep up with reality. Recent advances in Deep Learning, especially generative models, make it possible to generate text, audio, speech, and images. There's a wonderfully trippy world of neural nets &amp;quot;going wild&amp;quot; out there, with AI choreographed dancing moves, freestyle raps, impressionist paintings, and Trump impersonating bots. Such &amp;quot;bots&amp;quot; and experiments are but one novel use of this kind of &amp;quot;Creative AI&amp;quot;. Taking a more human-centered approach, allowing for control and agency, has the potential to turn these content-generating neural nets, into tools for creative use and explorations of human-machine interaction, where the main theorem is &amp;quot;augmentation, not automation&amp;quot;. The talk will particularly focus on &amp;quot;generative&amp;quot; models, and show the python fanatic how to make your move with these particular forms of Deep Neural Nets.&lt;/p&gt;
</summary></entry><entry><title>Analysing user comments with Doc2Vec and Machine Learning classification</title><link href="https://pyvideo.org/pydata-berlin-2017/analysing-user-comments-with-doc2vec-and-machine-learning-classification.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Robert Meyer</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/analysing-user-comments-with-doc2vec-and-machine-learning-classification.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;I used the Doc2Vec framework to analyze user comments on German online news articles and uncovered some interesting relations among the data. Furthermore, I fed the resulting Doc2Vec document embeddings as inputs to a supervised machine learning classifier. Can we determine for a particular user comment from which news site it originated?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Doc2Vec is a nice neural network framework for text analysis. The machine learning technique computes so called document and word embeddings, i.e. vector representations of documents and words. These representations can be used to uncover semantic relations. For instance, Doc2Vec may learn that the word &amp;quot;King&amp;quot; is similar to &amp;quot;Queen&amp;quot; but less so to &amp;quot;Database&amp;quot;.&lt;/p&gt;
&lt;p&gt;I used the Doc2Vec framework to analyze user comments on German online news articles and uncovered some interesting relations among the data. Furthermore, I fed the resulting Doc2Vec document embeddings as inputs to a supervised machine learning classifier. Accordingly, given a particular comment, can we determine from which news site it originated? Are there patterns among user comments? Can we identify stereotypical comments for different news sites? Besides presenting the results of my experiments, I will give a short introduction to Doc2Vec.&lt;/p&gt;
</summary></entry><entry><title>Are many of your worries about AI wrong?</title><link href="https://pyvideo.org/pydata-berlin-2017/are-many-of-your-worries-about-ai-wrong.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Toby Walsh</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/are-many-of-your-worries-about-ai-wrong.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Toby Walsh is one of the world's leading experts in artificial intelligence (AI). He was named by the Australian newspaper as a &amp;quot;rock star&amp;quot; of the digital revolution, and included in the inaugural Knowledge Nation 100, the list of the 100 most important digital innovators in Australia. Professor Walsh's research focuses on how computers can interact with humans to optimise decision-making for the common good. He is also a passionate advocate for limits to ensure AI is used to improve, not hurt, our lives. In 2015, Professor Walsh was behind an open letter calling for a ban on autonomous weapons or 'killer robots' that was signed by more than 20,000 AI researchers and high profile scientists, entrepreneurs and intellectuals, including Stephen Hawking, Noam Chomsky, Apple co-founder Steve Wozniak, and Tesla founder Elon Musk. He was subsequently invited by Human Rights Watch to talk at the United Nations in both New York and Geneva. Professor Walsh is a Fellow of the Australia Academy of Science, and winner of the Humboldt Award. His book, &amp;quot;Machines that Think: The Past, Present and Future of AI&amp;quot; will be published in early 2017 by Black Inc. His twitter account, &amp;#64;TobyWalsh was voted one of the top ten to keep abreast of developments in AI. His blog, &lt;a class="reference external" href="https://thefutureofai.blogspot.com"&gt;https://thefutureofai.blogspot.com&lt;/a&gt; attracts tens of thousands of readers every month.&lt;/p&gt;
&lt;p&gt;Many Fears About AI Are Wrong&lt;/p&gt;
&lt;p&gt;Should you be worried about progress in Artificial Intelligence? Will Artificial Intelligence destroy jobs? Should we fear killer robots? Does Artificial Intelligence threaten our very existence?  Artificial Intelligence (AI) is in the zeitgeist. Billions of dollars are being poured into the field, and spectacular advances are being announced regularly. Not surprisingly, many people are starting to worry where this will all end. The Chief Economist of the Bank of England predicted that Artificial Intelligence will destroy 50% of existing jobs. Thousands of Artificial Intelligence researchers signed an Open Letter predicting that Artificial Intelligence could transform warfare and lead to an arms race of &amp;quot;killer robots&amp;quot;. And Stephen Hawking, Elon Musk and others have predicted that Artificial Intelligence could end humanity itself. What should you make of all these predictions? Should you worry? Are you worrying about the right things? And what should we do now to ensure a safe and prosperous future for all?&lt;/p&gt;
&lt;p&gt;Link to Q&amp;amp;A: &lt;a class="reference external" href="https://youtu.be/SouUIKkgJLo"&gt;https://youtu.be/SouUIKkgJLo&lt;/a&gt; (Second pyvideo tab)&lt;/p&gt;
</summary></entry><entry><title>Best Practices for Debugging</title><link href="https://pyvideo.org/pydata-berlin-2017/best-practices-for-debugging.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Dr. Kristian Rother</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/best-practices-for-debugging.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial introduces concepts and techniques for systematic debugging. Participants will debug example programs with different kinds of bugs and with increasing difficulty.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Debugging is a daily activity of any programmer. Frequently, it is assumed that programmers can debug. However, programmers often have to deal with existing code that simply does not work. This tutorial attempts to change that by introducing concepts for debugging and corresponding programming techniques.&lt;/p&gt;
&lt;p&gt;In this tutorial, participants will learn strategies for systematically debugging Python programs. We will work through a series of examples, each with a different kind of bug and with increasing difficulty. The training will be interactive, combining one-person and group activities, to improve your debugging skills in an entertaining way.&lt;/p&gt;
&lt;p&gt;Contents:
Syntax Error against Runtime exceptions
Debugging with the scientific method
Inspection of variables with print and introspection functions
Using an interactive debugger
Logging
Code Review&lt;/p&gt;
</summary><category term="tutorial"></category></entry><entry><title>Biases are bugs: algorithm fairness and machine learning ethics</title><link href="https://pyvideo.org/pydata-berlin-2017/biases-are-bugs-algorithm-fairness-and-machine-learning-ethics.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Françoise Provencher</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/biases-are-bugs-algorithm-fairness-and-machine-learning-ethics.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Biases are bugs. They need to be found, fixed, and learnt from. A mix of good ethics and good engineering practices can get us a long way towards that goal.&lt;/p&gt;
&lt;p&gt;In this talk, you'll learn what biases are, what software tools can help, and how to adopt engineering practices that can make your algorithms fairer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Algorithms can make decisions, and these decisions can have an impact on people's lives. By feeding data into these algorithms, they can reproduce or amplify our societal biases and take unfair decisions.&lt;/p&gt;
&lt;p&gt;Biases are bugs. They need to be found, fixed, and learnt from. A mix of good ethics and good engineering practices can get us a long way towards that goal.&lt;/p&gt;
&lt;p&gt;In this talk, you will learn what biases are, see examples of algorithms gone wrong, and explore some software tools you can use and engineering practices you can adopt in your own work to make your algorithms more fair.&lt;/p&gt;
</summary></entry><entry><title>Blockchains for Artificial Intelligence</title><link href="https://pyvideo.org/pydata-berlin-2017/blockchains-for-artificial-intelligence.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Trent McConaghy</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/blockchains-for-artificial-intelligence.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk describes the various ways in which emerging blockchain technologies can be helpful for machine learning / artificial intelligence work, from audit trails on data to decentralized model exchanges.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In recent years, big data has transformed AI, to an almost unreasonable level. Now, blockchain technology could transform AI too, in its own particular ways. Some applications of blockchains to AI are mundane yet useful, like audit trails on AI models. Some appear almost unreasonable, like AI that can own itself — AI DAOs (decentralized autonomous organizations) leading to the first AI millionaires. All of them are opportunities. Blockchain technologies — especially planet-scale ones — can help realize some long-standing dreams of AI and data folks. This talk will explore these applications.&lt;/p&gt;
</summary></entry><entry><title>Building smart IoT applications with Python and Spark</title><link href="https://pyvideo.org/pydata-berlin-2017/building-smart-iot-applications-with-python-and-spark.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Rafael Schultze-Kraft</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/building-smart-iot-applications-with-python-and-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I will present how we use Python, PySpark and AWS as our preferred data science stack for the Internet of Things, which allows us to efficiently develop and deploy smart data applications on top of IoT sensor data. We use these technologies to analyse and model IoT timeseries data, as well as to build automated and scalable data pipelines for smart IoT data applications in the cloud.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Internet of Things and Industry 4.0 are here, bringing along a vast amount of connected devices and sensors producing even more data.&lt;/p&gt;
&lt;p&gt;In order to build smart applications on top of IoT sensor data we need to deal with the challenges that come along time-series data from a large amount of devices.&lt;/p&gt;
&lt;p&gt;At WATTx we build data application prototypes in the field of smart homes, smart buildings, and smart climate, which involves making use of data coming from many IoT sensors measuring -- amongst others -- temperature, humidity, motion, and luminance.&lt;/p&gt;
&lt;p&gt;The purpose of this talk is to present how we use Python and Spark to effectively analyse and model IoT data. In particular I will introduce how we use Python to process and model data from multiple IoT sensors, build machine learning models on top of it, and use Spark to scale and deploy our models in automated data pipelines in the cloud as smart IoT applications.&lt;/p&gt;
&lt;p&gt;I will use the development of predictive models for smart building applications as a real-world example to demonstrate this setup.&lt;/p&gt;
&lt;p&gt;I hope that this talk will give valuable insights on how Python and PySpark in conjunction with AWS are powerful tools to work with time-series sensor data from the Internet of Things and build data products on top of it.&lt;/p&gt;
</summary></entry><entry><title>Clean Code in Jupyter notebooks, using Python</title><link href="https://pyvideo.org/pydata-berlin-2017/clean-code-in-jupyter-notebooks-using-python.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Volodymyr (Vlad) Kazantsev</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/clean-code-in-jupyter-notebooks-using-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R.&lt;/p&gt;
&lt;p&gt;PyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases.&lt;/p&gt;
</summary></entry><entry><title>Compositional distributional semantics for modelling natural language</title><link href="https://pyvideo.org/pydata-berlin-2017/compositional-distributional-semantics-for-modelling-natural-language.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Thomas Kober</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/compositional-distributional-semantics-for-modelling-natural-language.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Distributional semantic word representations have become an integral part in numerous natural language processing pipelines in academia and industry. An open question is how these elementary representations can be composed to capture the meaning of longer units of text. In this talk, I will give an overview of compositional distributional models, their applications and current research directions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Representing words as vectors in a high-dimensional space has a long history in natural language processing. Recently, neural network based approaches such as word2vec and GloVe have gained a substantial amount of popularity and have become an ubiquituous part in many NLP pipelines for a variety tasks, ranging from sentiment analysis and text classification, to machine translation, recognising textual entailment or parsing.&lt;/p&gt;
&lt;p&gt;An important research problem is how to best leverage these word representations to form longer units of text such as phrases and full sentences. Proposals range from simple pointwise vector operations, to approaches inspired by formal semantics, deep learning based approaches that learn composition as part of an end-to-end system, and more structured approaches such as anchored packed dependency trees.&lt;/p&gt;
&lt;p&gt;In this talk I will introduce a variety of compositional distributional models and outline different approaches of how effective meaning representations beyond the word level can successfully be built. I will furthermore provide an overview of the advantages of using compositional distributional approaches, as well as their limitations. Lastly, I will discuss their merit for applications such as aspect oriented sentiment analysis and question answering.&lt;/p&gt;
</summary></entry><entry><title>Conversational AI: Building clever chatbots</title><link href="https://pyvideo.org/pydata-berlin-2017/conversational-ai-building-clever-chatbots.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Tom Bocklisch</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/conversational-ai-building-clever-chatbots.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most chatbots and voice skills are based on a state machine and too many if/else statements. Tom will show you how to move past that and build flexible, robust experiences using machine learning throughout the stack.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Conversational software is everywhere: messaging apps have opened up APIs to bot developers and millions of consumers now own voice controlled speakers. But the tools and frameworks for building these systems are still immature. Tom will talk about Rasa, an open source machine learning framework for building conversational software. The talk will cover the algorithms Rasa uses to build flexible and robust voice and text systems, the trade offs in using supervised versus reinforcement learning, and whether it's really such a good idea to generate text with LSTMs. Outline:&lt;/p&gt;
&lt;p&gt;Components : NLU , DM , integration , NLG
Overview of available tools and frameworks
Describe how Rasa does NLU
Motivation &amp;amp; a chatbot leading to state machine hell
How Rasa does dialogue management.
How to advance a bots capabilities - closing the loop and data collection.
Current research topics and challenges.&lt;/p&gt;
</summary></entry><entry><title>Data Science &amp; Data Visualization in Python. How to harness power of Python for social good?</title><link href="https://pyvideo.org/pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Radovan Kavicky</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python as an Open Data Science tool offers many libraries for data visualization and I will show you how to use and combine the best. I strongly believe that power of data is not only in the information &amp;amp; insight that data can provide us, Data is and can be really beautiful and can not only transform our perception but also the world that we all live in.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my talk I will primarily focus on answering/offer the answer to these questions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Why we need data science and why more and more people should be really interested in analyzing data and data visualization? (motivation)&lt;/li&gt;
&lt;li&gt;What is data science and how to start doing it in Python? (introduction of procedures, tools, most popular IDE-s for Python, etc.)&lt;/li&gt;
&lt;li&gt;What tools for data analysis and data visualization Python offers? (in each stage of analysis the best libraries will be shown for the specific purpose; as for data visualization we will focus particularly on Bokeh, Seaborn, Plotly and use of Jupyter Notebook and Plotly)&lt;/li&gt;
&lt;li&gt;How to 'unlock' the insight hidden in data through Python and how to use it to transform not only public administration or business, but ultimately the transformation of the whole society and economy towards the insight &amp;amp; knowledge based? (potential of data science)&lt;/li&gt;
&lt;li&gt;Open Data, Open Government Partnership, Open Public Administration &amp;amp; all the advantages of Open Data Science &amp;amp; Python. Data-Driven Approach. Everywhere. Now. (the end of talk +vision)&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="python"></category><category term="data-science"></category><category term="data-visualization"></category><category term="analytics"></category><category term="PyData"></category><category term="PyDataBLN"></category><category term="PyDataBerlin"></category><category term="PyDataBA"></category><category term="PyDataBratislava"></category><category term="talk"></category><category term="Data"></category><category term="Bokeh"></category><category term="Social Good"></category><category term="datascience"></category><category term="jupyter"></category><category term="open science"></category><category term="open data science"></category><category term="DataVisualization"></category><category term="data-analysis"></category><category term="analysis"></category><category term="matplotlib"></category><category term="numpy"></category><category term="data wrangling"></category><category term="jupyter notebook"></category><category term="pandas"></category><category term="machine learning"></category><category term="deep learning"></category><category term="Open Data"></category><category term="Citizen Data Science"></category></entry><entry><title>Data Science for Digital Humanities: Extracting meaning from Images and Text</title><link href="https://pyvideo.org/pydata-berlin-2017/data-science-for-digital-humanities-extracting-meaning-from-images-and-text.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Hendrik Heuer</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/data-science-for-digital-humanities-extracting-meaning-from-images-and-text.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Analyzing millions of images and enormous text sources using machine learning and deep learning techniques is simple and straightforward in the Python ecosystem. Powerful machine learning algorithms and interactive visualization frameworks make it easy to conduct and communicate large scale experiments. Exploring this data can yield new insights for researchers, journalists, and businesses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The focus of this talk is extracting meaning from data and making powerful methods usable by everybody. With the advent of big data, new approaches and technologies are needed to tackle the increase in volume, variety, and velocity of data. This talk illustrates how analysts, journalists, and scientists can benefit from exploratory data analysis and data science.&lt;/p&gt;
&lt;p&gt;Imagine a journalist who wants to cross-reference the names on the guest list of a parliament with online information about lobbyists to identify which party meets which company. A business analyst might want to quantify what topics certain customers are discussing on Twitter or how their sentiment towards a particular product is. Exploratory data analysis and data science techniques enable researchers, journalists and businesses to ask bigger and more ambitious questions than anybody before them and to leverage the abundance of information that is available today.&lt;/p&gt;
&lt;p&gt;The Digital Humanities are located at the intersection of computing and the disciplines of the humanities. They can benefit from the massive-scale automated analysis of content like images and text. Researchers, analysts, and journalists can quantify the state of society from publicly available data like tweets. It is now possible to construct an almost complete map of our civilization just by looking at the tags and GPS coordinates of Flickr photos.&lt;/p&gt;
&lt;p&gt;A vast Python ecosystem is supporting this including machine learning frameworks like scikit-learn, dedicated deep learning frameworks like Keras, and topic modeling tools like gensim. All these tools are open source and can be integrated into powerful data science pipelines. Rather than training neural networks from scratch, pretrained features for text and images can be adapted for fast results.&lt;/p&gt;
</summary></entry><entry><title>Deep Learning for detection on a phone</title><link href="https://pyvideo.org/pydata-berlin-2017/deep-learning-for-detection-on-a-phone.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Irina Vidal Migallon</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/deep-learning-for-detection-on-a-phone.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Deploying a deep model on a mobile device to be used for real-time detection is not quite trivial yet. Defining your Deep Learning architecture, gathering the right data, designing your training process, evaluating your models and turning this into a pipeline that keeps everyone on the team (somewhat) sane - these all have their pitfalls.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Deep Learning has gone through the hype phase where it seemed like a skeleton key, followed by a phase of despair for many who found the building blocks to be too esoteric and the training code and process too unreliable. Deploying on a device with strong hardware limitations adds that extra spice to the mix.&lt;/p&gt;
&lt;p&gt;This talk addresses a very specific use case: preparing a Deep Neural Network to be used for detection in real time in a mobile phone app. It is meant for hands-on engineers and data scientists who live in that area where writing scalable and testable code is every bit as important (and troublesome!) as understanding your loss function.&lt;/p&gt;
&lt;p&gt;We will cover different steps of the process, such as:&lt;/p&gt;
&lt;p&gt;Defining a good model for you:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Your device: it is what it is.&lt;/li&gt;
&lt;li&gt;Cargo cult or what is this layer doing and do we need it?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gathering the right training data: who, where and how will be using your app?&lt;/p&gt;
&lt;p&gt;Training:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Transfer learning as a small company's best friend.&lt;/li&gt;
&lt;li&gt;Integrating different sources in your data pipeline. Evaluating earlier rather than later.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on time and interest, we may also go over data augmentation and/or model persistence.&lt;/p&gt;
</summary></entry><entry><title>Developments in Test-Driven Data Analysis</title><link href="https://pyvideo.org/pydata-berlin-2017/developments-in-test-driven-data-analysis.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Nick Radcliffe</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/developments-in-test-driven-data-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Test-driven data analysis fuses and builds upon the ideas of test-driven development and reproducible research to support higher quality data analysis. This talk will extend the foundation parts of TDDA with extensions including tight constraints on string fields with automatically discovered regular expressions and automatically discovered relationships between datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Test-driven data analysis fuses and builds upon the ideas of test-driven development and reproducible research to support higher quality data analysis.&lt;/p&gt;
&lt;p&gt;Foundational concepts are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Level 0: Reference Tests&lt;/li&gt;
&lt;li&gt;Level 1: Automatic constraint discovery and validation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This talk will extend these to cover tight constraints on string fields with&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;automatically discovered regular expressions with rexpy&lt;/li&gt;
&lt;li&gt;constraints between datasets and probably more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Background material:&lt;/p&gt;
&lt;p&gt;PyCon UK Talk, Cardiff, Test-Driven Data Analysis &lt;a class="reference external" href="https://www.youtube.com/watch?v=FIw_7aUuY50"&gt;https://www.youtube.com/watch?v=FIw_7aUuY50&lt;/a&gt;
Blog: &lt;a class="reference external" href="http://tdda.info"&gt;http://tdda.info&lt;/a&gt;, especially posts &lt;a class="reference external" href="http://www.tdda.info/the-new-referencetest-class-for-tdda"&gt;http://www.tdda.info/the-new-referencetest-class-for-tdda&lt;/a&gt; and &lt;a class="reference external" href="http://www.tdda.info/constraint-discovery-and-verification-for-pandas-dataframes"&gt;http://www.tdda.info/constraint-discovery-and-verification-for-pandas-dataframes&lt;/a&gt;
Overview: &lt;a class="reference external" href="http://www.predictiveanalyticsworld.com/patimes/four-ways-data-science-goes-wrong-and-how-test-driven-data-analysis-can-help/"&gt;http://www.predictiveanalyticsworld.com/patimes/four-ways-data-science-goes-wrong-and-how-test-driven-data-analysis-can-help/&lt;/a&gt; In terms of some of the new material that will be covered in this talk, see
&lt;a class="reference external" href="http://www.tdda.info/introducing-rexpy-automatic-discovery-of-regular-expressions"&gt;http://www.tdda.info/introducing-rexpy-automatic-discovery-of-regular-expressions&lt;/a&gt;
&lt;a class="reference external" href="http://rexpy.herokuapp.com"&gt;http://rexpy.herokuapp.com&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Engage the Hyper-Python</title><link href="https://pyvideo.org/pydata-berlin-2017/engage-the-hyper-python.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Daniele Rapati</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/engage-the-hyper-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A fast paced high-level overview of speed optimisation in Python. What makes a program &amp;quot;slow&amp;quot;? How to tell what is making your program slow. Common speed-up paradigms: parallelization, alternatives to the regular Python interpreter and asynchronous processing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A fast paced high-level overview of speed optimisation in Python. We will start by looking systematically at the most common causes of poor speed, highlighting which resources are being the bottleneck in each case and giving practical advice on how to find out. We will then discuss parallelism with threads and processes, both in the standard library and using celery. We will discuss Pypy and Cython as alternatives to regular Python for CPU intensive tasks. We finish our tour with asynchronous processing in Python 3 using async.io.&lt;/p&gt;
</summary></entry><entry><title>Ethics in Machine Learning Panel</title><link href="https://pyvideo.org/pydata-berlin-2017/ethics-in-machine-learning-panel.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Roelof Pieters</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/ethics-in-machine-learning-panel.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Advances in AI are happening at a tremendous pace, and especially Machine Learning systems are hastily deployed in settings that span as wide as social media, search engines, advertising, military use, and legal systems. At first sight results are often promising, but more and more &amp;quot;outliers&amp;quot; are becoming visible as well. Systemic bias in culture, language, or business practices often become intensified: From the Tay-chatbot turning racist, to both Google and Flickr classifying images of African-Americans as &amp;quot;monkeys&amp;quot;, to discrimination and sexism encoded in language models used for everything from translation, to court sentencing decision. Luckily, initiatives that address some of these concerns is happening through more interpretable models, new privacy regulations, novel privacy-preserving learning algorithms, and researchers and engineers standing up for more just and transparent machine learning models and methods.&lt;/p&gt;
&lt;p&gt;The panel aims to debate these themes with speakers, and the audience, and go beyond the usual &amp;quot;AI hype&amp;quot; and discuss both the amazing progress, as well as the setbacks, in making societies better, more humane, and ready for the future!&lt;/p&gt;
&lt;p&gt;Speaking at the panel will be:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Roelof Pieters, Panel Host, Co-founder at creative.ai&lt;/li&gt;
&lt;li&gt;Marek Rosa, CEO/CTO at GoodAI&lt;/li&gt;
&lt;li&gt;Françoise Provencher, Data Team Lead at Shopify&lt;/li&gt;
&lt;li&gt;Hendrik Heuer, Researcher at Institute for Information Management (ifib) at the University of Bremen&lt;/li&gt;
&lt;li&gt;Andreas Dewes, Founder at 7scientists&lt;/li&gt;
&lt;li&gt;Katharine Jarmul, Founder at kjamistan&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Link to Q&amp;amp;A: &lt;a class="reference external" href="https://youtu.be/uOKhB1_vyAw"&gt;https://youtu.be/uOKhB1_vyAw&lt;/a&gt; (Second pyvideo tab)&lt;/p&gt;
</summary><category term="panel"></category><category term="ethics"></category><category term="machine learning"></category></entry><entry><title>Evaluating Topic Models</title><link href="https://pyvideo.org/pydata-berlin-2017/evaluating-topic-models.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Matti Lyra</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/evaluating-topic-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Unsupervised models in natural language processing (NLP) have become very popular recently. Word2vec, GloVe and LDA provide powerful computational tools to deal with natural language and make exploring large document collections feasible. We would like to be able to say if a model is objectively good or bad, and compare different models to each other, this is often tricky to do in practice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Supervised models are trained on labelled data and optimised to maximise an external metric such as log loss or accuracy. Unsupersived models on the other hand typically try to fit a predefined distribution to be consistent with the statistics of some large unlabelled data set or maximise the vector similarity of words that appear in similar contexts. Evaluating the trained model often starts by &amp;quot;eye-balling&amp;quot; the results, i.e. checking that your own expectations of similarity are fulfilled by the model.&lt;/p&gt;
&lt;p&gt;Documents that talk about football should be in the same category and &amp;quot;cat&amp;quot; is more similar with &amp;quot;dog&amp;quot; than with &amp;quot;pen&amp;quot;. Is &amp;quot;cat&amp;quot; more similar to &amp;quot;tiger&amp;quot; than to &amp;quot;dog&amp;quot;? Ideally this information should be captured in a single metric that can be maximised. Tools such as pyLDAvis and gensim provide many different ways to get an overview of the learned model or a single metric that can be maximised: topic coherence, perplexity, ontological similarity, term co-occurrence, word analogy. Using these methods without a good understanding of what the metric represents can give misleading results. The unsupervised models are also often used as part of larger processing pipelines, it is not clear if these intrinsic evaluation measures are approriate in such cases, perhaps the models should instead be evaluated against an external metric like accuracy for the entire pipeline.&lt;/p&gt;
&lt;p&gt;In this talk I will give an intuition of what the evaluation metrics are trying to achieve, give some recommendations for when to use them, what kind of pitfalls one should be aware of when using topic models and the inherent difficulty of measuring or even defining semantic similarity concisely.&lt;/p&gt;
&lt;p&gt;I assume that you are familiar with topic models, I will not cover how they are defined or trained. I talk specifically about the tools that are available for evaluating a topic model, irrespective of which algorithm you've used to learn one. The talk is accompanied by a notebook at &lt;a class="reference external" href="https://github.com/mattilyra/pydataberlin-2017"&gt;https://github.com/mattilyra/pydataberlin-2017&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Fairness and transparency in machine learning: Tools and techniques</title><link href="https://pyvideo.org/pydata-berlin-2017/fairness-and-transparency-in-machine-learning-tools-and-techniques.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Andreas Dewes</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/fairness-and-transparency-in-machine-learning-tools-and-techniques.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will try to answer a simple question: When building machine learning systems, how can we make sure that they treat people fairly and can be held accountable? While seemingly trivial, this question is not easy to answer, especially when using complex methods like deep learning. I will discuss tools and techniques that we can use to make sure our algorithms behave as they should.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When working with personal data, we need to make sure that our algorithms treat people fairly, are transparent and can be held accountable for their decisions. When using complex techniques like deep learning on very large datasets, it is not easy to prove that our algorithms behave they way we intend them to and e.g. do not discriminate against certain groups of people.&lt;/p&gt;
&lt;p&gt;In my talk, I will discuss why ensuring transparency and fairness in machine learning is not easy, and how we can use Python tools to investigate our machine learning systems and make sure they behave they way they should.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction: Why you should care about this (EU Data Protection Directive)&lt;/li&gt;
&lt;li&gt;What kind of problems can occur in machine learning systems (bias in the input data, leakage of sensitive information into the training data, hidden usage of protected attributes by the algorithm)?&lt;/li&gt;
&lt;li&gt;How can we measure and correct for bias in our systems (certifying and removing disparate impact)?&lt;/li&gt;
&lt;li&gt;How can we understand the decisions that our algorithms make (perturbation analysis, simplified modeling, blackbox testing)?&lt;/li&gt;
&lt;li&gt;How can we design our machine learning systems to make sure they're compliant and accountable (anonymization of data, monitoring of outcomes, auditing of algorithms)?&lt;/li&gt;
&lt;li&gt;Outlook: The future of transparency and accountability in machine learning&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Fast Multidimensional Signal Processing using Julia with Shearlab.jl</title><link href="https://pyvideo.org/pydata-berlin-2017/fast-multidimensional-signal-processing-using-julia-with-shearlabjl.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Héctor Andrade Loarca</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/fast-multidimensional-signal-processing-using-julia-with-shearlabjl.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Shearlab is a Julia Library with toolbox for two- and threedimensional data processing using the Shearlet system as basis functions which generates a sparse representation of cartoon-like functions with applications on Signal Processing, Compressed Sensing, 3D Imaging, MRI Imaging and a lot more, with visible improvements with respect of the Wavelet Transform in representing multidimensional data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Shearlet Transform was proposed by the Professor Gitta Kutyniok (&lt;a class="reference external" href="http://www.tu-berlin.de/?108957"&gt;http://www.tu-berlin.de/?108957&lt;/a&gt;) and her colleagues as a multidimensional generalization of the Wavelet Transform, and since then it has been adopted by a lot of Companies and Institutes by its stable and optimal representation of multidimensional signals. Shearlab.jl is a already registered Julia package (&lt;a class="reference external" href="https://github.com/arsenal9971/Shearlab.jl"&gt;https://github.com/arsenal9971/Shearlab.jl&lt;/a&gt;) based in the most used implementation of Shearlet Transform programmed in Matlab by the Research Group of Prof. Kutyniok (&lt;a class="reference external" href="http://www.shearlab.org/software"&gt;http://www.shearlab.org/software&lt;/a&gt;), it was developed as a project apart of my PhD studies but ended up being the main computational tool of them, used mainly to reconstruct the Light Field of a 3D Scene from Sparse Photographic Samples of Different Perspectives with Stereo Vision purposes.&lt;/p&gt;
&lt;p&gt;Why I think this will be an interesting thing to present at JuliaCon 2017?&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A lot of research institutes and companies have already adopted the Shearlet Transform in their work (e.g. Fraunhofer Institute in Berlin and Charité Hospital in Berlin, Mathematical Institute of TU Berlin) by its directional sensitivity, reconstruction stability and sparse representation; with applications that goes from MRI Imaging in Hospitals to Video Compression Decoding.&lt;/li&gt;
&lt;li&gt;I am convinced Shearlab.jl is the best implementation so far of Shearlet Transform, basing my arguments on the benchmarks already runned against the last Matlab version which is the most used at the moment (here benchmarks &lt;a class="reference external" href="https://github.com/arsenal9971/Shearlab.jl/blob/master/benchmarks/README.md"&gt;https://github.com/arsenal9971/Shearlab.jl/blob/master/benchmarks/README.md&lt;/a&gt;) beating it by at least double the speed on different experiments.&lt;/li&gt;
&lt;li&gt;Not everything is about performance and technical mathematics, so I also have cool usage examples to show with some algorithms that have been implemented lately, like: Image Decomposition and Recovery (&lt;a class="reference external" href="https://github.com/arsenal9971/Shearlab.jl/blob/master/examples/Shearlets/Examples%20of%20Image%20Decoding%20and%20Recovery.ipynb"&gt;https://github.com/arsenal9971/Shearlab.jl/blob/master/examples/Shearlets/Examples%20of%20Image%20Decoding%20and%20Recovery.ipynb&lt;/a&gt;), Image Denosing (&lt;a class="reference external" href="https://github.com/arsenal9971/Shearlab.jl/blob/master/examples/Shearlets/Image%20Denoising.ipynb"&gt;https://github.com/arsenal9971/Shearlab.jl/blob/master/examples/Shearlets/Image%20Denoising.ipynb&lt;/a&gt;) Image Inpainting (the coolest so far) (&lt;a class="reference external" href="https://github.com/arsenal9971/Shearlab.jl/blob/master/examples/Shearlets/Image%20Inpainting.ipynb"&gt;https://github.com/arsenal9971/Shearlab.jl/blob/master/examples/Shearlets/Image%20Inpainting.ipynb&lt;/a&gt;) which I am using at the moment for the Light Field Recovery of a sparsely sampled 3D scene.&lt;/li&gt;
&lt;li&gt;The Package was also already presented in the Berlin Julia Users MeetUp with a very good response and interest from the community (&lt;a class="reference external" href="https://www.meetup.com/es-ES/Julia-Users-Group/events/236791753/"&gt;https://www.meetup.com/es-ES/Julia-Users-Group/events/236791753/&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Finding Lane Lines for Self Driving Cars</title><link href="https://pyvideo.org/pydata-berlin-2017/finding-lane-lines-for-self-driving-cars.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Ross Kippenbrock</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/finding-lane-lines-for-self-driving-cars.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Self-driving cars might not be in our everyday lives yet, but they are coming! Analyzing images and figuring out where the lane lines are on a given roadway is one of the core competencies of any respectable self-driving car. Humans do this with ease and this talk will show you how to find these lines using Python and OpenCV.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction to the OpenCV library (loading images, plotting with matplotlib, etc…)&lt;/li&gt;
&lt;li&gt;Starting with single images, introduce Gaussian blur, region of interest filtering, canny edge detection, Hough transform and draw the lane lines.&lt;/li&gt;
&lt;li&gt;Create a lane line detection pipeline with those functions; extrapolating the lines to represent the boundaries of the lane.&lt;/li&gt;
&lt;li&gt;Process dash cam video using the single image techniques with the pipeline.&lt;/li&gt;
&lt;li&gt;Stitch together images from the processing pipeline to create a sweet video!&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Gold standard data: lessons from the trenches</title><link href="https://pyvideo.org/pydata-berlin-2017/gold-standard-data-lessons-from-the-trenches.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Miroslav Batchkarov</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/gold-standard-data-lessons-from-the-trenches.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The first stage in a data science project is often to collect training data. However, getting a good data set is surprisingly tricky and takes longer than one expects. This talk describes our experiences in labelling gold-standard data and the lessons we learnt the hard way. We will present three case studies from natural language processing and discuss the challenges we encountered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is often said that rather than spending a month figuring out how to apply unsupervised learning to a problem domain, a data scientist should spend a week labelling data. However, the difficulty of annotating data is often underestimated. Gathering a sufficiently large collection of good-quality labelled data requires careful problem definition and multiple iterations. In this talk, I will describe three case studies and lessons learnt from them. Each case shows several aspect of the process that should be considered in advance to ensure the project is successful.&lt;/p&gt;
</summary></entry><entry><title>Introduction to Data-Analysis with Pandas</title><link href="https://pyvideo.org/pydata-berlin-2017/introduction-to-data-analysis-with-pandas.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Alexander Hendorf</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/introduction-to-data-analysis-with-pandas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is the Swiss-Multipurpose Knife for Data Analysis in Python. With Pandas dealing with data-analysis is easy and simple but there are some things you need to get your head around first as Data-Frames and Data-Series. The tutorial provides a compact introduction to Pandas for beginners for I/O, data visualisation, statistical data analysis and aggregation within Jupiter notebooks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandas is the Swiss-Multipurpose Knife for Data Analysis in Python. With Pandas dealing with data-analysis is easy and simple but there are some things you need to get your head around first as Data-Frames and Data-Series.&lt;/p&gt;
&lt;p&gt;The tutorial provides a compact introduction to Pandas for beginners:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;reading and writing data across multiple formats (CSV, Excel, JSON, SQL, HTML,…)&lt;/li&gt;
&lt;li&gt;data visualisation&lt;/li&gt;
&lt;li&gt;statistical data analysis and aggregation.&lt;/li&gt;
&lt;li&gt;work with built-in data visualisation&lt;/li&gt;
&lt;li&gt;inner-mechanics of Pandas: Data-Frames, Data-Series &amp;amp; Numpy.&lt;/li&gt;
&lt;li&gt;working and making the most of indexes.&lt;/li&gt;
&lt;li&gt;how to mangle, reshape and pivot&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The tutorial will be provided as Jupiter notebooks.&lt;/p&gt;
</summary><category term="tutorial"></category></entry><entry><title>Introduction to Julia for Scientific Computing and Data Science</title><link href="https://pyvideo.org/pydata-berlin-2017/introduction-to-julia-for-scientific-computing-and-data-science.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>David Higgins</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/introduction-to-julia-for-scientific-computing-and-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Developed at MIT, with a focus on fast numerical computing, Julia has a syntactical complexity similar to that of Python or Matlab but a performance orders of magnitude faster. We will present an introduction to the language, followed by a sampling of some of our favourite packages. The focus is on which aspects of Julia are currently ready for use by numerical computing and data scientists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Julia is a new and exciting language, sponsored in part by NumFocus, and developed at MIT. With a focus on fast numerical computing, it has a syntactical complexity similar to that of Python or Matlab but a performance orders of magnitude faster. This means you can quickly code up your ideas in a scripting language, but you don't need to switch langauges when you later need to squeeze out every last ounce of performance on production data.&lt;/p&gt;
&lt;p&gt;Should you stick with Python, or is it time to move to Julia?&lt;/p&gt;
&lt;p&gt;We will try to answer this question by presenting an introduction to the language, followed by a sampling of packages from the most important projects relevant to data and numerical science. Python has become a powerhouse in these fields largely due to its available libraries. Julia is fast making up this ground, with an integrated package manager and the ability to call Python libraries, we will show where the gap has already been bridged and where there is still reason to hold back. We will demonstrate that the language syntax is already as easy as the simplest specialist alternatives (eg. Matlab/Mathematica/S-Plus), but this is a fully fledged programming language, and the libraries are no longer necessarily a reason to hold back.&lt;/p&gt;
&lt;p&gt;The Tutorial format&lt;/p&gt;
&lt;p&gt;We will begin by introducing the basic syntax, showing the paths of least resistance for users moving from Python, Matlab and other major languages. We will quickly move-on to explain the details of how Julia is different from other scripted programming languages. Typically, these differences, such as the system of multiple dispatch, have been specifically chosen as they allow for faster runtimes from scripted code without impacting on read/writeability. Finally, we will work through some specific examples of Visualisation (Plots.jl), Data Wrangling (ie DataFrame-type options), Numerical Optimisation (JuliaOpt/JuMP.jl) and High-Performance Computing (JuliaGPU/OpenCL.jl, also profiling, benchmarking and debugging if time allows) as an introduction to some of the packages currently available in the Julia package ecosystem.&lt;/p&gt;
&lt;p&gt;Note: this is an interactive tutorial please follow the Installation Instructions, which will be posted at &lt;a class="reference external" href="https://github.com/daveh19/pydataberlin2017"&gt;https://github.com/daveh19/pydataberlin2017&lt;/a&gt; shortly before the meeting, if you wish to participate!&lt;/p&gt;
</summary><category term="tutorial"></category></entry><entry><title>Introduction to Machine Learning with H2O and Python</title><link href="https://pyvideo.org/pydata-berlin-2017/introduction-to-machine-learning-with-h2o-and-python.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Jo-fai Chow</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/introduction-to-machine-learning-with-h2o-and-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;H2O.ai is focused on bringing AI to businesses through software. Its flagship product is H2O, the leading open source platform that makes it easy for financial services, insurance and healthcare companies to deploy machine learning and predictive analytics to solve complex problems.&lt;/p&gt;
&lt;p&gt;This tutorial aims to demonstrate the basic usage of H2O with worked examples in Python.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;About H2O.ai&lt;/p&gt;
&lt;p&gt;H2O.ai is focused on bringing AI to businesses through software. Its flagship product is H2O, the leading open source platform that makes it easy for financial services, insurance and healthcare companies to deploy machine learning and predictive analytics to solve complex problems. More than 8,500+ organizations and 75,000+ data scientists depend on H2O for critical applications like predictive maintenance and operational intelligence. The company accelerates business transformation for 107 Fortune 500 enterprises, 8 of the world’s 12 largest banks, 7 of the 10 largest auto insurance companies and all 5 major telecommunications providers. Notable customers include Capital One, Progressive Insurance, Transamerica, Comcast, Nielsen Catalina Solutions, Macy's, Walgreens, Kaiser Permanente, and Aetna.&lt;/p&gt;
&lt;p&gt;This tutorial aims to demonstrate the basic usage of H2O with worked examples in Python. Code and data for the worked examples will be provided.&lt;/p&gt;
&lt;p&gt;Learning Objectives
By the end of the tutorial, participants will be able to:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Start and connect to a local H2O cluster from Python.&lt;/li&gt;
&lt;li&gt;Start and connect to H2O cluster(s) on the cloud (e.g. AWS) (i.e. straight-forward distributed machine learning)&lt;/li&gt;
&lt;li&gt;Import data from Python data frames, local files or web.&lt;/li&gt;
&lt;li&gt;Perform basic data transformation and exploration.&lt;/li&gt;
&lt;li&gt;Train classification and regression models using H2O machine learning algorithms.&lt;/li&gt;
&lt;li&gt;Evaluate model performance and make predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Agenda&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;About H2O.ai&lt;/li&gt;
&lt;li&gt;H2O machine learning platform &amp;amp; algorithms&lt;/li&gt;
&lt;li&gt;H2O + Python API&lt;/li&gt;
&lt;li&gt;Basic Extract, Transform and Load (ETL) procedures&lt;/li&gt;
&lt;li&gt;Worked examples: classification and regression&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="tutorial"></category></entry><entry><title>Introduction to Search</title><link href="https://pyvideo.org/pydata-berlin-2017/introduction-to-search.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Sirin Odrowski</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/introduction-to-search.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Link to slides: &lt;a class="reference external" href="https://www.slideshare.net/secret/kvfTyzlrZuq3q7"&gt;https://www.slideshare.net/secret/kvfTyzlrZuq3q7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Description
Search engines make archives, inventories, websites, large publishing platforms, and the internet navigable. Using the search engine we built at WordPress.com, a platform where more than 90 million blog posts and web pages are published per month, as an example, I will explain how search engines work and how their performance can be evaluated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Search engines make archives, inventories, websites, large publishing platforms, and the internet navigable. In this talk, I will introduce the basic building blocks of a search engine.&lt;/p&gt;
&lt;p&gt;WordPress.com is a platform where anyone can create a blog or website, and read posts that are published there and elsewhere. More than 90 million posts and pages are published on WordPress.com per month. To make it easier for our users to find the ones they are most interested in, we built a search engine that is based on Elasticsearch.&lt;/p&gt;
&lt;p&gt;In order to keep improving our search algorithm, we need to track and analyze our user’s interactions with the search results. I will present an overview of the metrics we and others use to evaluate the performance of search algorithms.&lt;/p&gt;
</summary></entry><entry><title>Introductory tutorial on data exploration and statistical models</title><link href="https://pyvideo.org/pydata-berlin-2017/introductory-tutorial-on-data-exploration-and-statistical-models.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Alexandru Agachi</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/introductory-tutorial-on-data-exploration-and-statistical-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial will focus on analyzing a dataset and building statistical models from it. We will describe and visualize the data. We will then build and analyze statistical models, including linear and logistic regression, as well as chi-square tests of independence. We will then apply 4 machine learning techniques to the dataset: decision trees, random forests, lasso regression, and clustering.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I would be happy to conduct an introductory level tutorial on exploring a dataset with the pandas/StatsModels/scikit-learn framework:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Descriptive statistics. Here we will describe each variable depending on its type, as well as the dataset overall.&lt;/li&gt;
&lt;li&gt;Visualization for categorical and quantitative variables. We will learn effective visualization techniques for each type of variable in the dataset.&lt;/li&gt;
&lt;li&gt;Statistical modeling for quantitative and categorical, explanatory and response variables: chi-square tests of independence, linear regression and logistic regression. We will learn to test hypotheses, and to interpret our models, their strengths, and their limitations.&lt;/li&gt;
&lt;li&gt;I will then expand to the application of machine learning techniques, including decision trees, random forests, lasso regression, and clustering. Here we will explore the advantages and disadvantages of each of these techniques, as well as apply them to the dataset.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This would be a very applied, introductory tutorial, to the statistical exploration of a dataset and the building of statistical models from it. I would be happy to send you the ipython notebook for this tutorial as well.&lt;/p&gt;
</summary></entry><entry><title>Is That a Duplicate Quora Question?</title><link href="https://pyvideo.org/pydata-berlin-2017/is-that-a-duplicate-quora-question.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Abhishek Thakur</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/is-that-a-duplicate-quora-question.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Quora released its first ever dataset publicly on 24th Jan, 2017. This dataset consists of question pairs which are either duplicate or not. Duplicate questions mean the same thing. In this talk, we discuss methods which can be used to detect duplicate questions using Quora dataset. Of course, these methods can be used for other similar datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Quora released its first ever dataset publicly on 24th Jan, 2017. This dataset consists of question pairs which are either duplicate or not. Duplicate questions mean the same thing.&lt;/p&gt;
&lt;p&gt;For example, the question pairs below are duplicates (from the Quora dataset)&lt;/p&gt;
&lt;p&gt;How does Quora quickly mark questions as needing improvement? Why does Quora mark my questions as needing improvement/clarification before I have time to give it details? Literally within seconds…&lt;/p&gt;
&lt;p&gt;Why did Trump win the Presidency? How did Donald Trump win the 2016 Presidential Election?&lt;/p&gt;
&lt;p&gt;What practical applications might evolve from the discovery of the Higgs Boson? What are some practical benefits of discovery of the Higgs Boson?&lt;/p&gt;
&lt;p&gt;Some examples of non-duplicate questions are as follows:&lt;/p&gt;
&lt;p&gt;Who should I address my cover letter to if I'm applying for a big company like Mozilla? Which car is better from safety view?&amp;quot;&amp;quot;swift or grand i10&amp;quot;&amp;quot;.My first priority is safety?&lt;/p&gt;
&lt;p&gt;Mr. Robot (TV series): Is Mr. Robot a good representation of real-life hacking and hacking culture? Is the depiction of hacker societies realistic? What mistakes are made when depicting hacking in &amp;quot;&amp;quot;Mr. Robot&amp;quot;&amp;quot; compared to real-life cybersecurity breaches or just a regular use of technologies?&lt;/p&gt;
&lt;p&gt;How can I start an online shopping (e-commerce) website? Which web technology is best suitable for building a big E-Commerce website?&lt;/p&gt;
&lt;p&gt;In this talk, we discuss methods which can be used to detect duplicate questions using Quora dataset. Of course, these methods can be used for other similar datasets.&lt;/p&gt;
&lt;p&gt;Methods discussed in this talk range from simple TF-IDF, Singular Value Decomposition, Fuzzy Features, Word2Vec features, GloVe features, LSTMs and 1D CNN. We provide a comparison of performance of these algorithms on the Quora dataset.&lt;/p&gt;
&lt;p&gt;Using very simple methods and then using deep learning methods in python we achieved an accuracy similar to current state of the art on a normal machine with only one GPU.&lt;/p&gt;
</summary></entry><entry><title>Kickstarting projects with Cookiecutter</title><link href="https://pyvideo.org/pydata-berlin-2017/kickstarting-projects-with-cookiecutter.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Raphael Pierzina</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/kickstarting-projects-with-cookiecutter.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Cookiecutter is a command-line utility that generates projects from templates. You can use Cookiecutter to create new Python projects and to generate the initial code for different types of applications. In this talk, I will give an introduction to Cookiecutter, how to install it from PyPI, how to use it in the CLI, and finally how to author your own template.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Writing a Python script from scratch is fairly straightforward if you have some experience working in Python. You can usually get by with very little boilerplate code. Starting a new Python project, however, can be tiring if you decide to stick to best practices and plan on submitting it to PyPI later on. It requires great diligence and occasionally gets pretty cumbersome when if you're creating new tools on a regular basis.&lt;/p&gt;
&lt;p&gt;So why not use a template for your projects?&lt;/p&gt;
&lt;p&gt;Cookiecutter is a command-line utility that creates projects from templates. It is free and open-source software distributed under the terms of a permissive BSD-3 license. With around 150 individual contributors, more than 1000 public templates on GitHub, and multiple talks at conferences, it is fair to say that there is a solid community around it.&lt;/p&gt;
&lt;p&gt;In this talk, I will give an introduction on Cookiecutter, how to install it, how to use it in the CLI, and finally how to author your own template. You can use Cookiecutter for all sorts of projects: command-line scripts, Django webapps, and even non-Python projects.&lt;/p&gt;
&lt;p&gt;The community has authored and published several templates for Data Science projects, for instance widget-cookiecutter. I will demonstrate how to use Cookiecutter to create a custom Jupyter widget using that template.&lt;/p&gt;
&lt;p&gt;The goal of this talk is to teach how to integrate Cookiecutter into your own workflow and share learnings in the form of templates with your team at work and the open source community.&lt;/p&gt;
</summary></entry><entry><title>Large Scale Vandalism Detection in Knowledge Bases</title><link href="https://pyvideo.org/pydata-berlin-2017/large-scale-vandalism-detection-in-knowledge-bases.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Alexey Grigorev</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/large-scale-vandalism-detection-in-knowledge-bases.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Wikidata is a Knowledge Base where anybody can add new information. Unfortunately, it is targeted by vandals, who put inaccurate or offensive information there. To fight them, Wikidata employs moderators, who manually inspect each suggested edit. In this talk we will look into how we can use Machine Learning to automatically detect vandalic revisions and help the moderators.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Knowledge bases are an important source of information for many AI system: they rely on the bases for enriching the information they process to make better user experience. Obtaining such Knowledge Bases is difficult, and which is why this process is crowd-sourced. One of such bases is Wikidata: they allow everybody on the Internet to edit the content and add new information.&lt;/p&gt;
&lt;p&gt;Unfortunately, Wikidata is often targeted by vandals, who misuse the system and put false or offensive information there. This may lead to incorrect behaviour of the AI systems. To keep the base clean, Wikidata employs moderators who manually inspect each revision and revert vandalic ones.&lt;/p&gt;
&lt;p&gt;To help moderators fight vandals, the organizers of WSDM Cup 2017 challenged the participants to build a Machine Learning model which automatically detects if an edit should be rolled back. In this talk we will discuss the second place solution to the Cup: how to process half of terabyte of revisions, extract meaningful features and create a production ready model that scales to a large number of testing examples.&lt;/p&gt;
</summary></entry><entry><title>Leveling up your Jupyter notebook skills</title><link href="https://pyvideo.org/pydata-berlin-2017/leveling-up-your-jupyter-notebook-skills.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Gerrit Gruben</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/leveling-up-your-jupyter-notebook-skills.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most of us regularly work with Jupyter notebooks, but fail to see obvious productivity gains involving its usage. Did you know that the web interface works like a modal editor such as VIM? Do you know that you can actually profile AND debug code in notebooks? How about setting formulas or use pre-made style settings for visualizations? Let us go through the tricks of the trade together!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Overview of the Jupyter project + setup to get everyone on board.
Handling the UI, know the shortcuts
Different type of cells
Exporting notebooks for presentations
Handling different kernels
Set styles for visualizations for professional quality
Mod the style of the web interface yourself via CSS
Profiling code in notebooks, use Cython
Debugging in notebooks&lt;/p&gt;
</summary></entry><entry><title>lightning talks</title><link href="https://pyvideo.org/pydata-berlin-2017/lightning-talks.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/lightning-talks.html</id><summary type="html"></summary><category term="lightning talks"></category></entry><entry><title>Machine Learning to moderate ads in real world classified's business</title><link href="https://pyvideo.org/pydata-berlin-2017/machine-learning-to-moderate-ads-in-real-world-classifieds-business.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Vaibhav Singh</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/machine-learning-to-moderate-ads-in-real-world-classifieds-business.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In todays world of online business, it is difficult to moderate all the content coming to your site. In this talk we share our experiences on how we built machine learning models to moderate 100+ million classified ads every month. Audience will get a chance to experience a real world of content moderation and a race to beat online fraudsters and scammers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In an online classified's business, one may encounter a lot of spam and fraud once the business starts to grow. One way to inhibit this is to moderate all incoming advertisements by using static filters or having human moderators but this may not go a long way if the business deals with millions of advertisements every day. Static filters may catch good advertisements and flag them as bad and would also require humans to add, remove or improve them. On the other hand employing human moderators to moderate all incoming advertisements does not scale. Creating machine learning models is what we believe is the right way to address this kind of problem. Machine learning models identifies patterns in data and classifies ads thereby reducing the overhead of creating complex filters and reducing number of human moderators&lt;/p&gt;
&lt;p&gt;In this talk we share our experiences in building machine learning models to act as human moderators. This talk will cover mainly the following topics&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Creating a simple platform architecture that can do predictions on millions of requests without spending too much resources on devops and machines&lt;/li&gt;
&lt;li&gt;Batching of requests so as to use CPU's optimally.&lt;/li&gt;
&lt;li&gt;Containerising code so as to have ease of deployments&lt;/li&gt;
&lt;li&gt;Creating models from training set containing millions of rows and thousands of features which can be trained on simple machines rather than using complex Spark Hadoop Architectures.&lt;/li&gt;
&lt;li&gt;Using SVM files as a means data format rather than huge dataframes that can not fit in memory&lt;/li&gt;
&lt;li&gt;Orchestrate model generation pipeline using Luigi workflow.&lt;/li&gt;
&lt;li&gt;Controlling error rate using prediction probability thresholds&lt;/li&gt;
&lt;li&gt;Evaluating moderation/fraud detection models.&lt;/li&gt;
&lt;li&gt;Management of hundreds of models and manage their performance across all geographical regions.&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Natural Language Processing: Challenges and Next Frontiers</title><link href="https://pyvideo.org/pydata-berlin-2017/natural-language-processing-challenges-and-next-frontiers.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Barbara Plank</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/natural-language-processing-challenges-and-next-frontiers.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Barbara Plank is tenured Assistant Professor in Natural Language Processing at the University of Groningen, The Netherlands.&lt;/p&gt;
&lt;p&gt;Her research focuses on cross-domain and cross-language NLP. She is interested in robust language technology, learning under sample selection bias (domain adaptation, transfer learning), annotation bias (embracing annotator disagreements in learning), and generally, semi-supervised and weakly-supervised machine learning for a variety of NLP tasks and applications, including syntactic processing, opinion mining, information and relation extraction and personality prediction.&lt;/p&gt;
&lt;p&gt;Natural Language Processing: Challenges and Next Frontiers&lt;/p&gt;
&lt;p&gt;Despite many advances of Natural Language Processing (NLP) in recent years, largely due to the advent of deep learning approaches, there are still many challenges ahead to build successful NLP models. In this talk I will outline what makes NLP so challenging. Besides ambiguity, one major challenges is variability. In NLP, we typically deal with data from a variety of sources, like data from different domains, languages and media, while assuming that our models work well on a range of tasks, from classification to structured prediction. Data variability is an issue that affects all NLP models. I will then delineate one possible way to go about it, by combining recent success in deep multi-task learning with fortuitous data sources, which allows learning from distinct views and distinct sources. This will be one step towards one of the next frontiers: learning under limited (or absence) of annotated resources, for a variety of NLP tasks.&lt;/p&gt;
&lt;p&gt;Link to Q&amp;amp;A: &lt;a class="reference external" href="https://youtu.be/JtiCdsESuT0"&gt;https://youtu.be/JtiCdsESuT0&lt;/a&gt; (Second pyvideo tab)&lt;/p&gt;
</summary></entry><entry><title>On Bandits, Bayes, and swipes: gamification of search</title><link href="https://pyvideo.org/pydata-berlin-2017/on-bandits-bayes-and-swipes-gamification-of-search.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Stefan Otte</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/on-bandits-bayes-and-swipes-gamification-of-search.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The talk will show how to use active learning to work with Small Data. Active learning is an underappreciated subfield of ML where the algorithm actively gathers labeled data, e.g. it can query the user for the most informative data. I will discuss the basics of active learning theory, and look at a case study showing how to use active learning and tailor it to a practical problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first part of the talk is about active learning:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What is it?&lt;/li&gt;
&lt;li&gt;How does it work?&lt;/li&gt;
&lt;li&gt;What are the different flavours of active learning?&lt;/li&gt;
&lt;li&gt;Does it help me solve my problem?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second part presents a case study where we developed an engaging and fun way to search for your dream car.&lt;/p&gt;
&lt;p&gt;The case study&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;uses an intuitive interface (swipes),&lt;/li&gt;
&lt;li&gt;actively and efficiently explores and learns the user's preferences (with multi-armed bandits, an active learning algorithm),&lt;/li&gt;
&lt;li&gt;incorporates priors (clustering and Bayes to avoid popularity biases)&lt;/li&gt;
&lt;li&gt;is scalable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The talk is less about specific tools and libs - even though everything was done with python’s usual suspects (numpy, scipy, sklearn, flask, etc.) - but how to make active learning work for you.&lt;/p&gt;
</summary></entry><entry><title>Open Data Use Cases</title><link href="https://pyvideo.org/pydata-berlin-2017/open-data-use-cases.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Ulrike Thalheim</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/open-data-use-cases.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The talk will give an overview about the current state of Open Data in Germany including recent legislative changes that might lead to more Open Data. Leaving behind the basics, the talk will show the implications to the existing Open Data community. Three very different examples of current projects from the Code for Germany community will be presented showing you the bright side of Open Data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Open Data is the availability of (administrative) data to the public while there are no restrictions in analyzing or editing the data. The current stage of data availability depends largely on the the German states. In 2017, Open Data will regain importance and attention as the German Open Data Law has been passed in the German parliament in spring. Recently, Germany has also joint the Open Government Partnership. As many Open Data activists have been demanding better access to data in the past, how satisfactory are those changes on the government side? Those changes will be briefly discussed regarding their mean for our society and for those who have made use of Open Data already.&lt;/p&gt;
&lt;p&gt;In the second part, I will explain how Open Data can be quite awesome already and what obstacles we have been dealing with in the past by showing 3 different, exciting and very active projects: Measuring air quality (Luftdaten), making election data more accessible (Wahlsalons) and collaboratively collecting and sharing data in a huge database (Wikidata). Open Data does not only need coders. It needs those that lobby for it, those that manage the projects, those that spread the word and most importantly: those who use the applications built upon Open Data. Will it be you, too?&lt;/p&gt;
</summary></entry><entry><title>Pandas from the Inside / "Big Pandas"</title><link href="https://pyvideo.org/pydata-berlin-2017/pandas-from-the-inside-big-pandas.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Stephen Simmons</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/pandas-from-the-inside-big-pandas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is great for data analysis in Python. It promises intuitive DataFrames from R; speed like numpy; groupby like SQL. But there are plenty of pitfalls. This tutorial looks inside pandas to see how DataFrames actually work when building, indexing and grouping tables. You will learn how to write fast, efficient code, and how to scale up to bigger problems with libraries like Dask.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pandas is great way to quickly get started with data analysis in Python: intuitive DataFrames from R; fast numpy arrays under the hood; groupby just like SQL. But this familiarity is deceptive and both new and experienced pandas users often get stuck on things they feel should be simple.&lt;/p&gt;
&lt;p&gt;In the first part of this tutorial, we look inside pandas to see how DataFrames actually work when building, indexing and grouping tables. We will learn which pandas operations are fast and why, and how to avoid common performance pitfalls. By the end of the tutorial, you will develop a strong and reliable intuition about using pandas effectively.&lt;/p&gt;
&lt;p&gt;In the second part, we switch gear to bigger problems where our data sets can't fit in local memory. First we see how pandas behaves as we start to hit memory limits. Then we look at Dask, whose distributed/deferred DataFrames are a near drop-in replacement for pandas. Then we come back to pure pandas and look for ways to manage bigger datasets with clever data storage,.&lt;/p&gt;
&lt;p&gt;During this tutorial, you are welcome to follow along on your laptop with the sample data sets and example code in a Jupyter notebook. These will be made available on GitHub here just before the tutorial. The code targets Python 3 and the latest pandas/dask release.&lt;/p&gt;
</summary><category term="tutorial"></category></entry><entry><title>Patsy: The Lingua Franca to and from R</title><link href="https://pyvideo.org/pydata-berlin-2017/patsy-the-lingua-franca-to-and-from-r.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Max Humber</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/patsy-the-lingua-franca-to-and-from-r.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How to build R-like statistical models in Python with Patsy and scikit-learn.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Creating linear and logistic models in R is dead simple. If your numpy/panda-fu isn’t all that great than it’s a lot harder to do in Python. In R, for instance, you can declare a model with a formula as simple as y ~ x1 + x2. But in Python, you have to split out your target and input variables and make sure that the matrices work within the scikit-learn API.&lt;/p&gt;
&lt;p&gt;In this talk I will introduce the Patsy package for describing and creating statistical models in Python. I’ll walk through how to implement a logistic regression with Patsy and scikit-learn and I’ll emphasize Patsy as a bridge for those who want to better understand Python and/or R.&lt;/p&gt;
</summary></entry><entry><title>Patterns for Collaboration between Data Scientists And Software Engineers</title><link href="https://pyvideo.org/pydata-berlin-2017/patterns-for-collaboration-between-data-scientists-and-software-engineers.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Karolina Alexiou</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/patterns-for-collaboration-between-data-scientists-and-software-engineers.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The talk is going to present, with examples, how a software engineer team can work together with data scientists (both in-house and external collaborators) in order to leverage their unique domain knowledge and skills in analyzing data, while supporting them to work independently and making sure that their work can be constantly tested/evaluated and easily integrated into the larger product.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Collaboration between data scientists and software engineers can have the following issues:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Different tools used between data scientists and engineers (more interactive vs more automated, for example ipython notebook vs command line)&lt;/li&gt;
&lt;li&gt;If getting the latest data requires ops/engineering knowledge then the analysis may be done in &amp;quot;stale&amp;quot; data or a too-small subset of the data (As an example: data scientists working with manual exports )&lt;/li&gt;
&lt;li&gt;Regression testing/parameter tuning/evaluation of results/backfills and other common scenarios in data-driven applications also require more engineering knowledge. The engineers are in the best position to provide tools and processes for the data science team, but it can happen that this potential goes untapped&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Those issues lead to more time to production, unhappiness in the data science team if they end up fighting with operations work instead of doing mostly the work they like, less trustworthy results and less trust between teams in general. If collaboration is done right however, data science and engineering teams can have a very good symbiotic relationship where each person takes advantage of their strengths towards a common goal.&lt;/p&gt;
&lt;p&gt;Some collaboration patterns to foster a good relationship between data scientists and engineers are the following:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Continuous evaluation – making sure the data science algorithm continues to give good results with every commit (or combinations of commits, in case there is several repositories with different data scientists working on them)&lt;/li&gt;
&lt;li&gt;Report templating – data scientists can work with jupyter notebooks with an extension that allows those ipynb files to be used as templates (ie, where some variable values can be filled in later). Those notebooks can then be applied to different datasets to quickly diagnose issues.&lt;/li&gt;
&lt;li&gt;Data API – have a well documented API for the data scientists to have easy access to the data so that they can do their exploration without needing the software engineering team to manually provide exports&lt;/li&gt;
&lt;li&gt;Some flexibility regarding tools – if domain experts prefer to use SFTP to upload files to the server for analysis, let them. Too much flexibility can be an anti-pattern.&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Polynomial Chaos: A technique for modeling uncertainty</title><link href="https://pyvideo.org/pydata-berlin-2017/polynomial-chaos-a-technique-for-modeling-uncertainty.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Emily Gorcenski</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/polynomial-chaos-a-technique-for-modeling-uncertainty.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Parametric uncertainty is broadly difficult to quantify. In particular, when those parameters don't fit nice distributions it can be hard to generate reasonable simulations. Polynomial chaos is a somewhat obscure technique that leverages a natural connection between probability distributions and orthogonal polynomial families. This talk will demonstrate the technique and its applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There is an intricate link between orthogonal polynomial families and well-known probability distributions. Known as Polynomial Chaos, this technique is largely unknown outside of some engineering fields. Nevertheless, the method allows us to model arbitrary distributions (with finite second moment) using distributions that are more familiar, e.g. the uniform or normal distributions. The polynomial chaos technique shifts the burden of understanding random variables to one of understanding deterministic series coefficients.&lt;/p&gt;
&lt;p&gt;This method is particularly good for understanding dynamical systems with parametric uncertainty. The Polynomial Chaos expansion allows us to generate Monte Carlo simulations with far fewer simulation runs. In addition, we can use the method to quantify uncertainty in observations even when faced with small sample sizes. This talk will demonstrate the derivation of the technique and include some Python examples of ways it can be used to model systems and understand data in the presence of uncertainty. This will be a highly technical talk, touching on elements of measure-theoretic probability and functional analysis.&lt;/p&gt;
</summary></entry><entry><title>Semi-Supervised Bootstrapping of Relationship Extractors</title><link href="https://pyvideo.org/pydata-berlin-2017/semi-supervised-bootstrapping-of-relationship-extractors.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>David Soares Batista</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/semi-supervised-bootstrapping-of-relationship-extractors.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Semi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed relationships while limiting the semantic drift. This talk presents an approach to bootstrap relationship instances using word embeddings to find similar relationships. Results show that relying on word embeddings achieves a better performance than using TF-IDF weighted vectors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Relationship Extraction (RE) transforms unstructured text into relational triples, each representing a relationship between two named-entities. This relationships can then be used to populate knowledge bases, or build knowledge graphs, which can support several tasks, such as Question Answering.&lt;/p&gt;
&lt;p&gt;A bootstrapping system for RE starts with a collection of documents and a few seed instances. The system scans the document collection, collecting occurrence contexts for the seed instances. Then, based on these contexts, the system generates extraction patterns. The documents are scanned again using the patterns to match new relationship instances. These newly extracted instances are then added to the seed set, and the process is repeated until a certain stop criteria is met.&lt;/p&gt;
&lt;p&gt;Bootstrapping approaches relying on TF-IDF weighted vectors have limitations when trying to find similar instances, since the similarity between any two relationship instance vectors is only positive when the instances share at least one term. For instance, the phrases was &amp;quot;founded by&amp;quot; and is the &amp;quot;co-founder of&amp;quot; do not have any common words, but they have the same semantics. Stemming techniques can aid in these cases, but only for variations of the same root word. By relying on word embeddings, the similarity of two phrases can be captured even if no common words exist. For instance, the word embeddings for &amp;quot;co-founder&amp;quot;, &amp;quot;founded&amp;quot; and &amp;quot;creator&amp;quot; should be similar, since these words tend to occur in the same contexts.&lt;/p&gt;
&lt;p&gt;I propose to present a system which extracts relationship instances by bootstrapping and by relying on word embeddings. It was evaluated against a popular system which relies on TF-IDF weighted vectors, the paper describing the system was presented at EMNLP'15 and won an honorable mention for best short-paper award.&lt;/p&gt;
</summary></entry><entry><title>Social Networks and Protest Participation: Evidence from 130 Million Twitter Users</title><link href="https://pyvideo.org/pydata-berlin-2017/social-networks-and-protest-participation-evidence-from-130-million-twitter-users.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Jonathan Ronen</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/social-networks-and-protest-participation-evidence-from-130-million-twitter-users.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data mining social networks for evidence of political participation. A demonstration of python being used to data mine the twitter conversations around the #JeSuisCharlie hashtag, and analyzing it to learn about real world protest behavior.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pinning down the role of social ties in the decision to protest has been notoriously elusive, largely due to data limitations. The era of social media and its global use by protesters offers an unprecedented opportunity to observe real-time social ties and online behavior, though often without an attendant measure of real-world behavior. We collect data on Twitter activity during the 2015 Charlie Hebdo protest in Paris which, unusually, record real-world protest attendance and high-resolution network structure. We draw on a theory of participation in which protest decisions depend on exposure to others' intentions, and network position determines exposure. Our findings are strong and consistent with this theory, showing that, relative to comparable Twitter users, protesters are significantly more connected to one another via direct, indirect, triadic, and reciprocated ties. These results offer the first large-scale empirical support for the claim that social network structure has consequences for protest participation.&lt;/p&gt;
</summary></entry><entry><title>Spying on my Network for a Day: Data Analysis for Networks</title><link href="https://pyvideo.org/pydata-berlin-2017/spying-on-my-network-for-a-day-data-analysis-for-networks.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Aisha Bello</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/spying-on-my-network-for-a-day-data-analysis-for-networks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I would show how I used open source tools like Moloch, Wireshark, Bokeh and Jupyter to analyse my home network data for the day. Not just the volume of data generated daily, but how interesting it is to leverage data tools to discover when your network is experiencing downtime which could be as a result of packet loss, poorly placed Access points or just proximity away from your rout&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For every button on the webpage that is clicked , for every picture uploaded; There is a tremendous amount of data that goes over every network from users data via phones , tablets or even computers accessing a youtube or netflix Data center, to sending larger traffic between different Virtual Machines. But how does one actually analyze this volume of data sent.&lt;/p&gt;
&lt;p&gt;Network data analysis can go a long way in helping users find out the inefficiencies in their network. Understanding Networks makes for a very strong foundation on what everything else is built upon.&lt;/p&gt;
&lt;p&gt;In this talk I will cover the basics of :&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;An IP Packet data analysis: the basic and most fundamental thing everything on a network builds upon. A packet can carry a lot of information and interesting details about behavioural habits of a user or even attributes of a device. Understanding the nature of the IP packet data leads to a larger understanding of Networks when analysed on a scale. And like the data rule of thumb the more data over a period of time you’re able to learn from the better, and more accurate your intelligence become.&lt;/li&gt;
&lt;li&gt;Evaluating your local network using visualisation tools like IPython Notebook and Bokeh.&lt;/li&gt;
&lt;li&gt;Gathering insights from the data to answer questions like How much data on an average do I generate on a daily basis? Why is my network slow ? Am I really getting what was promised from my Service Provider ?, what applications do I spend the most of my bandwidth on ? Why is my network experiencing down time? How can I automate the process of analysing my network data and much more.&lt;/li&gt;
&lt;/ul&gt;
</summary></entry><entry><title>Text similiarity with the next generation of word embeddings in Gensim</title><link href="https://pyvideo.org/pydata-berlin-2017/text-similiarity-with-the-next-generation-of-word-embeddings-in-gensim.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Lev Konstantinovskiy</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/text-similiarity-with-the-next-generation-of-word-embeddings-in-gensim.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What is the closest word to &amp;quot;king&amp;quot;? Is it &amp;quot;Canute&amp;quot; or is it &amp;quot;crowned&amp;quot;? There are many ways to define &amp;quot;similar words&amp;quot; and &amp;quot;similar texts&amp;quot;. Depending on your definition you should choose a word embedding to use. There is a new generation of word embeddings added to Gensim open source NLP package using morphological information and learning-to-rank: Facebook's FastText, VarEmbed and WordRank.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are many ways to find similar words/docs with an open-source Natural Language processing library Gensim that I maintain. I will give an overview of modern word embeddings like Google's Word2vec, Facebook's FastText, GloVe, WordRank, VarEmbed and discuss what business tasks fit them best.&lt;/p&gt;
&lt;p&gt;What is the most similar word to &amp;quot;king&amp;quot;? It depends on what you mean by similar. &amp;quot;King&amp;quot; can be interchanged with &amp;quot;Canute&amp;quot;, but it's attribute is &amp;quot;crown&amp;quot;. We will discuss how to achieve these two kinds of similarity from word embeddings.&lt;/p&gt;
</summary></entry><entry><title>The Future of Cybersecurity Needs You, Here is Why.</title><link href="https://pyvideo.org/pydata-berlin-2017/the-future-of-cybersecurity-needs-you-here-is-why.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Verónica Valeros</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/the-future-of-cybersecurity-needs-you-here-is-why.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Verónica Valeros is a hacker, researcher and intelligence analyst from Argentina. Her research has a strong focus on helping people and involves different areas from wireless and bluetooth privacy issues to malware, botnets and intrusion analysis. She has presented her research on international conferences such as BlackHat, EkoParty, Botconf and others. She is the co-founder of the MatesLab hackerspace based in Argentina. Since 2013 she is part of the Cognitive Threat Analytics team (Cisco Systems) where she specialises on malware network traffic analysis and threats’ categorisation at big scale. She is also part of the core team of Security Without Borders, a collective of cyber security professionals who volunteer assisting people at risk and NGOs on cyber security issues.&lt;/p&gt;
&lt;p&gt;The Future of Cybersecurity Needs You, Here is Why.&lt;/p&gt;
&lt;p&gt;In the last decade we have observed a shift in cybersecurity. Cyber threats started to impact more and more our daily lives, even to the point of threatening our physical safety. We learnt that attackers are well aware of our weaknesses and limitations, that they take advantage of this knowledge and that for being successful they need to be just a little better than us. As defendants, we struggle. We perfected existing solutions to protect our environments with some degree of success but still today we fall behind adversaries more often than not. We got really good at collecting data until the point of not being able to use it in its full extent. This lead us to ask ourselves, Is this it? Is this all we can do? The future of cybersecurity needs you, join me on this talk to find out why.&lt;/p&gt;
&lt;p&gt;Link to Q&amp;amp;A: &lt;a class="reference external" href="https://youtu.be/Ln0wxTTpOvs"&gt;https://youtu.be/Ln0wxTTpOvs&lt;/a&gt; (Second pyvideo tab)&lt;/p&gt;
</summary></entry><entry><title>The path between developing and serving machine learning models.</title><link href="https://pyvideo.org/pydata-berlin-2017/the-path-between-developing-and-serving-machine-learning-models.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Adrin Jalali</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/the-path-between-developing-and-serving-machine-learning-models.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As a data scientist, one of the challenges after you develop and train your model, is to deploy it in production where other systems would use the output of the model in real time. In this tutorial we use PipelineIO, to deploy a cluster on the cloud, which gives us a JupyterHub to develop our method, and uses PMML to persist and deploy and serve the model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whenever you have a machine learning module in your pipeline, persisting and serving the model is not yet a trivial task. This tutorial shows how an open source framework using several open source technologies could potentially solve the problem.&lt;/p&gt;
&lt;p&gt;My journey started with this[1] question on StackOverflow. I wanted to be able to do my usual data science stuff, mostly in python, and then deploy them somewhere serving like a REST API, responding to requests in real-time, using the output of the trained models. My original line of thought was this workflow:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;train the model in python or pyspark or in scala in apache spark.&lt;/li&gt;
&lt;li&gt;get the model, put it in an apache flink stream and serve.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was the point at which I had been reading and watching tutorials and attending meetups related to these technologies. I was looking for a solution which is better than:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;train models in python&lt;/li&gt;
&lt;li&gt;write a web-service using flask, put it behind a apache2 server, and put a bunch of them behind a load balancer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This just sounded wrong, or at its best, not scalable. After a bit of research, I came across PipelineIO[2,3] which seems to promise exactly what I'm looking for. In this tutorial we use PipelineIO, to deply a cluster on the cloud, which gives us a JupyterHub to develop our method, and uses PMML to persist and deploy and serve the model. My own jurney and take from PipelineIO are documented github[4]. I'll use Amazon AWS, but PipelineIO uses Kubernetes and you can easily deploy in any environment in which you can use Kubernetes.&lt;/p&gt;
&lt;p&gt;If you work in an environment in which you have different machine learning modules, which should be used in production in real time and as a part of a stream processing pipeline, this talk is for you.&lt;/p&gt;
&lt;p&gt;[1] &lt;a class="reference external" href="http://stackoverflow.com/questions/42719953/how-to-develop-a-rest-api-using-an-ml-model-trained-on-apache-spark"&gt;http://stackoverflow.com/questions/42719953/how-to-develop-a-rest-api-using-an-ml-model-trained-on-apache-spark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a class="reference external" href="http://pipeline.io"&gt;http://pipeline.io&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a class="reference external" href="https://github.com/fluxcapacitor/pipeline"&gt;https://github.com/fluxcapacitor/pipeline&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a class="reference external" href="https://github.com/adrinjalali/pipeline-docs"&gt;https://github.com/adrinjalali/pipeline-docs&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>TNaaS - Tech Names as a Service</title><link href="https://pyvideo.org/pydata-berlin-2017/tnaas-tech-names-as-a-service.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Vincent D. Warmerdam</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/tnaas-tech-names-as-a-service.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I will explain how I built a service that generates Pokemon names. You'd be surprised how hard it is to do this properly and how easy it is to do it practically.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;TNaaS - Tech Names as a Service Deploying a Random Generator that's Phonetically Pokemon&lt;/p&gt;
&lt;p&gt;There is a striking phonetic similarity between big data technology and pokemon names. This has caused some hilarious conversations with recruiters on linkedin: &lt;a class="reference external" href="https://www.linkedin.com/in/vincentwarmerdam/"&gt;https://www.linkedin.com/in/vincentwarmerdam/&lt;/a&gt; but it also made me wonder. Can I create a service that generates strings that sound like potential pokemon names and what might be the simplest possible way to make that into a service? Also, would it be possible to generate pokemon names that start with three random characters and end with 'base' (KREBASE, MONBASE would be appropriate but IEYBASE would not be).&lt;/p&gt;
&lt;p&gt;Turns out that this is an interesting problem from a ML standpoint and that it is rediculously easy to build in the cloud. In my talk I will explain the ML behind it;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;markov chains&lt;/li&gt;
&lt;li&gt;probibalistic graphs&lt;/li&gt;
&lt;li&gt;rnn/lstm&lt;/li&gt;
&lt;li&gt;bidirectional lstm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During the talk I will also do a deep dive on the pros and cons of these methods.&lt;/p&gt;
&lt;p&gt;I currently have a 101 version of the service live here: &lt;a class="reference external" href="https://dyccxmwpz9.execute-api.eu-west-1.amazonaws.com/pokemon/poke-names/v2"&gt;https://dyccxmwpz9.execute-api.eu-west-1.amazonaws.com/pokemon/poke-names/v2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;By the time of the conference it will have a proper front end and I have already bought the domain name of tnaas.com.&lt;/p&gt;
&lt;p&gt;I've talked about a similar topic in the past: &lt;a class="reference external" href="https://youtu.be/TkHT3sLwtkY?t=22m10s"&gt;https://youtu.be/TkHT3sLwtkY?t=22m10s&lt;/a&gt;. The goal of this talk is to spend more time explaining how I actually built the service; both from a coding perspective as well as a deployment perspective.&lt;/p&gt;
&lt;p&gt;Let me know if there are any questions. I am submitting multiple talks that I think are interesting and relevant to the PyData crowd, I'll gladly leave it to the committee which (or if any) of them are relevant to the local community.&lt;/p&gt;
</summary></entry><entry><title>Topic Modelling (and more) with NLP framework Gensim</title><link href="https://pyvideo.org/pydata-berlin-2017/topic-modelling-and-more-with-nlp-framework-gensim.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Bhargav Srinivasa Desikan</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/topic-modelling-and-more-with-nlp-framework-gensim.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/bhargavvader/personal/tree/master/notebooks/text_analysis_tutorial"&gt;https://github.com/bhargavvader/personal/tree/master/notebooks/text_analysis_tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial will guide you through the process of analysing your textual data through topic modelling - from finding and cleaning your data, pre-processing using spaCy, applying topic modelling algorithms using gensim - before moving on to more advanced textual analysis techniques.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Topic Modelling is a great way to analyse completely unstructured textual data - and with the python NLP framework Gensim, it's very, very easy to do this. The purpose of this tutorial is to guide one through the whole process of topic modelling - right from pre-processing your raw textual data, creating your topic models, evaluating the topic models, to visualising them. Advanced topic modelling techniques will also be covered in this tutorial, such as Dynamic Topic Modelling, Topic Coherence, Document Word Coloring, and LSI/HDP.&lt;/p&gt;
&lt;p&gt;The python packages used during the tutorial will be spaCy (for pre-processing), gensim (for topic modelling), and pyLDAvis (for visualisation). The interface for the tutorial will be an Jupyter notebook.&lt;/p&gt;
&lt;p&gt;The takeaway from the tutorial would be the participants ability to get their hands dirty with analysing their own textual data, through the entire lifecycle of cleaning raw data to visualising topics.&lt;/p&gt;
</summary><category term="tutorial"></category></entry><entry><title>Towards Pythonic Innovation in Recommender Systems</title><link href="https://pyvideo.org/pydata-berlin-2017/towards-pythonic-innovation-in-recommender-systems.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Carlotta Schatten</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/towards-pythonic-innovation-in-recommender-systems.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recommender Systems are nowadays ubiquitous in our lives. Although many implementations of basic algorithms are well known, recent advances in the field are often less documented. This talks aims at reviewing available Recommender Systems libraries in Python, including cutting edge Time- and Context-aware state of the art models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Collaborative Filtering is the most commonly used Recommender System because of its ability to process sparse data. Many implementations of the basic Collaborative Filtering algorithms are available, however, recent advances in the field are often less documented and known. In order to provide a comparative analysis of available libraries and define minimal requirements for a performance comparison of Recommender System algorithms, the talk will be structured in two parts. In the first part, the speaker will introduce the last years of relevant literature in the field of Recommender System, giving an overview on advanced tensor approaches, such as those used for Context- and Time-aware Recommender Systems. In the second part of the talk, existing Python Collaborative Filtering libraries are reviewed. Among others, the following important criteria will be considered: availability of state of the art and benchmark algorithms, parallel computation, maintenance, and easiness of use.&lt;/p&gt;
</summary></entry><entry><title>Where are we looking? Predicting human gaze using deep networks.</title><link href="https://pyvideo.org/pydata-berlin-2017/where-are-we-looking-predicting-human-gaze-using-deep-networks.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Oliver Eberle</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/where-are-we-looking-predicting-human-gaze-using-deep-networks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Which features in an image draw our focus to a specific area while neglecting others entirely? This fascinating question has been motivating researchers for decades but also sparked interest in design and marketing. Thus, saliency models aim at identifying locations that stand out from their visual neighbourhood. Using tensorflow and matplotlib this talk will shed some light on these features..&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Visual saliency models aim at describing human eye fixations and finding the most relevant features in a visual scene. From experience one can justify two important processes that drive saliency: First, low level features like color, intensity or orientation contrast and second, high-level features like objects, faces or signs.&lt;/p&gt;
&lt;p&gt;Eye fixations serve as an experimental setup to study saliency phenomena and give insight into the ways we attend scenes. Hereby, computational modelling is used to explain what information processing might be responsible for saliency. Evaluating how well models explain observed fixations give a framework of identifying what features contribute to saliency by designing models with different, e.g. high- or low-level feature extractors.&lt;/p&gt;
&lt;p&gt;Many saliency models have used low-level features but were faced with drawbacks in explaining pronounced saliency caused by high-level contributions. Simply adding face or object detectors has been a plausible follow-up but revealed little about the underlying mechanisms.&lt;/p&gt;
&lt;p&gt;Recent advances in object classification by training convolutional neural networks (CNN) have revealed rich filter representations in a wide range of high-level features and are therefore a promising candidate in building models of visual processing (e.g. VGG-19 trained on ImageNet).&lt;/p&gt;
</summary></entry><entry><title>“Which car fits my life?” - mobile.de’s approach to recommendations</title><link href="https://pyvideo.org/pydata-berlin-2017/which-car-fits-my-life-mobiledes-approach-to-recommendations.html" rel="alternate"></link><published>2017-06-30T00:00:00+00:00</published><updated>2017-06-30T00:00:00+00:00</updated><author><name>Florian Wilhelm</name></author><id>tag:pyvideo.org,2017-06-30:pydata-berlin-2017/which-car-fits-my-life-mobiledes-approach-to-recommendations.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As Germany’s largest online vehicle marketplace mobile.de uses recommendations at scale to help users find the perfect car. We elaborate on collaborative &amp;amp; content-based filtering as well as a hybrid approach addressing the problem of a fast-changing inventory. We then dive into the technical implementation of the recommendation engine, outlining the various challenges faced and experiences made.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At mobile.de, Germany’s biggest car marketplace, a dedicated team of data engineers and scientists, supported by the IT project house inovex is responsible for creating intelligent data products. Driven by our company slogan “Find the car that fits your life”, we focus on personalised recommendations to address several user needs. Thereby we improve customer experience during browsing as well as finding the perfect offering. In an introduction to recommendation systems, we briefly mention the traditional approaches for recommendation engines, thereby motivating the need for sophisticated approaches. In particular, we explain the different concepts including collaborative and content-based filtering as well as hybrid approaches and general matrix factorisation methods. This is followed by a deep dive into the implementation and architecture at mobile.de that comprises ElasticSearch, Cassandra and Mahout. We explain how Python and Java is used simultaneously to create and serve recommendations.&lt;/p&gt;
&lt;p&gt;By presenting our car-model recommender that suggests similar car models of different brands as a concrete use-case, we reiterate on key-aspects during modelling and implementation. In particular, we present a matrix factorisation library that we used and share our experiences with it. We conclude by a brief demonstration of our results and discuss the improvements we achieved in terms of key performance indicators. Furthermore, we use our use case to exemplify the usage of deep learning for recommendations, comparing it with other traditional approaches and hence providing a brief account of the future of recommendation engines.&lt;/p&gt;
</summary></entry></feed>