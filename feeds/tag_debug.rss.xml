<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Sun, 30 Sep 2018 00:00:00 +0000</lastBuildDate><item><title>How to debug. Everything</title><link>https://pyvideo.org/odessapy-2018/how-to-debug-everything.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You know that person everybody comes to when the shit hits the fan and that bug is not going to fix itself? Want to become one? Come to learn about all the skills you'll need when your application freezes in production, leaks memory like crazy or dies under load in fifteen seconds&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Vsevolod Solovyov</dc:creator><pubDate>Sun, 30 Sep 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-09-30:odessapy-2018/how-to-debug-everything.html</guid><category>debug</category></item><item><title>Debugging PySpark -- Or trying to make sense of a JVM stack trace when you were minding your own bus</title><link>https://pyvideo.org/pycon-us-2018/debugging-pyspark-or-trying-to-make-sense-of-a-jvm-stack-trace-when-you-were-minding-your-own-bus.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common errors and how to detect them.&lt;/p&gt;
&lt;p&gt;Spark’s own internal logging can often be quite verbose, and this talk will examine how to effectively search logs from Apache Spark to spot common problems. In addition to the internal logging, this talk will look at options for logging from within our program itself.&lt;/p&gt;
&lt;p&gt;Spark’s accumulators have gotten a bad rap because of how they interact in the event of cache misses or partial recomputes, but this talk will look at how to effectively use Spark’s current accumulators for debugging as well as a look to future for data property type accumulators which may be coming to Spark in future version.&lt;/p&gt;
&lt;p&gt;In addition to reading logs, and instrumenting our program with accumulators, Spark’s UI can be of great help for quickly detecting certain types of problems.&lt;/p&gt;
&lt;p&gt;Debuggers are a wonderful tool, however when you have 100 computers the “wonder” can be a bit more like “pain”. This talk will look at how to connect remote debuggers, but also remind you that it’s probably not the easiest path forward.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Holden Karau</dc:creator><pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-11:pycon-us-2018/debugging-pyspark-or-trying-to-make-sense-of-a-jvm-stack-trace-when-you-were-minding-your-own-bus.html</guid><category>debug</category><category>pySpark</category></item><item><title>Love your bugs</title><link>https://pyvideo.org/pycon-us-2018/love-your-bugs.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Wrestling bugs can be one of the most frustrating parts of programming - but with the right framing, bugs can also be our best allies. I'll tell the tales of two of my favorite bugs, including the time I triggered a DDOS of a logging cluster, and explain why I love them. I'll also give you concrete strategies for approaching tricky bugs and making them easier and more fun.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Allison Kaptur</dc:creator><pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-11:pycon-us-2018/love-your-bugs.html</guid><category>debug</category></item></channel></rss>