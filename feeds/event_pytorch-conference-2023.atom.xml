<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - PyTorch Conference 2023</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pytorch-conference-2023.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2023-10-16T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Accelerating Explorations in Vision and Multimodal AI Using Pytorch Libraries</title><link href="https://pyvideo.org/pytorch-conference-2023/accelerating-explorations-in-vision-and-multimodal-ai-using-pytorch-libraries.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Nicolas Hug</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/accelerating-explorations-in-vision-and-multimodal-ai-using-pytorch-libraries.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyTorch Libraries provide building blocks (data processing transforms, modeling components, loss functions, etc.) on top of PyTorch as well as examples and tutorials on how to use these building blocks for training SoTA Models. In this talk, we’ll provide insights into ongoing work to accelerate exploration in multimodal …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyTorch Libraries provide building blocks (data processing transforms, modeling components, loss functions, etc.) on top of PyTorch as well as examples and tutorials on how to use these building blocks for training SoTA Models. In this talk, we’ll provide insights into ongoing work to accelerate exploration in multimodal understanding and generative AI using TorchMultimodal. We'll also present TorchVision's new transforms API, with added support for image detection, segmentation, and video tasks.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Accelerating Generative AI</title><link href="https://pyvideo.org/pytorch-conference-2023/accelerating-generative-ai.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Christian Puhrsch</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/accelerating-generative-ai.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a Cambrian explosion of performant and efficient methods to train and serve generative AI models within the community. The PyTorch team will present optimizations to transformer based Generative AI models, using pure, native PyTorch. In this talk we aim to cover both new techniques in PyTorch for …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is a Cambrian explosion of performant and efficient methods to train and serve generative AI models within the community. The PyTorch team will present optimizations to transformer based Generative AI models, using pure, native PyTorch. In this talk we aim to cover both new techniques in PyTorch for driving efficiency gains, as well as showcasing how they can be composed on popular Generative AI models. Highlights will include methods spanning torch compile, quantization, sparsity, memory efficient attention, reducing padding.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Composable Distributed PT2(D)</title><link href="https://pyvideo.org/pytorch-conference-2023/composable-distributed-pt2d.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Wanchao Liang</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/composable-distributed-pt2d.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this session, we will explore the technology advancements of PyTorch Distributed, and dive into the details of how multi-dimensional parallelism is made possible to train Large Language Models by composing different PyTorch native distributed training APIs.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Cost Effectively Deploy Thousands of Fine Tuned Gen AI Models Like Llama Using TorchServe on AWS</title><link href="https://pyvideo.org/pytorch-conference-2023/cost-effectively-deploy-thousands-of-fine-tuned-gen-ai-models-like-llama-using-torchserve-on-aws.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Saurabh Trikande</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/cost-effectively-deploy-thousands-of-fine-tuned-gen-ai-models-like-llama-using-torchserve-on-aws.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As Generative AI adoption accelerates across industry, organizations want to deliver hyper-personalized experiences to end users. For building such experiences, thousands of models are being developed by fine-tuning pre-trained large models. To meet their stringent latency and throughput goals, organizations use GPU instances to deploy such models. However, inference …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As Generative AI adoption accelerates across industry, organizations want to deliver hyper-personalized experiences to end users. For building such experiences, thousands of models are being developed by fine-tuning pre-trained large models. To meet their stringent latency and throughput goals, organizations use GPU instances to deploy such models. However, inference costs can add up quickly if deploying thousands of models and provisioning dedicated hardware for each. TorchServe offers feature likes open platform, deferred distribution initialization, model sharing and heterogeneous deployment that make it easy for users to deploy fine tuned large models and save cost. Learn how organization can use these features in conjunction with fine tuning techniques like PEFT (Parameter Efficient Fine Tuning) and use Amazon SageMaker Multi-Model Endpoint (MME) to deploy multiple GenAI models on the same GPU, share GPU instances across thousands of GenAI models, and dynamically load/unload models based on incoming traffic. All of which helps you significantly reduce the cost. Finally we showcase example code for deploying multiple Llama based models which are fine tuned using PEFT on MME.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Distributed Checkpoint</title><link href="https://pyvideo.org/pytorch-conference-2023/distributed-checkpoint.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Iris Zhang</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/distributed-checkpoint.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will present checkpoint features for distributed training. Distributed checkpoint support saving and loading from multiple ranks in parallel. It handles load-time resharding which enables saving in one cluster topolgy and loading to another. It also supports saving in one parallelism and loading into another. It is currently …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will present checkpoint features for distributed training. Distributed checkpoint support saving and loading from multiple ranks in parallel. It handles load-time resharding which enables saving in one cluster topolgy and loading to another. It also supports saving in one parallelism and loading into another. It is currently adopted by IBM, Mosaic, and XLA for FSDP checkpoint, and it is also being used for Shampoo OSS release checkpointing support. We will talk about distributed checkpoint support today and what is coming up next.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Getting Started with Pytorch 2.0 and Hugging Face Transformers</title><link href="https://pyvideo.org/pytorch-conference-2023/getting-started-with-pytorch-20-and-hugging-face-transformers.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Philipp Schmid</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/getting-started-with-pytorch-20-and-hugging-face-transformers.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The session will highlight the new features of PyTorch 2.0 and how to get started with PyTorch 2.0 and Hugging Face Transformers today. It will cover how to fine-tune a BERT model for Text Classification using the newest PyTorch 2.0 features.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Into Generative AI with PyTorch Lightning 2.0</title><link href="https://pyvideo.org/pytorch-conference-2023/into-generative-ai-with-pytorch-lightning-20.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Luca Antiga</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/into-generative-ai-with-pytorch-lightning-20.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Four years after its initial release, PyTorch Lightning has become one of the most used deep learning frameworks. It has been adopted by tens of thousands of companies and academic groups, contributing to drive the adoption of PyTorch to where it is today. The advent of generative AI has …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Four years after its initial release, PyTorch Lightning has become one of the most used deep learning frameworks. It has been adopted by tens of thousands of companies and academic groups, contributing to drive the adoption of PyTorch to where it is today. The advent of generative AI has led to new challenges in training large models, and PyTorch Lightning has empowered the industry by making many of the latest innovations accessible and robust. PyTorch Lightning powers state-of-the-art generative AI models like StableDiffusion and SDXL. It has been adopted in exciting new directions for LLMs like Hyena Hierarchy and State Space models, as well as the new RWKW recurrent architecture. On top of all that, the PyTorch Lightning-based NVIDIA NeMo, which includes NeMo Megatron, is enabling companies to train LLMs up to hundreds billions parameters. In this talk we will explore generative AI applications powered by PyTorch Lightning, and cover the latest PyTorch Lightning 2.0 features that make working with large models easy. We will also discuss how Lightning Fabric powers lit-gpt, which has been adopted as the starter kit for the recent LLM Efficiency Challenge at NeurIPS 2023.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Introducing ExecuTorch from PyTorch Edge: On-Device AI Stack and Ecosystem, and Our Unique Differentiators</title><link href="https://pyvideo.org/pytorch-conference-2023/introducing-executorch-from-pytorch-edge-on-device-ai-stack-and-ecosystem-and-our-unique-differentiators.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Mergen Nachin</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/introducing-executorch-from-pytorch-edge-on-device-ai-stack-and-ecosystem-and-our-unique-differentiators.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This high-level presentation focuses on the technological advancements in PyTorch Edge, our on-device AI stack. We will provide an overview of the current market landscape and delve into PyTorch Edge's architecture, unique differentiators, and design trade-offs. Discover how PyTorch Edge bridges the gap between research and production, offering performance …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This high-level presentation focuses on the technological advancements in PyTorch Edge, our on-device AI stack. We will provide an overview of the current market landscape and delve into PyTorch Edge's architecture, unique differentiators, and design trade-offs. Discover how PyTorch Edge bridges the gap between research and production, offering performance, portability, and productivity for on-device AI applications.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Keynote: AMD &amp; PyTorch: A Powerful Combination for Generative AI</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-amd-pytorch-a-powerful-combination-for-generative-ai.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Negin Oliver</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-amd-pytorch-a-powerful-combination-for-generative-ai.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Artificial Intelligence (AI) is a rapidly evolving field with diverse applications, and AMD is at the forefront of this revolution, offering a wide-ranging portfolio of AI solutions. In this keynote talk, learn about AMD’s extensive portfolio of AI solutions from Cloud to Edge to Endpoints and their support …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Artificial Intelligence (AI) is a rapidly evolving field with diverse applications, and AMD is at the forefront of this revolution, offering a wide-ranging portfolio of AI solutions. In this keynote talk, learn about AMD’s extensive portfolio of AI solutions from Cloud to Edge to Endpoints and their support for PyTorch framework. We will also showcase the growing AI ecosystem around AMD solutions facilitating a rich experience for AI users.
By the end of this talk, you will learn how to leverage the synergy of AMD and PyTorch to create amazing generative AI applications with ease and efficiency.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: Building an Interoperable Ecosystem for Generative AI</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-building-an-interoperable-ecosystem-for-generative-ai.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Stella Biderman</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-building-an-interoperable-ecosystem-for-generative-ai.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Generative AI can be daunting given the rapid growth and new information flooding the broader community. Leveraging her extensive experience in the construction of Generative AI ecosystems, Stella Biderman will highlight areas of the open-source AI community ready to offer invaluable support to industry beginners. Her keynote will discuss …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Generative AI can be daunting given the rapid growth and new information flooding the broader community. Leveraging her extensive experience in the construction of Generative AI ecosystems, Stella Biderman will highlight areas of the open-source AI community ready to offer invaluable support to industry beginners. Her keynote will discuss the endeavors of over a dozen organizations collaboratively dedicated to provide such resources and methods to become actively engaged in the field, as well as the importance of interoperability.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: How PyTorch Became the Foundation of the AI Revolution</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-how-pytorch-became-the-foundation-of-the-ai-revolution.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Joe Spisak</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-how-pytorch-became-the-foundation-of-the-ai-revolution.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Keynote: How PyTorch Became the Foundation of the AI Revolution - Joe Spisak, Product Director, Meta&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: How to Leverage PyTorch to Scale AI Training and Inferencing</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-how-to-leverage-pytorch-to-scale-ai-training-and-inferencing.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Raghu Ganti</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-how-to-leverage-pytorch-to-scale-ai-training-and-inferencing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As generative AI models grow larger and more complex, the ability to scale these models becomes a critical challenge facing enterprises today. How can developers leverage PyTorch to maximize the value of these large, multi-billion parameter models to make them run faster, more efficiently, and more affordably both on-prem …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As generative AI models grow larger and more complex, the ability to scale these models becomes a critical challenge facing enterprises today. How can developers leverage PyTorch to maximize the value of these large, multi-billion parameter models to make them run faster, more efficiently, and more affordably both on-prem and in the cloud? This keynote will highlight various levers that PyTorch FSDP provides to scale AI model training on hundreds of GPUs and how IBM applied them to obtain state-of-the-art training throughput in models with up to 70 billion parameters. It will also discuss how we combined the latest advancements in PyTorch compile with custom tensor parallel implementation to achieve significantly reduced inferencing latency.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: Intel and PyTorch: Enabling AI Everywhere with Ubiquitous Hardware and Open Software</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-intel-and-pytorch-enabling-ai-everywhere-with-ubiquitous-hardware-and-open-software.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Fan Zhao</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-intel-and-pytorch-enabling-ai-everywhere-with-ubiquitous-hardware-and-open-software.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Generative AI technologies have accelerated our journey to an “AI Everywhere” reality and enabling greater access to AI has great societal value. Ubiquitous hardware and open software are the keys to democratizing AI and its benefits. Intel’s rich AI hardware and software portfolios in conjunction with optimized open …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Generative AI technologies have accelerated our journey to an “AI Everywhere” reality and enabling greater access to AI has great societal value. Ubiquitous hardware and open software are the keys to democratizing AI and its benefits. Intel’s rich AI hardware and software portfolios in conjunction with optimized open frameworks such as PyTorch and its ecosystem libraries provide compelling options for any business or entity looking to innovate, develop, and deploy AI applications at scale. In this talk, we are going to provide a brief overview of Intel’s AI solutions, explore how we are working with the PyTorch community to advance “AI Everywhere”, and showcase how you can easily leverage hardware and software AI acceleration to seamlessly optimize your applications.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: PyTorch 2.1 Technical Deep Dive</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-pytorch-21-technical-deep-dive.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Mario Lezcano</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-pytorch-21-technical-deep-dive.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This Deep Dive provides an update on the PT2 development since last conference and dives into the key new features coming in PyTorch 2.1 This will provide high level updates on compile, distributed, inference, export and edge.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: PyTorch Lightning: Powering the GenAI Revolution from Research to the Enterprise</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-pytorch-lightning-powering-the-genai-revolution-from-research-to-the-enterprise.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>William Falcon</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-pytorch-lightning-powering-the-genai-revolution-from-research-to-the-enterprise.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Since its release in 2019, PyTorch Lightning has boosted the adoption of PyTorch in both research and the enterprise, and it is now at the center of the GenAI revolution. William Falcon, CEO at Lightning AI, introduces PyTorch Lightning 2.1, the latest release bringing key new features to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Since its release in 2019, PyTorch Lightning has boosted the adoption of PyTorch in both research and the enterprise, and it is now at the center of the GenAI revolution. William Falcon, CEO at Lightning AI, introduces PyTorch Lightning 2.1, the latest release bringing key new features to Trainer and Fabric specifically targeted at large model support. He lays down the vision for the future and announces the latest initiative around Lightning AI's commitment to PyTorch and its community.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: Refik Anadol Studio: Rainforest AI Research</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-refik-anadol-studio-rainforest-ai-research.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Christian Burke</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-refik-anadol-studio-rainforest-ai-research.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;During PyTorch 2022, Refik Anadol and Christian Burke discussed the Studio's history in AI art and unveiled its future endeavors in researching the applications of AI to model and preserve the Amazon Rainforest. This year, we would like to discuss and demonstrate our custom AI Rainforest models, showcasing generative …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;During PyTorch 2022, Refik Anadol and Christian Burke discussed the Studio's history in AI art and unveiled its future endeavors in researching the applications of AI to model and preserve the Amazon Rainforest. This year, we would like to discuss and demonstrate our custom AI Rainforest models, showcasing generative fauna, flora, and funga from the Amazon Rainforest, as well as highlight our research in AI-based Language Preservation for an indigenous tribe, the Yawanawa. In addition, we would like to exhibit how PyTorch enables us to conduct this research and how we are using it to create AI models to preserve the rainforest and humanity.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: The Llama Ecosystem: Past, Present and Future</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-the-llama-ecosystem-past-present-and-future.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Joe Spisak</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-the-llama-ecosystem-past-present-and-future.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Join Meta on a journey through the evolution of Llama — our new open source large language model (LLM). Understand its growth and the range of available models including the most recently released Code Llama. Discover the ways developers can harness Llama’s potential and learn about the significant community …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Join Meta on a journey through the evolution of Llama — our new open source large language model (LLM). Understand its growth and the range of available models including the most recently released Code Llama. Discover the ways developers can harness Llama’s potential and learn about the significant community impact Llama has had on the AI ecosystem.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: The Promise of PyTorch as a General-Purpose Array-Oriented Computational Backend</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-the-promise-of-pytorch-as-a-general-purpose-array-oriented-computational-backend.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Travis Oliphant</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-the-promise-of-pytorch-as-a-general-purpose-array-oriented-computational-backend.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Array-oriented programming is a key paradigm of the SciPy and PyData, or SciPyData, ecosystem. Most operations in science, engineering, and AI/ML are naturally based around N-dimensional array, or tensor, operations. Domain experts can typically write the algorithms they are implementing using high-level tensor primitives. For over 28 years …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Array-oriented programming is a key paradigm of the SciPy and PyData, or SciPyData, ecosystem. Most operations in science, engineering, and AI/ML are naturally based around N-dimensional array, or tensor, operations. Domain experts can typically write the algorithms they are implementing using high-level tensor primitives. For over 28 years this has been done in Python primarily using NumPy (previously Numeric) as the foundational Nd-array object. As many additional array objects have been built over the past 8 years to support the growing interest in deep-learning, the SciPyData ecosystem has produced the Array API as part of the Data APIs initiative (&lt;a class="reference external" href="https://data-apis.org"&gt;https://data-apis.org&lt;/a&gt;) to assist in meeting the needs of that community. New features in PyTorch have helped PyTorch implement this Array API, which has enabled libraries like SciPy and scikit-learn to add support for PyTorch as the backend. Quansight has worked closely with Meta and other PyTorch sponsors to enable libraries using the Array API to reliably use PyTorch in their general-purpose workflows outside of just deep-learning. This enables the entire scientific community to potentially take advantage of PyTorch investment in run-times on GPUs, TPUs, and other parallel-hardware. The promise of array-oriented computing has always been that by writing at a high-level, the code can be run with optimizations on a variety of hardware. With Data APIs and PyTorch, this promise is becoming a reality.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: The Value of Open Source for the Enterprise</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-the-value-of-open-source-for-the-enterprise.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Priya Nagpurkar</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-the-value-of-open-source-for-the-enterprise.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Open-source communities accelerate innovation by empowering members to harness collective insights and build on a vast prior body of work. However, achieving a successful and responsible open-source community, and how enterprise companies should contribute to these communities, can be a delicate balance. Priya Nagpurkar, who leads the strategy for …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Open-source communities accelerate innovation by empowering members to harness collective insights and build on a vast prior body of work. However, achieving a successful and responsible open-source community, and how enterprise companies should contribute to these communities, can be a delicate balance. Priya Nagpurkar, who leads the strategy for AI and cloud platforms at IBM Research, will discuss what IBM looks for in open-source collaborators, how PyTorch forwards IBM's strategic goals, and the role open-source technologies will play in generative AI’s future.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: Welcome &amp; Opening Remarks</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-welcome-opening-remarks.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Joe Spisak</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-welcome-opening-remarks.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Keynote: Welcome &amp;amp; Opening Remarks - Joe Spisak, Product Director, Meta&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Keynote: Welcome &amp; Opening Remarks 2</title><link href="https://pyvideo.org/pytorch-conference-2023/keynote-welcome-opening-remarks-2.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Ibrahim Haddad</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/keynote-welcome-opening-remarks-2.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Keynote: Welcome &amp;amp; Opening Remarks - Ibrahim Haddad, Executive Director, PyTorch Foundation&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Keynote"></category></entry><entry><title>Lessons Learned in WatsonX Training: Scaling Cloud-Native PyTorch FSDP to 20B Parameters</title><link href="https://pyvideo.org/pytorch-conference-2023/lessons-learned-in-watsonx-training-scaling-cloud-native-pytorch-fsdp-to-20b-parameters.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Davis Wertheimer</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lessons-learned-in-watsonx-training-scaling-cloud-native-pytorch-fsdp-to-20b-parameters.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will cover lessons learned along our almost year-and-a-half journey scaling up the WatsonX.AI stack for foundation model pretraining. Starting from 100M parameters on bare metal, we scaled PyTorch training to 20B parameters on cloud-based multi-node systems. We'll discuss the challenges encountered along the way …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will cover lessons learned along our almost year-and-a-half journey scaling up the WatsonX.AI stack for foundation model pretraining. Starting from 100M parameters on bare metal, we scaled PyTorch training to 20B parameters on cloud-based multi-node systems. We'll discuss the challenges encountered along the way, as well as the solutions we employed. This includes working with the PyTorch team to field test Fully-Sharded and Hybrid-Shard Data Parallel update protocols (FSDP/HSDP), as well as handling the associated communication vs computation bottlenecks, which are not always straightforward. We'll also review our collaboration on cloud-native distributed checkpointing, and development of a stateful and scalable distributed dataloader, allowing us to restart unstable jobs mid-epoch without revisiting stale data. And finally, we'll cover ongoing and upcoming challenges, like maintaining job stability and tensor parallelism integration.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Lightning Talk: A Novel Domain Generalization Technique for Medical Imaging Using PyTorch</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-a-novel-domain-generalization-technique-for-medical-imaging-using-pytorch.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Dinkar Juyal</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-a-novel-domain-generalization-technique-for-medical-imaging-using-pytorch.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Domain generalization is critical for real-world applications of machine learning models to medical imaging. Variation in histopathology images arises through a complex combination of factors relating to tissue collection and laboratory processing, as well as factors intrinsic to patient samples. Therefore, augmentation-based methods of domain generalization that require domain …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Domain generalization is critical for real-world applications of machine learning models to medical imaging. Variation in histopathology images arises through a complex combination of factors relating to tissue collection and laboratory processing, as well as factors intrinsic to patient samples. Therefore, augmentation-based methods of domain generalization that require domain identifiers and manual fine-tuning are inadequate in this setting. To overcome this challenge, we introduce ContriMix, a domain generalization technique that learns to generate synthetic images by disentangling and permuting the biological content (&amp;quot;content&amp;quot;) and technical variations (&amp;quot;attributes&amp;quot;) in images. ContriMix does not rely on domain identifiers or handcrafted augmentations and makes no assumptions about the input characteristics of images. ContriMix produces SOTA results on Camelyon17 dataset in Stanford WILDS public leaderboard. ContriMix is developed entirely in PyTorch. The modular nature of PyTorch enables the use of ContriMix as an easy and intuitive plug-and-play setup to generate realistic synthetic medical images at the time of model training. Inference code is available.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Accelerated Inference in PyTorch 2.X with Torch-TensorRT</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-accelerated-inference-in-pytorch-2x-with-torch-tensorrt.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>George Stefanakis</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-accelerated-inference-in-pytorch-2x-with-torch-tensorrt.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Torch-TensorRT accelerates the inference of deep learning models in PyTorch targeting NVIDIA GPUs. Torch-TensorRT now leverages Dynamo, the graph capture technology introduced in PyTorch 2.0, to offer a new and more pythonic user experience as well as to upgrade the existing compilation workflow. The new user experience includes …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Torch-TensorRT accelerates the inference of deep learning models in PyTorch targeting NVIDIA GPUs. Torch-TensorRT now leverages Dynamo, the graph capture technology introduced in PyTorch 2.0, to offer a new and more pythonic user experience as well as to upgrade the existing compilation workflow. The new user experience includes Just-In-Time compilation and support for arbitrary Python code (like dynamic control flow, complex I/O, and external libraries) used within your model, while still accelerating performance. A single line of code provides easy and robust acceleration of your model with full flexibility to configure the compilation process without ever leaving PyTorch: torch.compile(model, backend=”tensorrt”) The existing API has also been revamped to use Dynamo export under the hood, providing you with the same Ahead-of-Time whole-graph acceleration with fallback for custom operators and dynamic shape support as in previous versions: torch_tensorrt.compile(model, inputs=example_inputs) We will present descriptions of both paths as well as features coming soon. All of our work is open source and available at &lt;a class="reference external" href="https://github.com/pytorch/TensorRT"&gt;https://github.com/pytorch/TensorRT&lt;/a&gt;.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Accelerating Inference on CPU with Torch.Compile</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-accelerating-inference-on-cpu-with-torchcompile.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Jiong Gong</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-accelerating-inference-on-cpu-with-torchcompile.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For the torch.compile CPU backend, we have optimized the static shapes of the float32 path and achieved good performance speedups on popular models. Starting with PyTorch 2.0, we have further enhanced this feature by addressing several issues and optimizing the bfloat16 precision path. The dynamic shape path …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For the torch.compile CPU backend, we have optimized the static shapes of the float32 path and achieved good performance speedups on popular models. Starting with PyTorch 2.0, we have further enhanced this feature by addressing several issues and optimizing the bfloat16 precision path. The dynamic shape path is also supported, which allows users to get good performance on dynamic shape models, such as GPTJ and Llama, as well as using low precision bfloat16 data type to further improve performance on the 4th generation of Intel Xeon Scalable Processors (Sapphire Rapids) using Advanced Matrix Extensions (AMX) instruction set extension and lower memory footprint. In this topic, we will introduce the key optimization technologies used in the CPU inference path of torch.compile, such as GEMM fusions, vectorization of low precision bfloat16 path, and constant folding with freezing path. We will also discuss how to solve issues that arose when supporting the path of the dynamic shape. Currently, the dynamic shape and bfloat16 paths can work well as static shape path. The geometric mean speedup of the bfloat16 path can range from 1.4x to 2.3x compared to eager mode on Sapphire Rapids.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Accelerating LLM Training on Cerebras Wafer-Scale Cluster</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-accelerating-llm-training-on-cerebras-wafer-scale-cluster.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Mark Browning</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-accelerating-llm-training-on-cerebras-wafer-scale-cluster.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large Language Model (LLM) have taken the world by storm; however, a few handfuls of companies can train such foundational models. On this talk, we will discuss the integration of Cerebras Wafer-Scale Clusters with PyTorch 2.0 LTC backend and the technical challenges to enable training such large model …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Large Language Model (LLM) have taken the world by storm; however, a few handfuls of companies can train such foundational models. On this talk, we will discuss the integration of Cerebras Wafer-Scale Clusters with PyTorch 2.0 LTC backend and the technical challenges to enable training such large model efficiently and seamlessly in order to act as a single accelerator regardless of the number of systems used. Another crucial piece of such integration is our collaboration with the open-source community on Torch-MLIR which help benefit the PyTorch community at large especially in canonicalizing multiple PyTorch backend to a unified ATen MLIR dialect, which enable multiple hardware backend integration with multiple lowering frontend (i.e. TorchScript, LTC, TorchDynamo...etc). Furthermore, we present our architecture for representing weight sparsity with both static and dynamic model pruning. A few convenient PyTorch utilities enable practitioners to take advantage of our sparsity-first hardware to decrease training time and enable efficient model deployment.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Accelerating PyTorch Performance with OpenVINO</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-accelerating-pytorch-performance-with-openvino.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Yamini Nimmagadda</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-accelerating-pytorch-performance-with-openvino.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Intel® Distribution of OpenVINO™ Toolkit optimizes performance and efficiency of deep learning inference across diverse and heterogeneous hardware like CPUs, Intel integrated and discrete GPUs, and VPUs, with a simplified “write once, deploy everywhere” approach. In this session, we will show the benefits of optimizing PyTorch models with OpenVINO …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Intel® Distribution of OpenVINO™ Toolkit optimizes performance and efficiency of deep learning inference across diverse and heterogeneous hardware like CPUs, Intel integrated and discrete GPUs, and VPUs, with a simplified “write once, deploy everywhere” approach. In this session, we will show the benefits of optimizing PyTorch models with OpenVINO. Converting PyTorch models to ONNX and subsequently loading them into the OpenVINO runtime for optimized inference has been adopted by developers for a while. More recently, we have developed a PyTorch frontend that enables direct consumption of PyTorch models with OpenVINO, without needing the conversion to ONNX. Additionally, with the advent of PyTorch 2.0, we have pushed the boundaries further by seamlessly incorporating OpenVINO as a TorchDynamo backend with torch.compile to simplify the development process further while inferencing with PyTorch APIs. During our presentation, we will demonstrate the practical implementation of each of these techniques by providing example usage of the relevant APIs. We will also highlight the accelerated performance of state-of-the-art PyTorch models using OpenVINO across a range of Intel devices.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Adding Backends for TorchInductor: Case Study with Intel GPU</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-adding-backends-for-torchinductor-case-study-with-intel-gpu.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Eikan Wang</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-adding-backends-for-torchinductor-case-study-with-intel-gpu.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;There are two integration levels to add a new backend for the PyTorch compiler - AtenIR/PrimsIR level and Inductor loop IR level. The ATen/Prim level IR integration has been there via the custom backend registration infrastructure (&lt;a class="reference external" href="https://pytorch.org/docs/stable/dynamo/custom-backends.html"&gt;https://pytorch.org/docs/stable/dynamo/custom-backends.html&lt;/a&gt;). Yet, the latter offers …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;There are two integration levels to add a new backend for the PyTorch compiler - AtenIR/PrimsIR level and Inductor loop IR level. The ATen/Prim level IR integration has been there via the custom backend registration infrastructure (&lt;a class="reference external" href="https://pytorch.org/docs/stable/dynamo/custom-backends.html"&gt;https://pytorch.org/docs/stable/dynamo/custom-backends.html&lt;/a&gt;). Yet, the latter offers an option to integrate backend compiler at the lower loop-level IR, which can benefit from the existing compiler infrastructure of the Inductor, such as the loop fusion and memory planning. We developed a dynamic registration mechanism on the Inductor side for a new backend. The mechanism allows a backend to register its codegen for a particular device at runtime. And the new backend just needs to focus on generating optimal code for the device. - Case Study – Intel GPU Backend for Inductor Take Intel GPU Backend for Inductor as an example to study how to support Intel GPU via the proposed registration mechanism to prove the idea. Intel GPU Backend for Inductor is on top of Triton, as we have enabled Triton to support any new HW backend. In this context, the case study will show the power of “Inductor + Triton” to easily support any new accelerator.&lt;/li&gt;
&lt;/ul&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: AOTInductor: Ahead-of-Time Compilation for PT2 Exported Models</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-aotinductor-ahead-of-time-compilation-for-pt2-exported-models.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Bin Bao</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-aotinductor-ahead-of-time-compilation-for-pt2-exported-models.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk introduces AOTInductor, an Ahead-Of-Time version of TorchInductor. AOTInductor enables the compilation of exported models into shared libraries, offering a seamless and high-performance deployment solution for PyTorch models in non-Python environments. By leveraging torch.export, AOTInductor ensures a streamlined workflow for efficient PyTorch model deployment.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Building Intermediate Logging for PyTorch</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-building-intermediate-logging-for-pytorch.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Kunal Bhalla</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-building-intermediate-logging-for-pytorch.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;One of the best ways to understand what's going on with your model is to actually look at all the numbers flowing through it. In this talk I'll walk through implementing an API to capture all values as they flow through a PyTorch model: Module arguments, parameters, buffers, return …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;One of the best ways to understand what's going on with your model is to actually look at all the numbers flowing through it. In this talk I'll walk through implementing an API to capture all values as they flow through a PyTorch model: Module arguments, parameters, buffers, return values -- as well as gradients -- in a way that Just Works even if you're using TorchScript, torch.compile, transforming the model with Torch Fx, distributing it with torch.package or just pickling it and passing it around. We'll end up talking about several Python and PyTorch internals along the way. Having all the numbers available opens up a lot of opportunities to understand and debug your model as well, and I'll also talk through some case studies, though I'm far from a domain expert there.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: CUDAGraph in a Partial Graph World</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-cudagraph-in-a-partial-graph-world.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Elias Ellison</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-cudagraph-in-a-partial-graph-world.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;CUDAGraph made safe and easy to use in torch.compile.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Diffusers: Bringing Cutting-Edge Diffusion Models to the Masses</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-diffusers-bringing-cutting-edge-diffusion-models-to-the-masses.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Lysandre Debut</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-diffusers-bringing-cutting-edge-diffusion-models-to-the-masses.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Diffusers is a go-to library for state-of-the-art pretrained diffusion models, allowing users to generate images, audio, and 3D structures of molecules effortlessly. With a focus on usability, simplicity, and customization, Diffusers is a modular toolbox for both simple inference and training of diffusion models. By seamlessly integrating with PyTorch …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Diffusers is a go-to library for state-of-the-art pretrained diffusion models, allowing users to generate images, audio, and 3D structures of molecules effortlessly. With a focus on usability, simplicity, and customization, Diffusers is a modular toolbox for both simple inference and training of diffusion models. By seamlessly integrating with PyTorch, Diffusers streamlines the development process, making model implementation, training, and evaluation a breeze. Leveraging the power of PyTorch 2.0, Diffusers takes advantage of new features and improvements, empowering researchers and practitioners to push the boundaries of their projects. The library incorporates optimizations such as accelerated transformers and &lt;cite&gt;torch.compile()&lt;/cite&gt;, enhancing accessibility, inference speed, memory efficiency, and resource utilization. Through practical demonstrations, Diffusers showcases its potential in real-world scenarios, providing an indispensable tool for researchers and developers. Join us to explore how Diffusers democratizes diffusion models with PyTorch integration, optimizations, and practical demonstrations.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Dinosaur Bone Hunt</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-dinosaur-bone-hunt.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Bob Chesebrough</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-dinosaur-bone-hunt.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I will describe how to build an AI fossil-hunting tool based on PyTorch and the Intel AI Analytics Toolkit to create a bone likelihood map to help guide your next dinosaur hunt! All of this will be shown on CPUs using PyTorch. To start with, I …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk I will describe how to build an AI fossil-hunting tool based on PyTorch and the Intel AI Analytics Toolkit to create a bone likelihood map to help guide your next dinosaur hunt! All of this will be shown on CPUs using PyTorch. To start with, I will walk through dinosaur bone identification and the importance of sediment deposits in locating dinosaur bones. Then I will show you how to decompose an image classification problem, like dinosaur fossil hunting, into a few key components: building context from data, proper data representation to this model, model definition/training, and producing actionable insights from model predictions. The author has used this model to find new dinosaur bones in the Morrison Formation!&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Efficient Inference at the Edge: Performance You Need at the Lowest Power You Deserve</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-efficient-inference-at-the-edge-performance-you-need-at-the-lowest-power-you-deserve.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Felix Baum</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-efficient-inference-at-the-edge-performance-you-need-at-the-lowest-power-you-deserve.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most AI algorithms created for edge applications are initially developed on workstations. Developers then often struggle to get these workloads running on edge devices and achieve performance levels required for new and innovative use cases. This holds true for a wide range of applications, from IoT to automotive to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most AI algorithms created for edge applications are initially developed on workstations. Developers then often struggle to get these workloads running on edge devices and achieve performance levels required for new and innovative use cases. This holds true for a wide range of applications, from IoT to automotive to XR to mobile to compute. In this session we would cover the results of the collaborative effort between PyTorch and Qualcomm teams to integrate the Qualcomm AI Stack into PyTorch 2.0 workflow and how we streamlined the path for developers from initial algorithm development to edge deployment. This would make it easy to re-target algorithms to edge hardware by supporting framework and data types that PyTorch developers are familiar with and we provide a set of tools that empower developers to extract the best performance and energy efficiency from their Android handsets to enable advanced use cases with premium features, performance boosts and power savings.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Energy-Efficient Deep Learning with PyTorch and Zeus</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-energy-efficient-deep-learning-with-pytorch-and-zeus.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Jae-Won Chung</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-energy-efficient-deep-learning-with-pytorch-and-zeus.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Until now, we just wanted to make things faster and faster. However, especially with the recent growth of GenAI, Deep Learning has become one of the primary workloads of cloud datacenters, which already take up 2-3% of the world's electricity usage. Therefore we ask: How much room are there …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Until now, we just wanted to make things faster and faster. However, especially with the recent growth of GenAI, Deep Learning has become one of the primary workloads of cloud datacenters, which already take up 2-3% of the world's electricity usage. Therefore we ask: How much room are there for energy optimization? Can we get free energy reduction without slowdown? What knobs do we have available? How do we even measure energy consumption? In this talk, I aim to persuade the audience of the importance of regarding energy as a first-class metric for deep learning, and present the current state of deep learning energy optimization with Zeus (&lt;a class="reference external" href="https://ml.energy/zeus"&gt;https://ml.energy/zeus&lt;/a&gt;). Integrated with PyTorch, Zeus provides convenient tools for GPU time and energy measurement inside user training scripts and transparently profile and optimize GPU-side knobs to maximize energy efficiency. Finally, I'll share our vision towards making sustainable deep learning as easy as possible while being mindful of existing important metrics such as speed and model quality.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Enhancements Made to MPS Backend in PyTorch for Applications Running on Mac Platforms</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-enhancements-made-to-mps-backend-in-pytorch-for-applications-running-on-mac-platforms.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Kulin Seth</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-enhancements-made-to-mps-backend-in-pytorch-for-applications-running-on-mac-platforms.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Since PyTorch 2.0, MPS backend has qualified for “beta” stage which provides wider operator support (300+) and network coverage. We will provide details about new features introduced in MPS backend such as how to add custom operations to your network and profiling applications using MPSProfiler &amp;amp; Instruments. Finally we …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Since PyTorch 2.0, MPS backend has qualified for “beta” stage which provides wider operator support (300+) and network coverage. We will provide details about new features introduced in MPS backend such as how to add custom operations to your network and profiling applications using MPSProfiler &amp;amp; Instruments. Finally we will provide some debugging tips and best practices on MPS device and conclude with performance results on popular benchmarks.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Exploring PiPPY, Tensor Parallel and Torchserve for Large Model Inference</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-exploring-pippy-tensor-parallel-and-torchserve-for-large-model-inference.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Hamid Shojanazeri</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-exploring-pippy-tensor-parallel-and-torchserve-for-large-model-inference.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Here, we talk about large model inference with Torchserve, using PiPPy, Tensor Parallel, challenges of distributed inference and available solutions. Discuss the features that Torchserve provide today for serving LLMs in production today.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Harnessing NVIDIA Tensor Cores: An Exploration of CUTLASS &amp; OpenAI Triton</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-harnessing-nvidia-tensor-cores-an-exploration-of-cutlass-openai-triton.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Matthew Nicely</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-harnessing-nvidia-tensor-cores-an-exploration-of-cutlass-openai-triton.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Discover the power of NVIDIA Tensor Cores and accelerate your PyTorch development using two cutting-edge open-source libraries: CUTLASS and OpenAI Triton. This presentation aims to inspire both novice and seasoned PyTorch developers to unlock new efficiencies and capabilities in their work. We delve into the architecture of CUTLASS, revealing …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Discover the power of NVIDIA Tensor Cores and accelerate your PyTorch development using two cutting-edge open-source libraries: CUTLASS and OpenAI Triton. This presentation aims to inspire both novice and seasoned PyTorch developers to unlock new efficiencies and capabilities in their work. We delve into the architecture of CUTLASS, revealing its diverse use cases and value proposition. Simultaneously, we explore Triton's transformative capabilities for NVIDIA Tensor Cores, and the roadmaps for these essential tools. I hope to inspire the audience to engage with these tools for their projects, contributing to a high-performing PyTorch community.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Large-Scale Distributed Training with Dynamo and Triton</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-large-scale-distributed-training-with-dynamo-and-triton.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Yeounoh Chung</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-large-scale-distributed-training-with-dynamo-and-triton.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we cover PyTorch/XLA distributed API in relation with Torch.Dynamo. Specifically, we discuss the new PyTorch/XLA SPMD API for automatic parallelization and our latest LLaMA2 training results. PyTorch/XLA SPMD makes it simple for PyTorch developers to distribute their ML workloads (e.g., training …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we cover PyTorch/XLA distributed API in relation with Torch.Dynamo. Specifically, we discuss the new PyTorch/XLA SPMD API for automatic parallelization and our latest LLaMA2 training results. PyTorch/XLA SPMD makes it simple for PyTorch developers to distribute their ML workloads (e.g., training &amp;amp; inference with Dynamo) with easy-to-use API, and uses XLA GSPMD, high-performance automatic parallelization system. Under the hood, it transforms the user single-device program into a partitioned one. We will share how we enabled advanced 2D sharding strategies for LLaMA2 using PyTorch/XLA SPMD.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Lessons from Using Pytorch 2.0 Compile in IBM's Watsonx.AI Inference</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-lessons-from-using-pytorch-20-compile-in-ibms-watsonxai-inference.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Antoni Martin</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-lessons-from-using-pytorch-20-compile-in-ibms-watsonxai-inference.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will cover lessons learned about PT 2.0 compile after using it in IBM’s Watsonx.AI stack with NVIDIA GPUs and custom IBM accelerators as the main inference acceleration solution. Specifically, we will cover the results of our latency and throughput experiments with a …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will cover lessons learned about PT 2.0 compile after using it in IBM’s Watsonx.AI stack with NVIDIA GPUs and custom IBM accelerators as the main inference acceleration solution. Specifically, we will cover the results of our latency and throughput experiments with a range of LLM models, ranging from encoder-only, encoder-decoder, and decoder-only transformer models. We will talk about performance comparisons with other approaches in the field as well as our collaboration with the core PyTorch team to fix some of the bugs we have encountered when using features such as dynamic shapes and CUDA graph trees. We will also comment on how we have been using the torch.compile() API to compile and run models on IBM’s AIU accelerator and why we have made that choice. Finally, we will also cover the interaction of parallel approaches such as Tensor Parallel for bigger models combined with Compile for inference workloads.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Leveraging PyTorch 2.0 for Bias Reduction in AI</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-leveraging-pytorch-20-for-bias-reduction-in-ai.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Christina Zhu</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-leveraging-pytorch-20-for-bias-reduction-in-ai.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As a software developer, it's staggering to think about the incredible capabilities of AI, but it's vital not to overlook the ethical dimensions that surround its application. One of these ethical dimensions revolves around bias—an issue that, if left unchecked, can inadvertently reinforce societal prejudices and deepen existing …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As a software developer, it's staggering to think about the incredible capabilities of AI, but it's vital not to overlook the ethical dimensions that surround its application. One of these ethical dimensions revolves around bias—an issue that, if left unchecked, can inadvertently reinforce societal prejudices and deepen existing inequalities, especially when it comes to inclusivity. I'm excited to explore this topic at this year's PyTorch conference, focusing on the usage of PyTorch 2.0 to reduce bias in AI models. It should be a quick lightning talk with some easy tips on how to do it while training your AI models. The talk aims to underscore the silent challenge of bias in AI. It's an issue I feel warrants more attention, and I hope to contribute to that discussion. My hope is that attendees will walk away with a heightened understanding of bias in AI and the challenges it presents. More critically, I hope to arm them with the knowledge of how tools like PyTorch 2.0 can make a real difference in this fight.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Orchestrating Machine Learning on Edge Devices with PyTorch and WebAssembly</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-orchestrating-machine-learning-on-edge-devices-with-pytorch-and-webassembly.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Rishit Dagli</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-orchestrating-machine-learning-on-edge-devices-with-pytorch-and-webassembly.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Edge computing is becoming increasingly popular for applications that require low latency and high bandwidth. However, building machine learning runtimes for edge computing infra that is both scalable and optimized can be challenging. In this talk, we explore how PyTorch and WebAssembly (Wasm) can be used to build efficient …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Edge computing is becoming increasingly popular for applications that require low latency and high bandwidth. However, building machine learning runtimes for edge computing infra that is both scalable and optimized can be challenging. In this talk, we explore how PyTorch and WebAssembly (Wasm) can be used to build efficient edge computing runtimes. We then discuss how Wasm, a low-level bytecode format, can be used to effectively run PyTorch models and specific optimizations one could make in the models to run on the edge with Wasm, which provides a lightweight and portable runtime environment for edge applications, while we also introduce Akri, an open-source project in this context which allows us to easily discover edge devices to run the PyTorch model on. We will also cover some use cases where PyTorch and Wasm can be used together, such as building machine learning models that can run on edge devices or processing sensor data in real-time. We also share some best practices by showing how we run Neural Radiance Fields on the edge using this setup. The audience will gain a better understanding of how they can use PyTorch to run scalable and optimized machine learning on edge.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Profiling and Memory Debugging Tools for Distributed ML Workloads on GPUs</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-profiling-and-memory-debugging-tools-for-distributed-ml-workloads-on-gpus.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Aaron Shi</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-profiling-and-memory-debugging-tools-for-distributed-ml-workloads-on-gpus.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;An overview of PyTorch profiling tools and features (Profiler and Kineto) followed by a practical dive into our extensive GPU memory debugging tools. The PyTorch Profiler will introduce the Memory Profiler for better understanding of GPU memory, as well as newly released OSS repos such as Holistic Trace Analysis …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;An overview of PyTorch profiling tools and features (Profiler and Kineto) followed by a practical dive into our extensive GPU memory debugging tools. The PyTorch Profiler will introduce the Memory Profiler for better understanding of GPU memory, as well as newly released OSS repos such as Holistic Trace Analysis (used to understand distributed profiler traces and provide useful views), and Dynolog (used for triggering on-demand traces). Followed by a look into new GPU memory debugging tools for PyTorch: Memory Snapshot, and Reference Cycle Detector. May take a practical approach in understanding memory leaks, fragmentation and reference cycles.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: PT2 Export - A Sound Full Graph Capture Mechanism for PyTorch</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-pt2-export-a-sound-full-graph-capture-mechanism-for-pytorch.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Avik Chaudhuri</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-pt2-export-a-sound-full-graph-capture-mechanism-for-pytorch.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this session, we will introduce torch.export(), which is an Ahead-of-Time full graph capture mechanism with soundness guarantees. It is built on top of same foundational technology as torch.compile(). We recommend torch.export() for vendors integration and users who have requirements of running PyTorch program without Python …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this session, we will introduce torch.export(), which is an Ahead-of-Time full graph capture mechanism with soundness guarantees. It is built on top of same foundational technology as torch.compile(). We recommend torch.export() for vendors integration and users who have requirements of running PyTorch program without Python.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: PyTorch 2.0 on the ROCm Platform</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-pytorch-20-on-the-rocm-platform.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Douglas Lehr</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-pytorch-20-on-the-rocm-platform.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Talk about the current state of PyTorch on the ROCm platform. Including efforts to achieve day 0 support for Triton on Pytorch 2.0. As well as performance improvements, efforts with Huggingface, and other areas.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Seismic Data to Subsurface Models with OpenFWI</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-seismic-data-to-subsurface-models-with-openfwi.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Benjamin Consolvo</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-seismic-data-to-subsurface-models-with-openfwi.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Obtaining an accurate &amp;quot;picture&amp;quot; of the subsurface is not as simple as snapping a picture on a smartphone. Seismic exploration is a key component to creating images of the subsurface and finding essential minerals and oil and gas. The process of building images of the subsurface is akin to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Obtaining an accurate &amp;quot;picture&amp;quot; of the subsurface is not as simple as snapping a picture on a smartphone. Seismic exploration is a key component to creating images of the subsurface and finding essential minerals and oil and gas. The process of building images of the subsurface is akin to ultrasound technology used to image the human body. One of the best known physics-based methods to create geologically-accurate images of the subsurface is called full-waveform inversion (FWI). It is a process by which we can take raw seismic data and through applying an iterative physics-based approach recreate the velocities of sound waves in the subsurface (which can be understood as an image). However, one of the challenges of this physics-based approach is that it is computationally expensive and it typically relies heavily on a good initial velocity model that is close to the answer. I will walk you through how I quickly trained a neural network with PyTorch on the latest 4th Gen. Xeon CPU, going directly from seismic data to a subsurface model and bypassing the need for an accurate starting model.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Simulating Quantum Systems with PyTorch</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-simulating-quantum-systems-with-pytorch.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Pierre Guilmin</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-simulating-quantum-systems-with-pytorch.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, I propose to explain why simulating quantum systems is a formidable challenge, and how leveraging modern hardware and software can result in notable performance improvement. PyTorch is ideally suited for this task, first because running solvers on GPUs results in a significant speed-up, and second because …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, I propose to explain why simulating quantum systems is a formidable challenge, and how leveraging modern hardware and software can result in notable performance improvement. PyTorch is ideally suited for this task, first because running solvers on GPUs results in a significant speed-up, and second because numerous tasks related to the calibration and control of quantum systems require the computation of gradients based on the time-evolved quantum state. The emerging research effort to develop quantum computers heavily relies on such tools. The dynamiqs library (&lt;a class="reference external" href="https://github.com/dynamiqs/dynamiqs"&gt;https://github.com/dynamiqs/dynamiqs&lt;/a&gt;) is a Python library powered by PyTorch, designed to address this challenge. It provides differentiable solvers for the &lt;em&gt;Schrödinger Equation&lt;/em&gt; which governs closed quantum systems, the &lt;em&gt;Lindblad Master Equation&lt;/em&gt; for open quantum systems and the &lt;em&gt;Stochastic Master Equation&lt;/em&gt; for continuously measured quantum systems. Gradients can be computed with PyTorch’s automatic differentiation, or using a constant memory cost method. The library is being developed by several PhD students in physics, most of whom have substantial experience in software development.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Standardizing CPU Benchmarking with TorchBench for PyTorch Community</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-standardizing-cpu-benchmarking-with-torchbench-for-pytorch-community.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Xu Zhao</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-standardizing-cpu-benchmarking-with-torchbench-for-pytorch-community.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;TorchBench is a community-driven open-source benchmark for PyTorch that covers a wide range of popular and critical models. However, the default setup of TorchBench does not cover all CPU-specific features of Pytorch, and its benchmarking methodology is not optimized for CPU devices. This makes it difficult to use TorchBench …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;TorchBench is a community-driven open-source benchmark for PyTorch that covers a wide range of popular and critical models. However, the default setup of TorchBench does not cover all CPU-specific features of Pytorch, and its benchmarking methodology is not optimized for CPU devices. This makes it difficult to use TorchBench to benchmark new CPU features such as FX INT8, AMP and so on, as well as CPU-specific scenarios such as core binding. We worked closely with the PyTorch community to enable benchmarking for major CPU optimizations and features. And we leveraged the userbenchmark design to improve and standardize the benchmarking methodology for CPU, aligning it with common CPU benchmarking practices. We also increased model coverage by adding new models and fixing bugs, including GNN models and some fixes for serval models on CPU devices. With these improvements, TorchBench can be used to track regressions, prove the performance benefits of new optimizations, and easily replicate results on CPU devices. We will continue to improve TorchBench in line with the PyTorch roadmap, making it a valuable tool for improving PyTorch quality and showcasing PyTorch performance on CPU.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: State of PyTorch</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-state-of-pytorch.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Alban Desmaison</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-state-of-pytorch.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;It takes a village to build an open-source framework, all thanks to our awesome community of contributors, partners and ecosystem tools. This talk gives a run through of who builds PyTorch, new and upcoming improvements to the framework and how to get involved.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Streamlining Model Export with the New ONNX Exporter</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-streamlining-model-export-with-the-new-onnx-exporter.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Maanav Dalal</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-streamlining-model-export-with-the-new-onnx-exporter.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Join us for a 10-minute talk on the new TorchDynamo-based ONNX exporter, redefining how we convert machine learning models to the ONNX format, with the power of PyTorch 2.0. The talk covers: Exporting with torch.onnx.dynamo_export, Inference with ONNXRuntime, and a variety of of State-of-the-Art models being …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Join us for a 10-minute talk on the new TorchDynamo-based ONNX exporter, redefining how we convert machine learning models to the ONNX format, with the power of PyTorch 2.0. The talk covers: Exporting with torch.onnx.dynamo_export, Inference with ONNXRuntime, and a variety of of State-of-the-Art models being converted easily. Learn about how ONNX enables more PyTorch models to be cross-platform, all the great benefits of ONNX standard, and all the other features we have baked into the new exporter, including: Symbolic model tracing: Export large models without computation cost involving original data/parameters, saving time and preventing OOM issues. ONNX Script: A new way to architect ONNX Models and the backbone of the Dynamo Exporter Preserving original modules structure as ONNX functions: allowing for a better resulting model that is layered. Better diagnostics: Clearly identify the root of your conversion issues faster!&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Tensor and 2D Parallelism</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-tensor-and-2d-parallelism.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Rodrigo Kumpera</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-tensor-and-2d-parallelism.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Lightning Talk: Tensor and 2D Parallelism - Rodrigo Kumpera &amp;amp; Junjie Wang, Meta&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Tensor Query Processing</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-tensor-query-processing.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Matteo Interlandi</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-tensor-query-processing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The huge demand for computation in artificial intelligence (AI) is driving unparalleled investments in new hardware and software systems for AI. This leads to an explosion in the number of specialized hardware devices, which are now part of the offerings of major cloud providers. Meanwhile, by hiding the low-level …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The huge demand for computation in artificial intelligence (AI) is driving unparalleled investments in new hardware and software systems for AI. This leads to an explosion in the number of specialized hardware devices, which are now part of the offerings of major cloud providers. Meanwhile, by hiding the low-level complexity through a tensor-based interface, machine learning frameworks such as PyTorch allow data scientists to efficiently exploit the exciting capabilities offered by the new hardware. In this talk, we will present how databases can ride the wave of innovation happening in the AI space thanks to Tensor Query Processor (TQP): a SQL query processor leveraging the tensor interface of PyTorch. TQP can efficiently run the full TPC-H benchmark by implementing novel algorithms for executing relational operators on the specialized tensor routines provided by PyTorch. Meanwhile, TQP can target various hardware while only requiring a fraction of the usual development effort.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: The Fastest Path to Production: PyTorch Inference in Python</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-the-fastest-path-to-production-pytorch-inference-in-python.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Mark Saroufim</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-the-fastest-path-to-production-pytorch-inference-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Historically for inference, users have had to rewrite their models to be jit scriptable which required model rewrites and familiarity with C++ services. This is frustrating especially when the vast majority of real world pytorch users actually deploy python in production. When torch.compile was introduced, it encouraged a …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Historically for inference, users have had to rewrite their models to be jit scriptable which required model rewrites and familiarity with C++ services. This is frustrating especially when the vast majority of real world pytorch users actually deploy python in production. When torch.compile was introduced, it encouraged a UX of gradual model rewrites to optimize models but users would get value even without any. A C++ based option still represents a steep difficulty jump and torch.compile still suffers from long compile times which make it unsuited for server side inference where cold start times are critical. In this talk we introduce the options users have for the quickest possible path to production including new APIs to cache compilation artifacts across devices so users can compile models once for both training and inference and python bindings for AOT Inductor. We'll also end with some real world case studies inspired by users who faced the above problems within the context of torchserve. By which point we hope you'll be fully convinced that it's possible deploy python in production and retain performance.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: TorchFix - a Linter for PyTorch-Using Code with Autofix Support</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-torchfix-a-linter-for-pytorch-using-code-with-autofix-support.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Sergii Dymchenko</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-torchfix-a-linter-for-pytorch-using-code-with-autofix-support.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;TorchFix is a Python code static analysis tool - a linter with autofix capabilities - for users of PyTorch. It can be used to find and fix issues like usage of deprecated PyTorch functions and non-public symbols, and to adopt PyTorch best practices in general.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: TorchRL - RLHF Support</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-torchrl-rlhf-support.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Vincent Moens</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-torchrl-rlhf-support.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;RLHF is notoriously hard to implement, requiring technical knowledge across RL and other domains. For this reason, people often revert to packaged solutions with single entry points and complex configurations that leave little room for custom development. We present a new RLHF support in TorchRL that solves this problem …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;RLHF is notoriously hard to implement, requiring technical knowledge across RL and other domains. For this reason, people often revert to packaged solutions with single entry points and complex configurations that leave little room for custom development. We present a new RLHF support in TorchRL that solves this problem by giving developers users full control over the training pipeline at a reduced development cost on the RL side. This new set of primitives allow users to quickly prototype and train generative models across domains (language, CV and others). With the TorchRL-HF tooling, RL-specific classes and recipes are easily blended within one's code base, and multiple solutions (preprocessing techniques or RL algorithms) can seamlessly be implemented without the need for an in-depth understanding of the RL machinery. We demonstrate how this works in practice with examples from diverse domains, including LLMs and drug design.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Triton Compiler</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-triton-compiler.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Thomas Raoux</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-triton-compiler.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Triton is a language and compiler for writing highly efficient custom deep learning primitives. The aim of Triton is to provide an open-source environment to write fast code with higher productivity than CUDA, but also with greater flexibility than other existing DSLs. Triton has been adopted as a fundamental …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Triton is a language and compiler for writing highly efficient custom deep learning primitives. The aim of Triton is to provide an open-source environment to write fast code with higher productivity than CUDA, but also with greater flexibility than other existing DSLs. Triton has been adopted as a fundamental component of Torch inductor to synthesize efficient kernels targeting GPUs. This has multiple advantages compared to traditional library usage. It allows for the creation of a wide variety of fusions, it can be tuned independently, and it has a smaller memory footprint. This talk will present the Triton compiler and describe the process that enables it to generate lightning-fast kernels with minimal user effort.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Lightning Talk: Uplink Interference Optimizer, How to Optimize a Cellular Network in a Single Shot with GNNs</title><link href="https://pyvideo.org/pytorch-conference-2023/lightning-talk-uplink-interference-optimizer-how-to-optimize-a-cellular-network-in-a-single-shot-with-gnns.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Oscar Llorente Gonzalez</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightning-talk-uplink-interference-optimizer-how-to-optimize-a-cellular-network-in-a-single-shot-with-gnns.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Optimizing cellular networks has been a very difficult task for a long time. In these networks multiple problematic issues appear and the high number of parameters and variables to optimize makes it a difficult problem even for radio experts. Here an optimizer for a cellular network is presented, the …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Optimizing cellular networks has been a very difficult task for a long time. In these networks multiple problematic issues appear and the high number of parameters and variables to optimize makes it a difficult problem even for radio experts. Here an optimizer for a cellular network is presented, the Uplink Interference Optimizer. Specifically, the uplink interference problem (degradation of the signal transmitted from a user terminal to a base station) will be solved by constructing a model that predicts a variable that reflects the interference level (SINR) and then optimizing the parameters of the cellular network, based on the model that has been built, to reduce it. That way, we achieve the optimization in a single step, improving previous solutions based on RL that must iterate over the real cellular network for several weeks. The simulator model will be based on Graph Neural Networks (constructed with PyTorch and PyTorch Geometric), allowing us to consider the neighborhood of a cell to make a prediction, which enhances the prediction accuracy over all older models. Then, any algorithm could be run to improve the parameters based on the simulator model we have constructed.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>LightningTalk: MultiRay: An Accelerated Embedding Service for Content Understanding</title><link href="https://pyvideo.org/pytorch-conference-2023/lightningtalk-multiray-an-accelerated-embedding-service-for-content-understanding.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Michael Gschwind</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lightningtalk-multiray-an-accelerated-embedding-service-for-content-understanding.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We present an overview of MultiRay, our high performance embedding service. The service provides a shared inference architecture that provides embeddings for content, and serves a small set of foundation models shared by all use cases. At present, we are serving 3 embedding services, for text understanding (TextRay), image …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We present an overview of MultiRay, our high performance embedding service. The service provides a shared inference architecture that provides embeddings for content, and serves a small set of foundation models shared by all use cases. At present, we are serving 3 embedding services, for text understanding (TextRay), image understanding (ImageRay) and multi-modal whole-post understanding (text, image, etc) (PostRay). The system serves over over 800B requests daily, with up to 20M queries per second, serving over 125 different use cases.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category><category term="Lightning Talk"></category></entry><entry><title>Llama V2 in Azure AI for Finetuning, Evaluation and Deployment from the Model Catalog</title><link href="https://pyvideo.org/pytorch-conference-2023/llama-v2-in-azure-ai-for-finetuning-evaluation-and-deployment-from-the-model-catalog.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Swati Gharse</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/llama-v2-in-azure-ai-for-finetuning-evaluation-and-deployment-from-the-model-catalog.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Llama 2 is now available in the model catalog in Azure Machine Learning. The model catalog in AzureML is your hub for foundation models. Azure native support for Llama 2 in the model catalog enables you use these models, without having to manage any of the infrastructure or environment …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Llama 2 is now available in the model catalog in Azure Machine Learning. The model catalog in AzureML is your hub for foundation models. Azure native support for Llama 2 in the model catalog enables you use these models, without having to manage any of the infrastructure or environment dependencies. It provides out-of-the-box support for model finetuning and evaluation,  and includes options for optimizer libraries like DeepSpeed and ORT (ONNX RunTime), which speed up fine-tuning, and LoRA (Low-Rank Adaptation of Large Language Models), which greatly reduces memory and compute requirements for fine-tuning. Deployments of Llama 2 models in Azure have Azure AI Content Safety integrated by default, offering a built-in layered approach to safety, and following responsible AI best practices.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>PyTorch Edge: Developer Journey for Deploying AI Models Onto Edge Devices</title><link href="https://pyvideo.org/pytorch-conference-2023/pytorch-edge-developer-journey-for-deploying-ai-models-onto-edge-devices.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Mengwei Liu</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/pytorch-edge-developer-journey-for-deploying-ai-models-onto-edge-devices.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This technical session caters to on-device AI engineers, providing insights into developer flows with PyTorch Edge. Through our semi-live demos and code showcases, attendees will embark on a journey to understand the research-to-production workflow using PyTorch Edge. Discover how our APIs and tooling facilitate productivity while ensuring portability across …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This technical session caters to on-device AI engineers, providing insights into developer flows with PyTorch Edge. Through our semi-live demos and code showcases, attendees will embark on a journey to understand the research-to-production workflow using PyTorch Edge. Discover how our APIs and tooling facilitate productivity while ensuring portability across diverse hardware platforms, ranging from mobile phones, embedded devices and laptops.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>PyTorch Edge: Vendor Integration Journey for Compilers and Backends</title><link href="https://pyvideo.org/pytorch-conference-2023/pytorch-edge-vendor-integration-journey-for-compilers-and-backends.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Kimish Patel</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/pytorch-edge-vendor-integration-journey-for-compilers-and-backends.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Join this technical session tailored for partners and hardware vendors aiming to provide high-performance solutions to their customers, including on-device PyTorch users. We will showcase how to integrate backend and compiler toolchain natively with PyTorch Edge IR without compromising on performance. Explore our well-defined entry points and API for …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Join this technical session tailored for partners and hardware vendors aiming to provide high-performance solutions to their customers, including on-device PyTorch users. We will showcase how to integrate backend and compiler toolchain natively with PyTorch Edge IR without compromising on performance. Explore our well-defined entry points and API for integrating compiler passes, delegates, and custom kernel implementations, all without any intermediate conversions.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>PyTorch Korea User Group: The Beginning, Present, and Future</title><link href="https://pyvideo.org/pytorch-conference-2023/pytorch-korea-user-group-the-beginning-present-and-future.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Junghwan Park</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/pytorch-korea-user-group-the-beginning-present-and-future.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As a lead maintainer, I'd like to talk about the PyTorch community in Korea. Here’re some agenda I'm currently thinking about: - How we got started (since 2018) - How we're growing (during Covid-19) - What we want to do in the future - A guide for people who want to start …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;As a lead maintainer, I'd like to talk about the PyTorch community in Korea. Here’re some agenda I'm currently thinking about: - How we got started (since 2018) - How we're growing (during Covid-19) - What we want to do in the future - A guide for people who want to start a local community I’d like to talk about what inspired me to start the community, what I've learned along the way, and what other maintainers and I wish having done more of. I'll also include some tips and suggestions for those who want to start a local community like in Korea. With this talk, I hope to make more people aware of how the PyToch user community in Korea has been self-sustaining, and give them the courage to get involved.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>The Evolving Landscape of Dataloading</title><link href="https://pyvideo.org/pytorch-conference-2023/the-evolving-landscape-of-dataloading.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Laurence Rouesnel</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/the-evolving-landscape-of-dataloading.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data loading is a critical component in every ML training system. This session covers the dataloading in and around PyTorch today, the pain-points we hear from users, and industry trends. We focus on how we are thinking about our systems as we see models, datasets, and hardware continue to …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Data loading is a critical component in every ML training system. This session covers the dataloading in and around PyTorch today, the pain-points we hear from users, and industry trends. We focus on how we are thinking about our systems as we see models, datasets, and hardware continue to scale.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>TorchBench: Guarding the Performance of the PyTorch Ecosystem with Continuous Benchmarking</title><link href="https://pyvideo.org/pytorch-conference-2023/torchbench-guarding-the-performance-of-the-pytorch-ecosystem-with-continuous-benchmarking.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Xu Zhao</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/torchbench-guarding-the-performance-of-the-pytorch-ecosystem-with-continuous-benchmarking.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will present new features of TorchBench that make us to achieve even higher coverage of PyTorch &amp;amp; model benchmarking: More SOTA models in workloads; Integration with PT2 and other backends; Improvements on OSS service and infra, including userbenchmarks, model stableness improvements, and GCP A100 support.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>Training a LLaMA in your Backyard: Fine-tuning Very Large Models on Consumer Hardware</title><link href="https://pyvideo.org/pytorch-conference-2023/training-a-llama-in-your-backyard-fine-tuning-very-large-models-on-consumer-hardware.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Sourab Mangrulkar</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/training-a-llama-in-your-backyard-fine-tuning-very-large-models-on-consumer-hardware.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Training a LLaMA in your Backyard: fFne-tuning Very Large Models on Consumer Hardware - Sourab Mangrulkar &amp;amp; Younes Belkada, Hugging Face&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>What's New for Dynamic Shapes in PyTorch 2.1</title><link href="https://pyvideo.org/pytorch-conference-2023/whats-new-for-dynamic-shapes-in-pytorch-21.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Edward Yang</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/whats-new-for-dynamic-shapes-in-pytorch-21.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Last year, we announced dynamic shapes support in PT2. We have come a long way since that announcement, with a lot new features, case studies and debugging tools for using dynamic shapes. This talk will dive into all of the new developments for dynamic shapes in PT2, arguably one …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Last year, we announced dynamic shapes support in PT2. We have come a long way since that announcement, with a lot new features, case studies and debugging tools for using dynamic shapes. This talk will dive into all of the new developments for dynamic shapes in PT2, arguably one of the most new and unusual features of the PT2 compiler stack.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry><entry><title>What's New for PyTorch Developer Infrastructure</title><link href="https://pyvideo.org/pytorch-conference-2023/whats-new-for-pytorch-developer-infrastructure.html" rel="alternate"></link><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><author><name>Eli Uriegas</name></author><id>tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/whats-new-for-pytorch-developer-infrastructure.html</id><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What's New for PyTorch Developer Infrastructure - Eli Uriegas &amp;amp; Omkar Salpekar
A few updates on the state of the world for PyTorch Developer Infrastructure as well as a lookahead into future projects. Also some info on how we do releases for all of PyTorch at scale.&lt;/p&gt;
</content><category term="PyTorch Conference 2023"></category></entry></feed>