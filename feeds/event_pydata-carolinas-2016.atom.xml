<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/event_pydata-carolinas-2016.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2016-09-16T00:00:00+00:00</updated><entry><title>Balancing scale and interpretability in analytical applications with sklearn and ensembling methods</title><link href="https://pyvideo.org/pydata-carolinas-2016/balancing-scale-and-interpretability-in-analytical-applications-with-sklearn-and-ensembling-methods.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Lanhui Wang</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/balancing-scale-and-interpretability-in-analytical-applications-with-sklearn-and-ensembling-methods.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We present a machine learning framework using ensemble learning to
combine models developed by multiple analysts for distinct subsets of
a large feature space. We apply the framework to retail sales data,
but its design can accommodate other types of target variables. This
is a novel application of ensemble learning since it addresses both
analytical and organizational challenges.&lt;/p&gt;
</summary></entry><entry><title>Building self-evolving video game AI in Python</title><link href="https://pyvideo.org/pydata-carolinas-2016/building-self-evolving-video-game-ai-in-python.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Marshall Wang</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/building-self-evolving-video-game-ai-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will look at a few machine learning algorithms and
their Python implementations to build AI's that can play LaserCat, a
video game written with Pygame. We start with building an imitation AI
that can imitate a human player's decision-making. Then we build a
self-evolving AI that can explore and learn on its own to eventually
achieve super-human level.&lt;/p&gt;
</summary></entry><entry><title>Dynamic Object-Gaze Tracking with OpenCV</title><link href="https://pyvideo.org/pydata-carolinas-2016/dynamic-object-gaze-tracking-with-opencv.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Shariq Iqbal</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/dynamic-object-gaze-tracking-with-opencv.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Using computer vision techniques, we extended eye tracking technology
to allow for data normalization across dynamic environments. We
applied these techniques to subjects viewing artwork at Duke’s Nasher
Museum of Art.&lt;/p&gt;
&lt;p&gt;Most eye tracking solutions track gaze with respect to a static object
like a computer screen, making it easy to know exactly where a person
is looking with respect to an image on the screen. This makes analysis
easier, but doesn’t accurately reflect real-life. What happens when we
move eye tracking into a more realistic, dynamic setting, using eye
tracking glasses that allow people to move around? People can interact
with objects in a much more natural manner, but a new challenge is
introduced: We only have gaze data with respect to the glasses frame
of reference. In order to apply conventional analysis methods to these
data, we need to map dynamic gaze back onto a static reference image,
compensating for distance, head movement, and perspective.&lt;/p&gt;
&lt;p&gt;Using the OpenCV package and its efficient implementations of common
computer vision algorithms, we developed a method to find objects of
interest in video from eye tracking glasses and return gaze
coordinates over those objects, enabling experimenters to apply
conventional data analysis methods to eye tracking behavior obtained
in dynamic, real-world situations.&lt;/p&gt;
</summary></entry><entry><title>HistomicsTK: An open-source python toolkit for web-based analysis of digital histopathology data</title><link href="https://pyvideo.org/pydata-carolinas-2016/histomicstk-an-open-source-python-toolkit-for-web-based-analysis-of-digital-histopathology-data.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Deepak Roy Chittajallu</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/histomicstk-an-open-source-python-toolkit-for-web-based-analysis-of-digital-histopathology-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, we will present our ongoing effort behind the
development of HistomicsTK, an open-source python toolkit for
integrated web-based analysis of large collections of digital
histopathology data (2D images of a thin-slice of diseased tissue at a
microscopic resolution) used for the clinical diagnosis, staging, and
prognosis of several diseases including cancer.&lt;/p&gt;
</summary></entry><entry><title>How to get public data from public servants</title><link href="https://pyvideo.org/pydata-carolinas-2016/how-to-get-public-data-from-public-servants.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Hope E. Paasch</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/how-to-get-public-data-from-public-servants.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this short talk, you’ll learn the basics of making a request for
public information under both federal and state law, including how to
write the request, who to send it to and tactics for getting
compliance.&lt;/p&gt;
&lt;p&gt;A healthy democracy depends on open information so that both the
public and officials can participate in making good decisions. But a
great deal of information is essentially hidden from public view, held
in government databases that regular citizens can’t access. Even the
public officials who have access are often not fully aware of what
their data might show.&lt;/p&gt;
&lt;p&gt;So how does a developer get access to that information, in hopes of
shedding light on how government does or does not serve the needs of
citizens?&lt;/p&gt;
</summary></entry><entry><title>Identifying Racial Bias in Policing Practices: Open Data Policing</title><link href="https://pyvideo.org/pydata-carolinas-2016/identifying-racial-bias-in-policing-practices-open-data-policing.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Colin Copeland</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/identifying-racial-bias-in-policing-practices-open-data-policing.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;North Carolina developers and civil rights advocates used demographic
data from nearly 20,000,000 unique NC traffic stops in the state to
create a digital tool for identifying race-based policing practices:
&lt;a class="reference external" href="https://opendatapolicingnc.com/"&gt;https://opendatapolicingnc.com/&lt;/a&gt;. Come learn more about the technology
used (Python/Django) and how they're expanding into the state of
Maryland.&lt;/p&gt;
&lt;p&gt;North Carolina developers and civil rights advocates used demographic
data from nearly 20,000,000 unique NC traffic stops in the state to
create a digital tool for identifying race-based policing practices:
&lt;a class="reference external" href="https://opendatapolicingnc.com/"&gt;https://opendatapolicingnc.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The talk will cover: Background on NC data collection law Impediments
to access Open source project, the API architecture, and the
visualizations used to present and highlight key data points Expansion
to Maryland How the website helps: Community-based campaigns for
policy reforms organized largely around the data Criminal defense work
Civil litigation * Improved police management practices&lt;/p&gt;
&lt;p&gt;Come learn more about the process and how they using it to challenge
these practices in the courts.&lt;/p&gt;
</summary></entry><entry><title>Julia for Modern Data Analysis</title><link href="https://pyvideo.org/pydata-carolinas-2016/julia-for-modern-data-analysis.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Josh Day</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/julia-for-modern-data-analysis.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Julia is a dynamic, high-performance language for technical computing.
This talk will describe where the newcomer Julia may fit into a data
science ecosystem already full of rich libraries in R and Python.&lt;/p&gt;
</summary></entry><entry><title>JupyterHub: A "things explainer overview"</title><link href="https://pyvideo.org/pydata-carolinas-2016/jupyterhub-a-things-explainer-overview.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Carol Willing</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/jupyterhub-a-things-explainer-overview.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;With JupyterHub you can create a multi-user Hub which spawns, manages,
and proxies multiple instances of the single-user Jupyter notebook
(IPython notebook) server.&lt;/p&gt;
&lt;p&gt;JupyterHub provides single-user notebook servers to many users. For
example, JupyterHub could serve notebooks to a class of students, a
corporate workgroup, or a science research group.&lt;/p&gt;
</summary></entry><entry><title>Just Bring Glue - Leveraging Multiple Libraries To Quickly Build Powerful New Tools</title><link href="https://pyvideo.org/pydata-carolinas-2016/just-bring-glue-leveraging-multiple-libraries-to-quickly-build-powerful-new-tools.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Rob Agle</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/just-bring-glue-leveraging-multiple-libraries-to-quickly-build-powerful-new-tools.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;It has never been easier for developers to create simple-yet-powerful
data-driven or data-informed tools. Through case studies, we'll
explore a few projects that use a number of open source libraries or
modules in concert. Next, we'll cover strategies for learning these
new tools. Finally, we wrap up with pitfalls to keep in mind when
gluing powerful things together quickly.&lt;/p&gt;
</summary></entry><entry><title>Keynote: Data Science Community and You</title><link href="https://pyvideo.org/pydata-carolinas-2016/keynote-data-science-community-and-you.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Dr. Anthony Scopatz</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/keynote-data-science-community-and-you.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Big Data is a prominent, rapidly emerging discipline with far-reaching
scientific and economic potential, yet there remains a gap in the
translation of Big Data research findings into economic growth and
end-user impact. To exploit the full potential of Big Data, the Big
Data Innovation Hubs endeavor to foster innovation through
collaboration, diversity, education, and workforce development.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In recognition of the substantial and growing impact of big data to
the U.S., across sectors, in 2012 the White House launched a multi-
agency research initiative to foster and coordinate big data
innovation across the US. Under this initiative, the National
Foundation launched four Big Data Regional Innovation Hubs, new
organizations intended to develop the Big Data innovation ecosystem
and facilitate thematic communities’ use of data sciences for societal
benefit. Specifically, the Big Data Regional Innovation Hubs
accelerate partnerships among people in business, academia, and
government who apply data science and analytics to help solve regional
and national challenges. The Big Data Hubs cover all 50 states and
currently include several hundred universities, corporations, federal
agencies, and non-governmental organizations. Dr. Shanley will
introduce the Big Data Hubs and report on some of the significant
activities of the South Big Data Innovation Hub. Finally, she will
discuss opportunities to engage with the Big Data Hubs and our growing
networks of Public/Private partnerships. &lt;a class="reference external" href="http://southbigdatahub.org"&gt;http://southbigdatahub.org&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Keynote: How to Engage with the South Big Data Innovation Hub</title><link href="https://pyvideo.org/pydata-carolinas-2016/keynote-how-to-engage-with-the-south-big-data-innovation-hub.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Dr. Lea Shanley</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/keynote-how-to-engage-with-the-south-big-data-innovation-hub.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Big Data is a prominent, rapidly emerging discipline with far-reaching
scientific and economic potential, yet there remains a gap in the
translation of Big Data research findings into economic growth and
end-user impact. To exploit the full potential of Big Data, the Big
Data Innovation Hubs endeavor to foster innovation through
collaboration, diversity, education, and workforce development.&lt;/p&gt;
&lt;p&gt;In recognition of the substantial and growing impact of big data to
the U.S., across sectors, in 2012 the White House launched a multi-
agency research initiative to foster and coordinate big data
innovation across the US. Under this initiative, the National
Foundation launched four Big Data Regional Innovation Hubs, new
organizations intended to develop the Big Data innovation ecosystem
and facilitate thematic communities’ use of data sciences for societal
benefit. Specifically, the Big Data Regional Innovation Hubs
accelerate partnerships among people in business, academia, and
government who apply data science and analytics to help solve regional
and national challenges. The Big Data Hubs cover all 50 states and
currently include several hundred universities, corporations, federal
agencies, and non-governmental organizations. Dr. Shanley will
introduce the Big Data Hubs and report on some of the significant
activities of the South Big Data Innovation Hub. Finally, she will
discuss opportunities to engage with the Big Data Hubs and our growing
networks of Public/Private partnerships. &lt;a class="reference external" href="http://southbigdatahub.org"&gt;http://southbigdatahub.org&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Keynote: The NumFOCUS Ecosystem</title><link href="https://pyvideo.org/pydata-carolinas-2016/keynote-the-numfocus-ecosystem.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Leah Silen</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/keynote-the-numfocus-ecosystem.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The NumFOCUS Foundation is a 501(c)3 non-profit organization which
supports several open source Python packages, R tools, and the Julia
language; fosters diversity within the open source data science
community, and produces the internationally acclaimed PyData
conference series. Find out how NumFOCUS works.&lt;/p&gt;
</summary></entry><entry><title>Matplotlib 2.0 or "One does not simply change all the defaults"</title><link href="https://pyvideo.org/pydata-carolinas-2016/matplotlib-20-or-one-does-not-simply-change-all-the-defaults.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Thomas A Caswell</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/matplotlib-20-or-one-does-not-simply-change-all-the-defaults.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For the first time in over a decade, matplotlib is changing the
default styles. This talk will provide a high-level overview of the
changes, the reasoning behind the changes, and the challenges along
the way.&lt;/p&gt;
</summary></entry><entry><title>Online customer targeting as a classification problem</title><link href="https://pyvideo.org/pydata-carolinas-2016/online-customer-targeting-as-a-classification-problem.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Ulric Wong</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/online-customer-targeting-as-a-classification-problem.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Predicting user actions is a challenging and important part of any
online advertising business. The rarity of some user actions
introduces problems of class skew, overfitting, and data censoring to
modeling efforts. This presentation will delve into the business
problem and walk through an example of fitting a predictive model to
online advertising data provided by Maxpoint.&lt;/p&gt;
</summary></entry><entry><title>Principles of Reporting Systems: It's About Time we Talked About Bitemporality</title><link href="https://pyvideo.org/pydata-carolinas-2016/principles-of-reporting-systems-its-about-time-we-talked-about-bitemporality.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>James Powell</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/principles-of-reporting-systems-its-about-time-we-talked-about-bitemporality.html</id><summary type="html"></summary></entry><entry><title>PyData in Production: Lesson Learned from Various PyData Deployment Strategies</title><link href="https://pyvideo.org/pydata-carolinas-2016/pydata-in-production-lesson-learned-from-various-pydata-deployment-strategies.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Josh Howes</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/pydata-in-production-lesson-learned-from-various-pydata-deployment-strategies.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We all love Pandas, Sklearn and the rest of the PyData stack. They
allow us to conduct complex analysis and implement cutting-edge
machine learning models simply and easily. However after the initial
model fitting a common challenge often arises - how do we put these
models in production ensuring that it fits into a larger
organizational architecture? In this talk we outline the various
strateg&lt;/p&gt;
&lt;p&gt;The PyData stack offers a remarkably powerful toolkit for building
complex machine learning and analytical components quickly. However,
machine learning doesn't happen in a vacuum. It is part a large system
of enterprise software responsible for data processing and must play-
well with other tools in the ecosystem. In order to get the benefits
of rapid development while not sacrificing the non-functional
requirements, MaxPoint as implemented and tested multiple deployment
models for software relying on the PyData stack. This talk we walk
through these various deployment models and discuss the trade-offs of
the approach.&lt;/p&gt;
</summary></entry><entry><title>Visual diagnostics for more informed machine learning</title><link href="https://pyvideo.org/pydata-carolinas-2016/visual-diagnostics-for-more-informed-machine-learning.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Rebecca Bilbro</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/visual-diagnostics-for-more-informed-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Visualization has a critical role to play throughout the analytic
process. Where static outputs and tabular data can obscure patterns,
human visual analysis can open up insights that lead to more robust
data products. For Python programmers who dabble in machine learning,
visual diagnostics are a must-have for effective feature analysis,
model selection, and parameter tuning.&lt;/p&gt;
</summary></entry><entry><title>You Belong with Me: Scraping Taylor Swift Lyrics with Python and Celery</title><link href="https://pyvideo.org/pydata-carolinas-2016/you-belong-with-me-scraping-taylor-swift-lyrics-with-python-and-celery.html" rel="alternate"></link><published>2016-09-16T00:00:00+00:00</published><updated>2016-09-16T00:00:00+00:00</updated><author><name>Rebecca Conley</name></author><id>tag:pyvideo.org,2016-09-16:pydata-carolinas-2016/you-belong-with-me-scraping-taylor-swift-lyrics-with-python-and-celery.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will demonstrate an example application of using Celery to
extract all of the lyrics of the inspiring and influential Taylor
Swift from the Internet. Using a light-hearted approach and practical
Python examples, we aim to teach people the basics of using Celery for
data extraction.&lt;/p&gt;
&lt;p&gt;Celery is an open source, Python-based, asynchronous task framework
which is well-suited for extracting data from webpages, APIs, and text
files. With its power comes a bit of a learning curve. This talk
covers some of the first questions a new Celery user might have. It
will also point out some common pitfalls for beginning users.&lt;/p&gt;
</summary></entry><entry><title>A pythonista’s pipeline for large-scale geospatial analytics</title><link href="https://pyvideo.org/pydata-carolinas-2016/a-pythonistas-pipeline-for-large-scale-geospatial-analytics.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Alice Broadhead</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/a-pythonistas-pipeline-for-large-scale-geospatial-analytics.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We created an automated framework for analysis of large-scale
geospatial data using Spark, Impala, and Python. We use this framework
to join billions of device locations daily from mobile phone users to
millions of points of interest. We discuss our project structure and
workflow. Our work has broader application to movement of populations
through time and space.&lt;/p&gt;
&lt;p&gt;Large-scale geospatial data is a common analytical challenge. We
receive billions of location data points per day for user locations.
We need to filter this data and map it to millions of points of
interest. Our end use case is to provide clients with information as
to the attributes of users who visit their stores. This workflow,
however, has application to the broader analytical problem of mapping
geospatially located entities of interest to points of interest with
known locations. We also provide our approach to the frequently
encountered issue of needing both standardized and flexible reporting.
We have automated the standard analyses in our workflow, but created
an API for analysts to quickly develop ad-hoc analyses based on
customer requests.&lt;/p&gt;
&lt;p&gt;There are freely available tools to the Python user that can help us
complete all of the tasks in our data analysis pipeline. At a high
level, the architecture of our process is as follows: HDFS storage for
large-scale geospatial data, Spark for geospatial joins, Cloudera
Impala accessed via Ibis to query resultant datasets, and scientific
python for conducting analyses. We make the automated analyses to the
end users via a web portal created using Flask and Celery. We created
an API available to analysts as a Python package so that they can
quickly perform custom analyses created by clients. We used Sphinx to
aid in documentation for easier use of the API.&lt;/p&gt;
</summary></entry><entry><title>Dynamics in Graph Analysis: Adding Time as a Structure for Visual and Statistical Insight</title><link href="https://pyvideo.org/pydata-carolinas-2016/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical-insight.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Benjamin Bengfort</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/dynamics-in-graph-analysis-adding-time-as-a-structure-for-visual-and-statistical-insight.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Network analyses are powerful methods for both visual analytics and
machine learning but can suffer as their complexity increases. By
embedding time as a structural element rather than a property, we will
explore how time series and interactive analysis can be improved on
Graph structures. Primarily we will look at decomposition in NLP-
extracted concept graphs using NetworkX and Graph Tool.&lt;/p&gt;
&lt;p&gt;Modeling data as networks of relationships between entities can be a
powerful method for both visual analytics and machine learning; people
are very good at distinguishing patterns from interconnected
structures, and machine learning methods get a performance improvement
when applied to graph data structures. However, as these structures
become more complex or embed more information over time, both visual
and algorithmic methods get messy; visual analyses suffer from the
&amp;quot;hairball&amp;quot; effect, and graph algorithms require either more traversal
or increased computation at each vertex. A growing area to reduce this
complexity and optimize analytics is the use of interactive and
subgraph techniques that model how graph structures change over time.&lt;/p&gt;
</summary></entry><entry><title>Improving delivery of safe water to African communities with Jupyter notebooks</title><link href="https://pyvideo.org/pydata-carolinas-2016/improving-delivery-of-safe-water-to-african-communities-with-jupyter-notebooks.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Ginny Ghezzo</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/improving-delivery-of-safe-water-to-african-communities-with-jupyter-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Water Mission is a non-profit organization dedicated to bringing safe
water solutions where needed around the world.&lt;/p&gt;
&lt;p&gt;Water Mission is a non-profit organization dedicated to bringing safe
water solutions where needed around the world. The IBM jStart team is
helping Water Mission to analyze water consumption data from Water
Mission's treatment stations in Uganda and Tanzania. Jupiter notebooks
are used to combine and correlate data with socio-economic and weather
data, look for behavioral patterns, predict water shortages, and
suggest changes to the way the stations are operated in order to
increase delivery of safe drinking water within the local communities.&lt;/p&gt;
</summary></entry><entry><title>Integrating Scala/Java with your Python code</title><link href="https://pyvideo.org/pydata-carolinas-2016/integrating-scalajava-with-your-python-code.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Marius van Niekerk</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/integrating-scalajava-with-your-python-code.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Occasionally Python-focused data shops need to use JVM languages for
performance reasons. Generally this necessitates throwing away whole
repositories of Python code and starting over or resorting to interop
architectures (e.g., Apache thrift) which increase system complexity.
We provide a technique (and a new library, spylon) for interoperating
easily with the JVM from Python.&lt;/p&gt;
</summary></entry><entry><title>Introduction to Zeppelin Notebooks and PySpark 2.0</title><link href="https://pyvideo.org/pydata-carolinas-2016/introduction-to-zeppelin-notebooks-and-pyspark-20.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Kevin Prybol</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/introduction-to-zeppelin-notebooks-and-pyspark-20.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Zeppelin is interactive data analytics environment for
distributed data processing system. This talk will give a brief
overview of what Zeppelin is and where Zeppelin fits into the larger
data science/big data ecosystem, discuss how it differs from Jupyter
and cover several of Zeppelin's key features via a live demo use the
integrated (and just released) PySpark 2.0 interpreter .&lt;/p&gt;
&lt;p&gt;Apache Zeppelin is interactive, multi-purpose, data analytics
environment for distributed data processing system. It provides
beautiful interactive web-based interface, data visualization,
collaborative work environment and many other nice features to make
your data analytics more fun and enjoyable. This talk will provide a
brief overview (via live demo) of some of Zeppelin's key features such
as it's pluggable architecture for backend integration, drag and drop
visualizations, dynamic forms, notebook persistence, Shiro and
notebook authorization, and it's ability to share variables BETWEEN
contexts )E.g. the results of a Flink paragraph can be passed to a
Spark paragraph; the best tool can be used for the job can be used at
each step in analytics pipeline and a data scientist who loves Scala
Flink can easily work with a data scientist who loves pyspark.)&lt;/p&gt;
</summary></entry><entry><title>Keynote: Data, Decision Making, and Being Human</title><link href="https://pyvideo.org/pydata-carolinas-2016/keynote-data-decision-making-and-being-human.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Carol Willing</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/keynote-data-decision-making-and-being-human.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Our precious data. We are collecting more than ever before. Today,
we're analyzing large amounts. The results tell a story and imply
truth. In our rush to validate our beliefs, we often experience “Data
Fail”. Want to make a lasting impact on others while building your own
skills? Share your lessons from failure and be a mentor. You may be
surprised by your impact on humanity .&lt;/p&gt;
</summary></entry><entry><title>Let it shine - telling your data story</title><link href="https://pyvideo.org/pydata-carolinas-2016/let-it-shine-telling-your-data-story.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Sarah Bird</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/let-it-shine-telling-your-data-story.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You work for weeks on a new analysis, collecting, cleaning data,
modeling it, you arrive at a new result. What happens next? How many
people explore your results--and wider work? Bokeh is a data
visualization library that gives you the power of web-based custom
interactive visualizations, but lets you build them in Python. We'll
talk about moving seamlessly from analysis to engaging presentation.&lt;/p&gt;
&lt;p&gt;You work for weeks, maybe months, on a new analysis: collecting data,
cleaning it, extracting your features and modeling it, and you finally
come up with some meaningful results... What happens next? How many
people dig into and explore your work and your results?&lt;/p&gt;
&lt;p&gt;Bokeh is a data visualization library that gives you the power of
custom interactive visualizations that you can publish on the web, but
lets you build them in python.&lt;/p&gt;
</summary></entry><entry><title>Lightning Talks</title><link href="https://pyvideo.org/pydata-carolinas-2016/lightning-talks.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Various speakers</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/lightning-talks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools in Python. The goals are to provide Python enthusiasts a place to share ideas and learn from each other about how best to apply our language and tools to ever-evolving challenges in the vast realm of data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary><category term="lightning talks"></category></entry><entry><title>Making Sense Out of Flight Test Data with Python</title><link href="https://pyvideo.org/pydata-carolinas-2016/making-sense-out-of-flight-test-data-with-python.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Luke Starnes</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/making-sense-out-of-flight-test-data-with-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The assessment of complex algorithms like sensor fusion requires an
aggregate analysis across a large heterogeneous data set that
represents the possible operating conditions. This talk will discuss
the process and tools we use to analyze this data, where we want to
take things, hurdles we have left to overcome, lessons we have learned
along the way, and best practices we can recommend.&lt;/p&gt;
</summary></entry><entry><title>More than Words: Business Applications of Recurrent Neural Networks</title><link href="https://pyvideo.org/pydata-carolinas-2016/more-than-words-business-applications-of-recurrent-neural-networks.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Michael A. Alcorn</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/more-than-words-business-applications-of-recurrent-neural-networks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Recurrent neural networks have recently achieved spectacular results
in many different natural language processing tasks, but their utility
in more practical applications is largely undocumented. During this
talk, you’ll learn about how Red Hat is leveraging the power of
recurrent neural networks to make informed business decisions from
sequential customer data.&lt;/p&gt;
&lt;p&gt;Recurrent neural networks have recently achieved spectacular results
in many different natural language processing tasks, but their utility
in more practical applications is largely undocumented. For companies
that follow a subscription business model, sequential data can often
be found in abundance, seemingly making recurrent neural networks a
perfect fit for many prediction tasks. Unfortunately, resources
describing how to leverage the power of recurrent neural networks in
non-language settings are generally lacking. At Red Hat, we’re using
recurrent neural networks to tackle a number of different business
goals, including predicting customer churn and prioritizing support
cases. During this talk, you’ll learn about Red Hat’s full deep
learning pipeline, from data collection (e.g., mining website logs on
Hadoop clusters and pulling data from SQL databases), to data
preprocessing (e.g., dimensionality reduction in scikit-learn), to
prediction. By the end, you’ll have the foundation necessary to begin
implementing your own recurrent neural network solutions.&lt;/p&gt;
</summary></entry><entry><title>Open Data, Networks and the Law</title><link href="https://pyvideo.org/pydata-carolinas-2016/open-data-networks-and-the-law.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Iain Carmichael</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/open-data-networks-and-the-law.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;What does network science have to say about the law? Can we determine
which are the most the most influential cases in our legal system? Can
we understand how legal doctrine evolves? Using tools from network
statistics and data provided by Court Listener (an open legal data
project), we analyze the network of law case citations.&lt;/p&gt;
&lt;p&gt;Citation networks have recently been a topic of interest to network
scientists. Court Listener, an open data initiative, provides the
network of law case citations as well as the text of (almost every)
court case in the US. This network data set provides a rich array of
questions that are of interest to legal scholars as well as network
scientists.&lt;/p&gt;
&lt;p&gt;Can we determine which cases are the most influential in our legal
system? Can we understand how legal doctrine evolves? We will discuss
what we learned about how the network of law cases evolves and what
this means for legal practitioners.&lt;/p&gt;
&lt;p&gt;Inspired by this data set we develop new statistical methodology to
model how networks evolve. We also provide new techniques to asses the
goodness of fit for both standard and novel probabilistic network
growth models.&lt;/p&gt;
</summary></entry><entry><title>Python, C, C++, and Fortran Relationship Status: It’s Not That Complicated</title><link href="https://pyvideo.org/pydata-carolinas-2016/python-c-c-and-fortran-relationship-status-its-not-that-complicated.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Philip Semanchuk</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/python-c-c-and-fortran-relationship-status-its-not-that-complicated.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;One of Python’s strengths is that it can use code written in compiled
languages like C, Fortran and C++.&lt;/p&gt;
&lt;p&gt;This talk gives an overview of your many options for getting Python to
call and exchange data with code written in a compiled language. The
goal is to make attendees aware of choices they may not know they
have, and when to prefer one over another.&lt;/p&gt;
</summary></entry><entry><title>Reach More People: SMS Data Collection with RapidPro</title><link href="https://pyvideo.org/pydata-carolinas-2016/reach-more-people-sms-data-collection-with-rapidpro.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Erin Mullaney</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/reach-more-people-sms-data-collection-with-rapidpro.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Interested in gathering data via SMS but don't know how to get
started? Learn more about RapidPro, UNICEF's visual open-source SMS
platform. - How SMS apps are used for surveys, crises, elections and
data tracking - How to use RapidPro to jump start your own data
gathering application - Using RapidPro's API to create your own data
visualizations&lt;/p&gt;
</summary></entry><entry><title>Scalable Patient Records De-duplication using machine learning</title><link href="https://pyvideo.org/pydata-carolinas-2016/scalable-patient-records-de-duplication-using-machine-learning.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Jaafar Ben-Abdallah</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/scalable-patient-records-de-duplication-using-machine-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Simple matching to identify duplicates in patient records produces
numerous errors for various reasons. To improve the identification of
duplicates, we built an incremental model on top of an existing
machine learning based Python package. We made the model updatable and
scalable to accommodate an ever increasing patient record file.&lt;/p&gt;
&lt;p&gt;Objective:&lt;/p&gt;
&lt;p&gt;To produce an improved identification of a continuously increasing
patient records database.&lt;/p&gt;
&lt;p&gt;Problem:&lt;/p&gt;
&lt;p&gt;Proper identification of duplicated patient information remains an
arduous problem for hospitals, pharmacies and service providers.
Simple matching of these records does not result in the correct
identification of existing duplicates for various reasons such as
noisy and incomplete records.&lt;/p&gt;
</summary></entry><entry><title>Scikit-build: A build system generator for CPython C extensions</title><link href="https://pyvideo.org/pydata-carolinas-2016/scikit-build-a-build-system-generator-for-cpython-c-extensions.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Jean-Christophe Fillion-Robin</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/scikit-build-a-build-system-generator-for-cpython-c-extensions.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we present &amp;quot;scikit-build&amp;quot;, an improved build system
generator for CPython C extensions. It provides better support for
additional compilers, build systems, cross compilation, and locating
dependencies and determining their build requirements. We also
describe how Matplotlib and SymEngine are updated to make use of
&amp;quot;scikit-build&amp;quot;&lt;/p&gt;
&lt;p&gt;Although the scientific packages NumPy, SciPy, SymEngine or Matplotlib
can be used from Python today, since they bundle C, C++ or Fortan
extensions, successfully building the associated binary wheels is
complex and prone to errors. This is a significant barrier to allow
use of these scientific packages on a broader set of platforms like
mobile, Raspberry Pi or HPC.&lt;/p&gt;
&lt;p&gt;To facilitate the build process, the scikit-build package is
fundamentally just glue between the setuptools Python module and
CMake. Currently, the package is available to perform builds in a
setup.py file. In the future, the project aims to be a build tool
option in the currently developing pyproject.toml build system
specification.&lt;/p&gt;
</summary></entry><entry><title>Sharing Your Side Projects Online (and Making Your Github the Best Résumé It Can Be)</title><link href="https://pyvideo.org/pydata-carolinas-2016/sharing-your-side-projects-online-and-making-your-github-the-best-resume-it-can-be.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Timothy Hopper</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/sharing-your-side-projects-online-and-making-your-github-the-best-resume-it-can-be.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python's design makes it easy to create small programs to handle all
kinds of tasks. Tools like Github make it easy (and free!) to share
code with the world. However, code that solves a problem on your local
machine may not directly translate to solving the same problem for
someone else. This talk will provide basic practices and guidelines
for making your code usable and accessible to others.&lt;/p&gt;
&lt;p&gt;Python makes it easy to create small programs to handle all kinds of
tasks, and tools like Github make it easy and free to share code with
the world. However, simply adding a *.py to a Github repository (or
worse: a zip file on your personal website) doesn't mean other Python
programmers will be able to run and use your code.&lt;/p&gt;
&lt;p&gt;For years, I've written one-off scripts and small programs to automate
personal tasks and satisfy my curiosity. Until recently, I was never
comfortable sharing this code online. In this talk, I will share good
practices I've learned and developed for sharing my small projects
online.&lt;/p&gt;
&lt;p&gt;The talk will include tips on writing reusable scripts, the basics of
Git and Github, the importance of READMEs and software licenses, and
creation of reproducible Python environments with Conda.&lt;/p&gt;
&lt;p&gt;Besides making your code more usable and accessible to others, the
tips in this talk will help you make your Github profile a valuable
component of your online résumé and open the door for others to
improve your programs through Github pull requests.&lt;/p&gt;
</summary></entry><entry><title>Snakes on a Cloud: "Data science, meet DevOps"</title><link href="https://pyvideo.org/pydata-carolinas-2016/snakes-on-a-cloud-data-science-meet-devops.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Kyle Snavely</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/snakes-on-a-cloud-data-science-meet-devops.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Discover how IBM Cloudant engineers use data to inform development and
operations every day! Sources such as Splunk and Graphite are pivotal
for meeting our service needs. I will demonstrate a data processing
engine based on Pandas and NumPy that can help tame these sources.
Then discover how you can use Jupyter notebooks with IBM's Bluemix
platform to do more with data.&lt;/p&gt;
&lt;p&gt;Snakes on a Cloud: &amp;quot;Data science, meet DevOps&amp;quot;&lt;/p&gt;
&lt;p&gt;Running an always-on database service such as IBM Cloudant is a
complex task. Accomplishing this requires a mix of development and
operations informed by data.&lt;/p&gt;
&lt;p&gt;Various data sources such as application logs, system metrics, and
project management systems are used every day by Cloudant engineers.&lt;/p&gt;
&lt;p&gt;First I'll describe how DevOps works at Cloudant, and how data is used
to meet our service needs. Break through the buzzwords and see the
reality of a running service.&lt;/p&gt;
&lt;p&gt;I'll demonstrate a data processing engine called Forecast, which
combines various sources at Cloudant for analysis. Forecast allows
engineers to go beyond manual inspection by using Pandas and NumPy to
analyze information. We'll see how we can realize new insights from
our service metrics with tooling and automation.&lt;/p&gt;
&lt;p&gt;Finally we'll examine data using Jupyter Notebooks as provided by IBM
Bluemix's data science platform. I'll walk us through visualization of
Forecast's output and we'll learn how Bluemix can make data science
simpler.&lt;/p&gt;
</summary></entry><entry><title>Stemgraphic: A Stem-and-Leaf Plot for the Age of Big Data</title><link href="https://pyvideo.org/pydata-carolinas-2016/stemgraphic-a-stem-and-leaf-plot-for-the-age-of-big-data.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Francois Dion</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/stemgraphic-a-stem-and-leaf-plot-for-the-age-of-big-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;www.pydata.org&lt;/p&gt;
&lt;p&gt;PyData is a gathering of users and developers of data analysis tools
in Python. The goals are to provide Python enthusiasts a place to
share ideas and learn from each other about how best to apply our
language and tools to ever-evolving challenges in the vast realm of
data management, processing, analytics, and visualization.&lt;/p&gt;
&lt;p&gt;We aim to be an accessible, community-driven conference, with
tutorials for novices, advanced topical workshops for practitioners,
and opportunities for package developers and users to meet in person.&lt;/p&gt;
</summary></entry><entry><title>Testing is Fun in Python!</title><link href="https://pyvideo.org/pydata-carolinas-2016/testing-is-fun-in-python.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Andrew Knight</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/testing-is-fun-in-python.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Testing software is just as important in Python as it is in any other
programming language. Rather than treat testing as a “necessary evil,”
Python offers a number of versatile test frameworks to make it fun and
easy. This talk will cover basic testing best practices and introduce
a few of the popular frameworks, including unittest, doctest, py.test,
Nose, and Avocado.&lt;/p&gt;
&lt;p&gt;Testing is vital to the success of any software, including big data
and analytics code. Unfortunately, it is often regarded as a
“necessary evil” – extra work that slows down progress. In this
session, I will highlight how testing in Python can be fun, easy,
fast, and helpful.&lt;/p&gt;
&lt;p&gt;First, I will give a brief overview of basic best practices for
testing. We will talk about the difference between debugging and
testing, different types of tests, how to write good test cases, and
basic testing fixtures like assertions and results. I will focus on
unit testing, but the concepts can be applied to higher levels of
testing as well.&lt;/p&gt;
&lt;p&gt;Then, for the majority of the session, I will introduce different
Python test frameworks: - unittest as the standard module for unit
test classes. - doctest as a lightweight way to write short, self-
documenting assertions in docstrings. - py.test as a way to write very
concise test cases. - Nose as an extension of unittest with added
features. - Avocado as a comprehensive framework with parameters,
replay, and test discovery.&lt;/p&gt;
&lt;p&gt;This talk is designed to be useful to Python programmers of any skill
level. Only a basic understanding of Python is required.&lt;/p&gt;
</summary></entry><entry><title>The modern research skill set: Using Vagrant, Ansible, and Python to support researchers</title><link href="https://pyvideo.org/pydata-carolinas-2016/the-modern-research-skill-set-using-vagrant-ansible-and-python-to-support-researchers.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Bret Davidson</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/the-modern-research-skill-set-using-vagrant-ansible-and-python-to-support-researchers.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The NCSU Libraries are supporting the next generation of researchers
through basic training in core elements of the modern research skill
set like Python and scientific computing. We’ll show how we are
simplifying the learning experience for novice data scientists using
Vagrant and Ansible to provision reproducible computing environments
for use in our Summer of Open Science workshop series.&lt;/p&gt;
&lt;p&gt;Modern research practice asks researchers to engage with information
in new ways through the use of digital technologies. The landscape of
this skillset is rapidly changing and difficult to pinpoint at an
interdisciplinary level. The modern researcher is expected to navigate
digital tools that are not unique to the work that they do on a daily
basis within their discipline and to be able to share that work in
meaningful ways with collaborators.&lt;/p&gt;
&lt;p&gt;Open science has become increasingly relevant to modern scientific
practice and reflects the development of the modern research skill
set. There is a rising tide of policy that requires researchers to
navigate open methodology in order to gain access to grant funding.
Despite a shift towards support of open science in major policy making
bodies, such as the OECD, the training that early career researchers
receive has not caught up with this fast changing policy landscape.&lt;/p&gt;
</summary></entry><entry><title>Transforming Data to Unlock Its Latent Value</title><link href="https://pyvideo.org/pydata-carolinas-2016/transforming-data-to-unlock-its-latent-value.html" rel="alternate"></link><published>2016-09-15T00:00:00+00:00</published><updated>2016-09-15T00:00:00+00:00</updated><author><name>Tony Ojeda</name></author><id>tag:pyvideo.org,2016-09-15:pydata-carolinas-2016/transforming-data-to-unlock-its-latent-value.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will be about gaining an understanding of the real world
entities represented by our data, creatively conceptualizing different
perspectives from which our data can be analyzed, and then bringing
those conceptualizations to life with the help of Python libraries
such as Pandas, Scikit-Learn, Seaborn, and Yellowbrick so that we can
unlock the latent value and insights hidden in our data.&lt;/p&gt;
&lt;p&gt;At the heart of data analysis, there lies a need to understand the
real world entities being represented in the data. Every data set we
encounter is an attempt to capture a slice of our complex world and
communicate some information about it in a way that has potential to
be informative to humans, machines, or both. Moving from basic
analyses to advanced analytics requires the ability to imagine
multiple ways of conceptualizing the composition of entities and the
relationships present in our data. It also requires the realization
that different levels of aggregation, disaggregation, and
transformation can open up new pathways to understanding our data and
identifying the valuable insights it contains.&lt;/p&gt;
&lt;p&gt;In this talk, we’ll discuss several ways to think about the
composition and representation of our data. We’ll also demonstrate a
series of methods that leverage tools like networks, hierarchical
aggregations, and unsupervised clustering to visually explore our
data, transform it to discover new insights, help frame analytical
problems and questions, and even improve machine learning model
performance. In exploring these approaches, and with the help of
Python libraries such as Pandas, Scikit-Learn, Seaborn, and
Yellowbrick, we will provide a practical framework for thinking
creatively and visually about your data and unlocking latent value and
insights hidden deep beneath its surface.&lt;/p&gt;
</summary></entry><entry><title>Connected: A Social Network Analysis Tutorial with NetworkX</title><link href="https://pyvideo.org/pydata-carolinas-2016/connected-a-social-network-analysis-tutorial-with-networkx.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Rob Chew</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/connected-a-social-network-analysis-tutorial-with-networkx.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Social Network Analysis (SNA), the study of the relational structure
between actors, is used throughout the social and natural sciences to
discover insight from connected entities. In this tutorial, you will
learn how to use the NetworkX library to analyze network data in
Python, emphasizing intuition over theory.&lt;/p&gt;
&lt;p&gt;Methods will be illustrated using a dataset of the romantic
relationships between characters on &amp;quot;Grey's Anatomy&amp;quot;, an American
medical drama on the ABC television network. Analysis and intuition
will be emphasized over theory and mathematical rigor. An
IPython/Jupyter notebook format will be used as we code through the
examples together.&lt;/p&gt;
</summary></entry><entry><title>Data Science Driven Business Insights with Python and PySpark</title><link href="https://pyvideo.org/pydata-carolinas-2016/data-science-driven-business-insights-with-python-and-pyspark.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Chunhui Higgins</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/data-science-driven-business-insights-with-python-and-pyspark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most XaaS offerings in the cloud provide a discover-try-buy user
experience, how to predict which users have the highest propensity to
be customers, or predict customers churn, or provide service
recommendations become important to the XaaS. We will present how to
use Python on SaaS/PaaS to do data science driven actionable business
insights.&lt;/p&gt;
&lt;p&gt;Most XaaS offerings in the cloud provide a discover-try-buy user
experience, how to predict which users have the highest propensity to
be customers, or predict customers churn, or provide service
recommendations become important to the XaaS. We will present how to
use Python on SaaS/PaaS to do data science driven actionable business
insights.&lt;/p&gt;
</summary></entry><entry><title>Datascience on the web</title><link href="https://pyvideo.org/pydata-carolinas-2016/datascience-on-the-web.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Francois Dion</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/datascience-on-the-web.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Learn to deploy your research as a web application. You have been
using Jupyter and Python to do some interesting research, build
models, visualize results. In this tutorial, you’ll learn how to
easily go from a notebook to a Flask web application which you can
share.&lt;/p&gt;
&lt;p&gt;Jupyter is a great notebook environment for Python based data science
and exploratory data analysis. You can share the notebooks via a
github repository, as html or even on the web using something like
JupyterHub. How can we turn the work we have done in the notebook into
a real web application?&lt;/p&gt;
</summary></entry><entry><title>Deep Language Modeling for Question Answering using Keras</title><link href="https://pyvideo.org/pydata-carolinas-2016/deep-language-modeling-for-question-answering-using-keras.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Ben Bolte</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/deep-language-modeling-for-question-answering-using-keras.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Neural network models have revolutionized many areas of data analysis,
but have yet to make their way into mainstream usage in a number of
popular fields. Recent advances in question-answering have come
largely from creative applications of deep learning. In this tutorial,
I will demonstrate how to modify the open-source framework Keras to
build some of these models.&lt;/p&gt;
&lt;p&gt;Question answering has received more focus as large search engines
have basically mastered general information retrieval and are starting
to cover more edge cases. Question answering happens to be one of
those edge cases, because it could involve a lot of syntatic nuance
that doesn’t get captured by standard information retrieval models,
like BM-25 or LSI. Hypothetically, deep learning models are better
suited to this type of task because of their ability to capture
higher-order syntax. Two papers, “Applying deep learning to answer
selection: a study and an open task” (Feng et. al. 2015) and “LSTM-
based deep learning models for non-factoid answer selection” (Tan et.
al. 2016), are recent examples which have applied deep learning to
question-answering tasks with good results.&lt;/p&gt;
&lt;p&gt;Feng et. al. used an in-house Java framework for their work, and Tan
et. al. built their model entirely from Theano. This tutorial will
demonstrate how to replicate the models used by each group using the
popular open-source framework Keras, adding custom functions to
include recent advances from the neural networks community.&lt;/p&gt;
</summary></entry><entry><title>Getting Started with Bokeh</title><link href="https://pyvideo.org/pydata-carolinas-2016/getting-started-with-bokeh.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Sarah Bird</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/getting-started-with-bokeh.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Bokeh is a python interactive visualization library that uses web
browsers for its presentation. Bokeh supports a wide variety of
visualization tasks from basic exploration through to building
advanced data applications. In this tutorial we'll cover Bokeh's basic
concepts and go from plotting a scatter plot to building an
interactive dashboard that can run a clustering algorithm from a
dropdown.&lt;/p&gt;
&lt;p&gt;Bokeh is a python interactive visualization library that uses web
browsers for its presentation. Bokeh supports a wide variety of
visualization tasks from basic exploration through to building
advanced data applications. In this tutorial we'll cover Bokeh's basic
concepts and go from plotting a scatter plot to building an
interactive dashboard that can run a clustering algorithm from a
dropdown - you'll be surprised how easy that is.&lt;/p&gt;
</summary></entry><entry><title>Interactive Data Visualization Tools in R</title><link href="https://pyvideo.org/pydata-carolinas-2016/interactive-data-visualization-tools-in-r.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Ashton Drew</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/interactive-data-visualization-tools-in-r.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;An introduction to the R packages that generate interactive table and
graphics. The focus will be on R Shiny, but I will also summarize
value of other options, such as leaflet, plotly, ggvis, and rCharts.&lt;/p&gt;
&lt;p&gt;With interactive and reactive data visualizations, your audience
directly engages with your data for stronger communication and better
understanding. The Shiny apps can easily be launched directly to the
web via shinyapps.io or Shiny Server to be run by anyone (they don't
need to download your data or have R). This course assumes
participants can already perform data analysis and visualization in R,
but want to expand their skills with R Shiny. Therefore, the class
exercises focus on transcribing code from a static to an interactive
presentation of data products and information. Students must bring
their own laptop with a current version of R and R Studio. Outline: -
Overview of available interactive tools in R - Exercise to build a
simple Shiny app - Shiny setup and orientation - Introduction to Shiny
code structure - Overview of basic widgets - Example of conditional
reactivity&lt;/p&gt;
</summary></entry><entry><title>Introduction to Julia for Pythonistas</title><link href="https://pyvideo.org/pydata-carolinas-2016/introduction-to-julia-for-pythonistas.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>John Pearson</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/introduction-to-julia-for-pythonistas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Many Python users are curious about Julia, but the language is still
evolving, and best practices are not yet widespread. This tutorial
will introduce the Julia language, with a focus on two key areas —
multiple dispatch and the type system — that often trip up users
coming from dynamic languages.&lt;/p&gt;
&lt;p&gt;This workshop will introduce the Julia language to those coming from a
Python background.&lt;/p&gt;
</summary></entry><entry><title>Introduction to Pandas</title><link href="https://pyvideo.org/pydata-carolinas-2016/introduction-to-pandas.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Daniel Chen</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/introduction-to-pandas.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This tutorial aims to expose future data analysis with the Python
Pandas library. The tutorial is meant for absolute beginners to Pandas
and Python as a programming language. We will begin with loading
tabular data and various ways to calculate summary statistics and
visualize data. Next, we will learn various ways to join multiple
datasets, and how we can work with missing values etc.&lt;/p&gt;
&lt;p&gt;The purpose of the tutorial is to expose users to Pandas for data
analysis and move users into a more reproducible analysis workflow
compared to spreadsheet programs. The tutorial will begin with loading
in tabular data and various ways to view columns and rows of data. The
initial part of the tutorial will show users how to load in data and
quickly create descriptive statistical plots. Next, we will cover more
of pandas internal data structures. This will cover some fundamental
knowledge about Python as a programming language, mainly object
methods. Finally, before showing users basic data cleaning examples,
we will cover data visualizations using matplotlib, seaborn, and
pandas itself.&lt;/p&gt;
&lt;p&gt;The next section of the tutorial will show learners how to assemble
and merge multiple datasets, and how to work with missing values.
Lastly, before we get to fitting models, I will go over how to recode
variables for analysis.&lt;/p&gt;
&lt;p&gt;The main purpose is to show users how to use Pandas, and not how to
fit machine learning models. However, I show a simple model at the end
so users see how data cleaning and analysis all fit together in a
workflow.&lt;/p&gt;
</summary></entry><entry><title>ITK in Biomedical Research and Commercial Applications</title><link href="https://pyvideo.org/pydata-carolinas-2016/itk-in-biomedical-research-and-commercial-applications.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Matt McCormick</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/itk-in-biomedical-research-and-commercial-applications.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Insight Segmentation and Registration Toolkit (www.itk.org) has
become a standard in academia and industry for scientific,
N-dimensional image analysis. In this course we present best practices
for taking advantage of ITK in your imaging research and commercial
products.&lt;/p&gt;
&lt;p&gt;The Insight Segmentation and Registration Toolkit (www.itk.org) has
become a standard in academia and industry for medical image analysis.
In recent years, the ITK developers' community has focused on
providing programming interfaces to ITK from Python and Javascript and
making ITK available via leading applications such as Slicer and
ImageJ. In this course we present best practices for taking advantage
of ITK in your imaging research and commercial products. We
demonstrate how script writing and interactive GUIs can be used to
access the algorithms in ITK and the multitude of ITK extensions that
are freely available on the web.&lt;/p&gt;
</summary></entry><entry><title>Scalable Data Science with Spark and R</title><link href="https://pyvideo.org/pydata-carolinas-2016/scalable-data-science-with-spark-and-r.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Zeydy Ortiz</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/scalable-data-science-with-spark-and-r.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Processing large datasets in R have been limited by the amount of
memory in the local system. To overcome the native R limitation,
several cluster computing alternatives have recently emerged including
Apache Spark. In this session, we will discuss the architecture of
Spark and introduce the SparkR library. We will work through examples
of the API and discuss additional resources to learn more.&lt;/p&gt;
&lt;p&gt;In this tutorial, we will focus on SparkR. The outline of the tutorial
is as follows: - Introduction to cluster computing with Spark -
Getting started with SparkR - Deep dive into SparkR DataFrame API -
Additional resources&lt;/p&gt;
&lt;p&gt;In preparation for this tutorial please install.packages(&amp;quot;SparkR&amp;quot;) in
your system.&lt;/p&gt;
</summary></entry><entry><title>Turning Jupyter Notebooks into Data Applications</title><link href="https://pyvideo.org/pydata-carolinas-2016/turning-jupyter-notebooks-into-data-applications.html" rel="alternate"></link><published>2016-09-14T00:00:00+00:00</published><updated>2016-09-14T00:00:00+00:00</updated><author><name>Peter Parente</name></author><id>tag:pyvideo.org,2016-09-14:pydata-carolinas-2016/turning-jupyter-notebooks-into-data-applications.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Together, we will use Jupyter notebooks and the new Jupyter dashboard
and declarative widget extensions to create an interactive
application. By doing so, we hope to demonstrate the value of the
Jupyter ecosystem for rapidly building just-good-enough solutions.&lt;/p&gt;
&lt;p&gt;We will start by finding and analyzing relevant data in a Jupyter
notebook. We will drag-and-drop our notebook cells into a grid layout,
and publish it as a standalone web app. We will add interactivity to
our notebook and redeploy our app as we iterate.&lt;/p&gt;
</summary></entry></feed>