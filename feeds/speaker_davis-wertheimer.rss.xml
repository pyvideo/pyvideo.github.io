<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org - Davis Wertheimer</title><link>https://pyvideo.org/</link><description></description><lastBuildDate>Wed, 18 Sep 2024 00:00:00 +0000</lastBuildDate><item><title>Lessons Learned in WatsonX Training: Scaling Cloud-Native PyTorch FSDP to 20B Parameters</title><link>https://pyvideo.org/pytorch-conference-2023/lessons-learned-in-watsonx-training-scaling-cloud-native-pytorch-fsdp-to-20b-parameters.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk we will cover lessons learned along our almost year-and-a-half journey scaling up the WatsonX.AI stack for foundation model pretraining. Starting from 100M parameters on bare metal, we scaled PyTorch training to 20B parameters on cloud-based multi-node systems. We'll discuss the challenges encountered along the way, as well as the solutions we employed. This includes working with the PyTorch team to field test Fully-Sharded and Hybrid-Shard Data Parallel update protocols (FSDP/HSDP), as well as handling the associated communication vs computation bottlenecks, which are not always straightforward. We'll also review our collaboration on cloud-native distributed checkpointing, and development of a stateful and scalable distributed dataloader, allowing us to restart unstable jobs mid-epoch without revisiting stale data. And finally, we'll cover ongoing and upcoming challenges, like maintaining job stability and tensor parallelism integration.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Davis Wertheimer</dc:creator><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2023-10-16:/pytorch-conference-2023/lessons-learned-in-watsonx-training-scaling-cloud-native-pytorch-fsdp-to-20b-parameters.html</guid><category>PyTorch Conference 2023</category></item><item><title>A Distributed Stateful Dataloader for Large-Scale Pretraining</title><link>https://pyvideo.org/pytorch-conference-2024/a-distributed-stateful-dataloader-for-large-scale-pretraining.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A Distributed Stateful Dataloader for Large-Scale Pretraining - Davis Wertheimer, IBM &amp;amp; Linsong Chu, IBM Research&lt;/p&gt;
&lt;p&gt;Large-scale model pretraining crucially relies on specialized and dedicated dataloaders that can, for example, partition and stream data asynchronously across multiple processes and physical nodes. In this talk we discuss one of the torch-native dataloaders we built and use at IBM Research for addressing these needs. Intended for use in large-scale model pretraining, particularly in research settings where rapid iteration between datasets may be required, our dataloader is distributed, stateful, checkpointable, composable and rescalable â€“ while remaining a simple extension of the existing PyTorch dataloading framework. It automatically and invisibly handles data sharding, shuffling, subdataset weighting, checkpoint saving and loading, and custom user-defined preprocessing functions, with minimal overhead and high throughput. We discuss these properties and how we achieved them, such as reducing overhead by implementing a custom LCG random number generator, and demonstrate proof of concept on production-scale training of a 7B parameter Llama model over 4 trillion tokens.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Davis Wertheimer</dc:creator><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2024-09-18:/pytorch-conference-2024/a-distributed-stateful-dataloader-for-large-scale-pretraining.html</guid><category>PyTorch Conference 2024</category></item></channel></rss>