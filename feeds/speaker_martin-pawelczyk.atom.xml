<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org - Martin Pawelczyk</title><link href="https://pyvideo.org/" rel="alternate"></link><link href="https://pyvideo.org/feeds/speaker_martin-pawelczyk.atom.xml" rel="self"></link><id>https://pyvideo.org/</id><updated>2020-08-03T00:00:00+00:00</updated><subtitle></subtitle><entry><title>On Counterfactual Explanations under Predictive Multiplicity</title><link href="https://pyvideo.org/uai-2020/on-counterfactual-explanations-under-predictive-multiplicity.html" rel="alternate"></link><published>2020-08-03T00:00:00+00:00</published><updated>2020-08-03T00:00:00+00:00</updated><author><name>Martin Pawelczyk</name></author><id>tag:pyvideo.org,2020-08-03:/uai-2020/on-counterfactual-explanations-under-predictive-multiplicity.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;On Counterfactual Explanations under Predictive Multiplicity&lt;/p&gt;
&lt;p&gt;Martin Pawelczyk (University of Tuebingen)*; Klaus Broelemann (Schufa Holding AG); Gjergji. Kasneci (  University of Tuebingen)&lt;/p&gt;
&lt;p&gt;Counterfactual explanations are usually obtainedby identifying the smallest change made to an input to change a prediction made by a fixed model (hereafter called sparse methods). Recent work â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;On Counterfactual Explanations under Predictive Multiplicity&lt;/p&gt;
&lt;p&gt;Martin Pawelczyk (University of Tuebingen)*; Klaus Broelemann (Schufa Holding AG); Gjergji. Kasneci (  University of Tuebingen)&lt;/p&gt;
&lt;p&gt;Counterfactual explanations are usually obtainedby identifying the smallest change made to an input to change a prediction made by a fixed model (hereafter called sparse methods). Recent work, however, has revitalized an old insight: there often does not exist one superior solution to a prediction problem with respect to commonly used measures of interest (e.g. error rate). In fact, often multiple different classifiers give almost equal solutions. This phenomenon is known as predictive multiplicity (Breiman, 2001; Marx et al., 2019). In this work, we derive a general upper bound for the costs of counterfactual explanations under predictive multiplicity. Most notably, it depends on a discrepancy notion between two classifiers, which describes how differently they treat negatively predicted individuals. We then compare sparse and data support approaches empirically on real-world data. The results show that data support methods are more robust to multiplicity of different models. At the same time, we show that those methods have provably higher cost of generating counterfactual explanations under one fixed model. In summary, our theoretical and empirical results challenge the commonly held view that counterfactual recommendations should be sparse in general.&amp;quot;&lt;/p&gt;
</content><category term="UAI 2020"></category></entry></feed>